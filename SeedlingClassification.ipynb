{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from data_preprocess import *\n",
    "from image_preprocess import *\n",
    "from bcnn_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class vgg_bilinear_model:\n",
    "    def __init__(self, imgs, keep_prob, weights_file = None, weights_file_last = None, sess = None, finetune = False):\n",
    "        self.finetune = finetune                     # train all layers or only last layer\n",
    "        self.imgs = imgs                             # input img data\n",
    "        self.keep_prob = keep_prob                   # fc dropout ratio\n",
    "        self.last_layer_parameters = []              # param only last fc layer, normal train will use\n",
    "        self.parameters = []                         # param whole net, fine tune will use\n",
    "        self.weights_file = weights_file             # pretrained weights file path\n",
    "        self.weights_file_last = weights_file_last   # last fc layer weights file path\n",
    "        self.conv_layers()                           # model's conv part, here use vgg\n",
    "        self.bilinear_layers()                       # model's bilinear part, feature fusion\n",
    "        self.fc_layers()                             # model's classify part, full connect\n",
    "    \n",
    "    def conv_layers(self):\n",
    "        # preprovess, why???\n",
    "        #with tf.name_scope('preprocess') as scope:\n",
    "        #    mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "        #    images = self.imgs - mean\n",
    "        #    print('Adding Data Augmentation')\n",
    "            \n",
    "        # -------------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "        # conv1_1\n",
    "        with tf.variable_scope('conv1_1'):\n",
    "            weights = tf.get_variable('W', [3, 3, 3, 64], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [64], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.imgs, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv1_1 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # conv1_2\n",
    "        with tf.variable_scope('conv1_2'):\n",
    "            weights = tf.get_variable('W', [3, 3, 64, 64], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [64], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.conv1_1, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv1_2 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME', name = 'pool1')\n",
    "\n",
    "        # -------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.variable_scope('conv2_1'):\n",
    "            weights = tf.get_variable('W', [3, 3, 64, 128], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [128], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.pool1, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv2_1 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # conv2_2\n",
    "        with tf.variable_scope('conv2_2'):\n",
    "            weights = tf.get_variable('W', [3, 3, 128, 128], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [128], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.conv2_1, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv2_2 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME', name = 'pool2')\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.variable_scope('conv3_1'):\n",
    "            weights = tf.get_variable('W', [3, 3, 128, 256], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [256], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.pool2, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv3_1 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # conv3_2\n",
    "        with tf.variable_scope('conv3_2'):\n",
    "            weights = tf.get_variable('W', [3, 3, 256, 256], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [256], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.conv3_1, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv3_2 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # conv3_3\n",
    "        with tf.variable_scope('conv3_3'):\n",
    "            weights = tf.get_variable('W', [3, 3, 256, 256], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [256], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.conv3_2, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv3_3 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME', name = 'pool3')\n",
    "\n",
    "        # -------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.variable_scope('conv4_1'):\n",
    "            weights = tf.get_variable('W', [3, 3, 256, 512], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [512], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.pool3, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv4_1 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # conv4_2\n",
    "        with tf.variable_scope('conv4_2'):\n",
    "            weights = tf.get_variable('W', [3, 3, 512, 512], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [512], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.conv4_1, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv4_2 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # conv4_3\n",
    "        with tf.variable_scope('conv4_3'):\n",
    "            weights = tf.get_variable('W', [3, 3, 512, 512], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [512], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.conv4_2, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv4_3 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME', name = 'pool4')\n",
    "\n",
    "        # -------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.variable_scope('conv5_1'):\n",
    "            weights = tf.get_variable('W', [3, 3, 512, 512], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [512], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.pool4, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv5_1 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # conv5_2\n",
    "        with tf.variable_scope('conv5_2'):\n",
    "            weights = tf.get_variable('W', [3, 3, 512, 512], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [512], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.conv5_1, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv5_2 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "        \n",
    "        # conv5_3\n",
    "        with tf.variable_scope('conv5_3'):\n",
    "            weights = tf.get_variable('W', [3, 3, 512, 512], initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                      trainable = self.finetune)\n",
    "            biases = tf.get_variable('b', [512], initializer=tf.constant_initializer(0.1), trainable = self.finetune)\n",
    "            conv = tf.nn.conv2d(self.conv5_2, weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "            self.conv5_3 = tf.nn.relu(conv + biases)\n",
    "            self.parameters += [weights, biases]\n",
    "    \n",
    "    def bilinear_layers(self):\n",
    "        conv1 = tf.transpose(self.conv5_3, perm = [0, 3, 1, 2])\n",
    "        conv1 = tf.reshape(conv1, [-1, 512, 196])\n",
    "        \n",
    "        conv2 = tf.transpose(self.conv5_3, perm = [0, 3, 1, 2])\n",
    "        conv2 = tf.reshape(conv1, [-1, 512, 196])\n",
    "        conv2 = tf.transpose(conv2, perm = [0, 2, 1])\n",
    "        \n",
    "        phi_I = tf.matmul(conv1, conv2)\n",
    "        phi_I = tf.reshape(phi_I, [-1, 512 * 512])\n",
    "        phi_I = tf.divide(phi_I, 196.0)\n",
    "        \n",
    "        y_ssqrt = tf.multiply(tf.sign(phi_I), tf.sqrt(tf.abs(phi_I) + 1e-12))\n",
    "        \n",
    "        z = tf.nn.l2_normalize(y_ssqrt, dim = 1)\n",
    "        \n",
    "        print('Shape of z', z.get_shape())\n",
    "        \n",
    "        self.bilinear_feature = z\n",
    "    \n",
    "    def fc_layers(self):\n",
    "        with tf.variable_scope('fc-new') as scope:\n",
    "            fc3w = tf.get_variable('W', [512 * 512, 12], initializer = tf.contrib.layers.xavier_initializer(), trainable = True)\n",
    "            fc3b = tf.get_variable('b', [12], initializer=tf.constant_initializer(0.1), trainable = True)\n",
    "            fc3l = tf.nn.bias_add(tf.matmul(self.bilinear_feature, fc3w), fc3b)\n",
    "            self.fc3l = tf.nn.dropout(fc3l, self.keep_prob)\n",
    "            self.last_layer_parameters += [fc3w, fc3b]\n",
    "    \n",
    "    # for first train use, load pretrained vgg weights \n",
    "    def load_vgg_weights(self, session):\n",
    "        weights_dict = np.load(self.weights_file, encoding = 'bytes')\n",
    "        vgg_layers = ['conv1_1',\n",
    "                      'conv1_2',\n",
    "                      'conv2_1',\n",
    "                      'conv2_2',\n",
    "                      'conv3_1',\n",
    "                      'conv3_2',\n",
    "                      'conv3_3',\n",
    "                      'conv4_1',\n",
    "                      'conv4_2',\n",
    "                      'conv4_3',\n",
    "                      'conv5_1',\n",
    "                      'conv5_2',\n",
    "                      'conv5_3']\n",
    "\n",
    "        for op_name in vgg_layers:\n",
    "            with tf.variable_scope(op_name, reuse = True):\n",
    "                # biases\n",
    "                var = tf.get_variable('b')\n",
    "                print('Adding weights to',var.name)\n",
    "                session.run(var.assign(weights_dict[op_name + '_b']))\n",
    "                \n",
    "                # weights\n",
    "                var = tf.get_variable('W')\n",
    "                print('Adding weights to',var.name)\n",
    "                session.run(var.assign(weights_dict[op_name + '_W']))\n",
    "    \n",
    "    # for finetune train use, load last fc layer weights \n",
    "    def load_last_layer_weights(self, session):\n",
    "        weights_dict = np.load(self.weights_file_last, encoding = 'bytes')\n",
    "        \n",
    "        for i, var in enumerate(self.last_layer_parameters):\n",
    "            session.run(var.assign(weights_dict['arr_0'][i]))\n",
    "            print('Adding weights to',var.name)\n",
    "    \n",
    "    # for predict use, load all trained weights \n",
    "    def load_all_own_weights(self, session):\n",
    "        weights_dict_conv = np.load(self.weights_file, encoding = 'bytes')\n",
    "        weights_dict_fc = np.load(self.weights_file_last, encoding = 'bytes')\n",
    "        \n",
    "        for i, var in enumerate(self.parameters):\n",
    "            session.run(var.assign(weights_dict_conv['arr_0'][i]))\n",
    "            print('Adding weights to',var.name)\n",
    "            \n",
    "        for i, var in enumerate(self.last_layer_parameters):\n",
    "            session.run(var.assign(weights_dict_fc['arr_0'][i]))\n",
    "            print('Adding weights to',var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_file = \"vgg16_weights.npz\"\n",
    "weights_file_last_layer = \"bcnn_last_weights.npz\"\n",
    "weights_file_conv = \"bcnn_conv_weights.npz\"\n",
    "\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "finetune_step = -1\n",
    "\n",
    "image_size = 224\n",
    "n_classes = 12\n",
    "\n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of z (?, 262144)\n",
      "Adding weights to conv1_1/b:0\n",
      "Adding weights to conv1_1/W:0\n",
      "Adding weights to conv1_2/b:0\n",
      "Adding weights to conv1_2/W:0\n",
      "Adding weights to conv2_1/b:0\n",
      "Adding weights to conv2_1/W:0\n",
      "Adding weights to conv2_2/b:0\n",
      "Adding weights to conv2_2/W:0\n",
      "Adding weights to conv3_1/b:0\n",
      "Adding weights to conv3_1/W:0\n",
      "Adding weights to conv3_2/b:0\n",
      "Adding weights to conv3_2/W:0\n",
      "Adding weights to conv3_3/b:0\n",
      "Adding weights to conv3_3/W:0\n",
      "Adding weights to conv4_1/b:0\n",
      "Adding weights to conv4_1/W:0\n",
      "Adding weights to conv4_2/b:0\n",
      "Adding weights to conv4_2/W:0\n",
      "Adding weights to conv4_3/b:0\n",
      "Adding weights to conv4_3/W:0\n",
      "Adding weights to conv5_1/b:0\n",
      "Adding weights to conv5_1/W:0\n",
      "Adding weights to conv5_2/b:0\n",
      "Adding weights to conv5_2/W:0\n",
      "Adding weights to conv5_3/b:0\n",
      "Adding weights to conv5_3/W:0\n",
      "Trainable variables <tf.Variable 'fc-new/W:0' shape=(262144, 12) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'fc-new/b:0' shape=(12,) dtype=float32_ref>\n",
      "Starting training\n",
      "train loss:   2.388770\n",
      "########### epoch 1 ###########\n",
      "########### loop 0 ###########\n",
      "test loss:   2.525363   test accuracy:   0.031250\n",
      "########### loop 0 ###########\n",
      "train loss:   2.384774\n",
      "train loss:   2.377034\n",
      "train loss:   2.359334\n",
      "train loss:   2.356791\n",
      "train loss:   2.408705\n",
      "train loss:   2.437660\n",
      "train loss:   2.337476\n",
      "train loss:   2.522114\n",
      "train loss:   2.336606\n",
      "train loss:   2.613434\n",
      "train loss:   2.433068\n",
      "train loss:   2.428681\n",
      "train loss:   2.288691\n",
      "train loss:   2.339204\n",
      "train loss:   2.303048\n",
      "train loss:   2.309920\n",
      "train loss:   2.335598\n",
      "train loss:   2.447194\n",
      "train loss:   2.420578\n",
      "train loss:   2.343071\n",
      "train loss:   2.239587\n",
      "train loss:   2.476113\n",
      "train loss:   2.313022\n",
      "train loss:   2.521172\n",
      "train loss:   2.328105\n",
      "train loss:   2.354789\n",
      "train loss:   2.434455\n",
      "train loss:   2.333542\n",
      "train loss:   2.262105\n",
      "train loss:   2.258326\n",
      "train loss:   2.286616\n",
      "train loss:   2.336776\n",
      "train loss:   2.252189\n",
      "train loss:   2.225821\n",
      "train loss:   2.326461\n",
      "train loss:   2.274694\n",
      "train loss:   2.253533\n",
      "train loss:   2.344647\n",
      "train loss:   2.328106\n",
      "train loss:   2.268718\n",
      "train loss:   2.256803\n",
      "train loss:   2.063184\n",
      "train loss:   2.168254\n",
      "train loss:   2.285471\n",
      "train loss:   2.141580\n",
      "train loss:   2.272594\n",
      "train loss:   2.238329\n",
      "train loss:   2.479658\n",
      "train loss:   2.413082\n",
      "train loss:   2.133742\n",
      "########### epoch 1 ###########\n",
      "########### loop 50 ###########\n",
      "test loss:   2.196355   test accuracy:   0.312500\n",
      "########### loop 50 ###########\n",
      "train loss:   2.202669\n",
      "train loss:   1.953749\n",
      "train loss:   2.031299\n",
      "train loss:   2.022035\n",
      "train loss:   2.342952\n",
      "train loss:   2.213945\n",
      "train loss:   2.162710\n",
      "train loss:   2.265916\n",
      "train loss:   2.396677\n",
      "train loss:   2.351611\n",
      "train loss:   2.237570\n",
      "train loss:   2.297942\n",
      "train loss:   2.297241\n",
      "train loss:   2.314164\n",
      "train loss:   2.011913\n",
      "train loss:   2.284412\n",
      "train loss:   2.308233\n",
      "train loss:   2.180334\n",
      "train loss:   2.174819\n",
      "train loss:   2.365277\n",
      "train loss:   1.959019\n",
      "train loss:   2.103955\n",
      "train loss:   2.075862\n",
      "train loss:   2.181810\n",
      "train loss:   1.920429\n",
      "train loss:   2.172356\n",
      "train loss:   2.194029\n",
      "train loss:   2.073535\n",
      "train loss:   2.097806\n",
      "train loss:   2.015678\n",
      "train loss:   1.928625\n",
      "train loss:   2.092345\n",
      "train loss:   2.150943\n",
      "train loss:   1.966662\n",
      "train loss:   2.250589\n",
      "train loss:   2.176745\n",
      "train loss:   2.149426\n",
      "train loss:   2.285534\n",
      "train loss:   2.279273\n",
      "train loss:   2.089885\n",
      "train loss:   2.338419\n",
      "train loss:   2.207581\n",
      "train loss:   2.486525\n",
      "train loss:   2.366329\n",
      "train loss:   2.220965\n",
      "train loss:   2.146375\n",
      "train loss:   1.966320\n",
      "train loss:   2.207301\n",
      "train loss:   2.305941\n",
      "train loss:   2.215523\n",
      "########### epoch 1 ###########\n",
      "########### loop 100 ###########\n",
      "test loss:   2.352764   test accuracy:   0.312500\n",
      "########### loop 100 ###########\n",
      "train loss:   2.365625\n",
      "train loss:   2.167320\n",
      "train loss:   2.143841\n",
      "train loss:   1.929888\n",
      "train loss:   2.182518\n",
      "train loss:   2.153554\n",
      "train loss:   2.168510\n",
      "train loss:   2.490520\n",
      "train loss:   1.984395\n",
      "train loss:   2.021843\n",
      "train loss:   2.401584\n",
      "train loss:   1.845913\n",
      "train loss:   2.497364\n",
      "train loss:   2.219962\n",
      "train loss:   2.557453\n",
      "train loss:   2.319612\n",
      "train loss:   2.087971\n",
      "train loss:   2.249649\n",
      "train loss:   2.434579\n",
      "train loss:   2.215502\n",
      "train loss:   2.006933\n",
      "train loss:   2.035899\n",
      "train loss:   1.852103\n",
      "train loss:   2.207936\n",
      "train loss:   2.021981\n",
      "train loss:   2.038753\n",
      "train loss:   2.078887\n",
      "train loss:   2.061117\n",
      "train loss:   1.979912\n",
      "train loss:   2.064354\n",
      "train loss:   2.119012\n",
      "train loss:   2.200644\n",
      "train loss:   2.464268\n",
      "train loss:   1.893760\n",
      "train loss:   2.337090\n",
      "train loss:   2.393734\n",
      "train loss:   1.902980\n",
      "train loss:   2.058058\n",
      "train loss:   2.022123\n",
      "train loss:   2.325917\n",
      "train loss:   2.060816\n",
      "train loss:   1.956932\n",
      "train loss:   1.910863\n",
      "train loss:   1.943685\n",
      "train loss:   1.853038\n",
      "train loss:   2.041179\n",
      "train loss:   2.017103\n",
      "train loss:   1.914596\n",
      "train loss:   2.133234\n",
      "train loss:   1.915121\n",
      "########### epoch 2 ###########\n",
      "########### loop 150 ###########\n",
      "test loss:   2.629162   test accuracy:   0.125000\n",
      "########### loop 150 ###########\n",
      "train loss:   2.159503\n",
      "train loss:   2.201271\n",
      "train loss:   2.038833\n",
      "train loss:   2.129268\n",
      "train loss:   2.167346\n",
      "train loss:   1.816782\n",
      "train loss:   1.948650\n",
      "train loss:   1.823311\n",
      "train loss:   2.301845\n",
      "train loss:   2.020043\n",
      "train loss:   2.171792\n",
      "train loss:   1.829848\n",
      "train loss:   2.130412\n",
      "train loss:   1.730208\n",
      "train loss:   1.974008\n",
      "train loss:   1.819023\n",
      "train loss:   1.880318\n",
      "train loss:   1.919074\n",
      "train loss:   2.246661\n",
      "train loss:   2.153997\n",
      "train loss:   1.806026\n",
      "train loss:   2.075854\n",
      "train loss:   1.981860\n",
      "train loss:   1.912989\n",
      "train loss:   2.202334\n",
      "train loss:   2.006346\n",
      "train loss:   1.972383\n",
      "train loss:   2.051600\n",
      "train loss:   2.037153\n",
      "train loss:   1.951339\n",
      "train loss:   1.915493\n",
      "train loss:   2.020342\n",
      "train loss:   1.717454\n",
      "train loss:   1.964012\n",
      "train loss:   1.965091\n",
      "train loss:   1.823625\n",
      "train loss:   1.951663\n",
      "train loss:   1.762090\n",
      "train loss:   2.072093\n",
      "train loss:   2.270842\n",
      "train loss:   2.027822\n",
      "train loss:   2.053041\n",
      "train loss:   1.690996\n",
      "train loss:   1.857795\n",
      "train loss:   1.742853\n",
      "train loss:   1.987442\n",
      "train loss:   1.979992\n",
      "train loss:   1.862095\n",
      "train loss:   1.935112\n",
      "train loss:   2.001806\n",
      "########### epoch 2 ###########\n",
      "########### loop 200 ###########\n",
      "test loss:   1.748050   test accuracy:   0.562500\n",
      "########### loop 200 ###########\n",
      "train loss:   2.060019\n",
      "train loss:   2.094710\n",
      "train loss:   1.943149\n",
      "train loss:   1.877572\n",
      "train loss:   2.022352\n",
      "train loss:   1.747492\n",
      "train loss:   1.987107\n",
      "train loss:   1.865449\n",
      "train loss:   1.825584\n",
      "train loss:   1.896313\n",
      "train loss:   1.975338\n",
      "train loss:   1.750279\n",
      "train loss:   1.907341\n",
      "train loss:   1.856689\n",
      "train loss:   2.003923\n",
      "train loss:   1.742958\n",
      "train loss:   1.935272\n",
      "train loss:   1.952770\n",
      "train loss:   2.002578\n",
      "train loss:   1.856533\n",
      "train loss:   1.704210\n",
      "train loss:   1.723808\n",
      "train loss:   1.933357\n",
      "train loss:   1.959518\n",
      "train loss:   1.737150\n",
      "train loss:   1.982486\n",
      "train loss:   2.083455\n",
      "train loss:   1.766566\n",
      "train loss:   1.923054\n",
      "train loss:   1.876643\n",
      "train loss:   1.714271\n",
      "train loss:   1.843377\n",
      "train loss:   1.957835\n",
      "train loss:   2.042376\n",
      "train loss:   2.012175\n",
      "train loss:   2.033424\n",
      "train loss:   1.969460\n",
      "train loss:   1.697396\n",
      "train loss:   1.760853\n",
      "train loss:   1.930284\n",
      "train loss:   1.873549\n",
      "train loss:   2.030128\n",
      "train loss:   1.852396\n",
      "train loss:   1.925995\n",
      "train loss:   1.904911\n",
      "train loss:   1.950944\n",
      "train loss:   1.959329\n",
      "train loss:   1.896788\n",
      "train loss:   2.134148\n",
      "train loss:   1.768996\n",
      "########### epoch 2 ###########\n",
      "########### loop 250 ###########\n",
      "test loss:   1.766545   test accuracy:   0.375000\n",
      "########### loop 250 ###########\n",
      "train loss:   1.930009\n",
      "train loss:   2.070673\n",
      "train loss:   1.612540\n",
      "train loss:   2.070897\n",
      "train loss:   2.128247\n",
      "train loss:   1.709914\n",
      "train loss:   2.155534\n",
      "train loss:   2.102399\n",
      "train loss:   2.000617\n",
      "train loss:   2.031394\n",
      "train loss:   1.920796\n",
      "train loss:   1.958307\n",
      "train loss:   2.052075\n",
      "train loss:   1.816762\n",
      "train loss:   1.981235\n",
      "train loss:   1.769393\n",
      "train loss:   1.813080\n",
      "train loss:   1.757192\n",
      "train loss:   1.827686\n",
      "train loss:   2.038924\n",
      "train loss:   2.056346\n",
      "train loss:   1.918427\n",
      "train loss:   2.074505\n",
      "train loss:   2.209464\n",
      "train loss:   2.545068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   2.059854\n",
      "train loss:   2.166144\n",
      "train loss:   2.145414\n",
      "train loss:   2.029237\n",
      "train loss:   1.859474\n",
      "train loss:   2.216050\n",
      "train loss:   1.743692\n",
      "train loss:   1.630572\n",
      "train loss:   2.050435\n",
      "train loss:   1.835695\n",
      "train loss:   1.831742\n",
      "train loss:   2.020522\n",
      "train loss:   1.979112\n",
      "train loss:   1.856297\n",
      "train loss:   1.962246\n",
      "train loss:   1.786938\n",
      "train loss:   2.096676\n",
      "train loss:   1.962884\n",
      "train loss:   1.897461\n",
      "train loss:   1.781772\n",
      "train loss:   1.810292\n",
      "train loss:   1.771865\n",
      "train loss:   1.657670\n",
      "train loss:   1.678895\n",
      "train loss:   1.990570\n",
      "########### epoch 3 ###########\n",
      "########### loop 300 ###########\n",
      "test loss:   1.808197   test accuracy:   0.437500\n",
      "########### loop 300 ###########\n",
      "train loss:   1.979552\n",
      "train loss:   1.878313\n",
      "train loss:   1.723810\n",
      "train loss:   1.975907\n",
      "train loss:   1.726971\n",
      "train loss:   1.779817\n",
      "train loss:   1.673483\n",
      "train loss:   1.811248\n",
      "train loss:   1.732351\n",
      "train loss:   1.987812\n",
      "train loss:   1.989836\n",
      "train loss:   1.793826\n",
      "train loss:   1.903761\n",
      "train loss:   1.887180\n",
      "train loss:   1.868258\n",
      "train loss:   1.824297\n",
      "train loss:   1.609952\n",
      "train loss:   1.971915\n",
      "train loss:   1.970306\n",
      "train loss:   1.980829\n",
      "train loss:   1.904170\n",
      "train loss:   1.818966\n",
      "train loss:   2.011858\n",
      "train loss:   1.701352\n",
      "train loss:   1.834317\n",
      "train loss:   1.976622\n",
      "train loss:   1.875817\n",
      "train loss:   1.891104\n",
      "train loss:   1.756108\n",
      "train loss:   2.108254\n",
      "train loss:   2.092453\n",
      "train loss:   2.022372\n",
      "train loss:   2.101127\n",
      "train loss:   1.566048\n",
      "train loss:   1.857919\n",
      "train loss:   1.646083\n",
      "train loss:   1.960731\n",
      "train loss:   1.710954\n",
      "train loss:   1.784723\n",
      "train loss:   1.868196\n",
      "train loss:   2.039993\n",
      "train loss:   1.989491\n",
      "train loss:   1.983606\n",
      "train loss:   1.928589\n",
      "train loss:   1.885243\n",
      "train loss:   1.773098\n",
      "train loss:   1.474280\n",
      "train loss:   2.090080\n",
      "train loss:   1.920181\n",
      "train loss:   1.826763\n",
      "########### epoch 3 ###########\n",
      "########### loop 350 ###########\n",
      "test loss:   1.860054   test accuracy:   0.468750\n",
      "########### loop 350 ###########\n",
      "train loss:   1.875543\n",
      "train loss:   1.906963\n",
      "train loss:   1.883990\n",
      "train loss:   1.786302\n",
      "train loss:   1.674304\n",
      "train loss:   1.874203\n",
      "train loss:   1.581118\n",
      "train loss:   1.841076\n",
      "train loss:   1.721241\n",
      "train loss:   1.711658\n",
      "train loss:   1.698403\n",
      "train loss:   1.642822\n",
      "train loss:   1.666929\n",
      "train loss:   1.907590\n",
      "train loss:   1.845810\n",
      "train loss:   1.722329\n",
      "train loss:   2.124326\n",
      "train loss:   2.001630\n",
      "train loss:   1.542326\n",
      "train loss:   1.790831\n",
      "train loss:   1.727854\n",
      "train loss:   1.550594\n",
      "train loss:   1.660878\n",
      "train loss:   1.979731\n",
      "train loss:   1.986228\n",
      "train loss:   1.802938\n",
      "train loss:   2.098358\n",
      "train loss:   1.865709\n",
      "train loss:   1.646626\n",
      "train loss:   1.642576\n",
      "train loss:   1.785735\n",
      "train loss:   1.743830\n",
      "train loss:   2.172062\n",
      "train loss:   1.684170\n",
      "train loss:   1.784924\n",
      "train loss:   1.599215\n",
      "train loss:   1.627913\n",
      "train loss:   1.602419\n",
      "train loss:   1.742497\n",
      "train loss:   1.921927\n",
      "train loss:   1.631555\n",
      "train loss:   1.840653\n",
      "train loss:   1.858077\n",
      "train loss:   1.618264\n",
      "train loss:   1.914640\n",
      "train loss:   1.956665\n",
      "train loss:   1.733002\n",
      "train loss:   1.810803\n",
      "train loss:   1.694874\n",
      "train loss:   1.638525\n",
      "########### epoch 3 ###########\n",
      "########### loop 400 ###########\n",
      "test loss:   1.744273   test accuracy:   0.437500\n",
      "########### loop 400 ###########\n",
      "train loss:   1.715233\n",
      "train loss:   1.694327\n",
      "train loss:   1.601258\n",
      "train loss:   1.746868\n",
      "train loss:   1.581183\n",
      "train loss:   1.899522\n",
      "train loss:   1.664009\n",
      "train loss:   1.569302\n",
      "train loss:   1.658484\n",
      "train loss:   1.694243\n",
      "train loss:   1.529388\n",
      "train loss:   1.767176\n",
      "train loss:   1.872126\n",
      "train loss:   1.871263\n",
      "train loss:   1.881830\n",
      "train loss:   1.917186\n",
      "train loss:   1.822780\n",
      "train loss:   1.989714\n",
      "train loss:   1.733887\n",
      "train loss:   1.740017\n",
      "train loss:   1.654291\n",
      "train loss:   1.819714\n",
      "train loss:   1.548264\n",
      "train loss:   1.419852\n",
      "train loss:   1.632870\n",
      "train loss:   1.684847\n",
      "train loss:   1.605990\n",
      "train loss:   1.744490\n",
      "train loss:   1.839883\n",
      "train loss:   1.574453\n",
      "train loss:   1.690960\n",
      "train loss:   1.653726\n",
      "train loss:   1.941970\n",
      "train loss:   1.855698\n",
      "train loss:   1.780140\n",
      "train loss:   1.665803\n",
      "train loss:   1.615272\n",
      "train loss:   1.614752\n",
      "train loss:   1.544696\n",
      "train loss:   1.509335\n",
      "train loss:   1.886664\n",
      "train loss:   1.867608\n",
      "train loss:   1.770172\n",
      "train loss:   1.460246\n",
      "train loss:   1.810052\n",
      "train loss:   1.533537\n",
      "train loss:   1.628763\n",
      "train loss:   1.656058\n",
      "train loss:   1.912729\n",
      "train loss:   1.917730\n",
      "########### epoch 4 ###########\n",
      "########### loop 450 ###########\n",
      "test loss:   1.782205   test accuracy:   0.468750\n",
      "########### loop 450 ###########\n",
      "train loss:   1.952031\n",
      "train loss:   1.898646\n",
      "train loss:   1.556070\n",
      "train loss:   1.726781\n",
      "train loss:   1.889822\n",
      "train loss:   1.687637\n",
      "train loss:   2.369186\n",
      "train loss:   1.588978\n",
      "train loss:   1.830646\n",
      "train loss:   1.722420\n",
      "train loss:   1.770784\n",
      "train loss:   1.886450\n",
      "train loss:   1.675256\n",
      "train loss:   1.827260\n",
      "train loss:   1.479670\n",
      "train loss:   1.816887\n",
      "train loss:   1.852059\n",
      "train loss:   1.704979\n",
      "train loss:   1.712305\n",
      "train loss:   1.652983\n",
      "train loss:   1.929917\n",
      "train loss:   2.217885\n",
      "train loss:   1.714260\n",
      "train loss:   1.933580\n",
      "train loss:   1.586402\n",
      "train loss:   1.585080\n",
      "train loss:   1.506880\n",
      "train loss:   1.834124\n",
      "train loss:   1.794661\n",
      "train loss:   1.780248\n",
      "train loss:   1.756879\n",
      "train loss:   1.848403\n",
      "train loss:   1.797446\n",
      "train loss:   1.810606\n",
      "train loss:   1.864863\n",
      "train loss:   1.729314\n",
      "train loss:   1.635330\n",
      "train loss:   1.525829\n",
      "train loss:   1.861636\n",
      "train loss:   1.698817\n",
      "train loss:   1.763376\n",
      "train loss:   1.662485\n",
      "train loss:   1.767736\n",
      "train loss:   1.451585\n",
      "train loss:   1.710541\n",
      "train loss:   1.663965\n",
      "train loss:   1.785678\n",
      "train loss:   1.611719\n",
      "train loss:   1.768125\n",
      "train loss:   1.712821\n",
      "########### epoch 4 ###########\n",
      "########### loop 500 ###########\n",
      "test loss:   1.677466   test accuracy:   0.500000\n",
      "########### loop 500 ###########\n",
      "train loss:   1.582510\n",
      "train loss:   1.650730\n",
      "train loss:   1.687154\n",
      "train loss:   1.584492\n",
      "train loss:   1.791762\n",
      "train loss:   1.834722\n",
      "train loss:   1.672910\n",
      "train loss:   1.980040\n",
      "train loss:   1.808606\n",
      "train loss:   1.416093\n",
      "train loss:   1.745364\n",
      "train loss:   1.781010\n",
      "train loss:   1.585275\n",
      "train loss:   1.755377\n",
      "train loss:   1.886652\n",
      "train loss:   1.853090\n",
      "train loss:   1.859244\n",
      "train loss:   1.774099\n",
      "train loss:   1.684051\n",
      "train loss:   1.473420\n",
      "train loss:   1.537728\n",
      "train loss:   1.669645\n",
      "train loss:   1.696974\n",
      "train loss:   1.821924\n",
      "train loss:   1.426239\n",
      "train loss:   1.772655\n",
      "train loss:   1.553645\n",
      "train loss:   1.697501\n",
      "train loss:   1.602601\n",
      "train loss:   1.683162\n",
      "train loss:   1.903884\n",
      "train loss:   1.603560\n",
      "train loss:   1.844695\n",
      "train loss:   2.016411\n",
      "train loss:   1.449229\n",
      "train loss:   2.140817\n",
      "train loss:   1.845730\n",
      "train loss:   1.775809\n",
      "train loss:   1.747118\n",
      "train loss:   1.661872\n",
      "train loss:   1.639905\n",
      "train loss:   2.001368\n",
      "train loss:   1.879733\n",
      "train loss:   1.916557\n",
      "train loss:   1.927629\n",
      "train loss:   1.551909\n",
      "train loss:   1.646191\n",
      "train loss:   1.609650\n",
      "train loss:   1.696763\n",
      "train loss:   1.598297\n",
      "########### epoch 4 ###########\n",
      "########### loop 550 ###########\n",
      "test loss:   1.890067   test accuracy:   0.281250\n",
      "########### loop 550 ###########\n",
      "train loss:   1.584027\n",
      "train loss:   1.796167\n",
      "train loss:   1.796177\n",
      "train loss:   1.759420\n",
      "train loss:   1.861263\n",
      "train loss:   1.915771\n",
      "train loss:   2.034958\n",
      "train loss:   1.857638\n",
      "train loss:   1.825451\n",
      "train loss:   1.676820\n",
      "train loss:   1.576648\n",
      "train loss:   1.579147\n",
      "train loss:   1.903039\n",
      "train loss:   1.558632\n",
      "train loss:   1.580424\n",
      "train loss:   1.692029\n",
      "train loss:   1.529813\n",
      "train loss:   1.787907\n",
      "train loss:   1.793540\n",
      "train loss:   1.963261\n",
      "train loss:   1.966468\n",
      "train loss:   1.710986\n",
      "train loss:   1.642353\n",
      "train loss:   1.990228\n",
      "train loss:   1.929882\n",
      "train loss:   1.675010\n",
      "train loss:   1.625437\n",
      "train loss:   1.627144\n",
      "train loss:   1.541055\n",
      "train loss:   1.479403\n",
      "train loss:   1.545405\n",
      "train loss:   1.945117\n",
      "train loss:   1.770723\n",
      "train loss:   1.764039\n",
      "train loss:   1.476259\n",
      "train loss:   1.737367\n",
      "train loss:   1.432943\n",
      "train loss:   1.684783\n",
      "train loss:   1.505832\n",
      "train loss:   1.669052\n",
      "train loss:   1.664153\n",
      "train loss:   2.011069\n",
      "train loss:   1.861253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.475223\n",
      "train loss:   1.748424\n",
      "train loss:   1.854350\n",
      "train loss:   1.728695\n",
      "train loss:   1.653807\n",
      "train loss:   1.647550\n",
      "train loss:   1.696574\n",
      "########### epoch 5 ###########\n",
      "########### loop 600 ###########\n",
      "test loss:   1.924795   test accuracy:   0.343750\n",
      "########### loop 600 ###########\n",
      "train loss:   1.610030\n",
      "train loss:   1.776258\n",
      "train loss:   1.868029\n",
      "train loss:   1.784459\n",
      "train loss:   2.009964\n",
      "train loss:   1.673656\n",
      "train loss:   1.796841\n",
      "train loss:   1.832717\n",
      "train loss:   1.693540\n",
      "train loss:   1.665277\n",
      "train loss:   1.543631\n",
      "train loss:   1.784191\n",
      "train loss:   1.883344\n",
      "train loss:   1.546617\n",
      "train loss:   1.709311\n",
      "train loss:   1.466413\n",
      "train loss:   1.641239\n",
      "train loss:   1.488118\n",
      "train loss:   1.652622\n",
      "train loss:   1.510592\n",
      "train loss:   1.627752\n",
      "train loss:   1.711871\n",
      "train loss:   1.742021\n",
      "train loss:   1.623832\n",
      "train loss:   1.770029\n",
      "train loss:   1.724753\n",
      "train loss:   1.599714\n",
      "train loss:   1.599489\n",
      "train loss:   1.384137\n",
      "train loss:   1.879089\n",
      "train loss:   1.754770\n",
      "train loss:   1.540835\n",
      "train loss:   1.594091\n",
      "train loss:   1.683735\n",
      "train loss:   1.578524\n",
      "train loss:   1.628939\n",
      "train loss:   1.551926\n",
      "train loss:   1.702520\n",
      "train loss:   1.411497\n",
      "train loss:   1.626088\n",
      "train loss:   1.642594\n",
      "train loss:   1.655418\n",
      "train loss:   1.605298\n",
      "train loss:   1.554932\n",
      "train loss:   1.609224\n",
      "train loss:   1.684720\n",
      "train loss:   1.737118\n",
      "train loss:   1.636778\n",
      "train loss:   2.073579\n",
      "train loss:   1.953569\n",
      "########### epoch 5 ###########\n",
      "########### loop 650 ###########\n",
      "test loss:   1.749301   test accuracy:   0.468750\n",
      "########### loop 650 ###########\n",
      "train loss:   1.419099\n",
      "train loss:   1.676481\n",
      "train loss:   1.630965\n",
      "train loss:   1.533586\n",
      "train loss:   1.598649\n",
      "train loss:   1.871376\n",
      "train loss:   1.809759\n",
      "train loss:   1.652731\n",
      "train loss:   1.851694\n",
      "train loss:   1.691675\n",
      "train loss:   1.547704\n",
      "train loss:   1.643728\n",
      "train loss:   1.579956\n",
      "train loss:   1.625475\n",
      "train loss:   1.904980\n",
      "train loss:   1.358135\n",
      "train loss:   1.703517\n",
      "train loss:   1.447472\n",
      "train loss:   1.643713\n",
      "train loss:   1.634892\n",
      "train loss:   1.527901\n",
      "train loss:   1.757849\n",
      "train loss:   1.709480\n",
      "train loss:   1.699312\n",
      "train loss:   1.727880\n",
      "train loss:   1.422669\n",
      "train loss:   1.576102\n",
      "train loss:   1.704355\n",
      "train loss:   1.578395\n",
      "train loss:   1.812878\n",
      "train loss:   1.613490\n",
      "train loss:   1.620433\n",
      "train loss:   1.678862\n",
      "train loss:   1.432158\n",
      "train loss:   1.430180\n",
      "train loss:   1.721658\n",
      "train loss:   1.574775\n",
      "train loss:   1.601534\n",
      "train loss:   1.535696\n",
      "train loss:   1.452530\n",
      "train loss:   1.501332\n",
      "train loss:   1.581204\n",
      "train loss:   1.554208\n",
      "train loss:   1.750452\n",
      "train loss:   1.738932\n",
      "train loss:   1.699604\n",
      "train loss:   1.638455\n",
      "train loss:   1.899107\n",
      "train loss:   1.669715\n",
      "train loss:   1.832507\n",
      "########### epoch 5 ###########\n",
      "########### loop 700 ###########\n",
      "test loss:   1.619180   test accuracy:   0.531250\n",
      "########### loop 700 ###########\n",
      "train loss:   1.664853\n",
      "train loss:   1.632859\n",
      "train loss:   1.485425\n",
      "train loss:   1.892514\n",
      "train loss:   1.453481\n",
      "train loss:   1.642958\n",
      "train loss:   1.540284\n",
      "train loss:   1.524074\n",
      "train loss:   1.591890\n",
      "train loss:   1.797706\n",
      "train loss:   1.671234\n",
      "train loss:   1.541332\n",
      "train loss:   1.554610\n",
      "train loss:   1.601168\n",
      "train loss:   1.675606\n",
      "train loss:   1.744795\n",
      "train loss:   1.539288\n",
      "train loss:   1.600176\n",
      "train loss:   1.450448\n",
      "train loss:   1.368630\n",
      "train loss:   1.368233\n",
      "train loss:   1.401958\n",
      "train loss:   1.694726\n",
      "train loss:   1.723900\n",
      "train loss:   1.561580\n",
      "train loss:   1.343399\n",
      "train loss:   1.710288\n",
      "train loss:   1.554102\n",
      "train loss:   1.587181\n",
      "train loss:   1.493573\n",
      "train loss:   1.819170\n",
      "train loss:   1.593134\n",
      "train loss:   1.850134\n",
      "train loss:   1.741100\n",
      "train loss:   1.409239\n",
      "train loss:   1.783693\n",
      "train loss:   1.802209\n",
      "train loss:   1.706072\n",
      "train loss:   2.091182\n",
      "train loss:   1.442441\n",
      "train loss:   1.688502\n",
      "train loss:   1.654917\n",
      "train loss:   1.728894\n",
      "train loss:   2.052945\n",
      "train loss:   1.680547\n",
      "train loss:   1.872656\n",
      "train loss:   1.552170\n",
      "train loss:   1.653692\n",
      "train loss:   1.717265\n",
      "train loss:   1.676219\n",
      "########### epoch 6 ###########\n",
      "########### loop 750 ###########\n",
      "test loss:   1.710372   test accuracy:   0.500000\n",
      "########### loop 750 ###########\n",
      "train loss:   1.593870\n",
      "train loss:   1.491897\n",
      "train loss:   1.849761\n",
      "train loss:   1.887936\n",
      "train loss:   1.600552\n",
      "train loss:   1.680048\n",
      "train loss:   1.376887\n",
      "train loss:   1.501800\n",
      "train loss:   1.382399\n",
      "train loss:   1.677724\n",
      "train loss:   1.456267\n",
      "train loss:   1.760822\n",
      "train loss:   1.614356\n",
      "train loss:   1.704632\n",
      "train loss:   1.587679\n",
      "train loss:   1.677570\n",
      "train loss:   1.616324\n",
      "train loss:   1.713030\n",
      "train loss:   1.624944\n",
      "train loss:   1.373083\n",
      "train loss:   1.798224\n",
      "train loss:   1.641644\n",
      "train loss:   1.569535\n",
      "train loss:   1.651662\n",
      "train loss:   1.721081\n",
      "train loss:   1.463078\n",
      "train loss:   1.585817\n",
      "train loss:   1.514162\n",
      "train loss:   1.724957\n",
      "train loss:   1.461510\n",
      "train loss:   1.623625\n",
      "train loss:   1.644936\n",
      "train loss:   1.709672\n",
      "train loss:   1.645902\n",
      "train loss:   1.531340\n",
      "train loss:   1.437281\n",
      "train loss:   1.703970\n",
      "train loss:   1.716604\n",
      "train loss:   1.477365\n",
      "train loss:   1.826700\n",
      "train loss:   1.685899\n",
      "train loss:   1.337612\n",
      "train loss:   1.638882\n",
      "train loss:   1.553370\n",
      "train loss:   1.449306\n",
      "train loss:   1.476057\n",
      "train loss:   1.867501\n",
      "train loss:   1.662594\n",
      "train loss:   1.530047\n",
      "train loss:   1.643276\n",
      "########### epoch 6 ###########\n",
      "########### loop 800 ###########\n",
      "test loss:   1.706757   test accuracy:   0.500000\n",
      "########### loop 800 ###########\n",
      "train loss:   1.525575\n",
      "train loss:   1.368305\n",
      "train loss:   1.389069\n",
      "train loss:   1.498633\n",
      "train loss:   1.590386\n",
      "train loss:   1.760696\n",
      "train loss:   1.312273\n",
      "train loss:   1.514859\n",
      "train loss:   1.396203\n",
      "train loss:   1.508415\n",
      "train loss:   1.405125\n",
      "train loss:   1.446989\n",
      "train loss:   1.588934\n",
      "train loss:   1.426227\n",
      "train loss:   1.718208\n",
      "train loss:   1.838982\n",
      "train loss:   1.290865\n",
      "train loss:   1.647721\n",
      "train loss:   1.689459\n",
      "train loss:   1.635929\n",
      "train loss:   1.983542\n",
      "train loss:   1.863373\n",
      "train loss:   1.792177\n",
      "train loss:   1.847039\n",
      "train loss:   1.545486\n",
      "train loss:   1.666829\n",
      "train loss:   1.760529\n",
      "train loss:   1.556159\n",
      "train loss:   1.749836\n",
      "train loss:   1.699187\n",
      "train loss:   1.601171\n",
      "train loss:   1.610576\n",
      "train loss:   1.499617\n",
      "train loss:   1.650392\n",
      "train loss:   1.728682\n",
      "train loss:   1.728107\n",
      "train loss:   1.828427\n",
      "train loss:   2.132869\n",
      "train loss:   1.958789\n",
      "train loss:   1.767080\n",
      "train loss:   1.697066\n",
      "train loss:   1.577266\n",
      "train loss:   1.438297\n",
      "train loss:   1.503235\n",
      "train loss:   2.079733\n",
      "train loss:   1.420352\n",
      "train loss:   1.251662\n",
      "train loss:   1.562631\n",
      "train loss:   1.542080\n",
      "train loss:   1.972333\n",
      "########### epoch 7 ###########\n",
      "########### loop 850 ###########\n",
      "test loss:   2.011393   test accuracy:   0.468750\n",
      "########### loop 850 ###########\n",
      "train loss:   1.889096\n",
      "train loss:   2.085744\n",
      "train loss:   2.129132\n",
      "train loss:   1.601938\n",
      "train loss:   1.593091\n",
      "train loss:   1.921120\n",
      "train loss:   1.708731\n",
      "train loss:   1.765558\n",
      "train loss:   1.482951\n",
      "train loss:   1.511948\n",
      "train loss:   1.456240\n",
      "train loss:   1.571108\n",
      "train loss:   1.551817\n",
      "train loss:   1.815956\n",
      "train loss:   1.773803\n",
      "train loss:   1.838566\n",
      "train loss:   1.650488\n",
      "train loss:   1.904008\n",
      "train loss:   1.670222\n",
      "train loss:   1.540627\n",
      "train loss:   1.464375\n",
      "train loss:   1.535720\n",
      "train loss:   1.566379\n",
      "train loss:   1.899817\n",
      "train loss:   1.935948\n",
      "train loss:   1.300483\n",
      "train loss:   1.594536\n",
      "train loss:   1.641996\n",
      "train loss:   1.800703\n",
      "train loss:   1.601539\n",
      "train loss:   1.836292\n",
      "train loss:   2.038614\n",
      "train loss:   1.606968\n",
      "train loss:   1.659669\n",
      "train loss:   1.842510\n",
      "train loss:   1.602018\n",
      "train loss:   1.604774\n",
      "train loss:   1.403563\n",
      "train loss:   1.593969\n",
      "train loss:   1.747109\n",
      "train loss:   1.638528\n",
      "train loss:   1.619026\n",
      "train loss:   1.430114\n",
      "train loss:   1.822570\n",
      "train loss:   1.995349\n",
      "train loss:   1.484801\n",
      "train loss:   1.630810\n",
      "train loss:   1.504867\n",
      "train loss:   1.576526\n",
      "train loss:   1.364224\n",
      "########### epoch 7 ###########\n",
      "########### loop 900 ###########\n",
      "test loss:   1.884300   test accuracy:   0.218750\n",
      "########### loop 900 ###########\n",
      "train loss:   1.662152\n",
      "train loss:   1.475487\n",
      "train loss:   1.468551\n",
      "train loss:   1.766849\n",
      "train loss:   1.744199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.707645\n",
      "train loss:   1.981596\n",
      "train loss:   1.615044\n",
      "train loss:   1.703029\n",
      "train loss:   1.579448\n",
      "train loss:   1.260110\n",
      "train loss:   1.984511\n",
      "train loss:   1.680921\n",
      "train loss:   1.478911\n",
      "train loss:   1.595568\n",
      "train loss:   1.688447\n",
      "train loss:   1.508180\n",
      "train loss:   1.432497\n",
      "train loss:   1.499534\n",
      "train loss:   1.691497\n",
      "train loss:   1.436104\n",
      "train loss:   1.664922\n",
      "train loss:   1.696616\n",
      "train loss:   1.599530\n",
      "train loss:   1.788674\n",
      "train loss:   1.581938\n",
      "train loss:   1.331281\n",
      "train loss:   1.532670\n",
      "train loss:   1.725927\n",
      "train loss:   1.450769\n",
      "train loss:   1.775030\n",
      "train loss:   1.796053\n",
      "train loss:   1.272418\n",
      "train loss:   1.662580\n",
      "train loss:   1.601976\n",
      "train loss:   1.420797\n",
      "train loss:   1.404541\n",
      "train loss:   1.692815\n",
      "train loss:   1.685374\n",
      "train loss:   1.640227\n",
      "train loss:   1.761330\n",
      "train loss:   1.594606\n",
      "train loss:   1.586344\n",
      "train loss:   1.438642\n",
      "train loss:   1.530664\n",
      "train loss:   1.484700\n",
      "train loss:   1.824511\n",
      "train loss:   1.245412\n",
      "train loss:   1.581403\n",
      "train loss:   1.356176\n",
      "########### epoch 7 ###########\n",
      "########### loop 950 ###########\n",
      "test loss:   1.597444   test accuracy:   0.468750\n",
      "########### loop 950 ###########\n",
      "train loss:   1.555669\n",
      "train loss:   1.267008\n",
      "train loss:   1.308253\n",
      "train loss:   1.525287\n",
      "train loss:   1.422497\n",
      "train loss:   1.591705\n",
      "train loss:   1.741435\n",
      "train loss:   1.318418\n",
      "train loss:   1.664664\n",
      "train loss:   1.749332\n",
      "train loss:   1.431356\n",
      "train loss:   1.834777\n",
      "train loss:   1.587652\n",
      "train loss:   1.477767\n",
      "train loss:   1.599397\n",
      "train loss:   1.496415\n",
      "train loss:   1.500864\n",
      "train loss:   1.822928\n",
      "train loss:   1.505704\n",
      "train loss:   1.414084\n",
      "train loss:   1.420780\n",
      "train loss:   1.315408\n",
      "train loss:   1.380744\n",
      "train loss:   1.445561\n",
      "train loss:   1.493135\n",
      "train loss:   1.646065\n",
      "train loss:   1.671328\n",
      "train loss:   1.785289\n",
      "train loss:   1.875768\n",
      "train loss:   2.072556\n",
      "train loss:   1.784848\n",
      "train loss:   1.687129\n",
      "train loss:   1.514161\n",
      "train loss:   1.330860\n",
      "train loss:   1.367396\n",
      "train loss:   1.565947\n",
      "train loss:   1.336362\n",
      "train loss:   1.348348\n",
      "train loss:   1.431476\n",
      "train loss:   1.460089\n",
      "train loss:   1.627603\n",
      "train loss:   1.697573\n",
      "train loss:   1.662380\n",
      "train loss:   1.581878\n",
      "train loss:   1.548185\n",
      "train loss:   1.656458\n",
      "train loss:   1.903965\n",
      "train loss:   1.636348\n",
      "train loss:   1.571558\n",
      "train loss:   1.535620\n",
      "########### epoch 8 ###########\n",
      "########### loop 1000 ###########\n",
      "test loss:   1.585904   test accuracy:   0.406250\n",
      "########### loop 1000 ###########\n",
      "train loss:   1.629993\n",
      "train loss:   1.316032\n",
      "train loss:   1.437091\n",
      "train loss:   1.349974\n",
      "train loss:   1.842372\n",
      "train loss:   1.538114\n",
      "train loss:   1.688541\n",
      "train loss:   1.481699\n",
      "train loss:   1.809245\n",
      "train loss:   1.577901\n",
      "train loss:   1.298921\n",
      "train loss:   1.338035\n",
      "train loss:   1.530559\n",
      "train loss:   1.449263\n",
      "train loss:   1.736970\n",
      "train loss:   1.619654\n",
      "train loss:   1.283029\n",
      "train loss:   1.609920\n",
      "train loss:   1.778412\n",
      "train loss:   1.583717\n",
      "train loss:   1.478571\n",
      "train loss:   1.611270\n",
      "train loss:   1.730368\n",
      "train loss:   1.500334\n",
      "train loss:   1.756690\n",
      "train loss:   1.842689\n",
      "train loss:   1.559544\n",
      "train loss:   1.701936\n",
      "train loss:   1.349126\n",
      "train loss:   1.531714\n",
      "train loss:   1.584297\n",
      "train loss:   1.480660\n",
      "train loss:   1.562214\n",
      "train loss:   1.244386\n",
      "train loss:   1.666995\n",
      "train loss:   1.771024\n",
      "train loss:   1.483013\n",
      "train loss:   1.619398\n",
      "train loss:   1.282649\n",
      "train loss:   1.421680\n",
      "train loss:   1.296772\n",
      "train loss:   1.611691\n",
      "train loss:   1.472049\n",
      "train loss:   1.488567\n",
      "train loss:   1.616796\n",
      "train loss:   1.667094\n",
      "train loss:   1.572066\n",
      "train loss:   1.664386\n",
      "train loss:   1.567702\n",
      "train loss:   1.570791\n",
      "########### epoch 8 ###########\n",
      "########### loop 1050 ###########\n",
      "test loss:   1.404979   test accuracy:   0.593750\n",
      "########### loop 1050 ###########\n",
      "train loss:   1.412899\n",
      "train loss:   1.148096\n",
      "train loss:   1.712474\n",
      "train loss:   1.665217\n",
      "train loss:   1.689300\n",
      "train loss:   1.624777\n",
      "train loss:   1.544208\n",
      "train loss:   1.333000\n",
      "train loss:   1.546463\n",
      "train loss:   1.485698\n",
      "train loss:   1.691790\n",
      "train loss:   1.451416\n",
      "train loss:   1.380582\n",
      "train loss:   1.537336\n",
      "train loss:   1.290173\n",
      "train loss:   1.547830\n",
      "train loss:   1.289613\n",
      "train loss:   1.407114\n",
      "train loss:   1.493111\n",
      "train loss:   1.542655\n",
      "train loss:   1.454366\n",
      "train loss:   1.709855\n",
      "train loss:   1.695836\n",
      "train loss:   1.193745\n",
      "train loss:   1.790821\n",
      "train loss:   1.592575\n",
      "train loss:   1.504887\n",
      "train loss:   1.349068\n",
      "train loss:   1.620491\n",
      "train loss:   1.477747\n",
      "train loss:   1.530943\n",
      "train loss:   1.489922\n",
      "train loss:   1.535562\n",
      "train loss:   1.402589\n",
      "train loss:   1.421523\n",
      "train loss:   1.531565\n",
      "train loss:   1.514054\n",
      "train loss:   1.811814\n",
      "train loss:   1.242782\n",
      "train loss:   1.513571\n",
      "train loss:   1.331162\n",
      "train loss:   1.479670\n",
      "train loss:   1.198602\n",
      "train loss:   1.212093\n",
      "train loss:   1.526616\n",
      "train loss:   1.442090\n",
      "train loss:   1.527153\n",
      "train loss:   1.658373\n",
      "train loss:   1.313766\n",
      "train loss:   1.632330\n",
      "########### epoch 8 ###########\n",
      "########### loop 1100 ###########\n",
      "test loss:   1.642164   test accuracy:   0.437500\n",
      "########### loop 1100 ###########\n",
      "train loss:   1.752903\n",
      "train loss:   1.483963\n",
      "train loss:   1.787073\n",
      "train loss:   1.778172\n",
      "train loss:   1.456362\n",
      "train loss:   1.498011\n",
      "train loss:   1.446257\n",
      "train loss:   1.503228\n",
      "train loss:   1.733509\n",
      "train loss:   1.444589\n",
      "train loss:   1.749609\n",
      "train loss:   1.292775\n",
      "train loss:   1.319557\n",
      "train loss:   1.434001\n",
      "train loss:   1.479033\n",
      "train loss:   1.423578\n",
      "train loss:   1.684030\n",
      "train loss:   1.528542\n",
      "train loss:   1.621957\n",
      "train loss:   1.644636\n",
      "train loss:   1.753236\n",
      "train loss:   1.747736\n",
      "train loss:   1.623130\n",
      "train loss:   1.509899\n",
      "train loss:   1.372448\n",
      "train loss:   1.342664\n",
      "train loss:   1.741761\n",
      "train loss:   1.200729\n",
      "train loss:   1.198451\n",
      "train loss:   1.551893\n",
      "train loss:   1.455163\n",
      "train loss:   1.459057\n",
      "train loss:   1.676906\n",
      "train loss:   1.509628\n",
      "train loss:   1.433070\n",
      "train loss:   1.303480\n",
      "train loss:   1.465985\n",
      "train loss:   1.753905\n",
      "train loss:   1.581713\n",
      "train loss:   1.619480\n",
      "train loss:   1.345572\n",
      "train loss:   1.393399\n",
      "train loss:   1.288527\n",
      "train loss:   1.276162\n",
      "train loss:   1.394967\n",
      "train loss:   1.747212\n",
      "train loss:   1.493286\n",
      "train loss:   1.451547\n",
      "train loss:   1.313146\n",
      "train loss:   1.496483\n",
      "########### epoch 9 ###########\n",
      "########### loop 1150 ###########\n",
      "test loss:   1.645598   test accuracy:   0.437500\n",
      "########### loop 1150 ###########\n",
      "train loss:   1.301770\n",
      "train loss:   1.490329\n",
      "train loss:   1.436149\n",
      "train loss:   1.460889\n",
      "train loss:   1.409079\n",
      "train loss:   1.784526\n",
      "train loss:   1.637439\n",
      "train loss:   1.299911\n",
      "train loss:   1.620974\n",
      "train loss:   1.632439\n",
      "train loss:   1.574906\n",
      "train loss:   1.660954\n",
      "train loss:   1.378531\n",
      "train loss:   1.511010\n",
      "train loss:   1.456420\n",
      "train loss:   1.465854\n",
      "train loss:   1.709282\n",
      "train loss:   1.509772\n",
      "train loss:   1.645920\n",
      "train loss:   1.355583\n",
      "train loss:   1.550532\n",
      "train loss:   1.596830\n",
      "train loss:   1.503622\n",
      "train loss:   1.522713\n",
      "train loss:   1.222817\n",
      "train loss:   1.584182\n",
      "train loss:   1.791961\n",
      "train loss:   1.433399\n",
      "train loss:   1.694937\n",
      "train loss:   1.242684\n",
      "train loss:   1.438241\n",
      "train loss:   1.293236\n",
      "train loss:   1.429178\n",
      "train loss:   1.356856\n",
      "train loss:   1.421600\n",
      "train loss:   1.539856\n",
      "train loss:   1.613994\n",
      "train loss:   1.440235\n",
      "train loss:   1.706349\n",
      "train loss:   1.489617\n",
      "train loss:   1.447517\n",
      "train loss:   1.410122\n",
      "train loss:   1.184370\n",
      "train loss:   1.647752\n",
      "train loss:   1.398802\n",
      "train loss:   1.338761\n",
      "train loss:   1.436034\n",
      "train loss:   1.589395\n",
      "train loss:   1.349627\n",
      "train loss:   1.568802\n",
      "########### epoch 9 ###########\n",
      "########### loop 1200 ###########\n",
      "test loss:   1.564075   test accuracy:   0.531250\n",
      "########### loop 1200 ###########\n",
      "train loss:   1.480614\n",
      "train loss:   1.533475\n",
      "train loss:   1.363345\n",
      "train loss:   1.360318\n",
      "train loss:   1.427761\n",
      "train loss:   1.311869\n",
      "train loss:   1.533079\n",
      "train loss:   1.312477\n",
      "train loss:   1.401854\n",
      "train loss:   1.371422\n",
      "train loss:   1.547003\n",
      "train loss:   1.351416\n",
      "train loss:   1.691442\n",
      "train loss:   1.658989\n",
      "train loss:   1.217440\n",
      "train loss:   1.638932\n",
      "train loss:   1.462674\n",
      "train loss:   1.380195\n",
      "train loss:   1.434030\n",
      "train loss:   1.623994\n",
      "train loss:   1.489684\n",
      "train loss:   1.534342\n",
      "train loss:   1.562828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.489740\n",
      "train loss:   1.219646\n",
      "train loss:   1.320525\n",
      "train loss:   1.492426\n",
      "train loss:   1.459769\n",
      "train loss:   1.719492\n",
      "train loss:   1.234712\n",
      "train loss:   1.399006\n",
      "train loss:   1.261828\n",
      "train loss:   1.343151\n",
      "train loss:   1.267620\n",
      "train loss:   1.369664\n",
      "train loss:   1.492986\n",
      "train loss:   1.422379\n",
      "train loss:   1.501049\n",
      "train loss:   1.570751\n",
      "train loss:   1.195089\n",
      "train loss:   1.519068\n",
      "train loss:   1.617379\n",
      "train loss:   1.529949\n",
      "train loss:   1.777970\n",
      "train loss:   1.523162\n",
      "train loss:   1.379591\n",
      "train loss:   1.498520\n",
      "train loss:   1.374900\n",
      "train loss:   1.281172\n",
      "train loss:   1.627275\n",
      "########### epoch 9 ###########\n",
      "########### loop 1250 ###########\n",
      "test loss:   1.603232   test accuracy:   0.531250\n",
      "########### loop 1250 ###########\n",
      "train loss:   1.373057\n",
      "train loss:   1.456507\n",
      "train loss:   1.355876\n",
      "train loss:   1.279042\n",
      "train loss:   1.251114\n",
      "train loss:   1.429201\n",
      "train loss:   1.488788\n",
      "train loss:   1.387632\n",
      "train loss:   1.484648\n",
      "train loss:   1.511462\n",
      "train loss:   1.562639\n",
      "train loss:   1.622440\n",
      "train loss:   1.628021\n",
      "train loss:   1.460175\n",
      "train loss:   1.412896\n",
      "train loss:   1.358379\n",
      "train loss:   1.306532\n",
      "train loss:   1.648258\n",
      "train loss:   1.272094\n",
      "train loss:   1.453853\n",
      "train loss:   1.531310\n",
      "train loss:   1.532887\n",
      "train loss:   1.296247\n",
      "train loss:   1.586176\n",
      "train loss:   1.592115\n",
      "train loss:   1.643352\n",
      "train loss:   1.245576\n",
      "train loss:   1.358929\n",
      "train loss:   1.468201\n",
      "train loss:   1.409943\n",
      "train loss:   1.357228\n",
      "train loss:   1.399496\n",
      "train loss:   1.388940\n",
      "train loss:   1.292554\n",
      "train loss:   1.193672\n",
      "train loss:   1.174832\n",
      "train loss:   1.585481\n",
      "train loss:   1.651053\n",
      "train loss:   1.554083\n",
      "train loss:   1.308290\n",
      "train loss:   1.483892\n",
      "train loss:   1.217509\n",
      "train loss:   1.388393\n",
      "train loss:   1.366238\n",
      "train loss:   1.434166\n",
      "train loss:   1.266653\n",
      "train loss:   1.583182\n",
      "train loss:   1.614625\n",
      "train loss:   1.189339\n",
      "train loss:   1.738031\n",
      "########### epoch 10 ###########\n",
      "########### loop 1300 ###########\n",
      "test loss:   1.366018   test accuracy:   0.593750\n",
      "########### loop 1300 ###########\n",
      "train loss:   1.549917\n",
      "train loss:   1.537541\n",
      "train loss:   1.591150\n",
      "train loss:   1.237595\n",
      "train loss:   1.491851\n",
      "train loss:   1.455127\n",
      "train loss:   1.468570\n",
      "train loss:   1.612792\n",
      "train loss:   1.505374\n",
      "train loss:   1.689017\n",
      "train loss:   1.365493\n",
      "train loss:   1.487803\n",
      "train loss:   1.498254\n",
      "train loss:   1.507894\n",
      "train loss:   1.382845\n",
      "train loss:   1.194339\n",
      "train loss:   1.548965\n",
      "train loss:   1.662703\n",
      "train loss:   1.367853\n",
      "train loss:   1.454742\n",
      "train loss:   1.326576\n",
      "train loss:   1.370551\n",
      "train loss:   1.241152\n",
      "train loss:   1.508650\n",
      "train loss:   1.340549\n",
      "train loss:   1.443990\n",
      "train loss:   1.533216\n",
      "train loss:   1.489123\n",
      "train loss:   1.421026\n",
      "train loss:   1.628790\n",
      "train loss:   1.453635\n",
      "train loss:   1.564595\n",
      "train loss:   1.380742\n",
      "train loss:   1.163185\n",
      "train loss:   1.731426\n",
      "train loss:   1.394858\n",
      "train loss:   1.349728\n",
      "train loss:   1.416322\n",
      "train loss:   1.459322\n",
      "train loss:   1.323464\n",
      "train loss:   1.379435\n",
      "train loss:   1.349898\n",
      "train loss:   1.514117\n",
      "train loss:   1.410129\n",
      "train loss:   1.326771\n",
      "train loss:   1.363765\n",
      "train loss:   1.360905\n",
      "train loss:   1.481209\n",
      "train loss:   1.411846\n",
      "train loss:   1.412650\n",
      "########### epoch 10 ###########\n",
      "########### loop 1350 ###########\n",
      "test loss:   1.550354   test accuracy:   0.531250\n",
      "########### loop 1350 ###########\n",
      "train loss:   1.436356\n",
      "train loss:   1.550229\n",
      "train loss:   1.354019\n",
      "train loss:   1.664407\n",
      "train loss:   1.600085\n",
      "train loss:   1.149734\n",
      "train loss:   1.726225\n",
      "train loss:   1.456237\n",
      "train loss:   1.357668\n",
      "train loss:   1.399039\n",
      "train loss:   1.574879\n",
      "train loss:   1.517300\n",
      "train loss:   1.560394\n",
      "train loss:   1.465354\n",
      "train loss:   1.480658\n",
      "train loss:   1.345806\n",
      "train loss:   1.344998\n",
      "train loss:   1.449330\n",
      "train loss:   1.413468\n",
      "train loss:   1.688840\n",
      "train loss:   1.264544\n",
      "train loss:   1.409365\n",
      "train loss:   1.236098\n",
      "train loss:   1.522583\n",
      "train loss:   1.327726\n",
      "train loss:   1.393785\n",
      "train loss:   1.532602\n",
      "train loss:   1.395177\n",
      "train loss:   1.561087\n",
      "train loss:   1.525334\n",
      "train loss:   1.163387\n",
      "train loss:   1.595559\n",
      "train loss:   1.832929\n",
      "train loss:   1.466752\n",
      "train loss:   1.657924\n",
      "train loss:   1.468786\n",
      "train loss:   1.238881\n",
      "train loss:   1.419152\n",
      "train loss:   1.290106\n",
      "train loss:   1.480509\n",
      "train loss:   1.772788\n",
      "train loss:   1.443340\n",
      "train loss:   1.489539\n",
      "train loss:   1.224161\n",
      "train loss:   1.195295\n",
      "train loss:   1.279420\n",
      "train loss:   1.319923\n",
      "train loss:   1.364378\n",
      "train loss:   1.472908\n",
      "train loss:   1.478942\n",
      "########### epoch 10 ###########\n",
      "########### loop 1400 ###########\n",
      "test loss:   1.540208   test accuracy:   0.531250\n",
      "########### loop 1400 ###########\n",
      "train loss:   1.561263\n",
      "train loss:   1.578203\n",
      "train loss:   1.793243\n",
      "train loss:   1.728267\n",
      "train loss:   1.564323\n",
      "train loss:   1.592436\n",
      "train loss:   1.320481\n",
      "train loss:   1.383277\n",
      "train loss:   1.404260\n",
      "train loss:   1.174724\n",
      "train loss:   1.111486\n",
      "train loss:   1.408406\n",
      "train loss:   1.315434\n",
      "train loss:   1.444234\n",
      "train loss:   1.596198\n",
      "train loss:   1.486116\n",
      "train loss:   1.524691\n",
      "train loss:   1.332297\n",
      "train loss:   1.440834\n",
      "train loss:   1.623137\n",
      "train loss:   1.447233\n",
      "train loss:   1.425093\n",
      "train loss:   1.443361\n",
      "train loss:   1.382781\n",
      "train loss:   1.311301\n",
      "train loss:   1.242194\n",
      "train loss:   1.267216\n",
      "train loss:   1.628611\n",
      "train loss:   1.468378\n",
      "train loss:   1.401445\n",
      "train loss:   1.329482\n",
      "train loss:   1.466520\n",
      "train loss:   1.237353\n",
      "train loss:   1.347312\n",
      "train loss:   1.233112\n",
      "train loss:   1.326431\n",
      "train loss:   1.337059\n",
      "train loss:   1.633315\n",
      "train loss:   1.606190\n",
      "train loss:   1.196793\n",
      "train loss:   1.466330\n",
      "train loss:   1.540267\n",
      "train loss:   1.556635\n",
      "train loss:   1.526495\n",
      "train loss:   1.311094\n",
      "train loss:   1.510660\n",
      "train loss:   1.443061\n",
      "train loss:   1.416084\n",
      "train loss:   1.720509\n",
      "train loss:   1.371013\n",
      "########### epoch 11 ###########\n",
      "########### loop 1450 ###########\n",
      "test loss:   1.351427   test accuracy:   0.593750\n",
      "########### loop 1450 ###########\n",
      "train loss:   1.529108\n",
      "train loss:   1.233665\n",
      "train loss:   1.343697\n",
      "train loss:   1.356440\n",
      "train loss:   1.401239\n",
      "train loss:   1.504792\n",
      "train loss:   1.405563\n",
      "train loss:   1.784617\n",
      "train loss:   1.609033\n",
      "train loss:   1.322763\n",
      "train loss:   1.598171\n",
      "train loss:   1.214822\n",
      "train loss:   1.284349\n",
      "train loss:   1.238569\n",
      "train loss:   1.432318\n",
      "train loss:   1.388566\n",
      "train loss:   1.417000\n",
      "train loss:   1.408885\n",
      "train loss:   1.735400\n",
      "train loss:   1.514944\n",
      "train loss:   1.753533\n",
      "train loss:   1.607059\n",
      "train loss:   1.515234\n",
      "train loss:   1.342314\n",
      "train loss:   1.222372\n",
      "train loss:   1.767407\n",
      "train loss:   1.392877\n",
      "train loss:   1.435087\n",
      "train loss:   1.428340\n",
      "train loss:   1.445435\n",
      "train loss:   1.283777\n",
      "train loss:   1.371302\n",
      "train loss:   1.407273\n",
      "train loss:   1.424547\n",
      "train loss:   1.365824\n",
      "train loss:   1.305277\n",
      "train loss:   1.362067\n",
      "train loss:   1.225509\n",
      "train loss:   1.568344\n",
      "train loss:   1.391321\n",
      "train loss:   1.510107\n",
      "train loss:   1.442061\n",
      "train loss:   1.467826\n",
      "train loss:   1.448477\n",
      "train loss:   1.597931\n",
      "train loss:   1.634201\n",
      "train loss:   1.142713\n",
      "train loss:   1.644911\n",
      "train loss:   1.393424\n",
      "train loss:   1.282031\n",
      "########### epoch 11 ###########\n",
      "########### loop 1500 ###########\n",
      "test loss:   1.389980   test accuracy:   0.562500\n",
      "########### loop 1500 ###########\n",
      "train loss:   1.257824\n",
      "train loss:   1.556913\n",
      "train loss:   1.422804\n",
      "train loss:   1.403908\n",
      "train loss:   1.470301\n",
      "train loss:   1.440367\n",
      "train loss:   1.235937\n",
      "train loss:   1.321097\n",
      "train loss:   1.351195\n",
      "train loss:   1.322837\n",
      "train loss:   1.723818\n",
      "train loss:   1.081431\n",
      "train loss:   1.407576\n",
      "train loss:   1.155753\n",
      "train loss:   1.377188\n",
      "train loss:   1.146568\n",
      "train loss:   1.280164\n",
      "train loss:   1.358636\n",
      "train loss:   1.295764\n",
      "train loss:   1.547811\n",
      "train loss:   1.467137\n",
      "train loss:   1.183300\n",
      "train loss:   1.429970\n",
      "train loss:   1.583998\n",
      "train loss:   1.341140\n",
      "train loss:   1.533908\n",
      "train loss:   1.355204\n",
      "train loss:   1.266484\n",
      "train loss:   1.365744\n",
      "train loss:   1.313697\n",
      "train loss:   1.244009\n",
      "train loss:   1.523168\n",
      "train loss:   1.308501\n",
      "train loss:   1.301997\n",
      "train loss:   1.250726\n",
      "train loss:   1.094441\n",
      "train loss:   1.133616\n",
      "train loss:   1.367967\n",
      "train loss:   1.349573\n",
      "train loss:   1.391783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.495882\n",
      "train loss:   1.326204\n",
      "train loss:   1.407456\n",
      "train loss:   1.631979\n",
      "train loss:   1.537391\n",
      "train loss:   1.440015\n",
      "train loss:   1.450129\n",
      "train loss:   1.331387\n",
      "train loss:   1.316917\n",
      "train loss:   1.519743\n",
      "########### epoch 11 ###########\n",
      "########### loop 1550 ###########\n",
      "test loss:   1.420143   test accuracy:   0.562500\n",
      "########### loop 1550 ###########\n",
      "train loss:   1.153543\n",
      "train loss:   1.132959\n",
      "train loss:   1.370354\n",
      "train loss:   1.341528\n",
      "train loss:   1.276461\n",
      "train loss:   1.511494\n",
      "train loss:   1.398960\n",
      "train loss:   1.326983\n",
      "train loss:   1.275893\n",
      "train loss:   1.528821\n",
      "train loss:   1.637902\n",
      "train loss:   1.442715\n",
      "train loss:   1.387409\n",
      "train loss:   1.359814\n",
      "train loss:   1.257702\n",
      "train loss:   1.256683\n",
      "train loss:   1.224391\n",
      "train loss:   1.334418\n",
      "train loss:   1.508258\n",
      "train loss:   1.469682\n",
      "train loss:   1.534661\n",
      "train loss:   1.264891\n",
      "train loss:   1.414941\n",
      "train loss:   1.053573\n",
      "train loss:   1.288927\n",
      "train loss:   1.250434\n",
      "train loss:   1.275326\n",
      "train loss:   1.392060\n",
      "train loss:   1.640711\n",
      "train loss:   1.599000\n",
      "train loss:   1.243051\n",
      "train loss:   1.613136\n",
      "train loss:   1.620780\n",
      "train loss:   1.512708\n",
      "train loss:   1.487152\n",
      "train loss:   1.348813\n",
      "train loss:   1.487155\n",
      "train loss:   1.423461\n",
      "train loss:   1.553542\n",
      "train loss:   1.650242\n",
      "train loss:   1.397761\n",
      "train loss:   1.448150\n",
      "train loss:   1.320715\n",
      "train loss:   1.519368\n",
      "train loss:   1.632180\n",
      "train loss:   1.554936\n",
      "train loss:   1.424247\n",
      "train loss:   1.224582\n",
      "train loss:   1.624486\n",
      "train loss:   1.779001\n",
      "########### epoch 12 ###########\n",
      "########### loop 1600 ###########\n",
      "test loss:   1.405502   test accuracy:   0.562500\n",
      "########### loop 1600 ###########\n",
      "train loss:   1.385119\n",
      "train loss:   1.601328\n",
      "train loss:   1.066545\n",
      "train loss:   1.342844\n",
      "train loss:   1.211047\n",
      "train loss:   1.594247\n",
      "train loss:   1.265062\n",
      "train loss:   1.403691\n",
      "train loss:   1.585545\n",
      "train loss:   1.706669\n",
      "train loss:   1.448022\n",
      "train loss:   1.696591\n",
      "train loss:   1.446212\n",
      "train loss:   1.545580\n",
      "train loss:   1.364031\n",
      "train loss:   1.253241\n",
      "train loss:   1.830662\n",
      "train loss:   1.419475\n",
      "train loss:   1.355948\n",
      "train loss:   1.353342\n",
      "train loss:   1.492357\n",
      "train loss:   1.234010\n",
      "train loss:   1.396389\n",
      "train loss:   1.259065\n",
      "train loss:   1.535745\n",
      "train loss:   1.146876\n",
      "train loss:   1.460247\n",
      "train loss:   1.425991\n",
      "train loss:   1.375247\n",
      "train loss:   1.560606\n",
      "train loss:   1.264993\n",
      "train loss:   1.215216\n",
      "train loss:   1.348042\n",
      "train loss:   1.439472\n",
      "train loss:   1.354507\n",
      "train loss:   1.597443\n",
      "train loss:   1.603516\n",
      "train loss:   1.222257\n",
      "train loss:   1.477837\n",
      "train loss:   1.363395\n",
      "train loss:   1.281391\n",
      "train loss:   1.390710\n",
      "train loss:   1.658407\n",
      "train loss:   1.570116\n",
      "train loss:   1.621359\n",
      "train loss:   1.411831\n",
      "train loss:   1.521759\n",
      "train loss:   1.353754\n",
      "train loss:   1.364155\n",
      "train loss:   1.352001\n",
      "########### epoch 12 ###########\n",
      "########### loop 1650 ###########\n",
      "test loss:   1.593981   test accuracy:   0.437500\n",
      "########### loop 1650 ###########\n",
      "train loss:   1.411263\n",
      "train loss:   1.741797\n",
      "train loss:   1.191733\n",
      "train loss:   1.431773\n",
      "train loss:   1.184922\n",
      "train loss:   1.454172\n",
      "train loss:   1.163259\n",
      "train loss:   1.195480\n",
      "train loss:   1.508200\n",
      "train loss:   1.472354\n",
      "train loss:   1.423552\n",
      "train loss:   1.401485\n",
      "train loss:   1.156416\n",
      "train loss:   1.413914\n",
      "train loss:   1.573982\n",
      "train loss:   1.310569\n",
      "train loss:   1.552449\n",
      "train loss:   1.378921\n",
      "train loss:   1.114420\n",
      "train loss:   1.264672\n",
      "train loss:   1.208588\n",
      "train loss:   1.335577\n",
      "train loss:   1.586627\n",
      "train loss:   1.310916\n",
      "train loss:   1.329015\n",
      "train loss:   1.229623\n",
      "train loss:   1.099901\n",
      "train loss:   1.124771\n",
      "train loss:   1.263832\n",
      "train loss:   1.353696\n",
      "train loss:   1.460191\n",
      "train loss:   1.457266\n",
      "train loss:   1.415535\n",
      "train loss:   1.474492\n",
      "train loss:   1.716989\n",
      "train loss:   1.569401\n",
      "train loss:   1.526496\n",
      "train loss:   1.507967\n",
      "train loss:   1.325439\n",
      "train loss:   1.346227\n",
      "train loss:   1.561497\n",
      "train loss:   1.235271\n",
      "train loss:   1.240173\n",
      "train loss:   1.415503\n",
      "train loss:   1.252629\n",
      "train loss:   1.438062\n",
      "train loss:   1.709464\n",
      "train loss:   1.622672\n",
      "train loss:   1.546561\n",
      "train loss:   1.116202\n",
      "########### epoch 13 ###########\n",
      "########### loop 1700 ###########\n",
      "test loss:   1.430943   test accuracy:   0.625000\n",
      "########### loop 1700 ###########\n",
      "train loss:   1.337173\n",
      "train loss:   1.489744\n",
      "train loss:   1.426924\n",
      "train loss:   1.368584\n",
      "train loss:   1.197229\n",
      "train loss:   1.234950\n",
      "train loss:   1.198334\n",
      "train loss:   1.053863\n",
      "train loss:   1.099793\n",
      "train loss:   1.650359\n",
      "train loss:   1.613178\n",
      "train loss:   1.564769\n",
      "train loss:   1.252080\n",
      "train loss:   1.355034\n",
      "train loss:   1.183717\n",
      "train loss:   1.272541\n",
      "train loss:   1.274752\n",
      "train loss:   1.429401\n",
      "train loss:   1.197950\n",
      "train loss:   1.707706\n",
      "train loss:   1.586606\n",
      "train loss:   1.070053\n",
      "train loss:   1.446551\n",
      "train loss:   1.494535\n",
      "train loss:   1.561509\n",
      "train loss:   1.651218\n",
      "train loss:   1.331782\n",
      "train loss:   1.512771\n",
      "train loss:   1.458013\n",
      "train loss:   1.352390\n",
      "train loss:   1.687121\n",
      "train loss:   1.276301\n",
      "train loss:   1.450446\n",
      "train loss:   1.187472\n",
      "train loss:   1.386492\n",
      "train loss:   1.365993\n",
      "train loss:   1.440595\n",
      "train loss:   1.370610\n",
      "train loss:   1.209863\n",
      "train loss:   1.639780\n",
      "train loss:   1.744539\n",
      "train loss:   1.397580\n",
      "train loss:   1.562750\n",
      "train loss:   1.055543\n",
      "train loss:   1.351140\n",
      "train loss:   1.262002\n",
      "train loss:   1.453640\n",
      "train loss:   1.295877\n",
      "train loss:   1.270329\n",
      "train loss:   1.674129\n",
      "########### epoch 13 ###########\n",
      "########### loop 1750 ###########\n",
      "test loss:   1.333239   test accuracy:   0.593750\n",
      "########### loop 1750 ###########\n",
      "train loss:   1.689866\n",
      "train loss:   1.494553\n",
      "train loss:   1.571436\n",
      "train loss:   1.511669\n",
      "train loss:   1.561978\n",
      "train loss:   1.442019\n",
      "train loss:   1.155558\n",
      "train loss:   1.735858\n",
      "train loss:   1.344658\n",
      "train loss:   1.485098\n",
      "train loss:   1.277588\n",
      "train loss:   1.339446\n",
      "train loss:   1.275783\n",
      "train loss:   1.383186\n",
      "train loss:   1.274008\n",
      "train loss:   1.445860\n",
      "train loss:   1.294724\n",
      "train loss:   1.226105\n",
      "train loss:   1.250710\n",
      "train loss:   1.235175\n",
      "train loss:   1.381089\n",
      "train loss:   1.212641\n",
      "train loss:   1.291818\n",
      "train loss:   1.237228\n",
      "train loss:   1.408380\n",
      "train loss:   1.253719\n",
      "train loss:   1.557977\n",
      "train loss:   1.554808\n",
      "train loss:   1.059167\n",
      "train loss:   1.620387\n",
      "train loss:   1.478774\n",
      "train loss:   1.264037\n",
      "train loss:   1.261249\n",
      "train loss:   1.582956\n",
      "train loss:   1.495593\n",
      "train loss:   1.338604\n",
      "train loss:   1.403943\n",
      "train loss:   1.489333\n",
      "train loss:   1.387820\n",
      "train loss:   1.297556\n",
      "train loss:   1.355989\n",
      "train loss:   1.366606\n",
      "train loss:   1.578812\n",
      "train loss:   1.228677\n",
      "train loss:   1.274341\n",
      "train loss:   1.178315\n",
      "train loss:   1.496266\n",
      "train loss:   1.159866\n",
      "train loss:   1.208234\n",
      "train loss:   1.407094\n",
      "########### epoch 13 ###########\n",
      "########### loop 1800 ###########\n",
      "test loss:   1.293726   test accuracy:   0.593750\n",
      "########### loop 1800 ###########\n",
      "train loss:   1.324030\n",
      "train loss:   1.455465\n",
      "train loss:   1.458171\n",
      "train loss:   1.098521\n",
      "train loss:   1.475476\n",
      "train loss:   1.552469\n",
      "train loss:   1.298363\n",
      "train loss:   1.540353\n",
      "train loss:   1.326463\n",
      "train loss:   1.340872\n",
      "train loss:   1.408554\n",
      "train loss:   1.310256\n",
      "train loss:   1.376776\n",
      "train loss:   1.556468\n",
      "train loss:   1.300967\n",
      "train loss:   1.328727\n",
      "train loss:   1.236382\n",
      "train loss:   1.094504\n",
      "train loss:   1.130624\n",
      "train loss:   1.256523\n",
      "train loss:   1.260163\n",
      "train loss:   1.333156\n",
      "train loss:   1.411478\n",
      "train loss:   1.412516\n",
      "train loss:   1.454053\n",
      "train loss:   1.697801\n",
      "train loss:   1.714389\n",
      "train loss:   1.483938\n",
      "train loss:   1.327395\n",
      "train loss:   1.197507\n",
      "train loss:   1.144854\n",
      "train loss:   1.633513\n",
      "train loss:   1.177230\n",
      "train loss:   1.138857\n",
      "train loss:   1.316861\n",
      "train loss:   1.339693\n",
      "train loss:   1.231261\n",
      "train loss:   1.537249\n",
      "train loss:   1.335798\n",
      "train loss:   1.366747\n",
      "train loss:   1.141068\n",
      "train loss:   1.274928\n",
      "train loss:   1.631340\n",
      "train loss:   1.402829\n",
      "train loss:   1.347707\n",
      "train loss:   1.299572\n",
      "train loss:   1.239577\n",
      "train loss:   1.261422\n",
      "train loss:   1.152799\n",
      "train loss:   1.122194\n",
      "########### epoch 14 ###########\n",
      "########### loop 1850 ###########\n",
      "test loss:   1.355866   test accuracy:   0.531250\n",
      "########### loop 1850 ###########\n",
      "train loss:   1.545872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.375084\n",
      "train loss:   1.314497\n",
      "train loss:   1.177454\n",
      "train loss:   1.362526\n",
      "train loss:   1.196522\n",
      "train loss:   1.216820\n",
      "train loss:   1.232940\n",
      "train loss:   1.460734\n",
      "train loss:   1.258431\n",
      "train loss:   1.657140\n",
      "train loss:   1.616399\n",
      "train loss:   1.071929\n",
      "train loss:   1.318837\n",
      "train loss:   1.483572\n",
      "train loss:   1.444229\n",
      "train loss:   1.247687\n",
      "train loss:   1.503602\n",
      "train loss:   1.582518\n",
      "train loss:   1.332480\n",
      "train loss:   1.291846\n",
      "train loss:   1.650127\n",
      "train loss:   1.321032\n",
      "train loss:   1.630135\n",
      "train loss:   1.318153\n",
      "train loss:   1.336891\n",
      "train loss:   1.401541\n",
      "train loss:   1.334503\n",
      "train loss:   1.327493\n",
      "train loss:   1.141793\n",
      "train loss:   1.558251\n",
      "train loss:   1.770484\n",
      "train loss:   1.322044\n",
      "train loss:   1.463882\n",
      "train loss:   1.082660\n",
      "train loss:   1.359550\n",
      "train loss:   1.139437\n",
      "train loss:   1.526236\n",
      "train loss:   1.146071\n",
      "train loss:   1.598066\n",
      "train loss:   1.480227\n",
      "train loss:   1.452133\n",
      "train loss:   1.402590\n",
      "train loss:   1.605815\n",
      "train loss:   1.438290\n",
      "train loss:   1.459433\n",
      "train loss:   1.379247\n",
      "train loss:   1.249627\n",
      "train loss:   1.580230\n",
      "train loss:   1.238768\n",
      "########### epoch 14 ###########\n",
      "########### loop 1900 ###########\n",
      "test loss:   1.291865   test accuracy:   0.531250\n",
      "########### loop 1900 ###########\n",
      "train loss:   1.325681\n",
      "train loss:   1.341655\n",
      "train loss:   1.407356\n",
      "train loss:   1.231565\n",
      "train loss:   1.424802\n",
      "train loss:   1.368993\n",
      "train loss:   1.498949\n",
      "train loss:   1.382689\n",
      "train loss:   1.275244\n",
      "train loss:   1.355408\n",
      "train loss:   1.241096\n",
      "train loss:   1.368451\n",
      "train loss:   1.141358\n",
      "train loss:   1.337101\n",
      "train loss:   1.386733\n",
      "train loss:   1.299329\n",
      "train loss:   1.187685\n",
      "train loss:   1.448778\n",
      "train loss:   1.563641\n",
      "train loss:   1.089879\n",
      "train loss:   1.473256\n",
      "train loss:   1.294013\n",
      "train loss:   1.229479\n",
      "train loss:   1.295289\n",
      "train loss:   1.589429\n",
      "train loss:   1.404262\n",
      "train loss:   1.290350\n",
      "train loss:   1.419785\n",
      "train loss:   1.380145\n",
      "train loss:   1.039402\n",
      "train loss:   1.395973\n",
      "train loss:   1.468245\n",
      "train loss:   1.306306\n",
      "train loss:   1.491449\n",
      "train loss:   0.986878\n",
      "train loss:   1.441701\n",
      "train loss:   1.268083\n",
      "train loss:   1.355991\n",
      "train loss:   1.197518\n",
      "train loss:   1.221958\n",
      "train loss:   1.292078\n",
      "train loss:   1.271791\n",
      "train loss:   1.508517\n",
      "train loss:   1.439900\n",
      "train loss:   1.016692\n",
      "train loss:   1.366488\n",
      "train loss:   1.453948\n",
      "train loss:   1.287984\n",
      "train loss:   1.571900\n",
      "train loss:   1.285379\n",
      "########### epoch 14 ###########\n",
      "########### loop 1950 ###########\n",
      "test loss:   1.394175   test accuracy:   0.593750\n",
      "########### loop 1950 ###########\n",
      "train loss:   1.068727\n",
      "train loss:   1.267387\n",
      "train loss:   1.219567\n",
      "train loss:   1.234166\n",
      "train loss:   1.422755\n",
      "train loss:   1.271900\n",
      "train loss:   1.399242\n",
      "train loss:   1.151367\n",
      "train loss:   1.032521\n",
      "train loss:   1.099055\n",
      "train loss:   1.162035\n",
      "train loss:   1.382733\n",
      "train loss:   1.384606\n",
      "train loss:   1.419757\n",
      "train loss:   1.376707\n",
      "train loss:   1.381894\n",
      "train loss:   1.571238\n",
      "train loss:   1.384230\n",
      "train loss:   1.487738\n",
      "train loss:   1.292291\n",
      "train loss:   1.121147\n",
      "train loss:   1.180188\n",
      "train loss:   1.586917\n",
      "train loss:   1.254419\n",
      "train loss:   1.202691\n",
      "train loss:   1.430247\n",
      "train loss:   1.287760\n",
      "train loss:   1.206695\n",
      "train loss:   1.401516\n",
      "train loss:   1.439742\n",
      "train loss:   1.392020\n",
      "train loss:   1.108349\n",
      "train loss:   1.199392\n",
      "train loss:   1.327541\n",
      "train loss:   1.296714\n",
      "train loss:   1.302865\n",
      "train loss:   1.245305\n",
      "train loss:   1.244084\n",
      "train loss:   1.195440\n",
      "train loss:   1.058592\n",
      "train loss:   1.019696\n",
      "train loss:   1.521615\n",
      "train loss:   1.433897\n",
      "train loss:   1.367146\n",
      "train loss:   1.236636\n",
      "train loss:   1.346701\n",
      "train loss:   1.103212\n",
      "train loss:   1.146506\n",
      "train loss:   1.224600\n",
      "train loss:   1.339750\n",
      "########### epoch 15 ###########\n",
      "########### loop 2000 ###########\n",
      "test loss:   1.303507   test accuracy:   0.593750\n",
      "########### loop 2000 ###########\n",
      "train loss:   1.127731\n",
      "train loss:   1.545799\n",
      "train loss:   1.472695\n",
      "train loss:   1.073112\n",
      "train loss:   1.404995\n",
      "train loss:   1.511172\n",
      "train loss:   1.425618\n",
      "train loss:   1.616628\n",
      "train loss:   1.191808\n",
      "train loss:   1.401464\n",
      "train loss:   1.308537\n",
      "train loss:   1.294115\n",
      "train loss:   1.536441\n",
      "train loss:   1.363884\n",
      "train loss:   1.474157\n",
      "train loss:   1.147411\n",
      "train loss:   1.308535\n",
      "train loss:   1.452791\n",
      "train loss:   1.376544\n",
      "train loss:   1.359002\n",
      "train loss:   1.099787\n",
      "train loss:   1.295621\n",
      "train loss:   1.706256\n",
      "train loss:   1.252381\n",
      "train loss:   1.428964\n",
      "train loss:   1.243909\n",
      "train loss:   1.251676\n",
      "train loss:   1.084188\n",
      "train loss:   1.331585\n",
      "train loss:   1.232911\n",
      "train loss:   1.344788\n",
      "train loss:   1.580156\n",
      "train loss:   1.384794\n",
      "train loss:   1.329072\n",
      "train loss:   1.445030\n",
      "train loss:   1.431946\n",
      "train loss:   1.447989\n",
      "train loss:   1.317363\n",
      "train loss:   1.058244\n",
      "train loss:   1.540572\n",
      "train loss:   1.255864\n",
      "train loss:   1.262774\n",
      "train loss:   1.269668\n",
      "train loss:   1.387551\n",
      "train loss:   1.196996\n",
      "train loss:   1.277833\n",
      "train loss:   1.224242\n",
      "train loss:   1.469097\n",
      "train loss:   1.326955\n",
      "train loss:   1.288827\n",
      "########### epoch 15 ###########\n",
      "########### loop 2050 ###########\n",
      "test loss:   1.298345   test accuracy:   0.687500\n",
      "########### loop 2050 ###########\n",
      "train loss:   1.244073\n",
      "train loss:   1.417437\n",
      "train loss:   1.367702\n",
      "train loss:   1.197293\n",
      "train loss:   1.271776\n",
      "train loss:   1.359896\n",
      "train loss:   1.423031\n",
      "train loss:   1.155948\n",
      "train loss:   1.421221\n",
      "train loss:   1.604904\n",
      "train loss:   1.062257\n",
      "train loss:   1.461987\n",
      "train loss:   1.258682\n",
      "train loss:   1.147422\n",
      "train loss:   1.269547\n",
      "train loss:   1.557593\n",
      "train loss:   1.341636\n",
      "train loss:   1.332600\n",
      "train loss:   1.438550\n",
      "train loss:   1.424454\n",
      "train loss:   1.205532\n",
      "train loss:   1.328038\n",
      "train loss:   1.306368\n",
      "train loss:   1.342806\n",
      "train loss:   1.473080\n",
      "train loss:   1.044626\n",
      "train loss:   1.398387\n",
      "train loss:   1.221337\n",
      "train loss:   1.553967\n",
      "train loss:   1.183051\n",
      "train loss:   1.219731\n",
      "train loss:   1.249068\n",
      "train loss:   1.251118\n",
      "train loss:   1.435834\n",
      "train loss:   1.480222\n",
      "train loss:   1.007437\n",
      "train loss:   1.446726\n",
      "train loss:   1.477439\n",
      "train loss:   1.235838\n",
      "train loss:   1.502075\n",
      "train loss:   1.299830\n",
      "train loss:   1.073008\n",
      "train loss:   1.356224\n",
      "train loss:   1.249891\n",
      "train loss:   1.190607\n",
      "train loss:   1.539310\n",
      "train loss:   1.245073\n",
      "train loss:   1.314670\n",
      "train loss:   1.211545\n",
      "train loss:   1.128737\n",
      "########### epoch 15 ###########\n",
      "########### loop 2100 ###########\n",
      "test loss:   1.423748   test accuracy:   0.531250\n",
      "########### loop 2100 ###########\n",
      "train loss:   1.127301\n",
      "train loss:   1.188180\n",
      "train loss:   1.218952\n",
      "train loss:   1.372140\n",
      "train loss:   1.400899\n",
      "train loss:   1.385956\n",
      "train loss:   1.562826\n",
      "train loss:   1.521903\n",
      "train loss:   1.537490\n",
      "train loss:   1.357999\n",
      "train loss:   1.306357\n",
      "train loss:   1.290363\n",
      "train loss:   1.254539\n",
      "train loss:   1.679941\n",
      "train loss:   1.286797\n",
      "train loss:   1.246155\n",
      "train loss:   1.407916\n",
      "train loss:   1.178688\n",
      "train loss:   1.341133\n",
      "train loss:   1.716949\n",
      "train loss:   1.593403\n",
      "train loss:   1.715526\n",
      "train loss:   1.287253\n",
      "train loss:   1.346804\n",
      "train loss:   1.634921\n",
      "train loss:   1.371042\n",
      "train loss:   1.351240\n",
      "train loss:   1.209979\n",
      "train loss:   1.185659\n",
      "train loss:   1.167426\n",
      "train loss:   1.181317\n",
      "train loss:   1.249749\n",
      "train loss:   1.578745\n",
      "train loss:   1.390306\n",
      "train loss:   1.345205\n",
      "train loss:   1.142914\n",
      "train loss:   1.382287\n",
      "train loss:   1.273169\n",
      "train loss:   1.149000\n",
      "train loss:   1.299274\n",
      "train loss:   1.636523\n",
      "train loss:   1.197939\n",
      "train loss:   1.570700\n",
      "train loss:   1.578852\n",
      "train loss:   1.121012\n",
      "train loss:   1.360810\n",
      "train loss:   1.484308\n",
      "train loss:   1.708974\n",
      "train loss:   1.332209\n",
      "train loss:   1.283079\n",
      "########### epoch 16 ###########\n",
      "########### loop 2150 ###########\n",
      "test loss:   1.414550   test accuracy:   0.468750\n",
      "########### loop 2150 ###########\n",
      "train loss:   1.468990\n",
      "train loss:   1.248703\n",
      "train loss:   1.310487\n",
      "train loss:   1.487463\n",
      "train loss:   1.302858\n",
      "train loss:   1.407050\n",
      "train loss:   1.258034\n",
      "train loss:   1.330644\n",
      "train loss:   1.300640\n",
      "train loss:   1.293959\n",
      "train loss:   1.362613\n",
      "train loss:   1.059845\n",
      "train loss:   1.370238\n",
      "train loss:   1.578095\n",
      "train loss:   1.420745\n",
      "train loss:   1.591849\n",
      "train loss:   1.078006\n",
      "train loss:   1.162619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.033467\n",
      "train loss:   1.439887\n",
      "train loss:   1.155585\n",
      "train loss:   1.380205\n",
      "train loss:   1.391264\n",
      "train loss:   1.452377\n",
      "train loss:   1.301762\n",
      "train loss:   1.543820\n",
      "train loss:   1.391804\n",
      "train loss:   1.345830\n",
      "train loss:   1.354521\n",
      "train loss:   1.036253\n",
      "train loss:   1.541524\n",
      "train loss:   1.310318\n",
      "train loss:   1.383658\n",
      "train loss:   1.400703\n",
      "train loss:   1.398203\n",
      "train loss:   1.183374\n",
      "train loss:   1.269006\n",
      "train loss:   1.211048\n",
      "train loss:   1.415595\n",
      "train loss:   1.221478\n",
      "train loss:   1.185970\n",
      "train loss:   1.260032\n",
      "train loss:   1.225119\n",
      "train loss:   1.411842\n",
      "train loss:   1.125573\n",
      "train loss:   1.260566\n",
      "train loss:   1.362074\n",
      "train loss:   1.416600\n",
      "train loss:   1.197610\n",
      "train loss:   1.362861\n",
      "########### epoch 16 ###########\n",
      "########### loop 2200 ###########\n",
      "test loss:   1.392231   test accuracy:   0.625000\n",
      "########### loop 2200 ###########\n",
      "train loss:   1.395428\n",
      "train loss:   0.997852\n",
      "train loss:   1.415245\n",
      "train loss:   1.295943\n",
      "train loss:   1.162878\n",
      "train loss:   1.155140\n",
      "train loss:   1.449032\n",
      "train loss:   1.346858\n",
      "train loss:   1.233984\n",
      "train loss:   1.356763\n",
      "train loss:   1.313869\n",
      "train loss:   1.120398\n",
      "train loss:   1.203503\n",
      "train loss:   1.317198\n",
      "train loss:   1.261774\n",
      "train loss:   1.495257\n",
      "train loss:   0.994593\n",
      "train loss:   1.309926\n",
      "train loss:   1.140738\n",
      "train loss:   1.245312\n",
      "train loss:   1.001900\n",
      "train loss:   1.177549\n",
      "train loss:   1.359759\n",
      "train loss:   1.197498\n",
      "train loss:   1.366488\n",
      "train loss:   1.454678\n",
      "train loss:   1.096082\n",
      "train loss:   1.544544\n",
      "train loss:   1.571841\n",
      "train loss:   1.208792\n",
      "train loss:   1.514138\n",
      "train loss:   1.409593\n",
      "train loss:   1.056535\n",
      "train loss:   1.297313\n",
      "train loss:   1.235363\n",
      "train loss:   1.181603\n",
      "train loss:   1.544607\n",
      "train loss:   1.218492\n",
      "train loss:   1.240621\n",
      "train loss:   1.163734\n",
      "train loss:   1.064838\n",
      "train loss:   1.017836\n",
      "train loss:   1.213255\n",
      "train loss:   1.288047\n",
      "train loss:   1.421179\n",
      "train loss:   1.372306\n",
      "train loss:   1.343882\n",
      "train loss:   1.347923\n",
      "train loss:   1.560912\n",
      "train loss:   1.431640\n",
      "########### epoch 16 ###########\n",
      "########### loop 2250 ###########\n",
      "test loss:   1.300201   test accuracy:   0.531250\n",
      "########### loop 2250 ###########\n",
      "train loss:   1.377097\n",
      "train loss:   1.311094\n",
      "train loss:   1.192480\n",
      "train loss:   1.190494\n",
      "train loss:   1.429095\n",
      "train loss:   1.135369\n",
      "train loss:   1.163633\n",
      "train loss:   1.344400\n",
      "train loss:   1.147678\n",
      "train loss:   1.134311\n",
      "train loss:   1.311379\n",
      "train loss:   1.307244\n",
      "train loss:   1.255556\n",
      "train loss:   1.073749\n",
      "train loss:   1.250298\n",
      "train loss:   1.361989\n",
      "train loss:   1.259141\n",
      "train loss:   1.209955\n",
      "train loss:   1.209752\n",
      "train loss:   1.124606\n",
      "train loss:   1.141720\n",
      "train loss:   1.042616\n",
      "train loss:   1.108352\n",
      "train loss:   1.536969\n",
      "train loss:   1.406567\n",
      "train loss:   1.227864\n",
      "train loss:   1.082413\n",
      "train loss:   1.187402\n",
      "train loss:   1.068083\n",
      "train loss:   1.038738\n",
      "train loss:   1.180737\n",
      "train loss:   1.353587\n",
      "train loss:   1.218728\n",
      "train loss:   1.386958\n",
      "train loss:   1.433768\n",
      "train loss:   1.069519\n",
      "train loss:   1.290179\n",
      "train loss:   1.524836\n",
      "train loss:   1.402315\n",
      "train loss:   1.308405\n",
      "train loss:   1.250116\n",
      "train loss:   1.335532\n",
      "train loss:   1.414962\n",
      "train loss:   1.415812\n",
      "train loss:   1.588638\n",
      "train loss:   1.309167\n",
      "train loss:   1.595028\n",
      "train loss:   1.418189\n",
      "train loss:   1.320426\n",
      "train loss:   1.266818\n",
      "########### epoch 17 ###########\n",
      "########### loop 2300 ###########\n",
      "test loss:   1.265804   test accuracy:   0.562500\n",
      "########### loop 2300 ###########\n",
      "train loss:   1.363666\n",
      "train loss:   1.385578\n",
      "train loss:   1.208259\n",
      "train loss:   1.477596\n",
      "train loss:   1.631112\n",
      "train loss:   1.216623\n",
      "train loss:   1.271145\n",
      "train loss:   1.027625\n",
      "train loss:   1.192379\n",
      "train loss:   1.103116\n",
      "train loss:   1.409576\n",
      "train loss:   1.107541\n",
      "train loss:   1.402552\n",
      "train loss:   1.435318\n",
      "train loss:   1.346348\n",
      "train loss:   1.325761\n",
      "train loss:   1.481884\n",
      "train loss:   1.390811\n",
      "train loss:   1.384786\n",
      "train loss:   1.342247\n",
      "train loss:   1.161474\n",
      "train loss:   1.476572\n",
      "train loss:   1.236834\n",
      "train loss:   1.399547\n",
      "train loss:   1.326523\n",
      "train loss:   1.314839\n",
      "train loss:   1.076577\n",
      "train loss:   1.285132\n",
      "train loss:   1.245050\n",
      "train loss:   1.417478\n",
      "train loss:   1.224237\n",
      "train loss:   1.151152\n",
      "train loss:   1.308219\n",
      "train loss:   1.048566\n",
      "train loss:   1.377946\n",
      "train loss:   1.086450\n",
      "train loss:   1.308160\n",
      "train loss:   1.235632\n",
      "train loss:   1.278434\n",
      "train loss:   1.258861\n",
      "train loss:   1.567426\n",
      "train loss:   1.460585\n",
      "train loss:   1.078124\n",
      "train loss:   1.675990\n",
      "train loss:   1.353962\n",
      "train loss:   1.188291\n",
      "train loss:   1.253987\n",
      "train loss:   1.476516\n",
      "train loss:   1.333084\n",
      "train loss:   1.327169\n",
      "########### epoch 17 ###########\n",
      "########### loop 2350 ###########\n",
      "test loss:   1.287551   test accuracy:   0.718750\n",
      "########### loop 2350 ###########\n",
      "train loss:   1.250808\n",
      "train loss:   1.419686\n",
      "train loss:   1.125826\n",
      "train loss:   1.281741\n",
      "train loss:   1.223377\n",
      "train loss:   1.243872\n",
      "train loss:   1.581373\n",
      "train loss:   1.007364\n",
      "train loss:   1.282569\n",
      "train loss:   1.137226\n",
      "train loss:   1.498810\n",
      "train loss:   1.150908\n",
      "train loss:   1.124184\n",
      "train loss:   1.227667\n",
      "train loss:   1.240965\n",
      "train loss:   1.374538\n",
      "train loss:   1.384549\n",
      "train loss:   1.017723\n",
      "train loss:   1.352218\n",
      "train loss:   1.508463\n",
      "train loss:   1.237916\n",
      "train loss:   1.528599\n",
      "train loss:   1.432967\n",
      "train loss:   1.266407\n",
      "train loss:   1.389868\n",
      "train loss:   1.247097\n",
      "train loss:   1.322225\n",
      "train loss:   1.573362\n",
      "train loss:   1.236869\n",
      "train loss:   1.260231\n",
      "train loss:   1.123790\n",
      "train loss:   1.044651\n",
      "train loss:   1.040438\n",
      "train loss:   1.172018\n",
      "train loss:   1.312897\n",
      "train loss:   1.316699\n",
      "train loss:   1.305754\n",
      "train loss:   1.397802\n",
      "train loss:   1.378472\n",
      "train loss:   1.634804\n",
      "train loss:   1.638068\n",
      "train loss:   1.558036\n",
      "train loss:   1.396248\n",
      "train loss:   1.145044\n",
      "train loss:   1.105936\n",
      "train loss:   1.420296\n",
      "train loss:   1.086116\n",
      "train loss:   1.222602\n",
      "train loss:   1.331662\n",
      "train loss:   1.157009\n",
      "########### epoch 18 ###########\n",
      "########### loop 2400 ###########\n",
      "test loss:   1.289672   test accuracy:   0.593750\n",
      "########### loop 2400 ###########\n",
      "train loss:   1.195756\n",
      "train loss:   1.363972\n",
      "train loss:   1.318364\n",
      "train loss:   1.300499\n",
      "train loss:   1.010036\n",
      "train loss:   1.212981\n",
      "train loss:   1.369338\n",
      "train loss:   1.298357\n",
      "train loss:   1.245679\n",
      "train loss:   1.247784\n",
      "train loss:   1.280808\n",
      "train loss:   1.126580\n",
      "train loss:   1.061867\n",
      "train loss:   1.061107\n",
      "train loss:   1.397479\n",
      "train loss:   1.463388\n",
      "train loss:   1.297537\n",
      "train loss:   1.174063\n",
      "train loss:   1.251485\n",
      "train loss:   1.004097\n",
      "train loss:   1.112101\n",
      "train loss:   1.194740\n",
      "train loss:   1.371666\n",
      "train loss:   1.106744\n",
      "train loss:   1.578146\n",
      "train loss:   1.551587\n",
      "train loss:   1.059595\n",
      "train loss:   1.309141\n",
      "train loss:   1.401395\n",
      "train loss:   1.385073\n",
      "train loss:   1.277406\n",
      "train loss:   1.187190\n",
      "train loss:   1.341758\n",
      "train loss:   1.371959\n",
      "train loss:   1.301134\n",
      "train loss:   1.451003\n",
      "train loss:   1.181954\n",
      "train loss:   1.402259\n",
      "train loss:   1.160240\n",
      "train loss:   1.224165\n",
      "train loss:   1.282804\n",
      "train loss:   1.306999\n",
      "train loss:   1.254067\n",
      "train loss:   1.193574\n",
      "train loss:   1.471337\n",
      "train loss:   1.532979\n",
      "train loss:   1.176237\n",
      "train loss:   1.371236\n",
      "train loss:   1.087165\n",
      "train loss:   1.193792\n",
      "########### epoch 18 ###########\n",
      "########### loop 2450 ###########\n",
      "test loss:   1.369086   test accuracy:   0.593750\n",
      "########### loop 2450 ###########\n",
      "train loss:   1.084736\n",
      "train loss:   1.371464\n",
      "train loss:   1.013597\n",
      "train loss:   1.277614\n",
      "train loss:   1.369982\n",
      "train loss:   1.342183\n",
      "train loss:   1.207709\n",
      "train loss:   1.441086\n",
      "train loss:   1.337763\n",
      "train loss:   1.306504\n",
      "train loss:   1.194742\n",
      "train loss:   0.999685\n",
      "train loss:   1.596584\n",
      "train loss:   1.292377\n",
      "train loss:   1.237993\n",
      "train loss:   1.278239\n",
      "train loss:   1.472108\n",
      "train loss:   1.335791\n",
      "train loss:   1.383332\n",
      "train loss:   1.217455\n",
      "train loss:   1.469842\n",
      "train loss:   1.272410\n",
      "train loss:   1.164103\n",
      "train loss:   1.190806\n",
      "train loss:   1.350620\n",
      "train loss:   1.330506\n",
      "train loss:   1.268547\n",
      "train loss:   1.507199\n",
      "train loss:   1.414181\n",
      "train loss:   1.219645\n",
      "train loss:   1.067468\n",
      "train loss:   1.470839\n",
      "train loss:   1.552368\n",
      "train loss:   1.004950\n",
      "train loss:   1.547915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.302347\n",
      "train loss:   1.239771\n",
      "train loss:   1.181703\n",
      "train loss:   1.429987\n",
      "train loss:   1.252793\n",
      "train loss:   1.279425\n",
      "train loss:   1.420074\n",
      "train loss:   1.338429\n",
      "train loss:   1.021208\n",
      "train loss:   1.315118\n",
      "train loss:   1.372064\n",
      "train loss:   1.416450\n",
      "train loss:   1.478763\n",
      "train loss:   0.984636\n",
      "train loss:   1.239838\n",
      "########### epoch 18 ###########\n",
      "########### loop 2500 ###########\n",
      "test loss:   1.373246   test accuracy:   0.687500\n",
      "########### loop 2500 ###########\n",
      "train loss:   1.145460\n",
      "train loss:   1.333025\n",
      "train loss:   1.155590\n",
      "train loss:   1.304318\n",
      "train loss:   1.446699\n",
      "train loss:   1.380800\n",
      "train loss:   1.371380\n",
      "train loss:   1.302133\n",
      "train loss:   0.956154\n",
      "train loss:   1.492764\n",
      "train loss:   1.572844\n",
      "train loss:   1.220007\n",
      "train loss:   1.455181\n",
      "train loss:   1.375679\n",
      "train loss:   1.133753\n",
      "train loss:   1.216274\n",
      "train loss:   1.166762\n",
      "train loss:   1.047212\n",
      "train loss:   1.371225\n",
      "train loss:   1.202074\n",
      "train loss:   1.172437\n",
      "train loss:   1.168803\n",
      "train loss:   1.055898\n",
      "train loss:   1.141922\n",
      "train loss:   1.126672\n",
      "train loss:   1.375710\n",
      "train loss:   1.298258\n",
      "train loss:   1.365293\n",
      "train loss:   1.248182\n",
      "train loss:   1.282912\n",
      "train loss:   1.548667\n",
      "train loss:   1.469649\n",
      "train loss:   1.336750\n",
      "train loss:   1.301276\n",
      "train loss:   1.291304\n",
      "train loss:   1.175491\n",
      "train loss:   1.434427\n",
      "train loss:   1.083124\n",
      "train loss:   1.170042\n",
      "train loss:   1.340786\n",
      "train loss:   1.241299\n",
      "train loss:   1.298945\n",
      "train loss:   1.443324\n",
      "train loss:   1.329387\n",
      "train loss:   1.485994\n",
      "train loss:   1.006328\n",
      "train loss:   1.196026\n",
      "train loss:   1.438272\n",
      "train loss:   1.252036\n",
      "train loss:   1.255523\n",
      "########### epoch 19 ###########\n",
      "########### loop 2550 ###########\n",
      "test loss:   1.476438   test accuracy:   0.531250\n",
      "########### loop 2550 ###########\n",
      "train loss:   1.140265\n",
      "train loss:   1.154236\n",
      "train loss:   1.077034\n",
      "train loss:   1.049165\n",
      "train loss:   1.105677\n",
      "train loss:   1.431534\n",
      "train loss:   1.279464\n",
      "train loss:   1.307792\n",
      "train loss:   1.191073\n",
      "train loss:   1.292915\n",
      "train loss:   1.056707\n",
      "train loss:   1.155173\n",
      "train loss:   1.215042\n",
      "train loss:   1.121222\n",
      "train loss:   1.068002\n",
      "train loss:   1.518405\n",
      "train loss:   1.419816\n",
      "train loss:   1.031956\n",
      "train loss:   1.316732\n",
      "train loss:   1.403742\n",
      "train loss:   1.450777\n",
      "train loss:   1.236882\n",
      "train loss:   1.104290\n",
      "train loss:   1.367838\n",
      "train loss:   1.201392\n",
      "train loss:   1.260859\n",
      "train loss:   1.532862\n",
      "train loss:   1.143836\n",
      "train loss:   1.359282\n",
      "train loss:   1.213036\n",
      "train loss:   1.279431\n",
      "train loss:   1.189147\n",
      "train loss:   1.258368\n",
      "train loss:   1.250303\n",
      "train loss:   1.119991\n",
      "train loss:   1.424452\n",
      "train loss:   1.577184\n",
      "train loss:   1.143698\n",
      "train loss:   1.372129\n",
      "train loss:   1.069954\n",
      "train loss:   1.080162\n",
      "train loss:   1.012130\n",
      "train loss:   1.421715\n",
      "train loss:   1.152049\n",
      "train loss:   1.444800\n",
      "train loss:   1.432224\n",
      "train loss:   1.446397\n",
      "train loss:   1.215052\n",
      "train loss:   1.427102\n",
      "train loss:   1.302616\n",
      "########### epoch 19 ###########\n",
      "########### loop 2600 ###########\n",
      "test loss:   1.258987   test accuracy:   0.625000\n",
      "########### loop 2600 ###########\n",
      "train loss:   1.405902\n",
      "train loss:   1.377694\n",
      "train loss:   1.150378\n",
      "train loss:   1.528568\n",
      "train loss:   1.166859\n",
      "train loss:   1.195510\n",
      "train loss:   1.188508\n",
      "train loss:   1.233660\n",
      "train loss:   1.064387\n",
      "train loss:   1.134486\n",
      "train loss:   1.161578\n",
      "train loss:   1.310802\n",
      "train loss:   1.136544\n",
      "train loss:   1.101007\n",
      "train loss:   1.196481\n",
      "train loss:   1.098711\n",
      "train loss:   1.271415\n",
      "train loss:   1.058443\n",
      "train loss:   1.112679\n",
      "train loss:   1.187372\n",
      "train loss:   1.282193\n",
      "train loss:   1.129835\n",
      "train loss:   1.346436\n",
      "train loss:   1.426595\n",
      "train loss:   0.937594\n",
      "train loss:   1.485965\n",
      "train loss:   1.280910\n",
      "train loss:   1.194769\n",
      "train loss:   1.177096\n",
      "train loss:   1.554386\n",
      "train loss:   1.427715\n",
      "train loss:   1.243171\n",
      "train loss:   1.326452\n",
      "train loss:   1.476448\n",
      "train loss:   1.388654\n",
      "train loss:   1.167497\n",
      "train loss:   1.166830\n",
      "train loss:   1.143324\n",
      "train loss:   1.493384\n",
      "train loss:   1.008366\n",
      "train loss:   1.349405\n",
      "train loss:   1.096358\n",
      "train loss:   1.449775\n",
      "train loss:   1.097408\n",
      "train loss:   1.208016\n",
      "train loss:   1.546540\n",
      "train loss:   1.552792\n",
      "train loss:   1.410598\n",
      "train loss:   1.364704\n",
      "train loss:   1.093369\n",
      "########### epoch 19 ###########\n",
      "########### loop 2650 ###########\n",
      "test loss:   1.598783   test accuracy:   0.500000\n",
      "########### loop 2650 ###########\n",
      "train loss:   1.403105\n",
      "train loss:   1.381914\n",
      "train loss:   1.179715\n",
      "train loss:   1.553215\n",
      "train loss:   1.365699\n",
      "train loss:   1.078327\n",
      "train loss:   1.218676\n",
      "train loss:   1.189272\n",
      "train loss:   1.230160\n",
      "train loss:   1.483274\n",
      "train loss:   1.382627\n",
      "train loss:   1.264016\n",
      "train loss:   1.126465\n",
      "train loss:   1.076365\n",
      "train loss:   1.065011\n",
      "train loss:   1.180103\n",
      "train loss:   1.319039\n",
      "train loss:   1.503413\n",
      "train loss:   1.398316\n",
      "train loss:   1.220239\n",
      "train loss:   1.281609\n",
      "train loss:   1.516522\n",
      "train loss:   1.465140\n",
      "train loss:   1.363344\n",
      "train loss:   1.325676\n",
      "train loss:   1.241083\n",
      "train loss:   1.112293\n",
      "train loss:   1.373020\n",
      "train loss:   1.155095\n",
      "train loss:   1.216594\n",
      "train loss:   1.301271\n",
      "train loss:   1.136361\n",
      "train loss:   1.146526\n",
      "train loss:   1.465475\n",
      "train loss:   1.307658\n",
      "train loss:   1.467476\n",
      "train loss:   0.979896\n",
      "train loss:   1.188519\n",
      "train loss:   1.390835\n",
      "train loss:   1.269921\n",
      "train loss:   1.227560\n",
      "train loss:   1.174801\n",
      "train loss:   1.211136\n",
      "train loss:   1.212087\n",
      "train loss:   1.059561\n",
      "train loss:   1.028457\n",
      "train loss:   1.387714\n",
      "train loss:   1.375427\n",
      "train loss:   1.290166\n",
      "train loss:   1.320686\n",
      "########### epoch 20 ###########\n",
      "########### loop 2700 ###########\n",
      "test loss:   1.460980   test accuracy:   0.437500\n",
      "########### loop 2700 ###########\n",
      "train loss:   1.299954\n",
      "train loss:   1.150498\n",
      "train loss:   1.084336\n",
      "train loss:   1.161182\n",
      "train loss:   1.263558\n",
      "train loss:   1.031632\n",
      "train loss:   1.430246\n",
      "train loss:   1.454486\n",
      "train loss:   1.138474\n",
      "train loss:   1.265291\n",
      "train loss:   1.432181\n",
      "train loss:   1.327392\n",
      "train loss:   1.206202\n",
      "train loss:   1.059034\n",
      "train loss:   1.279084\n",
      "train loss:   1.260303\n",
      "train loss:   1.327691\n",
      "train loss:   1.576888\n",
      "train loss:   1.221637\n",
      "train loss:   1.294396\n",
      "train loss:   1.182472\n",
      "train loss:   1.265250\n",
      "train loss:   1.324487\n",
      "train loss:   1.298882\n",
      "train loss:   1.247004\n",
      "train loss:   1.045584\n",
      "train loss:   1.310306\n",
      "train loss:   1.526760\n",
      "train loss:   1.150891\n",
      "train loss:   1.300409\n",
      "train loss:   1.122949\n",
      "train loss:   1.106097\n",
      "train loss:   0.999932\n",
      "train loss:   1.368160\n",
      "train loss:   1.139730\n",
      "train loss:   1.212778\n",
      "train loss:   1.679718\n",
      "train loss:   1.286140\n",
      "train loss:   1.237176\n",
      "train loss:   1.352666\n",
      "train loss:   1.232971\n",
      "train loss:   1.216345\n",
      "train loss:   1.170465\n",
      "train loss:   1.001678\n",
      "train loss:   1.586361\n",
      "train loss:   1.254279\n",
      "train loss:   1.267114\n",
      "train loss:   1.261739\n",
      "train loss:   1.350520\n",
      "train loss:   1.051051\n",
      "########### epoch 20 ###########\n",
      "########### loop 2750 ###########\n",
      "test loss:   1.196651   test accuracy:   0.656250\n",
      "########### loop 2750 ###########\n",
      "train loss:   1.205576\n",
      "train loss:   1.083238\n",
      "train loss:   1.303883\n",
      "train loss:   1.205221\n",
      "train loss:   1.101345\n",
      "train loss:   1.130659\n",
      "train loss:   1.201847\n",
      "train loss:   1.252473\n",
      "train loss:   1.119038\n",
      "train loss:   1.371129\n",
      "train loss:   1.338207\n",
      "train loss:   1.324690\n",
      "train loss:   1.196087\n",
      "train loss:   1.373279\n",
      "train loss:   1.479662\n",
      "train loss:   0.967180\n",
      "train loss:   1.390918\n",
      "train loss:   1.166945\n",
      "train loss:   1.082879\n",
      "train loss:   1.118464\n",
      "train loss:   1.451372\n",
      "train loss:   1.303384\n",
      "train loss:   1.195727\n",
      "train loss:   1.367219\n",
      "train loss:   1.371187\n",
      "train loss:   1.101918\n",
      "train loss:   1.116174\n",
      "train loss:   1.200685\n",
      "train loss:   1.234279\n",
      "train loss:   1.285705\n",
      "train loss:   0.903586\n",
      "train loss:   1.222686\n",
      "train loss:   1.092948\n",
      "train loss:   1.318353\n",
      "train loss:   0.967868\n",
      "train loss:   1.080861\n",
      "train loss:   1.259370\n",
      "train loss:   1.197049\n",
      "train loss:   1.374596\n",
      "train loss:   1.275103\n",
      "train loss:   0.909442\n",
      "train loss:   1.294561\n",
      "train loss:   1.450925\n",
      "train loss:   1.139142\n",
      "train loss:   1.426802\n",
      "train loss:   1.202053\n",
      "train loss:   1.080438\n",
      "train loss:   1.236664\n",
      "train loss:   1.119014\n",
      "train loss:   1.040598\n",
      "########### epoch 20 ###########\n",
      "########### loop 2800 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   1.268121   test accuracy:   0.531250\n",
      "########### loop 2800 ###########\n",
      "train loss:   1.432952\n",
      "train loss:   1.149513\n",
      "train loss:   1.128096\n",
      "train loss:   1.135125\n",
      "train loss:   0.986151\n",
      "train loss:   1.067528\n",
      "train loss:   1.170100\n",
      "train loss:   1.231850\n",
      "train loss:   1.258266\n",
      "train loss:   1.350268\n",
      "train loss:   1.221158\n",
      "train loss:   1.243180\n",
      "train loss:   1.535233\n",
      "train loss:   1.406669\n",
      "train loss:   1.332188\n",
      "train loss:   1.175696\n",
      "train loss:   1.063294\n",
      "train loss:   1.072610\n",
      "train loss:   1.465669\n",
      "train loss:   1.048229\n",
      "train loss:   1.061362\n",
      "train loss:   1.288879\n",
      "train loss:   1.149524\n",
      "train loss:   1.187004\n",
      "train loss:   1.438661\n",
      "train loss:   1.237012\n",
      "train loss:   1.337256\n",
      "train loss:   0.958470\n",
      "train loss:   1.088114\n",
      "train loss:   1.274908\n",
      "train loss:   1.188983\n",
      "train loss:   1.187556\n",
      "train loss:   1.176559\n",
      "train loss:   1.135407\n",
      "train loss:   1.049352\n",
      "train loss:   0.985467\n",
      "train loss:   0.983029\n",
      "train loss:   1.414597\n",
      "train loss:   1.265131\n",
      "train loss:   1.216835\n",
      "train loss:   1.157133\n",
      "train loss:   1.261355\n",
      "train loss:   0.973946\n",
      "train loss:   1.013337\n",
      "train loss:   1.123663\n",
      "train loss:   1.230758\n",
      "train loss:   1.013157\n",
      "train loss:   1.517889\n",
      "train loss:   1.482294\n",
      "train loss:   0.916045\n",
      "########### epoch 21 ###########\n",
      "########### loop 2850 ###########\n",
      "test loss:   1.092423   test accuracy:   0.656250\n",
      "########### loop 2850 ###########\n",
      "train loss:   1.256823\n",
      "train loss:   1.376104\n",
      "train loss:   1.383506\n",
      "train loss:   1.318767\n",
      "train loss:   1.151183\n",
      "train loss:   1.332975\n",
      "train loss:   1.187297\n",
      "train loss:   1.234282\n",
      "train loss:   1.423660\n",
      "train loss:   1.153294\n",
      "train loss:   1.295310\n",
      "train loss:   1.225819\n",
      "train loss:   1.283481\n",
      "train loss:   1.296861\n",
      "train loss:   1.325389\n",
      "train loss:   1.214471\n",
      "train loss:   1.001740\n",
      "train loss:   1.392267\n",
      "train loss:   1.605930\n",
      "train loss:   1.213551\n",
      "train loss:   1.319447\n",
      "train loss:   0.978339\n",
      "train loss:   1.007743\n",
      "train loss:   0.997207\n",
      "train loss:   1.318701\n",
      "train loss:   1.058248\n",
      "train loss:   1.402573\n",
      "train loss:   1.357264\n",
      "train loss:   1.412859\n",
      "train loss:   1.250053\n",
      "train loss:   1.396398\n",
      "train loss:   1.355324\n",
      "train loss:   1.472936\n",
      "train loss:   1.405237\n",
      "train loss:   1.122381\n",
      "train loss:   1.413583\n",
      "train loss:   1.058456\n",
      "train loss:   1.246328\n",
      "train loss:   1.224647\n",
      "train loss:   1.263812\n",
      "train loss:   1.002980\n",
      "train loss:   1.174119\n",
      "train loss:   1.090889\n",
      "train loss:   1.308481\n",
      "train loss:   1.045526\n",
      "train loss:   1.121930\n",
      "train loss:   1.219985\n",
      "train loss:   1.087798\n",
      "train loss:   1.251534\n",
      "train loss:   1.007409\n",
      "########### epoch 21 ###########\n",
      "########### loop 2900 ###########\n",
      "test loss:   1.085329   test accuracy:   0.718750\n",
      "########### loop 2900 ###########\n",
      "train loss:   1.090342\n",
      "train loss:   1.239808\n",
      "train loss:   1.200138\n",
      "train loss:   1.087164\n",
      "train loss:   1.367283\n",
      "train loss:   1.435245\n",
      "train loss:   1.050192\n",
      "train loss:   1.362231\n",
      "train loss:   1.161686\n",
      "train loss:   1.079754\n",
      "train loss:   1.099279\n",
      "train loss:   1.382830\n",
      "train loss:   1.325871\n",
      "train loss:   1.249051\n",
      "train loss:   1.283313\n",
      "train loss:   1.339417\n",
      "train loss:   0.925947\n",
      "train loss:   1.060926\n",
      "train loss:   1.204926\n",
      "train loss:   1.191536\n",
      "train loss:   1.353678\n",
      "train loss:   0.866137\n",
      "train loss:   1.218965\n",
      "train loss:   1.087745\n",
      "train loss:   1.115817\n",
      "train loss:   0.955281\n",
      "train loss:   1.070759\n",
      "train loss:   1.337981\n",
      "train loss:   1.266269\n",
      "train loss:   1.436267\n",
      "train loss:   1.296545\n",
      "train loss:   1.059182\n",
      "train loss:   1.336270\n",
      "train loss:   1.437824\n",
      "train loss:   1.141234\n",
      "train loss:   1.453638\n",
      "train loss:   1.347826\n",
      "train loss:   1.028595\n",
      "train loss:   1.155908\n",
      "train loss:   1.121762\n",
      "train loss:   1.100986\n",
      "train loss:   1.398487\n",
      "train loss:   1.149562\n",
      "train loss:   1.237314\n",
      "train loss:   1.049415\n",
      "train loss:   0.918572\n",
      "train loss:   1.002061\n",
      "train loss:   1.157008\n",
      "train loss:   1.145454\n",
      "train loss:   1.294534\n",
      "########### epoch 21 ###########\n",
      "########### loop 2950 ###########\n",
      "test loss:   1.459962   test accuracy:   0.593750\n",
      "########### loop 2950 ###########\n",
      "train loss:   1.377356\n",
      "train loss:   1.228921\n",
      "train loss:   1.270372\n",
      "train loss:   1.339554\n",
      "train loss:   1.442672\n",
      "train loss:   1.352107\n",
      "train loss:   1.210836\n",
      "train loss:   1.102046\n",
      "train loss:   1.103126\n",
      "train loss:   1.285101\n",
      "train loss:   1.069822\n",
      "train loss:   1.120611\n",
      "train loss:   1.305146\n",
      "train loss:   1.134560\n",
      "train loss:   1.038653\n",
      "train loss:   1.294654\n",
      "train loss:   1.193619\n",
      "train loss:   1.331931\n",
      "train loss:   0.992390\n",
      "train loss:   1.159504\n",
      "train loss:   1.425226\n",
      "train loss:   1.185195\n",
      "train loss:   1.255756\n",
      "train loss:   1.207976\n",
      "train loss:   1.144794\n",
      "train loss:   1.105115\n",
      "train loss:   0.950612\n",
      "train loss:   1.018517\n",
      "train loss:   1.490559\n",
      "train loss:   1.317309\n",
      "train loss:   1.217432\n",
      "train loss:   1.212623\n",
      "train loss:   1.157172\n",
      "train loss:   0.964257\n",
      "train loss:   1.074179\n",
      "train loss:   1.142198\n",
      "train loss:   1.511357\n",
      "train loss:   1.122389\n",
      "train loss:   1.518849\n",
      "train loss:   1.531529\n",
      "train loss:   1.338930\n",
      "train loss:   1.305612\n",
      "train loss:   1.429843\n",
      "train loss:   1.227280\n",
      "train loss:   1.406391\n",
      "train loss:   1.181445\n",
      "train loss:   1.234643\n",
      "train loss:   1.276008\n",
      "train loss:   1.431770\n",
      "train loss:   1.438243\n",
      "########### epoch 22 ###########\n",
      "########### loop 3000 ###########\n",
      "test loss:   1.212197   test accuracy:   0.625000\n",
      "########### loop 3000 ###########\n",
      "train loss:   1.187441\n",
      "train loss:   1.372953\n",
      "train loss:   1.345156\n",
      "train loss:   1.477977\n",
      "train loss:   1.472980\n",
      "train loss:   1.447801\n",
      "train loss:   1.225187\n",
      "train loss:   1.036684\n",
      "train loss:   1.280932\n",
      "train loss:   1.659954\n",
      "train loss:   1.214016\n",
      "train loss:   1.292532\n",
      "train loss:   1.135788\n",
      "train loss:   1.180706\n",
      "train loss:   1.096056\n",
      "train loss:   1.317673\n",
      "train loss:   0.954774\n",
      "train loss:   1.413033\n",
      "train loss:   1.306445\n",
      "train loss:   1.415977\n",
      "train loss:   1.203052\n",
      "train loss:   1.414179\n",
      "train loss:   1.290903\n",
      "train loss:   1.394913\n",
      "train loss:   1.169812\n",
      "train loss:   1.094126\n",
      "train loss:   1.555397\n",
      "train loss:   1.067964\n",
      "train loss:   1.274694\n",
      "train loss:   1.163163\n",
      "train loss:   1.259183\n",
      "train loss:   1.051899\n",
      "train loss:   1.195988\n",
      "train loss:   1.081227\n",
      "train loss:   1.347814\n",
      "train loss:   1.131478\n",
      "train loss:   1.095176\n",
      "train loss:   1.160899\n",
      "train loss:   1.071010\n",
      "train loss:   1.302662\n",
      "train loss:   1.069617\n",
      "train loss:   1.153626\n",
      "train loss:   1.187924\n",
      "train loss:   1.199798\n",
      "train loss:   1.079507\n",
      "train loss:   1.401658\n",
      "train loss:   1.380572\n",
      "train loss:   0.973651\n",
      "train loss:   1.626533\n",
      "train loss:   1.183838\n",
      "########### epoch 22 ###########\n",
      "########### loop 3050 ###########\n",
      "test loss:   1.379234   test accuracy:   0.562500\n",
      "########### loop 3050 ###########\n",
      "train loss:   1.071664\n",
      "train loss:   1.256582\n",
      "train loss:   1.450655\n",
      "train loss:   1.459645\n",
      "train loss:   1.487650\n",
      "train loss:   1.407028\n",
      "train loss:   1.546502\n",
      "train loss:   1.251683\n",
      "train loss:   1.188607\n",
      "train loss:   1.138951\n",
      "train loss:   1.248176\n",
      "train loss:   1.462344\n",
      "train loss:   1.163214\n",
      "train loss:   1.323669\n",
      "train loss:   1.168473\n",
      "train loss:   1.272453\n",
      "train loss:   1.080321\n",
      "train loss:   1.280002\n",
      "train loss:   1.475267\n",
      "train loss:   1.329957\n",
      "train loss:   1.627145\n",
      "train loss:   1.336083\n",
      "train loss:   0.894515\n",
      "train loss:   1.371984\n",
      "train loss:   1.352117\n",
      "train loss:   1.229192\n",
      "train loss:   1.513022\n",
      "train loss:   1.502641\n",
      "train loss:   1.250885\n",
      "train loss:   1.333802\n",
      "train loss:   1.276207\n",
      "train loss:   1.087511\n",
      "train loss:   1.363527\n",
      "train loss:   1.162237\n",
      "train loss:   1.185614\n",
      "train loss:   1.039690\n",
      "train loss:   0.959485\n",
      "train loss:   1.011201\n",
      "train loss:   1.077591\n",
      "train loss:   1.136255\n",
      "train loss:   1.183838\n",
      "train loss:   1.262506\n",
      "train loss:   1.209890\n",
      "train loss:   1.280662\n",
      "train loss:   1.386274\n",
      "train loss:   1.427851\n",
      "train loss:   1.319036\n",
      "train loss:   1.167968\n",
      "train loss:   1.089184\n",
      "train loss:   1.091266\n",
      "########### epoch 22 ###########\n",
      "########### loop 3100 ###########\n",
      "test loss:   1.515801   test accuracy:   0.531250\n",
      "########### loop 3100 ###########\n",
      "train loss:   1.384354\n",
      "train loss:   1.058359\n",
      "train loss:   1.066585\n",
      "train loss:   1.172866\n",
      "train loss:   1.090827\n",
      "train loss:   1.200713\n",
      "train loss:   1.439472\n",
      "train loss:   1.271888\n",
      "train loss:   1.256559\n",
      "train loss:   1.102890\n",
      "train loss:   1.183889\n",
      "train loss:   1.398145\n",
      "train loss:   1.252293\n",
      "train loss:   1.158030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.222240\n",
      "train loss:   1.226290\n",
      "train loss:   1.072089\n",
      "train loss:   1.009934\n",
      "train loss:   0.990022\n",
      "train loss:   1.425849\n",
      "train loss:   1.306491\n",
      "train loss:   1.404922\n",
      "train loss:   1.243349\n",
      "train loss:   1.288505\n",
      "train loss:   1.116086\n",
      "train loss:   1.103874\n",
      "train loss:   1.242937\n",
      "train loss:   1.224470\n",
      "train loss:   1.049535\n",
      "train loss:   1.540985\n",
      "train loss:   1.429653\n",
      "train loss:   1.078570\n",
      "train loss:   1.284005\n",
      "train loss:   1.438416\n",
      "train loss:   1.502573\n",
      "train loss:   1.365571\n",
      "train loss:   1.076392\n",
      "train loss:   1.180190\n",
      "train loss:   1.181053\n",
      "train loss:   1.264550\n",
      "train loss:   1.528434\n",
      "train loss:   1.180046\n",
      "train loss:   1.450378\n",
      "train loss:   1.365215\n",
      "train loss:   1.334028\n",
      "train loss:   1.218128\n",
      "train loss:   1.321615\n",
      "train loss:   1.388995\n",
      "train loss:   1.231377\n",
      "train loss:   1.348486\n",
      "########### epoch 23 ###########\n",
      "########### loop 3150 ###########\n",
      "test loss:   1.107866   test accuracy:   0.656250\n",
      "########### loop 3150 ###########\n",
      "train loss:   1.577258\n",
      "train loss:   1.122730\n",
      "train loss:   1.301033\n",
      "train loss:   0.976391\n",
      "train loss:   1.140596\n",
      "train loss:   1.044806\n",
      "train loss:   1.497624\n",
      "train loss:   1.142481\n",
      "train loss:   1.176705\n",
      "train loss:   1.569445\n",
      "train loss:   1.242530\n",
      "train loss:   1.231064\n",
      "train loss:   1.491328\n",
      "train loss:   1.334360\n",
      "train loss:   1.178394\n",
      "train loss:   1.121017\n",
      "train loss:   1.109477\n",
      "train loss:   1.609071\n",
      "train loss:   1.138708\n",
      "train loss:   1.149351\n",
      "train loss:   1.258844\n",
      "train loss:   1.343941\n",
      "train loss:   1.202631\n",
      "train loss:   1.253986\n",
      "train loss:   1.064744\n",
      "train loss:   1.308866\n",
      "train loss:   0.999754\n",
      "train loss:   1.131755\n",
      "train loss:   1.121984\n",
      "train loss:   1.213566\n",
      "train loss:   1.309582\n",
      "train loss:   1.094876\n",
      "train loss:   1.136919\n",
      "train loss:   1.127034\n",
      "train loss:   1.237723\n",
      "train loss:   1.123851\n",
      "train loss:   1.423046\n",
      "train loss:   1.397718\n",
      "train loss:   1.024943\n",
      "train loss:   1.594327\n",
      "train loss:   1.226370\n",
      "train loss:   1.128211\n",
      "train loss:   0.987123\n",
      "train loss:   1.354538\n",
      "train loss:   1.283374\n",
      "train loss:   1.205114\n",
      "train loss:   1.207735\n",
      "train loss:   1.290885\n",
      "train loss:   1.110056\n",
      "train loss:   1.053759\n",
      "########### epoch 23 ###########\n",
      "########### loop 3200 ###########\n",
      "test loss:   1.382212   test accuracy:   0.562500\n",
      "########### loop 3200 ###########\n",
      "train loss:   1.180468\n",
      "train loss:   1.194715\n",
      "train loss:   1.467606\n",
      "train loss:   0.912242\n",
      "train loss:   1.298623\n",
      "train loss:   1.051091\n",
      "train loss:   1.170897\n",
      "train loss:   0.983662\n",
      "train loss:   1.200826\n",
      "train loss:   1.315334\n",
      "train loss:   1.214565\n",
      "train loss:   1.268980\n",
      "train loss:   1.205878\n",
      "train loss:   0.936419\n",
      "train loss:   1.398006\n",
      "train loss:   1.505406\n",
      "train loss:   1.107888\n",
      "train loss:   1.424707\n",
      "train loss:   1.369819\n",
      "train loss:   1.075945\n",
      "train loss:   1.248719\n",
      "train loss:   1.118607\n",
      "train loss:   1.051148\n",
      "train loss:   1.377838\n",
      "train loss:   1.159049\n",
      "train loss:   1.190219\n",
      "train loss:   1.040978\n",
      "train loss:   0.919822\n",
      "train loss:   0.955120\n",
      "train loss:   1.163715\n",
      "train loss:   1.192734\n",
      "train loss:   1.255040\n",
      "train loss:   1.285168\n",
      "train loss:   1.219544\n",
      "train loss:   1.129864\n",
      "train loss:   1.304602\n",
      "train loss:   1.385088\n",
      "train loss:   1.264751\n",
      "train loss:   1.209013\n",
      "train loss:   1.049606\n",
      "train loss:   1.131420\n",
      "train loss:   1.251661\n",
      "train loss:   0.986085\n",
      "train loss:   1.022127\n",
      "train loss:   1.183350\n",
      "train loss:   1.363510\n",
      "train loss:   1.133025\n",
      "train loss:   1.294600\n",
      "train loss:   1.074064\n",
      "train loss:   1.229196\n",
      "########### epoch 24 ###########\n",
      "########### loop 3250 ###########\n",
      "test loss:   1.231625   test accuracy:   0.625000\n",
      "########### loop 3250 ###########\n",
      "train loss:   0.882454\n",
      "train loss:   1.070875\n",
      "train loss:   1.106025\n",
      "train loss:   1.064624\n",
      "train loss:   1.176278\n",
      "train loss:   1.150375\n",
      "train loss:   1.118329\n",
      "train loss:   1.153984\n",
      "train loss:   0.946422\n",
      "train loss:   0.934830\n",
      "train loss:   1.432943\n",
      "train loss:   1.266250\n",
      "train loss:   1.165997\n",
      "train loss:   1.129323\n",
      "train loss:   1.172105\n",
      "train loss:   0.955160\n",
      "train loss:   1.021152\n",
      "train loss:   1.167936\n",
      "train loss:   1.178470\n",
      "train loss:   0.948712\n",
      "train loss:   1.320997\n",
      "train loss:   1.322942\n",
      "train loss:   0.987253\n",
      "train loss:   1.294107\n",
      "train loss:   1.429364\n",
      "train loss:   1.383088\n",
      "train loss:   1.330627\n",
      "train loss:   1.048891\n",
      "train loss:   1.234691\n",
      "train loss:   1.185685\n",
      "train loss:   1.250281\n",
      "train loss:   1.514693\n",
      "train loss:   1.221323\n",
      "train loss:   1.203460\n",
      "train loss:   1.103839\n",
      "train loss:   1.200098\n",
      "train loss:   1.254501\n",
      "train loss:   1.265849\n",
      "train loss:   1.210158\n",
      "train loss:   0.997921\n",
      "train loss:   1.258048\n",
      "train loss:   1.425239\n",
      "train loss:   1.090503\n",
      "train loss:   1.472569\n",
      "train loss:   1.068956\n",
      "train loss:   1.031916\n",
      "train loss:   1.016745\n",
      "train loss:   1.281545\n",
      "train loss:   1.046276\n",
      "train loss:   1.209474\n",
      "########### epoch 24 ###########\n",
      "########### loop 3300 ###########\n",
      "test loss:   1.397912   test accuracy:   0.562500\n",
      "########### loop 3300 ###########\n",
      "train loss:   1.550368\n",
      "train loss:   1.261904\n",
      "train loss:   1.208286\n",
      "train loss:   1.465232\n",
      "train loss:   1.300020\n",
      "train loss:   1.373220\n",
      "train loss:   1.140041\n",
      "train loss:   0.936432\n",
      "train loss:   1.515299\n",
      "train loss:   1.127804\n",
      "train loss:   1.186067\n",
      "train loss:   1.151255\n",
      "train loss:   1.225637\n",
      "train loss:   1.048571\n",
      "train loss:   1.225713\n",
      "train loss:   1.072058\n",
      "train loss:   1.289962\n",
      "train loss:   1.165331\n",
      "train loss:   1.094377\n",
      "train loss:   1.136628\n",
      "train loss:   1.071130\n",
      "train loss:   1.209248\n",
      "train loss:   1.022565\n",
      "train loss:   1.172348\n",
      "train loss:   1.153108\n",
      "train loss:   1.234397\n",
      "train loss:   1.112323\n",
      "train loss:   1.269970\n",
      "train loss:   1.445632\n",
      "train loss:   1.061623\n",
      "train loss:   1.648843\n",
      "train loss:   1.172820\n",
      "train loss:   1.153125\n",
      "train loss:   1.080997\n",
      "train loss:   1.348664\n",
      "train loss:   1.249376\n",
      "train loss:   1.211446\n",
      "train loss:   1.138661\n",
      "train loss:   1.341157\n",
      "train loss:   1.055751\n",
      "train loss:   0.988610\n",
      "train loss:   1.123544\n",
      "train loss:   1.149068\n",
      "train loss:   1.420222\n",
      "train loss:   0.971868\n",
      "train loss:   1.153040\n",
      "train loss:   1.056968\n",
      "train loss:   1.284985\n",
      "train loss:   1.038503\n",
      "train loss:   1.024979\n",
      "########### epoch 24 ###########\n",
      "########### loop 3350 ###########\n",
      "test loss:   1.236462   test accuracy:   0.687500\n",
      "########### loop 3350 ###########\n",
      "train loss:   1.411899\n",
      "train loss:   1.349439\n",
      "train loss:   1.307597\n",
      "train loss:   1.237916\n",
      "train loss:   1.037284\n",
      "train loss:   1.484839\n",
      "train loss:   1.445694\n",
      "train loss:   1.083036\n",
      "train loss:   1.459237\n",
      "train loss:   1.291163\n",
      "train loss:   1.017125\n",
      "train loss:   1.181580\n",
      "train loss:   1.120372\n",
      "train loss:   0.989221\n",
      "train loss:   1.365819\n",
      "train loss:   1.245208\n",
      "train loss:   1.156751\n",
      "train loss:   1.001741\n",
      "train loss:   0.886807\n",
      "train loss:   0.918286\n",
      "train loss:   1.072115\n",
      "train loss:   1.144135\n",
      "train loss:   1.222482\n",
      "train loss:   1.284270\n",
      "train loss:   1.248534\n",
      "train loss:   1.219849\n",
      "train loss:   1.334490\n",
      "train loss:   1.376522\n",
      "train loss:   1.277327\n",
      "train loss:   1.175499\n",
      "train loss:   1.000885\n",
      "train loss:   1.069344\n",
      "train loss:   1.257782\n",
      "train loss:   1.101738\n",
      "train loss:   1.174221\n",
      "train loss:   1.219091\n",
      "train loss:   1.165021\n",
      "train loss:   1.110221\n",
      "train loss:   1.309427\n",
      "train loss:   1.176251\n",
      "train loss:   1.370653\n",
      "train loss:   0.917631\n",
      "train loss:   1.124898\n",
      "train loss:   1.124506\n",
      "train loss:   1.185638\n",
      "train loss:   1.173564\n",
      "train loss:   1.115661\n",
      "train loss:   1.066266\n",
      "train loss:   1.053085\n",
      "train loss:   0.937770\n",
      "########### epoch 25 ###########\n",
      "########### loop 3400 ###########\n",
      "test loss:   1.348061   test accuracy:   0.593750\n",
      "########### loop 3400 ###########\n",
      "train loss:   0.939485\n",
      "train loss:   1.261987\n",
      "train loss:   1.354090\n",
      "train loss:   1.213697\n",
      "train loss:   1.078292\n",
      "train loss:   1.052505\n",
      "train loss:   0.859598\n",
      "train loss:   1.049734\n",
      "train loss:   1.129809\n",
      "train loss:   1.205402\n",
      "train loss:   0.934339\n",
      "train loss:   1.393564\n",
      "train loss:   1.349793\n",
      "train loss:   1.055233\n",
      "train loss:   1.288971\n",
      "train loss:   1.361737\n",
      "train loss:   1.291688\n",
      "train loss:   1.137411\n",
      "train loss:   1.118175\n",
      "train loss:   1.242420\n",
      "train loss:   1.171708\n",
      "train loss:   1.221611\n",
      "train loss:   1.337043\n",
      "train loss:   1.034223\n",
      "train loss:   1.356654\n",
      "train loss:   1.069937\n",
      "train loss:   1.127151\n",
      "train loss:   1.258339\n",
      "train loss:   1.300477\n",
      "train loss:   1.177111\n",
      "train loss:   0.948024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.212161\n",
      "train loss:   1.510749\n",
      "train loss:   1.039828\n",
      "train loss:   1.220797\n",
      "train loss:   0.935909\n",
      "train loss:   0.998582\n",
      "train loss:   0.925293\n",
      "train loss:   1.239345\n",
      "train loss:   0.926755\n",
      "train loss:   1.194101\n",
      "train loss:   1.340277\n",
      "train loss:   1.270354\n",
      "train loss:   1.162446\n",
      "train loss:   1.326603\n",
      "train loss:   1.224338\n",
      "train loss:   1.193010\n",
      "train loss:   1.107928\n",
      "train loss:   0.910079\n",
      "train loss:   1.454876\n",
      "########### epoch 25 ###########\n",
      "########### loop 3450 ###########\n",
      "test loss:   1.156978   test accuracy:   0.656250\n",
      "########### loop 3450 ###########\n",
      "train loss:   1.120405\n",
      "train loss:   1.210480\n",
      "train loss:   1.122481\n",
      "train loss:   1.288509\n",
      "train loss:   1.003119\n",
      "train loss:   1.129556\n",
      "train loss:   1.091268\n",
      "train loss:   1.338403\n",
      "train loss:   1.121611\n",
      "train loss:   1.008024\n",
      "train loss:   1.081591\n",
      "train loss:   1.069333\n",
      "train loss:   1.223364\n",
      "train loss:   1.033084\n",
      "train loss:   1.223537\n",
      "train loss:   1.241549\n",
      "train loss:   1.103664\n",
      "train loss:   1.010163\n",
      "train loss:   1.329300\n",
      "train loss:   1.389306\n",
      "train loss:   0.961017\n",
      "train loss:   1.363394\n",
      "train loss:   1.081419\n",
      "train loss:   1.026854\n",
      "train loss:   1.098608\n",
      "train loss:   1.425395\n",
      "train loss:   1.352928\n",
      "train loss:   1.341571\n",
      "train loss:   1.279378\n",
      "train loss:   1.335571\n",
      "train loss:   0.925457\n",
      "train loss:   1.052382\n",
      "train loss:   1.086078\n",
      "train loss:   1.219131\n",
      "train loss:   1.387783\n",
      "train loss:   0.818382\n",
      "train loss:   1.287772\n",
      "train loss:   1.037969\n",
      "train loss:   1.314980\n",
      "train loss:   0.922315\n",
      "train loss:   1.047016\n",
      "train loss:   1.318789\n",
      "train loss:   1.266508\n",
      "train loss:   1.349505\n",
      "train loss:   1.267720\n",
      "train loss:   0.974785\n",
      "train loss:   1.355993\n",
      "train loss:   1.556542\n",
      "train loss:   1.236163\n",
      "train loss:   1.586790\n",
      "########### epoch 25 ###########\n",
      "########### loop 3500 ###########\n",
      "test loss:   1.524194   test accuracy:   0.437500\n",
      "########### loop 3500 ###########\n",
      "train loss:   1.385947\n",
      "train loss:   1.086268\n",
      "train loss:   1.271935\n",
      "train loss:   1.203537\n",
      "train loss:   1.015282\n",
      "train loss:   1.421140\n",
      "train loss:   1.252101\n",
      "train loss:   1.163902\n",
      "train loss:   0.937218\n",
      "train loss:   0.820635\n",
      "train loss:   0.886345\n",
      "train loss:   1.109437\n",
      "train loss:   1.195586\n",
      "train loss:   1.257817\n",
      "train loss:   1.349039\n",
      "train loss:   1.324078\n",
      "train loss:   1.205287\n",
      "train loss:   1.502757\n",
      "train loss:   1.351023\n",
      "train loss:   1.288746\n",
      "train loss:   1.246704\n",
      "train loss:   0.978259\n",
      "train loss:   1.142875\n",
      "train loss:   1.368145\n",
      "train loss:   0.948318\n",
      "train loss:   1.037729\n",
      "train loss:   1.217617\n",
      "train loss:   1.224652\n",
      "train loss:   1.051258\n",
      "train loss:   1.334761\n",
      "train loss:   1.265643\n",
      "train loss:   1.440484\n",
      "train loss:   0.861774\n",
      "train loss:   1.159423\n",
      "train loss:   1.384661\n",
      "train loss:   1.166763\n",
      "train loss:   1.243067\n",
      "train loss:   1.246275\n",
      "train loss:   1.106664\n",
      "train loss:   1.146143\n",
      "train loss:   1.044085\n",
      "train loss:   1.012393\n",
      "train loss:   1.321723\n",
      "train loss:   1.244720\n",
      "train loss:   1.129207\n",
      "train loss:   1.168355\n",
      "train loss:   1.336509\n",
      "train loss:   1.096171\n",
      "train loss:   1.208997\n",
      "train loss:   1.273045\n",
      "########### epoch 26 ###########\n",
      "########### loop 3550 ###########\n",
      "test loss:   1.322478   test accuracy:   0.656250\n",
      "########### loop 3550 ###########\n",
      "train loss:   1.328728\n",
      "train loss:   1.118185\n",
      "train loss:   1.540334\n",
      "train loss:   1.463764\n",
      "train loss:   0.954426\n",
      "train loss:   1.270578\n",
      "train loss:   1.351611\n",
      "train loss:   1.414306\n",
      "train loss:   1.375033\n",
      "train loss:   1.002183\n",
      "train loss:   1.270843\n",
      "train loss:   1.156557\n",
      "train loss:   1.126767\n",
      "train loss:   1.433779\n",
      "train loss:   1.140179\n",
      "train loss:   1.223178\n",
      "train loss:   1.100914\n",
      "train loss:   1.180597\n",
      "train loss:   1.185231\n",
      "train loss:   1.209336\n",
      "train loss:   1.207661\n",
      "train loss:   1.027665\n",
      "train loss:   1.224511\n",
      "train loss:   1.443633\n",
      "train loss:   1.060535\n",
      "train loss:   1.182708\n",
      "train loss:   0.964154\n",
      "train loss:   0.991013\n",
      "train loss:   0.967644\n",
      "train loss:   1.255297\n",
      "train loss:   0.967847\n",
      "train loss:   1.172304\n",
      "train loss:   1.309243\n",
      "train loss:   1.327011\n",
      "train loss:   1.196290\n",
      "train loss:   1.458216\n",
      "train loss:   1.249235\n",
      "train loss:   1.273473\n",
      "train loss:   1.189566\n",
      "train loss:   0.965625\n",
      "train loss:   1.363488\n",
      "train loss:   1.025397\n",
      "train loss:   1.183522\n",
      "train loss:   1.202986\n",
      "train loss:   1.251789\n",
      "train loss:   1.046048\n",
      "train loss:   1.137483\n",
      "train loss:   1.116538\n",
      "train loss:   1.246323\n",
      "train loss:   1.210847\n",
      "########### epoch 26 ###########\n",
      "########### loop 3600 ###########\n",
      "test loss:   1.291191   test accuracy:   0.625000\n",
      "########### loop 3600 ###########\n",
      "train loss:   1.023607\n",
      "train loss:   1.123573\n",
      "train loss:   1.209503\n",
      "train loss:   1.322295\n",
      "train loss:   1.024127\n",
      "train loss:   1.144438\n",
      "train loss:   1.090120\n",
      "train loss:   1.202626\n",
      "train loss:   1.200077\n",
      "train loss:   1.304073\n",
      "train loss:   1.323861\n",
      "train loss:   0.967641\n",
      "train loss:   1.491893\n",
      "train loss:   1.268279\n",
      "train loss:   1.140882\n",
      "train loss:   1.192970\n",
      "train loss:   1.452744\n",
      "train loss:   1.422631\n",
      "train loss:   1.295196\n",
      "train loss:   1.245714\n",
      "train loss:   1.327969\n",
      "train loss:   0.908107\n",
      "train loss:   1.068312\n",
      "train loss:   1.062626\n",
      "train loss:   1.172534\n",
      "train loss:   1.301758\n",
      "train loss:   0.979802\n",
      "train loss:   1.240890\n",
      "train loss:   1.024921\n",
      "train loss:   1.278007\n",
      "train loss:   1.056694\n",
      "train loss:   1.095338\n",
      "train loss:   1.262721\n",
      "train loss:   1.255409\n",
      "train loss:   1.343599\n",
      "train loss:   1.230196\n",
      "train loss:   0.876197\n",
      "train loss:   1.360699\n",
      "train loss:   1.382156\n",
      "train loss:   1.092147\n",
      "train loss:   1.395839\n",
      "train loss:   1.100522\n",
      "train loss:   0.948668\n",
      "train loss:   1.209218\n",
      "train loss:   1.134589\n",
      "train loss:   0.965810\n",
      "train loss:   1.371438\n",
      "train loss:   1.135709\n",
      "train loss:   1.142778\n",
      "train loss:   0.987969\n",
      "########### epoch 26 ###########\n",
      "########### loop 3650 ###########\n",
      "test loss:   1.183266   test accuracy:   0.593750\n",
      "########### loop 3650 ###########\n",
      "train loss:   0.948285\n",
      "train loss:   0.954829\n",
      "train loss:   1.108757\n",
      "train loss:   1.185877\n",
      "train loss:   1.270528\n",
      "train loss:   1.352292\n",
      "train loss:   1.321613\n",
      "train loss:   1.259513\n",
      "train loss:   1.344365\n",
      "train loss:   1.209754\n",
      "train loss:   1.283937\n",
      "train loss:   1.159693\n",
      "train loss:   1.017398\n",
      "train loss:   1.044166\n",
      "train loss:   1.358209\n",
      "train loss:   1.040864\n",
      "train loss:   1.005521\n",
      "train loss:   1.184051\n",
      "train loss:   1.033553\n",
      "train loss:   1.151616\n",
      "train loss:   1.318111\n",
      "train loss:   1.116424\n",
      "train loss:   1.250823\n",
      "train loss:   0.892088\n",
      "train loss:   1.080790\n",
      "train loss:   1.048318\n",
      "train loss:   1.159102\n",
      "train loss:   1.087689\n",
      "train loss:   1.090655\n",
      "train loss:   1.023943\n",
      "train loss:   0.949865\n",
      "train loss:   0.923159\n",
      "train loss:   0.885797\n",
      "train loss:   1.263794\n",
      "train loss:   1.256393\n",
      "train loss:   1.225965\n",
      "train loss:   1.025675\n",
      "train loss:   1.245852\n",
      "train loss:   1.021152\n",
      "train loss:   0.933238\n",
      "train loss:   1.004528\n",
      "train loss:   1.106143\n",
      "train loss:   0.990788\n",
      "train loss:   1.414811\n",
      "train loss:   1.387507\n",
      "train loss:   0.934859\n",
      "train loss:   1.212502\n",
      "train loss:   1.281802\n",
      "train loss:   1.272408\n",
      "train loss:   1.403948\n",
      "########### epoch 27 ###########\n",
      "########### loop 3700 ###########\n",
      "test loss:   1.205622   test accuracy:   0.656250\n",
      "########### loop 3700 ###########\n",
      "train loss:   1.012039\n",
      "train loss:   1.347514\n",
      "train loss:   1.217350\n",
      "train loss:   1.201847\n",
      "train loss:   1.326056\n",
      "train loss:   1.162692\n",
      "train loss:   1.238378\n",
      "train loss:   1.026900\n",
      "train loss:   1.108457\n",
      "train loss:   1.115785\n",
      "train loss:   1.210727\n",
      "train loss:   1.133961\n",
      "train loss:   0.893767\n",
      "train loss:   1.191488\n",
      "train loss:   1.401920\n",
      "train loss:   1.074927\n",
      "train loss:   1.195799\n",
      "train loss:   0.950117\n",
      "train loss:   1.020150\n",
      "train loss:   0.905563\n",
      "train loss:   1.153143\n",
      "train loss:   0.942626\n",
      "train loss:   1.113148\n",
      "train loss:   1.296292\n",
      "train loss:   1.260137\n",
      "train loss:   1.115614\n",
      "train loss:   1.317672\n",
      "train loss:   1.197635\n",
      "train loss:   1.299389\n",
      "train loss:   1.096619\n",
      "train loss:   0.855979\n",
      "train loss:   1.407000\n",
      "train loss:   1.150789\n",
      "train loss:   1.199252\n",
      "train loss:   1.115202\n",
      "train loss:   1.198851\n",
      "train loss:   0.984121\n",
      "train loss:   1.084319\n",
      "train loss:   1.006800\n",
      "train loss:   1.311247\n",
      "train loss:   1.080457\n",
      "train loss:   1.040211\n",
      "train loss:   1.050508\n",
      "train loss:   1.065566\n",
      "train loss:   1.224620\n",
      "train loss:   1.049016\n",
      "train loss:   1.222571\n",
      "train loss:   1.308529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.130784\n",
      "train loss:   1.006419\n",
      "########### epoch 27 ###########\n",
      "########### loop 3750 ###########\n",
      "test loss:   1.162608   test accuracy:   0.718750\n",
      "########### loop 3750 ###########\n",
      "train loss:   1.229302\n",
      "train loss:   1.275157\n",
      "train loss:   0.920800\n",
      "train loss:   1.291743\n",
      "train loss:   1.123614\n",
      "train loss:   0.999078\n",
      "train loss:   1.013734\n",
      "train loss:   1.318289\n",
      "train loss:   1.194905\n",
      "train loss:   1.176984\n",
      "train loss:   1.218710\n",
      "train loss:   1.207093\n",
      "train loss:   0.860990\n",
      "train loss:   1.071668\n",
      "train loss:   1.119290\n",
      "train loss:   1.116425\n",
      "train loss:   1.310013\n",
      "train loss:   0.804248\n",
      "train loss:   1.129622\n",
      "train loss:   1.002151\n",
      "train loss:   1.276013\n",
      "train loss:   0.933101\n",
      "train loss:   0.972926\n",
      "train loss:   1.125982\n",
      "train loss:   1.095893\n",
      "train loss:   1.313244\n",
      "train loss:   1.241591\n",
      "train loss:   0.850065\n",
      "train loss:   1.300370\n",
      "train loss:   1.401144\n",
      "train loss:   1.048500\n",
      "train loss:   1.451383\n",
      "train loss:   1.241811\n",
      "train loss:   0.920938\n",
      "train loss:   1.108084\n",
      "train loss:   1.100904\n",
      "train loss:   1.058380\n",
      "train loss:   1.426030\n",
      "train loss:   1.117157\n",
      "train loss:   1.164090\n",
      "train loss:   0.951270\n",
      "train loss:   0.864830\n",
      "train loss:   0.893199\n",
      "train loss:   0.968285\n",
      "train loss:   1.155477\n",
      "train loss:   1.197357\n",
      "train loss:   1.189107\n",
      "train loss:   1.202239\n",
      "train loss:   1.210602\n",
      "train loss:   1.427241\n",
      "########### epoch 27 ###########\n",
      "########### loop 3800 ###########\n",
      "test loss:   1.260129   test accuracy:   0.593750\n",
      "########### loop 3800 ###########\n",
      "train loss:   1.264236\n",
      "train loss:   1.253012\n",
      "train loss:   1.141392\n",
      "train loss:   0.897377\n",
      "train loss:   1.036504\n",
      "train loss:   1.264785\n",
      "train loss:   1.077925\n",
      "train loss:   1.100202\n",
      "train loss:   1.280451\n",
      "train loss:   1.369495\n",
      "train loss:   1.151279\n",
      "train loss:   1.268843\n",
      "train loss:   1.034995\n",
      "train loss:   1.182533\n",
      "train loss:   0.878590\n",
      "train loss:   1.092744\n",
      "train loss:   1.002770\n",
      "train loss:   1.116392\n",
      "train loss:   1.061681\n",
      "train loss:   1.193618\n",
      "train loss:   1.170859\n",
      "train loss:   0.994796\n",
      "train loss:   0.931637\n",
      "train loss:   0.863890\n",
      "train loss:   1.313240\n",
      "train loss:   1.270489\n",
      "train loss:   1.257225\n",
      "train loss:   1.108771\n",
      "train loss:   1.327614\n",
      "train loss:   1.141359\n",
      "train loss:   0.947865\n",
      "train loss:   1.048957\n",
      "train loss:   1.045211\n",
      "train loss:   0.948258\n",
      "train loss:   1.355328\n",
      "train loss:   1.336941\n",
      "train loss:   0.899924\n",
      "train loss:   1.137912\n",
      "train loss:   1.314004\n",
      "train loss:   1.348483\n",
      "train loss:   1.129559\n",
      "train loss:   1.058703\n",
      "train loss:   1.231979\n",
      "train loss:   1.166446\n",
      "train loss:   1.197586\n",
      "train loss:   1.430923\n",
      "train loss:   1.083222\n",
      "train loss:   1.203748\n",
      "train loss:   1.070847\n",
      "train loss:   1.092380\n",
      "########### epoch 28 ###########\n",
      "########### loop 3850 ###########\n",
      "test loss:   1.262981   test accuracy:   0.625000\n",
      "########### loop 3850 ###########\n",
      "train loss:   1.116988\n",
      "train loss:   1.185141\n",
      "train loss:   1.229682\n",
      "train loss:   1.072745\n",
      "train loss:   1.346133\n",
      "train loss:   1.344953\n",
      "train loss:   0.971972\n",
      "train loss:   1.203997\n",
      "train loss:   1.043964\n",
      "train loss:   1.038899\n",
      "train loss:   0.931166\n",
      "train loss:   1.193206\n",
      "train loss:   1.005300\n",
      "train loss:   1.218126\n",
      "train loss:   1.451062\n",
      "train loss:   1.230370\n",
      "train loss:   1.139015\n",
      "train loss:   1.457988\n",
      "train loss:   1.154624\n",
      "train loss:   1.124216\n",
      "train loss:   1.176367\n",
      "train loss:   0.878559\n",
      "train loss:   1.371718\n",
      "train loss:   1.062786\n",
      "train loss:   1.121183\n",
      "train loss:   1.126415\n",
      "train loss:   1.333656\n",
      "train loss:   1.024118\n",
      "train loss:   1.207943\n",
      "train loss:   1.096138\n",
      "train loss:   1.233149\n",
      "train loss:   1.078997\n",
      "train loss:   1.000627\n",
      "train loss:   1.039002\n",
      "train loss:   1.054784\n",
      "train loss:   1.201306\n",
      "train loss:   0.987082\n",
      "train loss:   1.085041\n",
      "train loss:   1.093321\n",
      "train loss:   1.202186\n",
      "train loss:   1.110777\n",
      "train loss:   1.338400\n",
      "train loss:   1.357119\n",
      "train loss:   0.960876\n",
      "train loss:   1.300038\n",
      "train loss:   1.204265\n",
      "train loss:   1.103853\n",
      "train loss:   1.020282\n",
      "train loss:   1.373262\n",
      "train loss:   1.320163\n",
      "########### epoch 28 ###########\n",
      "########### loop 3900 ###########\n",
      "test loss:   1.234933   test accuracy:   0.625000\n",
      "########### loop 3900 ###########\n",
      "train loss:   1.096455\n",
      "train loss:   1.153456\n",
      "train loss:   1.184428\n",
      "train loss:   0.930003\n",
      "train loss:   0.992193\n",
      "train loss:   1.151598\n",
      "train loss:   1.070948\n",
      "train loss:   1.345511\n",
      "train loss:   0.884956\n",
      "train loss:   1.212116\n",
      "train loss:   1.070799\n",
      "train loss:   1.357462\n",
      "train loss:   1.059201\n",
      "train loss:   0.963130\n",
      "train loss:   1.186807\n",
      "train loss:   1.190184\n",
      "train loss:   1.285529\n",
      "train loss:   1.250780\n",
      "train loss:   0.819679\n",
      "train loss:   1.289323\n",
      "train loss:   1.425485\n",
      "train loss:   1.094196\n",
      "train loss:   1.442572\n",
      "train loss:   1.291542\n",
      "train loss:   1.041937\n",
      "train loss:   1.245850\n",
      "train loss:   1.122931\n",
      "train loss:   0.965407\n",
      "train loss:   1.333503\n",
      "train loss:   1.108060\n",
      "train loss:   1.157265\n",
      "train loss:   1.005837\n",
      "train loss:   0.859437\n",
      "train loss:   0.848934\n",
      "train loss:   1.005612\n",
      "train loss:   1.125395\n",
      "train loss:   1.188063\n",
      "train loss:   1.212840\n",
      "train loss:   1.109501\n",
      "train loss:   1.119742\n",
      "train loss:   1.340279\n",
      "train loss:   1.263116\n",
      "train loss:   1.307716\n",
      "train loss:   1.116442\n",
      "train loss:   1.022100\n",
      "train loss:   1.002820\n",
      "train loss:   1.275178\n",
      "train loss:   1.011106\n",
      "train loss:   1.009176\n",
      "train loss:   1.171424\n",
      "########### epoch 29 ###########\n",
      "########### loop 3950 ###########\n",
      "test loss:   1.257231   test accuracy:   0.500000\n",
      "########### loop 3950 ###########\n",
      "train loss:   1.191575\n",
      "train loss:   1.066624\n",
      "train loss:   1.173786\n",
      "train loss:   1.209191\n",
      "train loss:   1.201234\n",
      "train loss:   0.835598\n",
      "train loss:   1.031241\n",
      "train loss:   1.133325\n",
      "train loss:   1.096535\n",
      "train loss:   1.021362\n",
      "train loss:   1.021267\n",
      "train loss:   1.042569\n",
      "train loss:   0.968080\n",
      "train loss:   0.907905\n",
      "train loss:   0.868180\n",
      "train loss:   1.326794\n",
      "train loss:   1.231445\n",
      "train loss:   1.153840\n",
      "train loss:   1.011017\n",
      "train loss:   1.163214\n",
      "train loss:   0.965614\n",
      "train loss:   0.902517\n",
      "train loss:   1.073033\n",
      "train loss:   1.076057\n",
      "train loss:   0.944109\n",
      "train loss:   1.320202\n",
      "train loss:   1.274082\n",
      "train loss:   0.871790\n",
      "train loss:   1.158133\n",
      "train loss:   1.316832\n",
      "train loss:   1.222940\n",
      "train loss:   1.468501\n",
      "train loss:   1.044599\n",
      "train loss:   1.288249\n",
      "train loss:   1.115182\n",
      "train loss:   1.091835\n",
      "train loss:   1.315899\n",
      "train loss:   1.042333\n",
      "train loss:   1.190633\n",
      "train loss:   1.026789\n",
      "train loss:   1.145328\n",
      "train loss:   1.144688\n",
      "train loss:   1.266424\n",
      "train loss:   1.245376\n",
      "train loss:   0.985245\n",
      "train loss:   1.129275\n",
      "train loss:   1.431189\n",
      "train loss:   1.063234\n",
      "train loss:   1.209343\n",
      "train loss:   1.007280\n",
      "########### epoch 29 ###########\n",
      "########### loop 4000 ###########\n",
      "test loss:   1.121483   test accuracy:   0.718750\n",
      "########### loop 4000 ###########\n",
      "train loss:   1.034319\n",
      "train loss:   0.983103\n",
      "train loss:   1.221026\n",
      "train loss:   0.968174\n",
      "train loss:   1.048267\n",
      "train loss:   1.351108\n",
      "train loss:   1.255089\n",
      "train loss:   1.183172\n",
      "train loss:   1.387422\n",
      "train loss:   1.350816\n",
      "train loss:   1.290858\n",
      "train loss:   1.137529\n",
      "train loss:   0.947441\n",
      "train loss:   1.436240\n",
      "train loss:   0.956489\n",
      "train loss:   1.149644\n",
      "train loss:   1.188113\n",
      "train loss:   1.304454\n",
      "train loss:   1.079785\n",
      "train loss:   1.149185\n",
      "train loss:   1.084162\n",
      "train loss:   1.198157\n",
      "train loss:   1.024539\n",
      "train loss:   0.969219\n",
      "train loss:   1.062525\n",
      "train loss:   1.118746\n",
      "train loss:   1.194637\n",
      "train loss:   0.999762\n",
      "train loss:   1.108569\n",
      "train loss:   1.110346\n",
      "train loss:   1.218608\n",
      "train loss:   1.144714\n",
      "train loss:   1.184683\n",
      "train loss:   1.194235\n",
      "train loss:   0.904432\n",
      "train loss:   1.281898\n",
      "train loss:   1.031210\n",
      "train loss:   0.970338\n",
      "train loss:   1.014674\n",
      "train loss:   1.247536\n",
      "train loss:   1.266589\n",
      "train loss:   1.160507\n",
      "train loss:   1.242681\n",
      "train loss:   1.214787\n",
      "train loss:   0.975107\n",
      "train loss:   1.105909\n",
      "train loss:   1.138705\n",
      "train loss:   1.120402\n",
      "train loss:   1.283229\n",
      "train loss:   0.990624\n",
      "########### epoch 29 ###########\n",
      "########### loop 4050 ###########\n",
      "test loss:   1.098465   test accuracy:   0.687500\n",
      "########### loop 4050 ###########\n",
      "train loss:   1.108289\n",
      "train loss:   1.072165\n",
      "train loss:   1.182143\n",
      "train loss:   0.916981\n",
      "train loss:   1.004529\n",
      "train loss:   1.152618\n",
      "train loss:   1.116608\n",
      "train loss:   1.279243\n",
      "train loss:   1.295245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.954940\n",
      "train loss:   1.464283\n",
      "train loss:   1.506034\n",
      "train loss:   1.044550\n",
      "train loss:   1.346918\n",
      "train loss:   1.224780\n",
      "train loss:   0.922539\n",
      "train loss:   1.056290\n",
      "train loss:   1.076614\n",
      "train loss:   1.086815\n",
      "train loss:   1.495610\n",
      "train loss:   1.135182\n",
      "train loss:   1.127997\n",
      "train loss:   1.000158\n",
      "train loss:   0.839173\n",
      "train loss:   0.931895\n",
      "train loss:   0.992245\n",
      "train loss:   1.292680\n",
      "train loss:   1.351194\n",
      "train loss:   1.358098\n",
      "train loss:   1.404177\n",
      "train loss:   1.185228\n",
      "train loss:   1.295667\n",
      "train loss:   1.386123\n",
      "train loss:   1.312522\n",
      "train loss:   1.237716\n",
      "train loss:   0.856741\n",
      "train loss:   1.051217\n",
      "train loss:   1.209495\n",
      "train loss:   0.907689\n",
      "train loss:   0.999772\n",
      "train loss:   1.082732\n",
      "train loss:   1.116945\n",
      "train loss:   0.989808\n",
      "train loss:   1.222214\n",
      "train loss:   1.045449\n",
      "train loss:   1.222737\n",
      "train loss:   0.809925\n",
      "train loss:   1.010901\n",
      "train loss:   1.043858\n",
      "train loss:   1.064777\n",
      "########### epoch 30 ###########\n",
      "########### loop 4100 ###########\n",
      "test loss:   1.188331   test accuracy:   0.625000\n",
      "########### loop 4100 ###########\n",
      "train loss:   1.084615\n",
      "train loss:   1.125728\n",
      "train loss:   1.097970\n",
      "train loss:   1.093454\n",
      "train loss:   0.929010\n",
      "train loss:   0.861408\n",
      "train loss:   1.304107\n",
      "train loss:   1.322495\n",
      "train loss:   1.229772\n",
      "train loss:   1.032746\n",
      "train loss:   1.187153\n",
      "train loss:   0.932792\n",
      "train loss:   0.931913\n",
      "train loss:   0.971165\n",
      "train loss:   1.181867\n",
      "train loss:   0.981757\n",
      "train loss:   1.462060\n",
      "train loss:   1.353654\n",
      "train loss:   0.912489\n",
      "train loss:   1.177897\n",
      "train loss:   1.317308\n",
      "train loss:   1.194538\n",
      "train loss:   1.241557\n",
      "train loss:   1.091497\n",
      "train loss:   1.297889\n",
      "train loss:   1.084512\n",
      "train loss:   1.242498\n",
      "train loss:   1.439013\n",
      "train loss:   1.110792\n",
      "train loss:   1.230196\n",
      "train loss:   1.098171\n",
      "train loss:   1.148766\n",
      "train loss:   1.178556\n",
      "train loss:   1.199180\n",
      "train loss:   1.161278\n",
      "train loss:   0.916436\n",
      "train loss:   1.186503\n",
      "train loss:   1.415829\n",
      "train loss:   1.060188\n",
      "train loss:   1.246840\n",
      "train loss:   1.010851\n",
      "train loss:   0.955255\n",
      "train loss:   0.923162\n",
      "train loss:   1.160606\n",
      "train loss:   0.926701\n",
      "train loss:   1.309458\n",
      "train loss:   1.429569\n",
      "train loss:   1.215905\n",
      "train loss:   1.129740\n",
      "train loss:   1.299231\n",
      "########### epoch 30 ###########\n",
      "########### loop 4150 ###########\n",
      "test loss:   1.120995   test accuracy:   0.781250\n",
      "########### loop 4150 ###########\n",
      "train loss:   1.121791\n",
      "train loss:   1.213002\n",
      "train loss:   1.128104\n",
      "train loss:   0.853496\n",
      "train loss:   1.355670\n",
      "train loss:   0.971718\n",
      "train loss:   1.089814\n",
      "train loss:   1.162161\n",
      "train loss:   1.260142\n",
      "train loss:   0.937924\n",
      "train loss:   1.087654\n",
      "train loss:   1.074644\n",
      "train loss:   1.233575\n",
      "train loss:   1.043751\n",
      "train loss:   0.970908\n",
      "train loss:   1.064548\n",
      "train loss:   0.968972\n",
      "train loss:   1.255424\n",
      "train loss:   1.052845\n",
      "train loss:   1.207283\n",
      "train loss:   1.142497\n",
      "train loss:   1.089277\n",
      "train loss:   1.114722\n",
      "train loss:   1.284355\n",
      "train loss:   1.441575\n",
      "train loss:   0.978467\n",
      "train loss:   1.367658\n",
      "train loss:   1.121314\n",
      "train loss:   1.105152\n",
      "train loss:   1.010975\n",
      "train loss:   1.369180\n",
      "train loss:   1.297876\n",
      "train loss:   1.365356\n",
      "train loss:   1.224601\n",
      "train loss:   1.275028\n",
      "train loss:   0.776838\n",
      "train loss:   0.973251\n",
      "train loss:   1.117221\n",
      "train loss:   1.148211\n",
      "train loss:   1.206124\n",
      "train loss:   0.826329\n",
      "train loss:   1.188567\n",
      "train loss:   1.007092\n",
      "train loss:   1.085941\n",
      "train loss:   0.956553\n",
      "train loss:   0.995027\n",
      "train loss:   1.206908\n",
      "train loss:   1.251883\n",
      "train loss:   1.253815\n",
      "train loss:   1.222041\n",
      "########### epoch 30 ###########\n",
      "########### loop 4200 ###########\n",
      "test loss:   1.331705   test accuracy:   0.531250\n",
      "########### loop 4200 ###########\n",
      "train loss:   0.919718\n",
      "train loss:   1.310275\n",
      "train loss:   1.329236\n",
      "train loss:   1.028201\n",
      "train loss:   1.488515\n",
      "train loss:   1.341773\n",
      "train loss:   1.014985\n",
      "train loss:   1.254182\n",
      "train loss:   1.026905\n",
      "train loss:   0.954583\n",
      "train loss:   1.220481\n",
      "train loss:   1.119051\n",
      "train loss:   1.189198\n",
      "train loss:   0.940793\n",
      "train loss:   0.840118\n",
      "train loss:   0.810641\n",
      "train loss:   1.014096\n",
      "train loss:   1.096569\n",
      "train loss:   1.161949\n",
      "train loss:   1.188222\n",
      "train loss:   1.172847\n",
      "train loss:   1.148335\n",
      "train loss:   1.315759\n",
      "train loss:   1.318424\n",
      "train loss:   1.226372\n",
      "train loss:   1.168378\n",
      "train loss:   0.942190\n",
      "train loss:   1.000595\n",
      "train loss:   1.370497\n",
      "train loss:   0.926485\n",
      "train loss:   0.907777\n",
      "train loss:   1.171910\n",
      "train loss:   1.095303\n",
      "train loss:   1.089885\n",
      "train loss:   1.378361\n",
      "train loss:   1.138909\n",
      "train loss:   1.330071\n",
      "train loss:   0.811526\n",
      "train loss:   0.986970\n",
      "train loss:   1.079982\n",
      "train loss:   1.172273\n",
      "train loss:   1.210126\n",
      "train loss:   0.986938\n",
      "train loss:   1.089974\n",
      "train loss:   1.040941\n",
      "train loss:   0.912771\n",
      "train loss:   0.944807\n",
      "train loss:   1.262357\n",
      "train loss:   1.266602\n",
      "train loss:   1.175913\n",
      "########### epoch 31 ###########\n",
      "########### loop 4250 ###########\n",
      "test loss:   1.273884   test accuracy:   0.625000\n",
      "########### loop 4250 ###########\n",
      "train loss:   0.992127\n",
      "train loss:   1.178853\n",
      "train loss:   1.014812\n",
      "train loss:   0.966660\n",
      "train loss:   1.000816\n",
      "train loss:   1.120983\n",
      "train loss:   0.964767\n",
      "train loss:   1.424711\n",
      "train loss:   1.376115\n",
      "train loss:   0.921155\n",
      "train loss:   1.295990\n",
      "train loss:   1.284106\n",
      "train loss:   1.186513\n",
      "train loss:   1.347399\n",
      "train loss:   1.202308\n",
      "train loss:   1.281788\n",
      "train loss:   1.151725\n",
      "train loss:   1.323264\n",
      "train loss:   1.446205\n",
      "train loss:   1.110847\n",
      "train loss:   1.251698\n",
      "train loss:   0.968031\n",
      "train loss:   1.169593\n",
      "train loss:   1.181266\n",
      "train loss:   1.254487\n",
      "train loss:   1.239590\n",
      "train loss:   1.009832\n",
      "train loss:   1.225774\n",
      "train loss:   1.395694\n",
      "train loss:   1.151097\n",
      "train loss:   1.280591\n",
      "train loss:   0.997739\n",
      "train loss:   1.051718\n",
      "train loss:   0.977934\n",
      "train loss:   1.212079\n",
      "train loss:   0.941307\n",
      "train loss:   1.049595\n",
      "train loss:   1.414018\n",
      "train loss:   1.204633\n",
      "train loss:   1.113884\n",
      "train loss:   1.262605\n",
      "train loss:   1.267616\n",
      "train loss:   1.261126\n",
      "train loss:   1.078607\n",
      "train loss:   0.820879\n",
      "train loss:   1.236587\n",
      "train loss:   1.011320\n",
      "train loss:   1.130733\n",
      "train loss:   1.159058\n",
      "train loss:   1.287668\n",
      "########### epoch 31 ###########\n",
      "########### loop 4300 ###########\n",
      "test loss:   1.473323   test accuracy:   0.500000\n",
      "########### loop 4300 ###########\n",
      "train loss:   1.106672\n",
      "train loss:   1.167715\n",
      "train loss:   0.999098\n",
      "train loss:   1.299967\n",
      "train loss:   1.186423\n",
      "train loss:   1.118585\n",
      "train loss:   1.140328\n",
      "train loss:   1.281481\n",
      "train loss:   1.160242\n",
      "train loss:   0.962835\n",
      "train loss:   1.084610\n",
      "train loss:   1.155246\n",
      "train loss:   1.028354\n",
      "train loss:   0.970647\n",
      "train loss:   1.157253\n",
      "train loss:   1.299289\n",
      "train loss:   0.976517\n",
      "train loss:   1.327884\n",
      "train loss:   1.090878\n",
      "train loss:   0.925786\n",
      "train loss:   0.948795\n",
      "train loss:   1.251713\n",
      "train loss:   1.205354\n",
      "train loss:   1.140151\n",
      "train loss:   1.261471\n",
      "train loss:   1.209333\n",
      "train loss:   0.949127\n",
      "train loss:   1.147413\n",
      "train loss:   1.091753\n",
      "train loss:   1.072031\n",
      "train loss:   1.185533\n",
      "train loss:   0.793468\n",
      "train loss:   1.096703\n",
      "train loss:   1.014496\n",
      "train loss:   1.203021\n",
      "train loss:   0.940976\n",
      "train loss:   1.020348\n",
      "train loss:   1.201168\n",
      "train loss:   1.151384\n",
      "train loss:   1.272194\n",
      "train loss:   1.145236\n",
      "train loss:   0.818036\n",
      "train loss:   1.219221\n",
      "train loss:   1.458444\n",
      "train loss:   1.150708\n",
      "train loss:   1.466373\n",
      "train loss:   1.183774\n",
      "train loss:   0.965247\n",
      "train loss:   1.182329\n",
      "train loss:   1.051985\n",
      "########### epoch 31 ###########\n",
      "########### loop 4350 ###########\n",
      "test loss:   1.149853   test accuracy:   0.625000\n",
      "########### loop 4350 ###########\n",
      "train loss:   1.060488\n",
      "train loss:   1.285185\n",
      "train loss:   1.224161\n",
      "train loss:   1.164367\n",
      "train loss:   0.917845\n",
      "train loss:   0.843101\n",
      "train loss:   0.835058\n",
      "train loss:   1.293036\n",
      "train loss:   1.212999\n",
      "train loss:   1.254773\n",
      "train loss:   1.308915\n",
      "train loss:   1.258041\n",
      "train loss:   1.283580\n",
      "train loss:   1.478890\n",
      "train loss:   1.378265\n",
      "train loss:   1.297139\n",
      "train loss:   1.250025\n",
      "train loss:   0.983073\n",
      "train loss:   1.083915\n",
      "train loss:   1.189264\n",
      "train loss:   1.031668\n",
      "train loss:   0.988003\n",
      "train loss:   1.188050\n",
      "train loss:   1.191160\n",
      "train loss:   1.059736\n",
      "train loss:   1.171764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.088604\n",
      "train loss:   1.115392\n",
      "train loss:   0.821605\n",
      "train loss:   1.044919\n",
      "train loss:   0.993928\n",
      "train loss:   1.016226\n",
      "train loss:   1.037116\n",
      "train loss:   1.012239\n",
      "train loss:   1.051594\n",
      "train loss:   0.993775\n",
      "train loss:   0.858803\n",
      "train loss:   0.888926\n",
      "train loss:   1.331223\n",
      "train loss:   1.275928\n",
      "train loss:   1.107804\n",
      "train loss:   0.953042\n",
      "train loss:   1.029520\n",
      "train loss:   0.848952\n",
      "train loss:   0.959604\n",
      "train loss:   0.979694\n",
      "train loss:   1.032519\n",
      "train loss:   0.912455\n",
      "train loss:   1.309300\n",
      "train loss:   1.301889\n",
      "########### epoch 32 ###########\n",
      "########### loop 4400 ###########\n",
      "test loss:   1.102099   test accuracy:   0.750000\n",
      "########### loop 4400 ###########\n",
      "train loss:   0.854559\n",
      "train loss:   1.181773\n",
      "train loss:   1.250375\n",
      "train loss:   1.173155\n",
      "train loss:   1.113701\n",
      "train loss:   1.045526\n",
      "train loss:   1.166067\n",
      "train loss:   1.108725\n",
      "train loss:   1.240865\n",
      "train loss:   1.456153\n",
      "train loss:   1.087149\n",
      "train loss:   1.294365\n",
      "train loss:   1.034271\n",
      "train loss:   1.137068\n",
      "train loss:   1.177323\n",
      "train loss:   1.269200\n",
      "train loss:   1.193059\n",
      "train loss:   0.916693\n",
      "train loss:   1.152621\n",
      "train loss:   1.495273\n",
      "train loss:   0.967698\n",
      "train loss:   1.196664\n",
      "train loss:   0.959537\n",
      "train loss:   0.999651\n",
      "train loss:   0.879926\n",
      "train loss:   1.128548\n",
      "train loss:   0.993062\n",
      "train loss:   1.135989\n",
      "train loss:   1.373651\n",
      "train loss:   1.245627\n",
      "train loss:   1.076847\n",
      "train loss:   1.361510\n",
      "train loss:   1.141810\n",
      "train loss:   1.134982\n",
      "train loss:   1.096885\n",
      "train loss:   0.841629\n",
      "train loss:   1.288051\n",
      "train loss:   1.044973\n",
      "train loss:   1.060765\n",
      "train loss:   1.138959\n",
      "train loss:   1.285545\n",
      "train loss:   0.933408\n",
      "train loss:   1.048488\n",
      "train loss:   0.961991\n",
      "train loss:   1.236459\n",
      "train loss:   1.067218\n",
      "train loss:   1.120677\n",
      "train loss:   1.077372\n",
      "train loss:   1.023387\n",
      "train loss:   1.160996\n",
      "########### epoch 32 ###########\n",
      "########### loop 4450 ###########\n",
      "test loss:   1.107983   test accuracy:   0.625000\n",
      "########### loop 4450 ###########\n",
      "train loss:   0.886592\n",
      "train loss:   1.055449\n",
      "train loss:   1.127273\n",
      "train loss:   1.149453\n",
      "train loss:   0.984314\n",
      "train loss:   1.221377\n",
      "train loss:   1.375562\n",
      "train loss:   0.891291\n",
      "train loss:   1.304565\n",
      "train loss:   1.021732\n",
      "train loss:   0.929972\n",
      "train loss:   0.973000\n",
      "train loss:   1.243731\n",
      "train loss:   1.211051\n",
      "train loss:   1.114925\n",
      "train loss:   1.119948\n",
      "train loss:   1.266534\n",
      "train loss:   1.050500\n",
      "train loss:   1.038067\n",
      "train loss:   1.014240\n",
      "train loss:   1.072975\n",
      "train loss:   1.222291\n",
      "train loss:   0.839756\n",
      "train loss:   1.111395\n",
      "train loss:   1.050725\n",
      "train loss:   1.182690\n",
      "train loss:   1.021100\n",
      "train loss:   1.001965\n",
      "train loss:   1.161735\n",
      "train loss:   1.272743\n",
      "train loss:   1.235874\n",
      "train loss:   1.120513\n",
      "train loss:   0.828538\n",
      "train loss:   1.099666\n",
      "train loss:   1.270654\n",
      "train loss:   1.037942\n",
      "train loss:   1.363325\n",
      "train loss:   1.127574\n",
      "train loss:   0.992922\n",
      "train loss:   1.101571\n",
      "train loss:   1.048190\n",
      "train loss:   0.941747\n",
      "train loss:   1.251672\n",
      "train loss:   1.085451\n",
      "train loss:   1.210803\n",
      "train loss:   0.929468\n",
      "train loss:   0.819590\n",
      "train loss:   0.873834\n",
      "train loss:   0.984698\n",
      "train loss:   1.046133\n",
      "########### epoch 32 ###########\n",
      "########### loop 4500 ###########\n",
      "test loss:   1.156880   test accuracy:   0.625000\n",
      "########### loop 4500 ###########\n",
      "train loss:   1.210537\n",
      "train loss:   1.211910\n",
      "train loss:   1.111315\n",
      "train loss:   1.102625\n",
      "train loss:   1.383240\n",
      "train loss:   1.342970\n",
      "train loss:   1.345149\n",
      "train loss:   1.175419\n",
      "train loss:   0.987643\n",
      "train loss:   0.965193\n",
      "train loss:   1.385738\n",
      "train loss:   1.041769\n",
      "train loss:   1.094386\n",
      "train loss:   1.129532\n",
      "train loss:   1.096555\n",
      "train loss:   1.041198\n",
      "train loss:   1.241623\n",
      "train loss:   1.126311\n",
      "train loss:   1.112222\n",
      "train loss:   0.849644\n",
      "train loss:   1.025418\n",
      "train loss:   1.108366\n",
      "train loss:   1.035430\n",
      "train loss:   1.083246\n",
      "train loss:   1.060921\n",
      "train loss:   1.055543\n",
      "train loss:   1.004388\n",
      "train loss:   0.882327\n",
      "train loss:   0.885269\n",
      "train loss:   1.492058\n",
      "train loss:   1.186760\n",
      "train loss:   1.056607\n",
      "train loss:   1.089794\n",
      "train loss:   1.190504\n",
      "train loss:   1.141725\n",
      "train loss:   0.956238\n",
      "train loss:   1.202812\n",
      "train loss:   1.250308\n",
      "train loss:   1.000400\n",
      "train loss:   1.292132\n",
      "train loss:   1.297601\n",
      "train loss:   0.902767\n",
      "train loss:   1.045749\n",
      "train loss:   1.259463\n",
      "train loss:   1.158173\n",
      "train loss:   1.343455\n",
      "train loss:   1.023794\n",
      "train loss:   1.252884\n",
      "train loss:   1.064176\n",
      "train loss:   1.157940\n",
      "########### epoch 33 ###########\n",
      "########### loop 4550 ###########\n",
      "test loss:   1.128974   test accuracy:   0.687500\n",
      "########### loop 4550 ###########\n",
      "train loss:   1.442176\n",
      "train loss:   1.061902\n",
      "train loss:   1.241148\n",
      "train loss:   1.015359\n",
      "train loss:   1.092050\n",
      "train loss:   1.067630\n",
      "train loss:   1.141845\n",
      "train loss:   1.189271\n",
      "train loss:   0.984434\n",
      "train loss:   1.235001\n",
      "train loss:   1.368187\n",
      "train loss:   1.038759\n",
      "train loss:   1.233129\n",
      "train loss:   0.985726\n",
      "train loss:   0.989899\n",
      "train loss:   0.857399\n",
      "train loss:   1.192802\n",
      "train loss:   0.925016\n",
      "train loss:   1.094252\n",
      "train loss:   1.286927\n",
      "train loss:   1.279821\n",
      "train loss:   1.161971\n",
      "train loss:   1.362460\n",
      "train loss:   1.282118\n",
      "train loss:   1.247156\n",
      "train loss:   1.086424\n",
      "train loss:   0.937267\n",
      "train loss:   1.326199\n",
      "train loss:   0.911456\n",
      "train loss:   1.123445\n",
      "train loss:   1.087914\n",
      "train loss:   1.174544\n",
      "train loss:   0.871332\n",
      "train loss:   1.036543\n",
      "train loss:   0.984949\n",
      "train loss:   1.257473\n",
      "train loss:   0.949648\n",
      "train loss:   0.953302\n",
      "train loss:   1.008445\n",
      "train loss:   1.017373\n",
      "train loss:   1.229864\n",
      "train loss:   1.092139\n",
      "train loss:   1.136033\n",
      "train loss:   1.101791\n",
      "train loss:   1.109282\n",
      "train loss:   0.960806\n",
      "train loss:   1.225189\n",
      "train loss:   1.236549\n",
      "train loss:   0.786004\n",
      "train loss:   1.173988\n",
      "########### epoch 33 ###########\n",
      "########### loop 4600 ###########\n",
      "test loss:   1.158175   test accuracy:   0.687500\n",
      "########### loop 4600 ###########\n",
      "train loss:   1.063632\n",
      "train loss:   0.966904\n",
      "train loss:   0.918640\n",
      "train loss:   1.263109\n",
      "train loss:   1.235746\n",
      "train loss:   1.080624\n",
      "train loss:   1.175167\n",
      "train loss:   1.271068\n",
      "train loss:   0.891298\n",
      "train loss:   1.030022\n",
      "train loss:   1.068810\n",
      "train loss:   1.094474\n",
      "train loss:   1.396396\n",
      "train loss:   0.770149\n",
      "train loss:   1.153595\n",
      "train loss:   1.081650\n",
      "train loss:   1.199519\n",
      "train loss:   0.984116\n",
      "train loss:   0.941994\n",
      "train loss:   1.047973\n",
      "train loss:   1.172676\n",
      "train loss:   1.328033\n",
      "train loss:   1.124923\n",
      "train loss:   0.787941\n",
      "train loss:   1.132519\n",
      "train loss:   1.292798\n",
      "train loss:   0.996835\n",
      "train loss:   1.309129\n",
      "train loss:   1.125978\n",
      "train loss:   0.892738\n",
      "train loss:   1.056949\n",
      "train loss:   1.044198\n",
      "train loss:   1.047486\n",
      "train loss:   1.333974\n",
      "train loss:   1.143796\n",
      "train loss:   1.122963\n",
      "train loss:   0.939981\n",
      "train loss:   0.860208\n",
      "train loss:   0.871247\n",
      "train loss:   1.014143\n",
      "train loss:   1.098137\n",
      "train loss:   1.124189\n",
      "train loss:   1.151147\n",
      "train loss:   1.129704\n",
      "train loss:   1.113908\n",
      "train loss:   1.371267\n",
      "train loss:   1.303087\n",
      "train loss:   1.239305\n",
      "train loss:   1.167053\n",
      "train loss:   0.894154\n",
      "########### epoch 33 ###########\n",
      "########### loop 4650 ###########\n",
      "test loss:   1.251928   test accuracy:   0.562500\n",
      "########### loop 4650 ###########\n",
      "train loss:   0.973827\n",
      "train loss:   1.338854\n",
      "train loss:   0.988241\n",
      "train loss:   1.092929\n",
      "train loss:   1.284903\n",
      "train loss:   1.101884\n",
      "train loss:   0.967110\n",
      "train loss:   1.154784\n",
      "train loss:   1.165431\n",
      "train loss:   1.298089\n",
      "train loss:   0.776386\n",
      "train loss:   1.068108\n",
      "train loss:   1.103768\n",
      "train loss:   1.056165\n",
      "train loss:   1.110696\n",
      "train loss:   1.057468\n",
      "train loss:   1.071355\n",
      "train loss:   1.048656\n",
      "train loss:   0.900801\n",
      "train loss:   0.873646\n",
      "train loss:   1.265839\n",
      "train loss:   1.240073\n",
      "train loss:   1.067019\n",
      "train loss:   1.074707\n",
      "train loss:   1.073596\n",
      "train loss:   1.020230\n",
      "train loss:   0.842497\n",
      "train loss:   1.066426\n",
      "train loss:   0.984414\n",
      "train loss:   0.920371\n",
      "train loss:   1.275892\n",
      "train loss:   1.257187\n",
      "train loss:   0.900542\n",
      "train loss:   1.118564\n",
      "train loss:   1.304836\n",
      "train loss:   1.186717\n",
      "train loss:   1.243146\n",
      "train loss:   1.049181\n",
      "train loss:   1.151899\n",
      "train loss:   1.082253\n",
      "train loss:   1.114645\n",
      "train loss:   1.281205\n",
      "train loss:   1.014287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.283527\n",
      "train loss:   1.137625\n",
      "train loss:   1.178658\n",
      "train loss:   1.189099\n",
      "train loss:   1.258627\n",
      "train loss:   1.172892\n",
      "train loss:   0.889083\n",
      "########### epoch 34 ###########\n",
      "########### loop 4700 ###########\n",
      "test loss:   1.188633   test accuracy:   0.625000\n",
      "########### loop 4700 ###########\n",
      "train loss:   1.136741\n",
      "train loss:   1.445677\n",
      "train loss:   1.009984\n",
      "train loss:   1.316386\n",
      "train loss:   1.101856\n",
      "train loss:   0.956114\n",
      "train loss:   0.881814\n",
      "train loss:   1.351766\n",
      "train loss:   1.157075\n",
      "train loss:   1.038289\n",
      "train loss:   1.708643\n",
      "train loss:   1.095110\n",
      "train loss:   1.223964\n",
      "train loss:   1.366129\n",
      "train loss:   1.109177\n",
      "train loss:   1.116684\n",
      "train loss:   1.091684\n",
      "train loss:   0.814934\n",
      "train loss:   1.372893\n",
      "train loss:   1.055698\n",
      "train loss:   1.021259\n",
      "train loss:   1.042310\n",
      "train loss:   1.273708\n",
      "train loss:   0.936959\n",
      "train loss:   1.066667\n",
      "train loss:   0.908446\n",
      "train loss:   1.249248\n",
      "train loss:   0.983876\n",
      "train loss:   1.029727\n",
      "train loss:   1.056379\n",
      "train loss:   1.132506\n",
      "train loss:   1.147063\n",
      "train loss:   0.917632\n",
      "train loss:   1.056488\n",
      "train loss:   1.073864\n",
      "train loss:   0.963690\n",
      "train loss:   0.941663\n",
      "train loss:   1.159859\n",
      "train loss:   1.261023\n",
      "train loss:   0.917062\n",
      "train loss:   1.215564\n",
      "train loss:   1.017933\n",
      "train loss:   0.923082\n",
      "train loss:   0.977450\n",
      "train loss:   1.222842\n",
      "train loss:   1.113085\n",
      "train loss:   1.170271\n",
      "train loss:   1.298274\n",
      "train loss:   1.225529\n",
      "train loss:   0.848105\n",
      "########### epoch 34 ###########\n",
      "########### loop 4750 ###########\n",
      "test loss:   1.097898   test accuracy:   0.656250\n",
      "########### loop 4750 ###########\n",
      "train loss:   0.931291\n",
      "train loss:   1.050201\n",
      "train loss:   1.051702\n",
      "train loss:   1.180626\n",
      "train loss:   0.727402\n",
      "train loss:   1.133770\n",
      "train loss:   0.992484\n",
      "train loss:   1.107781\n",
      "train loss:   0.886279\n",
      "train loss:   0.957407\n",
      "train loss:   1.133761\n",
      "train loss:   1.221256\n",
      "train loss:   1.166321\n",
      "train loss:   1.140052\n",
      "train loss:   0.886867\n",
      "train loss:   1.142474\n",
      "train loss:   1.279253\n",
      "train loss:   1.016111\n",
      "train loss:   1.376469\n",
      "train loss:   1.145662\n",
      "train loss:   0.858432\n",
      "train loss:   1.086732\n",
      "train loss:   1.057728\n",
      "train loss:   0.916715\n",
      "train loss:   1.215506\n",
      "train loss:   1.088645\n",
      "train loss:   1.056445\n",
      "train loss:   0.931412\n",
      "train loss:   0.787657\n",
      "train loss:   0.827005\n",
      "train loss:   0.944386\n",
      "train loss:   1.209795\n",
      "train loss:   1.186915\n",
      "train loss:   1.195792\n",
      "train loss:   1.221873\n",
      "train loss:   1.105281\n",
      "train loss:   1.187694\n",
      "train loss:   1.244349\n",
      "train loss:   1.234283\n",
      "train loss:   1.199410\n",
      "train loss:   0.906698\n",
      "train loss:   0.958480\n",
      "train loss:   1.136201\n",
      "train loss:   0.868146\n",
      "train loss:   0.926413\n",
      "train loss:   1.061327\n",
      "train loss:   1.067929\n",
      "train loss:   0.952601\n",
      "train loss:   1.223637\n",
      "train loss:   1.035731\n",
      "########### epoch 35 ###########\n",
      "########### loop 4800 ###########\n",
      "test loss:   1.166279   test accuracy:   0.625000\n",
      "########### loop 4800 ###########\n",
      "train loss:   1.160344\n",
      "train loss:   0.759335\n",
      "train loss:   0.987406\n",
      "train loss:   1.090746\n",
      "train loss:   1.053264\n",
      "train loss:   1.042777\n",
      "train loss:   1.120003\n",
      "train loss:   1.080532\n",
      "train loss:   1.012214\n",
      "train loss:   0.881075\n",
      "train loss:   0.849220\n",
      "train loss:   1.236445\n",
      "train loss:   1.178067\n",
      "train loss:   1.069442\n",
      "train loss:   1.039715\n",
      "train loss:   1.117325\n",
      "train loss:   0.859660\n",
      "train loss:   0.860603\n",
      "train loss:   1.014245\n",
      "train loss:   1.159299\n",
      "train loss:   0.906998\n",
      "train loss:   1.348950\n",
      "train loss:   1.342808\n",
      "train loss:   0.828266\n",
      "train loss:   1.159924\n",
      "train loss:   1.266969\n",
      "train loss:   1.148291\n",
      "train loss:   1.146713\n",
      "train loss:   0.965383\n",
      "train loss:   1.126115\n",
      "train loss:   1.072549\n",
      "train loss:   1.162470\n",
      "train loss:   1.404819\n",
      "train loss:   0.969577\n",
      "train loss:   1.201702\n",
      "train loss:   1.027433\n",
      "train loss:   1.102804\n",
      "train loss:   1.301123\n",
      "train loss:   1.319287\n",
      "train loss:   1.152851\n",
      "train loss:   0.974689\n",
      "train loss:   1.101198\n",
      "train loss:   1.279608\n",
      "train loss:   0.945388\n",
      "train loss:   1.102372\n",
      "train loss:   0.982945\n",
      "train loss:   1.040101\n",
      "train loss:   0.986418\n",
      "train loss:   1.126745\n",
      "train loss:   0.820911\n",
      "########### epoch 35 ###########\n",
      "########### loop 4850 ###########\n",
      "test loss:   1.189414   test accuracy:   0.562500\n",
      "########### loop 4850 ###########\n",
      "train loss:   1.191196\n",
      "train loss:   1.289263\n",
      "train loss:   1.176554\n",
      "train loss:   1.027802\n",
      "train loss:   1.271666\n",
      "train loss:   1.137498\n",
      "train loss:   1.246525\n",
      "train loss:   1.058346\n",
      "train loss:   0.867005\n",
      "train loss:   1.289218\n",
      "train loss:   0.943920\n",
      "train loss:   1.008631\n",
      "train loss:   1.065994\n",
      "train loss:   1.246872\n",
      "train loss:   0.959336\n",
      "train loss:   1.089315\n",
      "train loss:   1.017340\n",
      "train loss:   1.157148\n",
      "train loss:   0.901134\n",
      "train loss:   1.014952\n",
      "train loss:   1.080597\n",
      "train loss:   0.993973\n",
      "train loss:   1.191236\n",
      "train loss:   0.943812\n",
      "train loss:   1.137084\n",
      "train loss:   1.027054\n",
      "train loss:   1.081244\n",
      "train loss:   1.055208\n",
      "train loss:   1.251189\n",
      "train loss:   1.367025\n",
      "train loss:   0.873029\n",
      "train loss:   1.235858\n",
      "train loss:   0.960364\n",
      "train loss:   0.951156\n",
      "train loss:   0.927603\n",
      "train loss:   1.183239\n",
      "train loss:   1.147415\n",
      "train loss:   1.061159\n",
      "train loss:   1.111820\n",
      "train loss:   1.125003\n",
      "train loss:   0.814892\n",
      "train loss:   0.982056\n",
      "train loss:   1.087770\n",
      "train loss:   1.017435\n",
      "train loss:   1.308626\n",
      "train loss:   0.700726\n",
      "train loss:   1.142333\n",
      "train loss:   0.999021\n",
      "train loss:   1.144117\n",
      "train loss:   0.884089\n",
      "########### epoch 35 ###########\n",
      "########### loop 4900 ###########\n",
      "test loss:   1.116367   test accuracy:   0.531250\n",
      "########### loop 4900 ###########\n",
      "train loss:   0.983768\n",
      "train loss:   1.045290\n",
      "train loss:   1.067158\n",
      "train loss:   1.180346\n",
      "train loss:   1.158422\n",
      "train loss:   0.776759\n",
      "train loss:   1.236942\n",
      "train loss:   1.326944\n",
      "train loss:   0.996032\n",
      "train loss:   1.306071\n",
      "train loss:   1.084477\n",
      "train loss:   0.866246\n",
      "train loss:   1.022895\n",
      "train loss:   1.000140\n",
      "train loss:   0.906321\n",
      "train loss:   1.246152\n",
      "train loss:   1.033584\n",
      "train loss:   1.091305\n",
      "train loss:   0.942266\n",
      "train loss:   0.800556\n",
      "train loss:   0.825724\n",
      "train loss:   1.025227\n",
      "train loss:   1.129215\n",
      "train loss:   1.162218\n",
      "train loss:   1.206880\n",
      "train loss:   1.133588\n",
      "train loss:   1.090043\n",
      "train loss:   1.281076\n",
      "train loss:   1.396918\n",
      "train loss:   1.180787\n",
      "train loss:   1.180134\n",
      "train loss:   0.911898\n",
      "train loss:   0.995853\n",
      "train loss:   1.166164\n",
      "train loss:   0.971672\n",
      "train loss:   1.117957\n",
      "train loss:   1.188117\n",
      "train loss:   1.224929\n",
      "train loss:   0.945270\n",
      "train loss:   1.085439\n",
      "train loss:   1.055149\n",
      "train loss:   1.135734\n",
      "train loss:   0.795703\n",
      "train loss:   1.000697\n",
      "train loss:   0.925931\n",
      "train loss:   1.028663\n",
      "train loss:   1.028234\n",
      "train loss:   1.050578\n",
      "train loss:   0.995940\n",
      "train loss:   0.997031\n",
      "########### epoch 36 ###########\n",
      "########### loop 4950 ###########\n",
      "test loss:   1.144036   test accuracy:   0.593750\n",
      "########### loop 4950 ###########\n",
      "train loss:   0.855459\n",
      "train loss:   0.902825\n",
      "train loss:   1.239794\n",
      "train loss:   1.196540\n",
      "train loss:   0.986091\n",
      "train loss:   0.965254\n",
      "train loss:   1.031177\n",
      "train loss:   0.861291\n",
      "train loss:   0.986788\n",
      "train loss:   0.920127\n",
      "train loss:   1.085211\n",
      "train loss:   0.918720\n",
      "train loss:   1.292395\n",
      "train loss:   1.298943\n",
      "train loss:   0.832014\n",
      "train loss:   1.186563\n",
      "train loss:   1.203292\n",
      "train loss:   1.139442\n",
      "train loss:   1.128942\n",
      "train loss:   0.952758\n",
      "train loss:   1.125858\n",
      "train loss:   1.037260\n",
      "train loss:   1.160411\n",
      "train loss:   1.341798\n",
      "train loss:   1.044605\n",
      "train loss:   1.174969\n",
      "train loss:   1.032316\n",
      "train loss:   1.103184\n",
      "train loss:   1.140450\n",
      "train loss:   1.213452\n",
      "train loss:   1.120933\n",
      "train loss:   0.929125\n",
      "train loss:   1.064792\n",
      "train loss:   1.347544\n",
      "train loss:   1.023289\n",
      "train loss:   1.109599\n",
      "train loss:   0.940318\n",
      "train loss:   1.010398\n",
      "train loss:   0.927836\n",
      "train loss:   1.046435\n",
      "train loss:   0.855972\n",
      "train loss:   1.207570\n",
      "train loss:   1.315377\n",
      "train loss:   1.143785\n",
      "train loss:   1.055013\n",
      "train loss:   1.177927\n",
      "train loss:   1.174475\n",
      "train loss:   1.368487\n",
      "train loss:   1.174861\n",
      "train loss:   0.902800\n",
      "########### epoch 36 ###########\n",
      "########### loop 5000 ###########\n",
      "test loss:   1.185775   test accuracy:   0.625000\n",
      "########### loop 5000 ###########\n",
      "train loss:   1.385368\n",
      "train loss:   0.978848\n",
      "train loss:   1.130800\n",
      "train loss:   1.054128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.130428\n",
      "train loss:   0.846582\n",
      "train loss:   1.064923\n",
      "train loss:   1.012306\n",
      "train loss:   1.140309\n",
      "train loss:   0.946130\n",
      "train loss:   0.938403\n",
      "train loss:   1.024209\n",
      "train loss:   0.953858\n",
      "train loss:   1.150275\n",
      "train loss:   0.853277\n",
      "train loss:   1.087938\n",
      "train loss:   1.107811\n",
      "train loss:   1.180990\n",
      "train loss:   1.048565\n",
      "train loss:   1.296234\n",
      "train loss:   1.226851\n",
      "train loss:   0.862889\n",
      "train loss:   1.188143\n",
      "train loss:   0.986118\n",
      "train loss:   0.952602\n",
      "train loss:   0.924547\n",
      "train loss:   1.181881\n",
      "train loss:   1.138239\n",
      "train loss:   1.048359\n",
      "train loss:   1.057473\n",
      "train loss:   1.144308\n",
      "train loss:   0.814590\n",
      "train loss:   0.880530\n",
      "train loss:   0.990029\n",
      "train loss:   1.051657\n",
      "train loss:   1.335271\n",
      "train loss:   0.751416\n",
      "train loss:   1.156931\n",
      "train loss:   1.058269\n",
      "train loss:   1.136942\n",
      "train loss:   0.867938\n",
      "train loss:   0.963786\n",
      "train loss:   1.086549\n",
      "train loss:   1.152240\n",
      "train loss:   1.203838\n",
      "train loss:   1.161641\n",
      "train loss:   0.944825\n",
      "train loss:   1.316827\n",
      "train loss:   1.420423\n",
      "train loss:   1.003727\n",
      "########### epoch 36 ###########\n",
      "########### loop 5050 ###########\n",
      "test loss:   1.267013   test accuracy:   0.437500\n",
      "########### loop 5050 ###########\n",
      "train loss:   1.215306\n",
      "train loss:   1.136186\n",
      "train loss:   0.871895\n",
      "train loss:   1.063641\n",
      "train loss:   0.977250\n",
      "train loss:   0.857393\n",
      "train loss:   1.248897\n",
      "train loss:   1.043864\n",
      "train loss:   1.063472\n",
      "train loss:   0.929861\n",
      "train loss:   0.822304\n",
      "train loss:   0.827309\n",
      "train loss:   0.936125\n",
      "train loss:   0.962803\n",
      "train loss:   1.104427\n",
      "train loss:   1.146522\n",
      "train loss:   1.102070\n",
      "train loss:   1.088094\n",
      "train loss:   1.180574\n",
      "train loss:   1.249757\n",
      "train loss:   1.290123\n",
      "train loss:   1.170391\n",
      "train loss:   0.960379\n",
      "train loss:   1.007462\n",
      "train loss:   1.159883\n",
      "train loss:   0.971624\n",
      "train loss:   0.875181\n",
      "train loss:   1.024461\n",
      "train loss:   0.988814\n",
      "train loss:   0.921511\n",
      "train loss:   1.166392\n",
      "train loss:   1.020575\n",
      "train loss:   1.111316\n",
      "train loss:   0.776559\n",
      "train loss:   0.937895\n",
      "train loss:   1.060377\n",
      "train loss:   0.989668\n",
      "train loss:   1.100914\n",
      "train loss:   0.959009\n",
      "train loss:   1.020470\n",
      "train loss:   1.056666\n",
      "train loss:   0.930682\n",
      "train loss:   0.848658\n",
      "train loss:   1.206737\n",
      "train loss:   1.154088\n",
      "train loss:   1.026290\n",
      "train loss:   0.966498\n",
      "train loss:   1.113309\n",
      "train loss:   0.888694\n",
      "train loss:   0.880362\n",
      "########### epoch 37 ###########\n",
      "########### loop 5100 ###########\n",
      "test loss:   1.289766   test accuracy:   0.593750\n",
      "########### loop 5100 ###########\n",
      "train loss:   0.929249\n",
      "train loss:   1.069306\n",
      "train loss:   0.901937\n",
      "train loss:   1.250359\n",
      "train loss:   1.253246\n",
      "train loss:   0.842029\n",
      "train loss:   1.049120\n",
      "train loss:   1.197266\n",
      "train loss:   1.174047\n",
      "train loss:   1.166790\n",
      "train loss:   0.954286\n",
      "train loss:   1.127444\n",
      "train loss:   1.093617\n",
      "train loss:   1.062486\n",
      "train loss:   1.215471\n",
      "train loss:   0.972071\n",
      "train loss:   1.285242\n",
      "train loss:   1.003728\n",
      "train loss:   0.993009\n",
      "train loss:   1.067832\n",
      "train loss:   1.144792\n",
      "train loss:   1.044245\n",
      "train loss:   0.888367\n",
      "train loss:   1.232964\n",
      "train loss:   1.338532\n",
      "train loss:   1.082541\n",
      "train loss:   1.113781\n",
      "train loss:   0.961632\n",
      "train loss:   0.963046\n",
      "train loss:   0.891373\n",
      "train loss:   1.242601\n",
      "train loss:   0.831949\n",
      "train loss:   1.145247\n",
      "train loss:   1.202304\n",
      "train loss:   1.236750\n",
      "train loss:   1.046218\n",
      "train loss:   1.321898\n",
      "train loss:   1.156443\n",
      "train loss:   1.172294\n",
      "train loss:   1.101125\n",
      "train loss:   0.819528\n",
      "train loss:   1.378030\n",
      "train loss:   1.025108\n",
      "train loss:   1.123061\n",
      "train loss:   1.078917\n",
      "train loss:   1.238668\n",
      "train loss:   0.930084\n",
      "train loss:   0.977385\n",
      "train loss:   1.000558\n",
      "train loss:   1.213648\n",
      "########### epoch 37 ###########\n",
      "########### loop 5150 ###########\n",
      "test loss:   1.369959   test accuracy:   0.562500\n",
      "########### loop 5150 ###########\n",
      "train loss:   1.025612\n",
      "train loss:   0.928917\n",
      "train loss:   1.004834\n",
      "train loss:   1.001008\n",
      "train loss:   1.194204\n",
      "train loss:   1.026526\n",
      "train loss:   1.193159\n",
      "train loss:   1.143051\n",
      "train loss:   0.999317\n",
      "train loss:   1.004552\n",
      "train loss:   1.227144\n",
      "train loss:   1.234605\n",
      "train loss:   0.852617\n",
      "train loss:   1.130585\n",
      "train loss:   1.037523\n",
      "train loss:   0.892440\n",
      "train loss:   1.043322\n",
      "train loss:   1.242341\n",
      "train loss:   1.231582\n",
      "train loss:   1.216432\n",
      "train loss:   1.201734\n",
      "train loss:   1.187273\n",
      "train loss:   0.818031\n",
      "train loss:   1.085464\n",
      "train loss:   1.071452\n",
      "train loss:   1.119394\n",
      "train loss:   1.143201\n",
      "train loss:   0.767170\n",
      "train loss:   1.079905\n",
      "train loss:   0.888154\n",
      "train loss:   1.290937\n",
      "train loss:   0.940649\n",
      "train loss:   1.072904\n",
      "train loss:   1.256386\n",
      "train loss:   1.196190\n",
      "train loss:   1.286154\n",
      "train loss:   1.170534\n",
      "train loss:   0.786873\n",
      "train loss:   1.346417\n",
      "train loss:   1.449325\n",
      "train loss:   0.992425\n",
      "train loss:   1.468871\n",
      "train loss:   1.037756\n",
      "train loss:   0.826546\n",
      "train loss:   1.033333\n",
      "train loss:   1.004858\n",
      "train loss:   0.930419\n",
      "train loss:   1.271130\n",
      "train loss:   1.119504\n",
      "train loss:   1.017360\n",
      "########### epoch 37 ###########\n",
      "########### loop 5200 ###########\n",
      "test loss:   1.231334   test accuracy:   0.593750\n",
      "########### loop 5200 ###########\n",
      "train loss:   0.936202\n",
      "train loss:   0.817082\n",
      "train loss:   0.820150\n",
      "train loss:   1.047871\n",
      "train loss:   1.060133\n",
      "train loss:   1.152085\n",
      "train loss:   1.174327\n",
      "train loss:   1.166497\n",
      "train loss:   1.104634\n",
      "train loss:   1.230054\n",
      "train loss:   1.203044\n",
      "train loss:   1.255174\n",
      "train loss:   1.218231\n",
      "train loss:   0.933574\n",
      "train loss:   0.950308\n",
      "train loss:   1.131723\n",
      "train loss:   0.974665\n",
      "train loss:   0.885604\n",
      "train loss:   1.125184\n",
      "train loss:   1.097004\n",
      "train loss:   1.089688\n",
      "train loss:   1.255671\n",
      "train loss:   1.074019\n",
      "train loss:   1.075697\n",
      "train loss:   0.785567\n",
      "train loss:   0.969277\n",
      "train loss:   0.949198\n",
      "train loss:   0.987813\n",
      "train loss:   0.984425\n",
      "train loss:   0.936380\n",
      "train loss:   0.956788\n",
      "train loss:   0.823996\n",
      "train loss:   0.840005\n",
      "train loss:   0.890321\n",
      "train loss:   1.215142\n",
      "train loss:   1.223606\n",
      "train loss:   1.121330\n",
      "train loss:   0.985709\n",
      "train loss:   1.079295\n",
      "train loss:   0.913985\n",
      "train loss:   0.803926\n",
      "train loss:   0.999926\n",
      "train loss:   0.991308\n",
      "train loss:   0.847350\n",
      "train loss:   1.422071\n",
      "train loss:   1.277620\n",
      "train loss:   0.988932\n",
      "train loss:   1.181787\n",
      "train loss:   1.333402\n",
      "train loss:   1.247955\n",
      "########### epoch 38 ###########\n",
      "########### loop 5250 ###########\n",
      "test loss:   1.273090   test accuracy:   0.625000\n",
      "########### loop 5250 ###########\n",
      "train loss:   1.323064\n",
      "train loss:   0.993593\n",
      "train loss:   1.154781\n",
      "train loss:   1.095658\n",
      "train loss:   1.084176\n",
      "train loss:   1.229979\n",
      "train loss:   1.006205\n",
      "train loss:   1.130411\n",
      "train loss:   1.030107\n",
      "train loss:   1.043975\n",
      "train loss:   1.145122\n",
      "train loss:   1.128070\n",
      "train loss:   1.140839\n",
      "train loss:   0.928107\n",
      "train loss:   1.124426\n",
      "train loss:   1.368791\n",
      "train loss:   1.029345\n",
      "train loss:   1.152855\n",
      "train loss:   0.930510\n",
      "train loss:   0.948307\n",
      "train loss:   0.925158\n",
      "train loss:   1.227220\n",
      "train loss:   0.857765\n",
      "train loss:   1.056398\n",
      "train loss:   1.263948\n",
      "train loss:   1.246106\n",
      "train loss:   1.059683\n",
      "train loss:   1.309120\n",
      "train loss:   1.315913\n",
      "train loss:   1.126368\n",
      "train loss:   1.063198\n",
      "train loss:   0.926710\n",
      "train loss:   1.309300\n",
      "train loss:   0.929015\n",
      "train loss:   1.106155\n",
      "train loss:   1.035678\n",
      "train loss:   1.166087\n",
      "train loss:   0.926315\n",
      "train loss:   1.162868\n",
      "train loss:   1.039036\n",
      "train loss:   1.313534\n",
      "train loss:   1.057666\n",
      "train loss:   0.902342\n",
      "train loss:   0.999642\n",
      "train loss:   0.986317\n",
      "train loss:   1.188236\n",
      "train loss:   1.112726\n",
      "train loss:   1.319341\n",
      "train loss:   1.176722\n",
      "train loss:   1.175829\n",
      "########### epoch 38 ###########\n",
      "########### loop 5300 ###########\n",
      "test loss:   1.217091   test accuracy:   0.656250\n",
      "########### loop 5300 ###########\n",
      "train loss:   1.014616\n",
      "train loss:   1.083184\n",
      "train loss:   1.287800\n",
      "train loss:   0.759123\n",
      "train loss:   1.121968\n",
      "train loss:   1.031629\n",
      "train loss:   1.018339\n",
      "train loss:   0.881861\n",
      "train loss:   1.229692\n",
      "train loss:   1.137037\n",
      "train loss:   1.042167\n",
      "train loss:   1.243472\n",
      "train loss:   1.146391\n",
      "train loss:   0.862024\n",
      "train loss:   0.923308\n",
      "train loss:   1.009634\n",
      "train loss:   1.062318\n",
      "train loss:   1.154057\n",
      "train loss:   0.767480\n",
      "train loss:   1.118283\n",
      "train loss:   0.957715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.361065\n",
      "train loss:   0.886254\n",
      "train loss:   0.990448\n",
      "train loss:   1.131378\n",
      "train loss:   1.168398\n",
      "train loss:   1.370040\n",
      "train loss:   1.135641\n",
      "train loss:   0.933876\n",
      "train loss:   1.131884\n",
      "train loss:   1.487707\n",
      "train loss:   0.956136\n",
      "train loss:   1.274007\n",
      "train loss:   1.013443\n",
      "train loss:   0.867978\n",
      "train loss:   1.020815\n",
      "train loss:   1.032540\n",
      "train loss:   0.848518\n",
      "train loss:   1.158189\n",
      "train loss:   1.087155\n",
      "train loss:   1.036911\n",
      "train loss:   0.884211\n",
      "train loss:   0.754513\n",
      "train loss:   0.810296\n",
      "train loss:   0.910798\n",
      "train loss:   1.045827\n",
      "train loss:   1.047979\n",
      "train loss:   1.162715\n",
      "train loss:   1.150102\n",
      "train loss:   1.079692\n",
      "########### epoch 38 ###########\n",
      "########### loop 5350 ###########\n",
      "test loss:   1.088308   test accuracy:   0.656250\n",
      "########### loop 5350 ###########\n",
      "train loss:   1.241679\n",
      "train loss:   1.335106\n",
      "train loss:   1.222329\n",
      "train loss:   1.216973\n",
      "train loss:   0.899861\n",
      "train loss:   1.026035\n",
      "train loss:   1.070646\n",
      "train loss:   0.975721\n",
      "train loss:   0.901235\n",
      "train loss:   1.074742\n",
      "train loss:   1.001582\n",
      "train loss:   0.914181\n",
      "train loss:   1.007052\n",
      "train loss:   1.086333\n",
      "train loss:   0.932430\n",
      "train loss:   0.749941\n",
      "train loss:   0.948194\n",
      "train loss:   0.972219\n",
      "train loss:   0.982119\n",
      "train loss:   1.029451\n",
      "train loss:   1.003667\n",
      "train loss:   0.993861\n",
      "train loss:   0.920057\n",
      "train loss:   0.825189\n",
      "train loss:   0.781893\n",
      "train loss:   1.180804\n",
      "train loss:   1.226929\n",
      "train loss:   1.051656\n",
      "train loss:   0.851709\n",
      "train loss:   1.018217\n",
      "train loss:   0.813510\n",
      "train loss:   0.845861\n",
      "train loss:   0.926784\n",
      "train loss:   1.009912\n",
      "train loss:   0.961436\n",
      "train loss:   1.262684\n",
      "train loss:   1.266619\n",
      "train loss:   0.822793\n",
      "train loss:   1.219215\n",
      "train loss:   1.171353\n",
      "train loss:   1.166595\n",
      "train loss:   1.134559\n",
      "train loss:   0.982193\n",
      "train loss:   1.086769\n",
      "train loss:   1.050957\n",
      "train loss:   1.089111\n",
      "train loss:   1.223852\n",
      "train loss:   0.970083\n",
      "train loss:   1.199699\n",
      "train loss:   1.007820\n",
      "########### epoch 39 ###########\n",
      "########### loop 5400 ###########\n",
      "test loss:   1.157018   test accuracy:   0.750000\n",
      "########### loop 5400 ###########\n",
      "train loss:   1.042051\n",
      "train loss:   1.114931\n",
      "train loss:   1.183930\n",
      "train loss:   0.999974\n",
      "train loss:   0.930388\n",
      "train loss:   1.141652\n",
      "train loss:   1.338047\n",
      "train loss:   0.990801\n",
      "train loss:   1.092938\n",
      "train loss:   0.960560\n",
      "train loss:   1.023079\n",
      "train loss:   0.954912\n",
      "train loss:   1.075234\n",
      "train loss:   0.863392\n",
      "train loss:   1.059482\n",
      "train loss:   1.285445\n",
      "train loss:   1.168516\n",
      "train loss:   1.017024\n",
      "train loss:   1.211998\n",
      "train loss:   1.165789\n",
      "train loss:   1.233350\n",
      "train loss:   1.115530\n",
      "train loss:   0.857765\n",
      "train loss:   1.352881\n",
      "train loss:   1.008561\n",
      "train loss:   1.062577\n",
      "train loss:   1.089614\n",
      "train loss:   1.235675\n",
      "train loss:   0.903188\n",
      "train loss:   1.018331\n",
      "train loss:   0.928558\n",
      "train loss:   1.147699\n",
      "train loss:   0.922891\n",
      "train loss:   0.891382\n",
      "train loss:   0.957580\n",
      "train loss:   0.999253\n",
      "train loss:   1.097935\n",
      "train loss:   0.909915\n",
      "train loss:   1.036044\n",
      "train loss:   1.007785\n",
      "train loss:   0.950555\n",
      "train loss:   1.020829\n",
      "train loss:   1.277376\n",
      "train loss:   1.155682\n",
      "train loss:   0.781617\n",
      "train loss:   1.200986\n",
      "train loss:   1.011741\n",
      "train loss:   0.927734\n",
      "train loss:   1.046329\n",
      "train loss:   1.175807\n",
      "########### epoch 39 ###########\n",
      "########### loop 5450 ###########\n",
      "test loss:   1.182894   test accuracy:   0.750000\n",
      "########### loop 5450 ###########\n",
      "train loss:   1.133864\n",
      "train loss:   1.201441\n",
      "train loss:   1.176558\n",
      "train loss:   1.086089\n",
      "train loss:   0.756123\n",
      "train loss:   0.925185\n",
      "train loss:   1.022022\n",
      "train loss:   1.037221\n",
      "train loss:   1.187872\n",
      "train loss:   0.688064\n",
      "train loss:   1.036597\n",
      "train loss:   0.879581\n",
      "train loss:   1.103633\n",
      "train loss:   0.888255\n",
      "train loss:   1.075392\n",
      "train loss:   1.208233\n",
      "train loss:   1.107912\n",
      "train loss:   1.194999\n",
      "train loss:   1.056009\n",
      "train loss:   0.761203\n",
      "train loss:   1.163072\n",
      "train loss:   1.172847\n",
      "train loss:   0.950287\n",
      "train loss:   1.279811\n",
      "train loss:   1.039201\n",
      "train loss:   0.821564\n",
      "train loss:   1.033850\n",
      "train loss:   1.035367\n",
      "train loss:   0.791507\n",
      "train loss:   1.111015\n",
      "train loss:   0.949911\n",
      "train loss:   0.972503\n",
      "train loss:   0.938112\n",
      "train loss:   0.731257\n",
      "train loss:   0.869308\n",
      "train loss:   0.818594\n",
      "train loss:   1.059735\n",
      "train loss:   1.133450\n",
      "train loss:   1.152133\n",
      "train loss:   1.064272\n",
      "train loss:   1.073647\n",
      "train loss:   1.156268\n",
      "train loss:   1.313803\n",
      "train loss:   1.221404\n",
      "train loss:   1.061047\n",
      "train loss:   0.882030\n",
      "train loss:   0.910782\n",
      "train loss:   1.115499\n",
      "train loss:   0.911715\n",
      "train loss:   0.880932\n",
      "########### epoch 40 ###########\n",
      "########### loop 5500 ###########\n",
      "test loss:   1.183407   test accuracy:   0.656250\n",
      "########### loop 5500 ###########\n",
      "train loss:   1.031203\n",
      "train loss:   1.031585\n",
      "train loss:   0.962808\n",
      "train loss:   1.093255\n",
      "train loss:   1.031594\n",
      "train loss:   1.141910\n",
      "train loss:   0.789585\n",
      "train loss:   0.971217\n",
      "train loss:   0.993061\n",
      "train loss:   1.038128\n",
      "train loss:   1.048868\n",
      "train loss:   1.049872\n",
      "train loss:   1.039634\n",
      "train loss:   0.894981\n",
      "train loss:   0.856526\n",
      "train loss:   0.801042\n",
      "train loss:   1.289160\n",
      "train loss:   1.178082\n",
      "train loss:   0.975584\n",
      "train loss:   0.929033\n",
      "train loss:   1.098832\n",
      "train loss:   0.881242\n",
      "train loss:   0.930847\n",
      "train loss:   0.980414\n",
      "train loss:   0.965724\n",
      "train loss:   0.845310\n",
      "train loss:   1.282253\n",
      "train loss:   1.287788\n",
      "train loss:   0.805439\n",
      "train loss:   1.225661\n",
      "train loss:   1.215735\n",
      "train loss:   1.114821\n",
      "train loss:   1.153330\n",
      "train loss:   1.055098\n",
      "train loss:   1.193856\n",
      "train loss:   1.035637\n",
      "train loss:   1.085786\n",
      "train loss:   1.224176\n",
      "train loss:   0.977740\n",
      "train loss:   1.185101\n",
      "train loss:   0.960406\n",
      "train loss:   1.023255\n",
      "train loss:   1.127259\n",
      "train loss:   1.100670\n",
      "train loss:   1.028026\n",
      "train loss:   0.906970\n",
      "train loss:   1.183804\n",
      "train loss:   1.367423\n",
      "train loss:   0.927899\n",
      "train loss:   1.225397\n",
      "########### epoch 40 ###########\n",
      "########### loop 5550 ###########\n",
      "test loss:   1.383407   test accuracy:   0.562500\n",
      "########### loop 5550 ###########\n",
      "train loss:   0.882712\n",
      "train loss:   0.944622\n",
      "train loss:   0.900709\n",
      "train loss:   1.039810\n",
      "train loss:   0.839156\n",
      "train loss:   1.055295\n",
      "train loss:   1.288679\n",
      "train loss:   1.183020\n",
      "train loss:   1.104058\n",
      "train loss:   1.358135\n",
      "train loss:   1.061513\n",
      "train loss:   1.148539\n",
      "train loss:   1.126242\n",
      "train loss:   0.875242\n",
      "train loss:   1.197373\n",
      "train loss:   0.899008\n",
      "train loss:   0.993837\n",
      "train loss:   1.044147\n",
      "train loss:   1.248780\n",
      "train loss:   0.875461\n",
      "train loss:   1.047216\n",
      "train loss:   0.998137\n",
      "train loss:   1.195206\n",
      "train loss:   1.013792\n",
      "train loss:   0.849621\n",
      "train loss:   0.906241\n",
      "train loss:   1.011997\n",
      "train loss:   1.123854\n",
      "train loss:   0.966019\n",
      "train loss:   1.000347\n",
      "train loss:   1.083537\n",
      "train loss:   1.006121\n",
      "train loss:   0.929913\n",
      "train loss:   1.112791\n",
      "train loss:   1.200994\n",
      "train loss:   0.904505\n",
      "train loss:   1.116687\n",
      "train loss:   0.985606\n",
      "train loss:   0.897966\n",
      "train loss:   0.951509\n",
      "train loss:   1.220780\n",
      "train loss:   1.117238\n",
      "train loss:   1.080687\n",
      "train loss:   1.160847\n",
      "train loss:   1.115492\n",
      "train loss:   0.745446\n",
      "train loss:   0.868926\n",
      "train loss:   1.009501\n",
      "train loss:   1.100579\n",
      "train loss:   1.247633\n",
      "########### epoch 40 ###########\n",
      "########### loop 5600 ###########\n",
      "test loss:   1.311825   test accuracy:   0.562500\n",
      "########### loop 5600 ###########\n",
      "train loss:   0.725313\n",
      "train loss:   1.068854\n",
      "train loss:   0.903621\n",
      "train loss:   1.244969\n",
      "train loss:   0.872617\n",
      "train loss:   1.094149\n",
      "train loss:   1.177649\n",
      "train loss:   1.191096\n",
      "train loss:   1.202255\n",
      "train loss:   1.116894\n",
      "train loss:   0.797686\n",
      "train loss:   1.291827\n",
      "train loss:   1.558471\n",
      "train loss:   1.092080\n",
      "train loss:   1.547646\n",
      "train loss:   1.323601\n",
      "train loss:   1.101849\n",
      "train loss:   1.137394\n",
      "train loss:   1.023142\n",
      "train loss:   0.959041\n",
      "train loss:   1.327938\n",
      "train loss:   1.329224\n",
      "train loss:   1.423905\n",
      "train loss:   1.016973\n",
      "train loss:   0.913372\n",
      "train loss:   0.896119\n",
      "train loss:   1.008968\n",
      "train loss:   1.056019\n",
      "train loss:   1.194629\n",
      "train loss:   1.277257\n",
      "train loss:   1.388304\n",
      "train loss:   1.304407\n",
      "train loss:   1.321883\n",
      "train loss:   1.498381\n",
      "train loss:   1.425745\n",
      "train loss:   1.219510\n",
      "train loss:   0.925786\n",
      "train loss:   0.969865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.046719\n",
      "train loss:   0.933847\n",
      "train loss:   0.880104\n",
      "train loss:   1.051723\n",
      "train loss:   1.046502\n",
      "train loss:   0.876217\n",
      "train loss:   1.130427\n",
      "train loss:   0.917036\n",
      "train loss:   1.081050\n",
      "train loss:   0.746501\n",
      "train loss:   1.007907\n",
      "train loss:   0.989419\n",
      "########### epoch 41 ###########\n",
      "########### loop 5650 ###########\n",
      "test loss:   1.152499   test accuracy:   0.656250\n",
      "########### loop 5650 ###########\n",
      "train loss:   0.993085\n",
      "train loss:   0.972416\n",
      "train loss:   1.064806\n",
      "train loss:   0.925182\n",
      "train loss:   0.866962\n",
      "train loss:   0.793125\n",
      "train loss:   0.831015\n",
      "train loss:   1.193813\n",
      "train loss:   1.105921\n",
      "train loss:   0.979159\n",
      "train loss:   0.898869\n",
      "train loss:   1.063449\n",
      "train loss:   0.828761\n",
      "train loss:   0.868173\n",
      "train loss:   0.924512\n",
      "train loss:   1.151483\n",
      "train loss:   0.891134\n",
      "train loss:   1.200025\n",
      "train loss:   1.274432\n",
      "train loss:   0.771992\n",
      "train loss:   1.039882\n",
      "train loss:   1.216772\n",
      "train loss:   1.124282\n",
      "train loss:   1.262925\n",
      "train loss:   1.019615\n",
      "train loss:   1.109037\n",
      "train loss:   1.020571\n",
      "train loss:   1.076276\n",
      "train loss:   1.275262\n",
      "train loss:   0.979898\n",
      "train loss:   1.151385\n",
      "train loss:   0.994914\n",
      "train loss:   1.040623\n",
      "train loss:   1.086006\n",
      "train loss:   1.156924\n",
      "train loss:   1.145093\n",
      "train loss:   1.048835\n",
      "train loss:   1.116793\n",
      "train loss:   1.255226\n",
      "train loss:   0.984442\n",
      "train loss:   1.095228\n",
      "train loss:   0.886985\n",
      "train loss:   0.944407\n",
      "train loss:   0.826122\n",
      "train loss:   1.161997\n",
      "train loss:   0.843626\n",
      "train loss:   1.017638\n",
      "train loss:   1.249676\n",
      "train loss:   1.219771\n",
      "train loss:   1.105888\n",
      "########### epoch 41 ###########\n",
      "########### loop 5700 ###########\n",
      "test loss:   1.275319   test accuracy:   0.531250\n",
      "########### loop 5700 ###########\n",
      "train loss:   1.351156\n",
      "train loss:   1.247630\n",
      "train loss:   1.258379\n",
      "train loss:   1.210813\n",
      "train loss:   0.978724\n",
      "train loss:   1.282461\n",
      "train loss:   1.042986\n",
      "train loss:   1.148769\n",
      "train loss:   1.088730\n",
      "train loss:   1.224245\n",
      "train loss:   0.957718\n",
      "train loss:   0.946821\n",
      "train loss:   0.911974\n",
      "train loss:   1.181629\n",
      "train loss:   1.036644\n",
      "train loss:   0.967362\n",
      "train loss:   1.037401\n",
      "train loss:   1.002039\n",
      "train loss:   1.188665\n",
      "train loss:   0.924457\n",
      "train loss:   1.127063\n",
      "train loss:   1.304862\n",
      "train loss:   1.109847\n",
      "train loss:   1.055578\n",
      "train loss:   1.378259\n",
      "train loss:   1.142972\n",
      "train loss:   0.770339\n",
      "train loss:   1.142022\n",
      "train loss:   0.936364\n",
      "train loss:   0.912933\n",
      "train loss:   1.042420\n",
      "train loss:   1.201945\n",
      "train loss:   1.142795\n",
      "train loss:   1.013737\n",
      "train loss:   1.089366\n",
      "train loss:   1.192222\n",
      "train loss:   0.847726\n",
      "train loss:   0.926168\n",
      "train loss:   1.029483\n",
      "train loss:   1.065628\n",
      "train loss:   1.253382\n",
      "train loss:   0.710837\n",
      "train loss:   1.052311\n",
      "train loss:   0.989737\n",
      "train loss:   0.941327\n",
      "train loss:   0.827026\n",
      "train loss:   0.927581\n",
      "train loss:   1.156344\n",
      "train loss:   1.172189\n",
      "train loss:   1.145474\n",
      "########### epoch 41 ###########\n",
      "########### loop 5750 ###########\n",
      "test loss:   1.001099   test accuracy:   0.750000\n",
      "########### loop 5750 ###########\n",
      "train loss:   1.067727\n",
      "train loss:   0.768684\n",
      "train loss:   1.161462\n",
      "train loss:   1.237103\n",
      "train loss:   0.959608\n",
      "train loss:   1.240865\n",
      "train loss:   1.092170\n",
      "train loss:   0.863093\n",
      "train loss:   1.112730\n",
      "train loss:   0.973407\n",
      "train loss:   0.968683\n",
      "train loss:   1.261860\n",
      "train loss:   1.118038\n",
      "train loss:   0.975433\n",
      "train loss:   0.895517\n",
      "train loss:   0.740331\n",
      "train loss:   0.801178\n",
      "train loss:   0.826658\n",
      "train loss:   0.995641\n",
      "train loss:   1.124635\n",
      "train loss:   1.176158\n",
      "train loss:   1.075877\n",
      "train loss:   1.106878\n",
      "train loss:   1.187749\n",
      "train loss:   1.167579\n",
      "train loss:   1.162309\n",
      "train loss:   1.101486\n",
      "train loss:   0.879330\n",
      "train loss:   0.954849\n",
      "train loss:   1.074800\n",
      "train loss:   0.833261\n",
      "train loss:   0.858413\n",
      "train loss:   1.057136\n",
      "train loss:   1.097560\n",
      "train loss:   0.967511\n",
      "train loss:   1.172005\n",
      "train loss:   0.957464\n",
      "train loss:   1.092087\n",
      "train loss:   0.713389\n",
      "train loss:   0.948264\n",
      "train loss:   0.797560\n",
      "train loss:   0.975453\n",
      "train loss:   0.939888\n",
      "train loss:   1.013921\n",
      "train loss:   0.999073\n",
      "train loss:   0.990248\n",
      "train loss:   0.841192\n",
      "train loss:   0.823259\n",
      "train loss:   1.225547\n",
      "train loss:   1.229434\n",
      "########### epoch 42 ###########\n",
      "########### loop 5800 ###########\n",
      "test loss:   1.113272   test accuracy:   0.718750\n",
      "########### loop 5800 ###########\n",
      "train loss:   1.067009\n",
      "train loss:   0.866967\n",
      "train loss:   1.112335\n",
      "train loss:   0.814222\n",
      "train loss:   0.822340\n",
      "train loss:   0.909538\n",
      "train loss:   0.997842\n",
      "train loss:   0.819248\n",
      "train loss:   1.298573\n",
      "train loss:   1.283314\n",
      "train loss:   0.762715\n",
      "train loss:   1.029932\n",
      "train loss:   1.148790\n",
      "train loss:   1.120362\n",
      "train loss:   1.197257\n",
      "train loss:   1.015296\n",
      "train loss:   1.158641\n",
      "train loss:   0.973392\n",
      "train loss:   1.040804\n",
      "train loss:   1.193283\n",
      "train loss:   0.948140\n",
      "train loss:   1.098890\n",
      "train loss:   0.968210\n",
      "train loss:   1.020383\n",
      "train loss:   1.033474\n",
      "train loss:   1.111507\n",
      "train loss:   1.064404\n",
      "train loss:   0.823541\n",
      "train loss:   1.033819\n",
      "train loss:   1.290329\n",
      "train loss:   0.977906\n",
      "train loss:   1.140582\n",
      "train loss:   0.915661\n",
      "train loss:   0.963887\n",
      "train loss:   0.814079\n",
      "train loss:   1.216715\n",
      "train loss:   0.887783\n",
      "train loss:   0.987370\n",
      "train loss:   1.333956\n",
      "train loss:   1.172440\n",
      "train loss:   1.078171\n",
      "train loss:   1.236562\n",
      "train loss:   1.176718\n",
      "train loss:   1.155628\n",
      "train loss:   1.099086\n",
      "train loss:   0.936323\n",
      "train loss:   1.566041\n",
      "train loss:   1.030563\n",
      "train loss:   1.060509\n",
      "train loss:   1.038616\n",
      "########### epoch 42 ###########\n",
      "########### loop 5850 ###########\n",
      "test loss:   1.297284   test accuracy:   0.656250\n",
      "########### loop 5850 ###########\n",
      "train loss:   1.216378\n",
      "train loss:   0.863262\n",
      "train loss:   1.105143\n",
      "train loss:   1.009053\n",
      "train loss:   1.229829\n",
      "train loss:   0.982991\n",
      "train loss:   0.947115\n",
      "train loss:   1.033917\n",
      "train loss:   0.911595\n",
      "train loss:   1.098272\n",
      "train loss:   0.886442\n",
      "train loss:   1.015811\n",
      "train loss:   0.994817\n",
      "train loss:   1.030360\n",
      "train loss:   1.017677\n",
      "train loss:   1.221790\n",
      "train loss:   1.159401\n",
      "train loss:   0.753531\n",
      "train loss:   1.162951\n",
      "train loss:   0.972513\n",
      "train loss:   0.943283\n",
      "train loss:   0.916810\n",
      "train loss:   1.185787\n",
      "train loss:   1.146143\n",
      "train loss:   1.000313\n",
      "train loss:   1.023912\n",
      "train loss:   1.090697\n",
      "train loss:   0.809267\n",
      "train loss:   0.933863\n",
      "train loss:   1.085996\n",
      "train loss:   1.047876\n",
      "train loss:   1.202463\n",
      "train loss:   0.683977\n",
      "train loss:   1.051726\n",
      "train loss:   1.018420\n",
      "train loss:   1.019555\n",
      "train loss:   0.913921\n",
      "train loss:   0.912407\n",
      "train loss:   1.125837\n",
      "train loss:   1.197141\n",
      "train loss:   1.236117\n",
      "train loss:   1.065310\n",
      "train loss:   0.758905\n",
      "train loss:   1.053272\n",
      "train loss:   1.144757\n",
      "train loss:   0.918496\n",
      "train loss:   1.283791\n",
      "train loss:   0.962271\n",
      "train loss:   0.818704\n",
      "train loss:   1.045430\n",
      "########### epoch 42 ###########\n",
      "########### loop 5900 ###########\n",
      "test loss:   1.120676   test accuracy:   0.687500\n",
      "########### loop 5900 ###########\n",
      "train loss:   0.979328\n",
      "train loss:   0.808473\n",
      "train loss:   1.148263\n",
      "train loss:   0.987431\n",
      "train loss:   0.990632\n",
      "train loss:   0.860916\n",
      "train loss:   0.729135\n",
      "train loss:   0.805165\n",
      "train loss:   0.959957\n",
      "train loss:   0.996652\n",
      "train loss:   1.056311\n",
      "train loss:   1.085607\n",
      "train loss:   1.066097\n",
      "train loss:   1.054568\n",
      "train loss:   1.112552\n",
      "train loss:   1.185468\n",
      "train loss:   1.139798\n",
      "train loss:   0.999698\n",
      "train loss:   0.864451\n",
      "train loss:   0.896342\n",
      "train loss:   1.071950\n",
      "train loss:   0.891144\n",
      "train loss:   0.789783\n",
      "train loss:   0.981200\n",
      "train loss:   1.025954\n",
      "train loss:   0.979890\n",
      "train loss:   1.142054\n",
      "train loss:   0.976469\n",
      "train loss:   1.148103\n",
      "train loss:   0.772910\n",
      "train loss:   0.927388\n",
      "train loss:   1.083711\n",
      "train loss:   1.019354\n",
      "train loss:   0.936872\n",
      "train loss:   0.925589\n",
      "train loss:   0.940928\n",
      "train loss:   0.940500\n",
      "train loss:   0.851675\n",
      "train loss:   0.770227\n",
      "train loss:   1.334658\n",
      "train loss:   1.230863\n",
      "train loss:   1.124211\n",
      "train loss:   0.973264\n",
      "train loss:   1.166261\n",
      "train loss:   0.974505\n",
      "train loss:   0.870202\n",
      "train loss:   0.899705\n",
      "train loss:   1.124164\n",
      "train loss:   0.887402\n",
      "train loss:   1.265949\n",
      "########### epoch 43 ###########\n",
      "########### loop 5950 ###########\n",
      "test loss:   1.118413   test accuracy:   0.593750\n",
      "########### loop 5950 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.330807\n",
      "train loss:   0.766654\n",
      "train loss:   1.080571\n",
      "train loss:   1.161346\n",
      "train loss:   1.121749\n",
      "train loss:   1.022264\n",
      "train loss:   0.975457\n",
      "train loss:   1.081824\n",
      "train loss:   0.968406\n",
      "train loss:   1.080095\n",
      "train loss:   1.115038\n",
      "train loss:   0.884869\n",
      "train loss:   1.118079\n",
      "train loss:   0.969159\n",
      "train loss:   1.030005\n",
      "train loss:   1.018822\n",
      "train loss:   1.167848\n",
      "train loss:   1.023306\n",
      "train loss:   0.837923\n",
      "train loss:   1.052520\n",
      "train loss:   1.270187\n",
      "train loss:   1.069423\n",
      "train loss:   1.168351\n",
      "train loss:   0.768723\n",
      "train loss:   0.876875\n",
      "train loss:   0.804059\n",
      "train loss:   1.047686\n",
      "train loss:   0.790476\n",
      "train loss:   1.043147\n",
      "train loss:   1.173326\n",
      "train loss:   1.186060\n",
      "train loss:   0.949485\n",
      "train loss:   1.161660\n",
      "train loss:   1.086540\n",
      "train loss:   1.012028\n",
      "train loss:   1.024881\n",
      "train loss:   0.729683\n",
      "train loss:   1.246740\n",
      "train loss:   0.934347\n",
      "train loss:   1.098209\n",
      "train loss:   1.071903\n",
      "train loss:   1.241468\n",
      "train loss:   0.835934\n",
      "train loss:   1.085959\n",
      "train loss:   0.964320\n",
      "train loss:   1.128568\n",
      "train loss:   0.927796\n",
      "train loss:   0.904796\n",
      "train loss:   0.963212\n",
      "train loss:   0.853885\n",
      "########### epoch 43 ###########\n",
      "########### loop 6000 ###########\n",
      "test loss:   1.219004   test accuracy:   0.531250\n",
      "########### loop 6000 ###########\n",
      "train loss:   1.143932\n",
      "train loss:   0.947509\n",
      "train loss:   1.099455\n",
      "train loss:   1.107547\n",
      "train loss:   1.029117\n",
      "train loss:   0.960788\n",
      "train loss:   1.151495\n",
      "train loss:   1.095291\n",
      "train loss:   0.793195\n",
      "train loss:   1.145436\n",
      "train loss:   0.904380\n",
      "train loss:   0.845604\n",
      "train loss:   0.927498\n",
      "train loss:   1.113045\n",
      "train loss:   1.059038\n",
      "train loss:   1.037089\n",
      "train loss:   1.096727\n",
      "train loss:   1.067418\n",
      "train loss:   0.697473\n",
      "train loss:   0.893411\n",
      "train loss:   1.056086\n",
      "train loss:   1.027462\n",
      "train loss:   1.238710\n",
      "train loss:   0.668195\n",
      "train loss:   1.065928\n",
      "train loss:   0.919793\n",
      "train loss:   1.202201\n",
      "train loss:   0.810664\n",
      "train loss:   0.922789\n",
      "train loss:   1.029099\n",
      "train loss:   1.075302\n",
      "train loss:   1.206320\n",
      "train loss:   1.042954\n",
      "train loss:   0.771072\n",
      "train loss:   1.073524\n",
      "train loss:   1.211597\n",
      "train loss:   0.886872\n",
      "train loss:   1.178082\n",
      "train loss:   0.979808\n",
      "train loss:   0.826939\n",
      "train loss:   0.957755\n",
      "train loss:   0.934708\n",
      "train loss:   0.884586\n",
      "train loss:   1.254559\n",
      "train loss:   1.068231\n",
      "train loss:   1.029429\n",
      "train loss:   0.866716\n",
      "train loss:   0.733937\n",
      "train loss:   0.711458\n",
      "train loss:   0.910965\n",
      "########### epoch 43 ###########\n",
      "########### loop 6050 ###########\n",
      "test loss:   1.365743   test accuracy:   0.562500\n",
      "########### loop 6050 ###########\n",
      "train loss:   1.019541\n",
      "train loss:   1.083294\n",
      "train loss:   1.126364\n",
      "train loss:   1.026009\n",
      "train loss:   1.018257\n",
      "train loss:   1.327385\n",
      "train loss:   1.258597\n",
      "train loss:   1.272974\n",
      "train loss:   1.054818\n",
      "train loss:   0.832818\n",
      "train loss:   0.895494\n",
      "train loss:   1.161341\n",
      "train loss:   0.941331\n",
      "train loss:   1.039682\n",
      "train loss:   1.081835\n",
      "train loss:   1.014254\n",
      "train loss:   0.874211\n",
      "train loss:   1.031152\n",
      "train loss:   1.014966\n",
      "train loss:   1.224118\n",
      "train loss:   0.731290\n",
      "train loss:   0.964944\n",
      "train loss:   0.925572\n",
      "train loss:   0.918333\n",
      "train loss:   0.974691\n",
      "train loss:   0.916175\n",
      "train loss:   0.972881\n",
      "train loss:   0.969736\n",
      "train loss:   0.776489\n",
      "train loss:   0.736980\n",
      "train loss:   1.299366\n",
      "train loss:   1.175744\n",
      "train loss:   1.161330\n",
      "train loss:   1.040469\n",
      "train loss:   1.136817\n",
      "train loss:   0.923190\n",
      "train loss:   0.790342\n",
      "train loss:   0.878347\n",
      "train loss:   0.995621\n",
      "train loss:   0.832193\n",
      "train loss:   1.222864\n",
      "train loss:   1.224640\n",
      "train loss:   0.860132\n",
      "train loss:   1.012720\n",
      "train loss:   1.199260\n",
      "train loss:   1.064753\n",
      "train loss:   1.086765\n",
      "train loss:   0.957658\n",
      "train loss:   1.080842\n",
      "train loss:   0.956128\n",
      "########### epoch 44 ###########\n",
      "########### loop 6100 ###########\n",
      "test loss:   0.986643   test accuracy:   0.625000\n",
      "########### loop 6100 ###########\n",
      "train loss:   1.035367\n",
      "train loss:   1.221841\n",
      "train loss:   0.928224\n",
      "train loss:   1.067952\n",
      "train loss:   0.956568\n",
      "train loss:   1.021616\n",
      "train loss:   1.087100\n",
      "train loss:   1.092512\n",
      "train loss:   1.020314\n",
      "train loss:   0.818478\n",
      "train loss:   1.029951\n",
      "train loss:   1.306656\n",
      "train loss:   0.932169\n",
      "train loss:   1.023541\n",
      "train loss:   0.919839\n",
      "train loss:   0.932319\n",
      "train loss:   0.809503\n",
      "train loss:   1.074659\n",
      "train loss:   0.828729\n",
      "train loss:   1.073837\n",
      "train loss:   1.226166\n",
      "train loss:   1.150615\n",
      "train loss:   0.999950\n",
      "train loss:   1.260810\n",
      "train loss:   1.059330\n",
      "train loss:   1.053411\n",
      "train loss:   1.087898\n",
      "train loss:   0.765069\n",
      "train loss:   1.108157\n",
      "train loss:   0.845373\n",
      "train loss:   1.022220\n",
      "train loss:   0.978222\n",
      "train loss:   1.130953\n",
      "train loss:   0.805981\n",
      "train loss:   0.993515\n",
      "train loss:   0.952832\n",
      "train loss:   1.150258\n",
      "train loss:   0.933264\n",
      "train loss:   0.862528\n",
      "train loss:   0.959258\n",
      "train loss:   0.941160\n",
      "train loss:   1.092976\n",
      "train loss:   0.853565\n",
      "train loss:   1.049453\n",
      "train loss:   1.027228\n",
      "train loss:   0.937325\n",
      "train loss:   1.009767\n",
      "train loss:   1.137702\n",
      "train loss:   1.146513\n",
      "train loss:   0.796703\n",
      "########### epoch 44 ###########\n",
      "########### loop 6150 ###########\n",
      "test loss:   1.088502   test accuracy:   0.625000\n",
      "########### loop 6150 ###########\n",
      "train loss:   1.174084\n",
      "train loss:   0.992511\n",
      "train loss:   0.922315\n",
      "train loss:   0.897348\n",
      "train loss:   1.155387\n",
      "train loss:   1.089763\n",
      "train loss:   1.024179\n",
      "train loss:   1.025772\n",
      "train loss:   1.092319\n",
      "train loss:   0.800613\n",
      "train loss:   0.872777\n",
      "train loss:   0.923983\n",
      "train loss:   0.943870\n",
      "train loss:   1.196604\n",
      "train loss:   0.717574\n",
      "train loss:   0.984455\n",
      "train loss:   0.917862\n",
      "train loss:   1.143096\n",
      "train loss:   0.886027\n",
      "train loss:   0.899543\n",
      "train loss:   1.066776\n",
      "train loss:   1.021692\n",
      "train loss:   1.212603\n",
      "train loss:   1.128789\n",
      "train loss:   0.744130\n",
      "train loss:   1.199853\n",
      "train loss:   1.214995\n",
      "train loss:   0.931429\n",
      "train loss:   1.219420\n",
      "train loss:   0.946929\n",
      "train loss:   0.816606\n",
      "train loss:   1.042290\n",
      "train loss:   0.975672\n",
      "train loss:   0.849301\n",
      "train loss:   1.156511\n",
      "train loss:   1.055319\n",
      "train loss:   0.963678\n",
      "train loss:   0.896327\n",
      "train loss:   0.712259\n",
      "train loss:   0.702371\n",
      "train loss:   0.935931\n",
      "train loss:   0.958131\n",
      "train loss:   1.050893\n",
      "train loss:   1.104464\n",
      "train loss:   0.985023\n",
      "train loss:   1.014109\n",
      "train loss:   1.155465\n",
      "train loss:   1.097213\n",
      "train loss:   1.140707\n",
      "train loss:   0.999341\n",
      "########### epoch 44 ###########\n",
      "########### loop 6200 ###########\n",
      "test loss:   1.088978   test accuracy:   0.687500\n",
      "########### loop 6200 ###########\n",
      "train loss:   0.877512\n",
      "train loss:   0.917493\n",
      "train loss:   1.105505\n",
      "train loss:   0.876438\n",
      "train loss:   0.881041\n",
      "train loss:   0.943743\n",
      "train loss:   0.939416\n",
      "train loss:   1.022969\n",
      "train loss:   1.207050\n",
      "train loss:   1.167034\n",
      "train loss:   1.185744\n",
      "train loss:   0.737487\n",
      "train loss:   0.928502\n",
      "train loss:   0.970008\n",
      "train loss:   0.955170\n",
      "train loss:   0.953727\n",
      "train loss:   0.935461\n",
      "train loss:   1.031021\n",
      "train loss:   0.986167\n",
      "train loss:   0.893420\n",
      "train loss:   0.829473\n",
      "train loss:   1.110333\n",
      "train loss:   1.163965\n",
      "train loss:   1.064027\n",
      "train loss:   0.919036\n",
      "train loss:   1.025342\n",
      "train loss:   0.889202\n",
      "train loss:   0.866639\n",
      "train loss:   0.926554\n",
      "train loss:   0.849975\n",
      "train loss:   0.938532\n",
      "train loss:   1.215664\n",
      "train loss:   1.240737\n",
      "train loss:   0.819639\n",
      "train loss:   1.210355\n",
      "train loss:   1.285467\n",
      "train loss:   1.123491\n",
      "train loss:   1.101268\n",
      "train loss:   0.963636\n",
      "train loss:   1.047537\n",
      "train loss:   0.953923\n",
      "train loss:   1.025548\n",
      "train loss:   1.234209\n",
      "train loss:   0.969578\n",
      "train loss:   1.170543\n",
      "train loss:   0.993579\n",
      "train loss:   1.022470\n",
      "train loss:   1.071302\n",
      "train loss:   1.156306\n",
      "train loss:   1.031587\n",
      "########### epoch 45 ###########\n",
      "########### loop 6250 ###########\n",
      "test loss:   1.114624   test accuracy:   0.656250\n",
      "########### loop 6250 ###########\n",
      "train loss:   0.897590\n",
      "train loss:   1.052653\n",
      "train loss:   1.288494\n",
      "train loss:   0.990203\n",
      "train loss:   1.139434\n",
      "train loss:   1.024130\n",
      "train loss:   0.962542\n",
      "train loss:   0.879611\n",
      "train loss:   0.983530\n",
      "train loss:   0.828061\n",
      "train loss:   1.058143\n",
      "train loss:   1.246314\n",
      "train loss:   1.095522\n",
      "train loss:   0.918600\n",
      "train loss:   1.178907\n",
      "train loss:   1.004369\n",
      "train loss:   1.052219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.133940\n",
      "train loss:   0.767798\n",
      "train loss:   1.157560\n",
      "train loss:   0.880933\n",
      "train loss:   1.020085\n",
      "train loss:   0.956124\n",
      "train loss:   1.165756\n",
      "train loss:   0.751846\n",
      "train loss:   0.972718\n",
      "train loss:   0.864704\n",
      "train loss:   1.128606\n",
      "train loss:   0.890558\n",
      "train loss:   0.853506\n",
      "train loss:   0.960807\n",
      "train loss:   0.954551\n",
      "train loss:   1.040436\n",
      "train loss:   0.804755\n",
      "train loss:   0.941564\n",
      "train loss:   1.038485\n",
      "train loss:   0.946528\n",
      "train loss:   0.869655\n",
      "train loss:   1.110514\n",
      "train loss:   1.156235\n",
      "train loss:   0.828365\n",
      "train loss:   1.252324\n",
      "train loss:   0.941937\n",
      "train loss:   0.938582\n",
      "train loss:   0.894424\n",
      "train loss:   1.146423\n",
      "train loss:   1.213450\n",
      "train loss:   1.034196\n",
      "train loss:   0.995021\n",
      "train loss:   1.157476\n",
      "########### epoch 45 ###########\n",
      "########### loop 6300 ###########\n",
      "test loss:   1.065382   test accuracy:   0.750000\n",
      "########### loop 6300 ###########\n",
      "train loss:   0.697653\n",
      "train loss:   0.842534\n",
      "train loss:   0.983355\n",
      "train loss:   0.968956\n",
      "train loss:   1.176284\n",
      "train loss:   0.717554\n",
      "train loss:   1.081113\n",
      "train loss:   1.028248\n",
      "train loss:   1.126262\n",
      "train loss:   0.840392\n",
      "train loss:   0.866231\n",
      "train loss:   1.045340\n",
      "train loss:   1.066516\n",
      "train loss:   1.200114\n",
      "train loss:   1.149921\n",
      "train loss:   0.815770\n",
      "train loss:   1.088420\n",
      "train loss:   1.261847\n",
      "train loss:   0.920193\n",
      "train loss:   1.299724\n",
      "train loss:   1.084380\n",
      "train loss:   0.827927\n",
      "train loss:   1.100165\n",
      "train loss:   1.061204\n",
      "train loss:   0.975073\n",
      "train loss:   1.212176\n",
      "train loss:   1.100098\n",
      "train loss:   1.007688\n",
      "train loss:   0.907459\n",
      "train loss:   0.766471\n",
      "train loss:   0.712654\n",
      "train loss:   0.946484\n",
      "train loss:   0.983642\n",
      "train loss:   1.052921\n",
      "train loss:   1.103266\n",
      "train loss:   1.035595\n",
      "train loss:   1.063358\n",
      "train loss:   1.205154\n",
      "train loss:   1.295723\n",
      "train loss:   1.227782\n",
      "train loss:   1.049924\n",
      "train loss:   0.762338\n",
      "train loss:   0.963818\n",
      "train loss:   1.050127\n",
      "train loss:   0.873426\n",
      "train loss:   0.981639\n",
      "train loss:   0.967686\n",
      "train loss:   1.093626\n",
      "train loss:   0.887665\n",
      "train loss:   1.162989\n",
      "########### epoch 46 ###########\n",
      "########### loop 6350 ###########\n",
      "test loss:   1.364191   test accuracy:   0.468750\n",
      "########### loop 6350 ###########\n",
      "train loss:   0.972438\n",
      "train loss:   1.037425\n",
      "train loss:   0.706167\n",
      "train loss:   1.041697\n",
      "train loss:   1.101779\n",
      "train loss:   0.994730\n",
      "train loss:   0.956503\n",
      "train loss:   1.032655\n",
      "train loss:   0.941850\n",
      "train loss:   0.881098\n",
      "train loss:   0.830215\n",
      "train loss:   0.723467\n",
      "train loss:   1.176548\n",
      "train loss:   1.136794\n",
      "train loss:   1.037988\n",
      "train loss:   0.944207\n",
      "train loss:   1.037347\n",
      "train loss:   0.789894\n",
      "train loss:   0.805569\n",
      "train loss:   1.020493\n",
      "train loss:   1.173306\n",
      "train loss:   0.817536\n",
      "train loss:   1.342564\n",
      "train loss:   1.298804\n",
      "train loss:   0.787004\n",
      "train loss:   1.159980\n",
      "train loss:   1.249676\n",
      "train loss:   1.184525\n",
      "train loss:   1.203928\n",
      "train loss:   0.932921\n",
      "train loss:   1.049853\n",
      "train loss:   0.961406\n",
      "train loss:   1.039371\n",
      "train loss:   1.205594\n",
      "train loss:   0.913508\n",
      "train loss:   1.133164\n",
      "train loss:   0.979047\n",
      "train loss:   1.061347\n",
      "train loss:   1.051657\n",
      "train loss:   1.088063\n",
      "train loss:   0.989727\n",
      "train loss:   0.861340\n",
      "train loss:   0.997799\n",
      "train loss:   1.287818\n",
      "train loss:   0.989418\n",
      "train loss:   1.143673\n",
      "train loss:   0.856614\n",
      "train loss:   0.864513\n",
      "train loss:   0.831147\n",
      "train loss:   1.073301\n",
      "########### epoch 46 ###########\n",
      "########### loop 6400 ###########\n",
      "test loss:   1.274157   test accuracy:   0.562500\n",
      "########### loop 6400 ###########\n",
      "train loss:   0.838092\n",
      "train loss:   1.053681\n",
      "train loss:   1.291904\n",
      "train loss:   1.057603\n",
      "train loss:   0.964621\n",
      "train loss:   1.219618\n",
      "train loss:   1.141557\n",
      "train loss:   1.175590\n",
      "train loss:   0.926625\n",
      "train loss:   0.843798\n",
      "train loss:   1.334635\n",
      "train loss:   0.949200\n",
      "train loss:   1.025920\n",
      "train loss:   0.974221\n",
      "train loss:   1.103277\n",
      "train loss:   0.845419\n",
      "train loss:   0.983281\n",
      "train loss:   0.889180\n",
      "train loss:   1.174061\n",
      "train loss:   0.863651\n",
      "train loss:   0.921749\n",
      "train loss:   1.006660\n",
      "train loss:   1.019416\n",
      "train loss:   1.106923\n",
      "train loss:   0.899365\n",
      "train loss:   0.882398\n",
      "train loss:   1.015242\n",
      "train loss:   0.961407\n",
      "train loss:   0.887813\n",
      "train loss:   1.208652\n",
      "train loss:   1.197506\n",
      "train loss:   0.782134\n",
      "train loss:   1.091563\n",
      "train loss:   0.980722\n",
      "train loss:   0.876257\n",
      "train loss:   0.878023\n",
      "train loss:   1.132687\n",
      "train loss:   1.146173\n",
      "train loss:   0.954834\n",
      "train loss:   1.165301\n",
      "train loss:   1.110324\n",
      "train loss:   0.791908\n",
      "train loss:   0.870924\n",
      "train loss:   0.980362\n",
      "train loss:   0.976851\n",
      "train loss:   1.197144\n",
      "train loss:   0.696737\n",
      "train loss:   1.016337\n",
      "train loss:   0.902916\n",
      "train loss:   1.133282\n",
      "########### epoch 46 ###########\n",
      "########### loop 6450 ###########\n",
      "test loss:   1.231442   test accuracy:   0.437500\n",
      "########### loop 6450 ###########\n",
      "train loss:   0.855772\n",
      "train loss:   0.849132\n",
      "train loss:   0.957393\n",
      "train loss:   1.016280\n",
      "train loss:   1.139669\n",
      "train loss:   1.097587\n",
      "train loss:   0.720964\n",
      "train loss:   1.080956\n",
      "train loss:   1.265756\n",
      "train loss:   0.901947\n",
      "train loss:   1.178594\n",
      "train loss:   0.962513\n",
      "train loss:   0.730819\n",
      "train loss:   0.942445\n",
      "train loss:   0.913909\n",
      "train loss:   0.831287\n",
      "train loss:   1.121169\n",
      "train loss:   1.026418\n",
      "train loss:   0.945379\n",
      "train loss:   0.892611\n",
      "train loss:   0.709544\n",
      "train loss:   0.790117\n",
      "train loss:   0.844309\n",
      "train loss:   1.002180\n",
      "train loss:   1.073141\n",
      "train loss:   1.124335\n",
      "train loss:   1.038046\n",
      "train loss:   1.040030\n",
      "train loss:   1.183865\n",
      "train loss:   1.202242\n",
      "train loss:   1.101386\n",
      "train loss:   1.091347\n",
      "train loss:   0.831651\n",
      "train loss:   0.972877\n",
      "train loss:   1.016430\n",
      "train loss:   0.870045\n",
      "train loss:   0.921134\n",
      "train loss:   1.025042\n",
      "train loss:   1.028613\n",
      "train loss:   0.962825\n",
      "train loss:   1.110696\n",
      "train loss:   0.997480\n",
      "train loss:   1.093189\n",
      "train loss:   0.699216\n",
      "train loss:   0.895527\n",
      "train loss:   0.963996\n",
      "train loss:   0.901499\n",
      "train loss:   0.937428\n",
      "train loss:   1.064616\n",
      "train loss:   0.980992\n",
      "########### epoch 47 ###########\n",
      "########### loop 6500 ###########\n",
      "test loss:   1.237863   test accuracy:   0.625000\n",
      "########### loop 6500 ###########\n",
      "train loss:   0.895562\n",
      "train loss:   0.808208\n",
      "train loss:   0.732082\n",
      "train loss:   1.212439\n",
      "train loss:   1.069623\n",
      "train loss:   0.954367\n",
      "train loss:   0.896857\n",
      "train loss:   1.072706\n",
      "train loss:   0.845046\n",
      "train loss:   0.867622\n",
      "train loss:   0.874204\n",
      "train loss:   0.957630\n",
      "train loss:   0.815709\n",
      "train loss:   1.208057\n",
      "train loss:   1.275039\n",
      "train loss:   0.724678\n",
      "train loss:   1.053878\n",
      "train loss:   1.143634\n",
      "train loss:   1.106598\n",
      "train loss:   1.045306\n",
      "train loss:   0.961605\n",
      "train loss:   1.129020\n",
      "train loss:   0.914439\n",
      "train loss:   1.040513\n",
      "train loss:   1.264578\n",
      "train loss:   0.881910\n",
      "train loss:   1.112832\n",
      "train loss:   0.943600\n",
      "train loss:   1.013408\n",
      "train loss:   1.020522\n",
      "train loss:   1.091145\n",
      "train loss:   0.977140\n",
      "train loss:   0.840683\n",
      "train loss:   1.176123\n",
      "train loss:   1.336129\n",
      "train loss:   1.001924\n",
      "train loss:   1.121155\n",
      "train loss:   0.814264\n",
      "train loss:   0.932698\n",
      "train loss:   0.837956\n",
      "train loss:   1.176336\n",
      "train loss:   0.847488\n",
      "train loss:   1.080434\n",
      "train loss:   1.229230\n",
      "train loss:   1.144167\n",
      "train loss:   0.976916\n",
      "train loss:   1.407223\n",
      "train loss:   1.054198\n",
      "train loss:   1.084448\n",
      "train loss:   1.103168\n",
      "########### epoch 47 ###########\n",
      "########### loop 6550 ###########\n",
      "test loss:   1.110110   test accuracy:   0.718750\n",
      "########### loop 6550 ###########\n",
      "train loss:   0.921784\n",
      "train loss:   1.303106\n",
      "train loss:   0.993277\n",
      "train loss:   1.028096\n",
      "train loss:   0.998118\n",
      "train loss:   1.130706\n",
      "train loss:   0.813938\n",
      "train loss:   0.942493\n",
      "train loss:   0.895302\n",
      "train loss:   1.158465\n",
      "train loss:   0.885698\n",
      "train loss:   0.832750\n",
      "train loss:   0.924472\n",
      "train loss:   0.861221\n",
      "train loss:   1.050387\n",
      "train loss:   0.858656\n",
      "train loss:   0.913866\n",
      "train loss:   1.081982\n",
      "train loss:   0.913414\n",
      "train loss:   0.957019\n",
      "train loss:   1.106418\n",
      "train loss:   1.139746\n",
      "train loss:   0.774788\n",
      "train loss:   1.104593\n",
      "train loss:   0.952742\n",
      "train loss:   0.894431\n",
      "train loss:   0.857482\n",
      "train loss:   1.103719\n",
      "train loss:   1.046394\n",
      "train loss:   0.973749\n",
      "train loss:   1.048500\n",
      "train loss:   1.085772\n",
      "train loss:   0.710968\n",
      "train loss:   0.852170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.968601\n",
      "train loss:   0.958404\n",
      "train loss:   1.131198\n",
      "train loss:   0.698046\n",
      "train loss:   1.018837\n",
      "train loss:   0.961167\n",
      "train loss:   0.980764\n",
      "train loss:   0.812325\n",
      "train loss:   0.824003\n",
      "train loss:   1.025961\n",
      "train loss:   1.003710\n",
      "train loss:   1.160568\n",
      "train loss:   1.086808\n",
      "train loss:   0.704758\n",
      "train loss:   1.121930\n",
      "train loss:   1.243156\n",
      "########### epoch 47 ###########\n",
      "########### loop 6600 ###########\n",
      "test loss:   1.072160   test accuracy:   0.718750\n",
      "########### loop 6600 ###########\n",
      "train loss:   0.921780\n",
      "train loss:   1.246829\n",
      "train loss:   1.042160\n",
      "train loss:   0.939568\n",
      "train loss:   1.086866\n",
      "train loss:   0.991049\n",
      "train loss:   0.978111\n",
      "train loss:   1.314099\n",
      "train loss:   1.062486\n",
      "train loss:   0.997507\n",
      "train loss:   0.838383\n",
      "train loss:   0.698737\n",
      "train loss:   0.693536\n",
      "train loss:   0.953498\n",
      "train loss:   0.958469\n",
      "train loss:   1.083915\n",
      "train loss:   1.081614\n",
      "train loss:   1.132052\n",
      "train loss:   1.074455\n",
      "train loss:   1.328189\n",
      "train loss:   1.323128\n",
      "train loss:   1.224367\n",
      "train loss:   1.232892\n",
      "train loss:   0.905190\n",
      "train loss:   0.967188\n",
      "train loss:   0.992171\n",
      "train loss:   0.830550\n",
      "train loss:   1.068885\n",
      "train loss:   1.049159\n",
      "train loss:   1.162450\n",
      "train loss:   0.814325\n",
      "train loss:   1.027379\n",
      "train loss:   0.950411\n",
      "train loss:   0.921900\n",
      "train loss:   0.650908\n",
      "train loss:   0.976635\n",
      "train loss:   0.920126\n",
      "train loss:   0.956178\n",
      "train loss:   0.940299\n",
      "train loss:   0.881536\n",
      "train loss:   0.872904\n",
      "train loss:   0.903576\n",
      "train loss:   0.879001\n",
      "train loss:   0.726895\n",
      "train loss:   1.159179\n",
      "train loss:   1.232729\n",
      "train loss:   1.040896\n",
      "train loss:   0.882130\n",
      "train loss:   0.956841\n",
      "train loss:   0.806590\n",
      "########### epoch 48 ###########\n",
      "########### loop 6650 ###########\n",
      "test loss:   1.108565   test accuracy:   0.687500\n",
      "########### loop 6650 ###########\n",
      "train loss:   0.778643\n",
      "train loss:   0.916497\n",
      "train loss:   1.000845\n",
      "train loss:   0.814604\n",
      "train loss:   1.169342\n",
      "train loss:   1.173114\n",
      "train loss:   0.745622\n",
      "train loss:   1.052971\n",
      "train loss:   1.117391\n",
      "train loss:   1.117103\n",
      "train loss:   1.137486\n",
      "train loss:   0.932121\n",
      "train loss:   1.096298\n",
      "train loss:   0.979932\n",
      "train loss:   1.011911\n",
      "train loss:   1.203244\n",
      "train loss:   0.925682\n",
      "train loss:   1.148216\n",
      "train loss:   0.999013\n",
      "train loss:   1.042063\n",
      "train loss:   1.006978\n",
      "train loss:   1.121989\n",
      "train loss:   1.008180\n",
      "train loss:   0.942660\n",
      "train loss:   1.113326\n",
      "train loss:   1.307860\n",
      "train loss:   0.983252\n",
      "train loss:   1.048655\n",
      "train loss:   0.791047\n",
      "train loss:   0.812157\n",
      "train loss:   0.827777\n",
      "train loss:   1.115354\n",
      "train loss:   0.913365\n",
      "train loss:   0.992027\n",
      "train loss:   1.303659\n",
      "train loss:   1.166239\n",
      "train loss:   1.004637\n",
      "train loss:   1.303861\n",
      "train loss:   1.065215\n",
      "train loss:   1.086572\n",
      "train loss:   1.045227\n",
      "train loss:   0.759694\n",
      "train loss:   1.151501\n",
      "train loss:   0.886670\n",
      "train loss:   0.974586\n",
      "train loss:   0.986685\n",
      "train loss:   1.153086\n",
      "train loss:   0.751044\n",
      "train loss:   0.945111\n",
      "train loss:   0.870382\n",
      "########### epoch 48 ###########\n",
      "########### loop 6700 ###########\n",
      "test loss:   1.130741   test accuracy:   0.781250\n",
      "########### loop 6700 ###########\n",
      "train loss:   1.149594\n",
      "train loss:   0.959522\n",
      "train loss:   0.922810\n",
      "train loss:   1.064063\n",
      "train loss:   1.074180\n",
      "train loss:   1.110130\n",
      "train loss:   0.789721\n",
      "train loss:   0.894578\n",
      "train loss:   1.035908\n",
      "train loss:   1.008061\n",
      "train loss:   0.906762\n",
      "train loss:   1.165650\n",
      "train loss:   1.177547\n",
      "train loss:   0.774990\n",
      "train loss:   1.232510\n",
      "train loss:   0.984726\n",
      "train loss:   0.857030\n",
      "train loss:   0.942690\n",
      "train loss:   1.088979\n",
      "train loss:   1.111405\n",
      "train loss:   1.018110\n",
      "train loss:   1.061650\n",
      "train loss:   1.088078\n",
      "train loss:   0.764191\n",
      "train loss:   0.859741\n",
      "train loss:   0.996643\n",
      "train loss:   0.978177\n",
      "train loss:   1.233896\n",
      "train loss:   0.719037\n",
      "train loss:   0.995725\n",
      "train loss:   0.972788\n",
      "train loss:   1.031219\n",
      "train loss:   0.895567\n",
      "train loss:   0.967223\n",
      "train loss:   1.187391\n",
      "train loss:   1.222055\n",
      "train loss:   1.231590\n",
      "train loss:   1.044061\n",
      "train loss:   0.852170\n",
      "train loss:   1.139853\n",
      "train loss:   1.160571\n",
      "train loss:   0.936939\n",
      "train loss:   1.223303\n",
      "train loss:   1.034788\n",
      "train loss:   0.771375\n",
      "train loss:   1.057862\n",
      "train loss:   0.977219\n",
      "train loss:   0.938845\n",
      "train loss:   1.277862\n",
      "train loss:   1.112358\n",
      "########### epoch 48 ###########\n",
      "########### loop 6750 ###########\n",
      "test loss:   1.203489   test accuracy:   0.656250\n",
      "########### loop 6750 ###########\n",
      "train loss:   1.024926\n",
      "train loss:   0.932985\n",
      "train loss:   0.742498\n",
      "train loss:   0.728762\n",
      "train loss:   0.983109\n",
      "train loss:   0.970516\n",
      "train loss:   1.052148\n",
      "train loss:   1.106514\n",
      "train loss:   0.920470\n",
      "train loss:   1.043746\n",
      "train loss:   1.162855\n",
      "train loss:   1.227441\n",
      "train loss:   1.180267\n",
      "train loss:   1.102688\n",
      "train loss:   0.871414\n",
      "train loss:   0.918709\n",
      "train loss:   1.062577\n",
      "train loss:   0.914939\n",
      "train loss:   0.879577\n",
      "train loss:   0.956556\n",
      "train loss:   0.988133\n",
      "train loss:   0.813439\n",
      "train loss:   0.976458\n",
      "train loss:   0.986003\n",
      "train loss:   0.915894\n",
      "train loss:   0.668064\n",
      "train loss:   0.957219\n",
      "train loss:   0.892392\n",
      "train loss:   0.930164\n",
      "train loss:   0.870510\n",
      "train loss:   0.868525\n",
      "train loss:   0.880218\n",
      "train loss:   0.850393\n",
      "train loss:   0.787216\n",
      "train loss:   0.754379\n",
      "train loss:   1.144769\n",
      "train loss:   1.109682\n",
      "train loss:   1.019832\n",
      "train loss:   0.942694\n",
      "train loss:   1.021900\n",
      "train loss:   0.806943\n",
      "train loss:   0.942843\n",
      "train loss:   0.973158\n",
      "train loss:   0.891851\n",
      "train loss:   0.801863\n",
      "train loss:   1.285880\n",
      "train loss:   1.236337\n",
      "train loss:   0.782823\n",
      "train loss:   1.085063\n",
      "train loss:   1.180525\n",
      "########### epoch 49 ###########\n",
      "########### loop 6800 ###########\n",
      "test loss:   1.120639   test accuracy:   0.687500\n",
      "########### loop 6800 ###########\n",
      "train loss:   1.118375\n",
      "train loss:   1.087548\n",
      "train loss:   1.011784\n",
      "train loss:   1.172318\n",
      "train loss:   1.027238\n",
      "train loss:   1.112509\n",
      "train loss:   1.235176\n",
      "train loss:   0.917649\n",
      "train loss:   1.113288\n",
      "train loss:   0.916309\n",
      "train loss:   0.981584\n",
      "train loss:   0.968243\n",
      "train loss:   1.049939\n",
      "train loss:   0.978921\n",
      "train loss:   0.891425\n",
      "train loss:   1.097221\n",
      "train loss:   1.198016\n",
      "train loss:   0.970297\n",
      "train loss:   1.076623\n",
      "train loss:   0.850082\n",
      "train loss:   0.869946\n",
      "train loss:   0.852909\n",
      "train loss:   1.004591\n",
      "train loss:   0.831729\n",
      "train loss:   1.024196\n",
      "train loss:   1.258453\n",
      "train loss:   1.116646\n",
      "train loss:   0.932090\n",
      "train loss:   1.269901\n",
      "train loss:   1.052289\n",
      "train loss:   1.118883\n",
      "train loss:   1.056308\n",
      "train loss:   0.736100\n",
      "train loss:   1.152505\n",
      "train loss:   0.833788\n",
      "train loss:   1.009431\n",
      "train loss:   0.946926\n",
      "train loss:   1.133680\n",
      "train loss:   0.809293\n",
      "train loss:   0.946438\n",
      "train loss:   0.862552\n",
      "train loss:   1.082233\n",
      "train loss:   0.854227\n",
      "train loss:   0.872409\n",
      "train loss:   0.872423\n",
      "train loss:   0.948435\n",
      "train loss:   1.052304\n",
      "train loss:   0.851634\n",
      "train loss:   0.888412\n",
      "train loss:   0.961926\n",
      "########### epoch 49 ###########\n",
      "########### loop 6850 ###########\n",
      "test loss:   1.107993   test accuracy:   0.656250\n",
      "########### loop 6850 ###########\n",
      "train loss:   0.889306\n",
      "train loss:   0.901025\n",
      "train loss:   1.077331\n",
      "train loss:   1.146730\n",
      "train loss:   0.803360\n",
      "train loss:   1.133634\n",
      "train loss:   0.973228\n",
      "train loss:   0.904350\n",
      "train loss:   1.056741\n",
      "train loss:   1.187936\n",
      "train loss:   1.078082\n",
      "train loss:   1.018244\n",
      "train loss:   1.034485\n",
      "train loss:   1.152311\n",
      "train loss:   0.793095\n",
      "train loss:   1.038980\n",
      "train loss:   1.075421\n",
      "train loss:   1.017315\n",
      "train loss:   1.161896\n",
      "train loss:   0.726157\n",
      "train loss:   0.997514\n",
      "train loss:   0.895400\n",
      "train loss:   1.110618\n",
      "train loss:   0.839834\n",
      "train loss:   1.007477\n",
      "train loss:   1.319116\n",
      "train loss:   1.309896\n",
      "train loss:   1.301509\n",
      "train loss:   1.074474\n",
      "train loss:   0.778655\n",
      "train loss:   1.232341\n",
      "train loss:   1.212718\n",
      "train loss:   0.949714\n",
      "train loss:   1.356626\n",
      "train loss:   1.128758\n",
      "train loss:   0.879460\n",
      "train loss:   1.194276\n",
      "train loss:   1.054475\n",
      "train loss:   0.827348\n",
      "train loss:   1.268407\n",
      "train loss:   1.155973\n",
      "train loss:   1.062830\n",
      "train loss:   0.960846\n",
      "train loss:   0.842081\n",
      "train loss:   0.720475\n",
      "train loss:   1.132564\n",
      "train loss:   1.025733\n",
      "train loss:   1.080033\n",
      "train loss:   1.121939\n",
      "train loss:   1.038314\n",
      "########### epoch 49 ###########\n",
      "########### loop 6900 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   1.173199   test accuracy:   0.562500\n",
      "########### loop 6900 ###########\n",
      "train loss:   1.077999\n",
      "train loss:   1.335022\n",
      "train loss:   1.474054\n",
      "train loss:   1.364521\n",
      "train loss:   1.238243\n",
      "train loss:   0.840225\n",
      "train loss:   0.890547\n",
      "train loss:   1.091238\n",
      "train loss:   0.865919\n",
      "train loss:   0.953234\n",
      "train loss:   1.005538\n",
      "train loss:   1.029920\n",
      "train loss:   0.842889\n",
      "train loss:   1.130614\n",
      "train loss:   1.000174\n",
      "train loss:   1.075754\n",
      "train loss:   0.686283\n",
      "train loss:   0.912851\n",
      "train loss:   0.979979\n",
      "train loss:   0.921264\n",
      "train loss:   0.963908\n",
      "train loss:   0.946323\n",
      "train loss:   0.906704\n",
      "train loss:   0.907604\n",
      "train loss:   0.768248\n",
      "train loss:   0.680014\n",
      "train loss:   1.204033\n",
      "train loss:   1.066075\n",
      "train loss:   0.987422\n",
      "train loss:   0.905220\n",
      "train loss:   0.997044\n",
      "train loss:   0.751698\n",
      "train loss:   0.816882\n",
      "train loss:   0.927313\n",
      "train loss:   1.086213\n",
      "train loss:   0.846413\n",
      "train loss:   1.138269\n",
      "train loss:   1.209811\n",
      "train loss:   0.808226\n",
      "train loss:   1.067180\n",
      "train loss:   1.202887\n",
      "train loss:   1.152320\n",
      "train loss:   1.209894\n",
      "train loss:   0.768882\n",
      "train loss:   1.058359\n",
      "train loss:   0.911824\n",
      "train loss:   1.022526\n",
      "train loss:   1.181264\n",
      "train loss:   0.997605\n",
      "train loss:   1.217318\n",
      "########### epoch 50 ###########\n",
      "########### loop 6950 ###########\n",
      "test loss:   1.077862   test accuracy:   0.625000\n",
      "########### loop 6950 ###########\n",
      "train loss:   1.049265\n",
      "train loss:   1.082696\n",
      "train loss:   1.080007\n",
      "train loss:   1.241120\n",
      "train loss:   1.000369\n",
      "train loss:   0.919272\n",
      "train loss:   1.015597\n",
      "train loss:   1.198741\n",
      "train loss:   0.975838\n",
      "train loss:   1.061389\n",
      "train loss:   0.785365\n",
      "train loss:   0.877340\n",
      "train loss:   0.875015\n",
      "train loss:   0.995794\n",
      "train loss:   0.815631\n",
      "train loss:   1.032789\n",
      "train loss:   1.229590\n",
      "train loss:   1.025712\n",
      "train loss:   0.896526\n",
      "train loss:   1.091808\n",
      "train loss:   0.993438\n",
      "train loss:   1.104132\n",
      "train loss:   1.008725\n",
      "train loss:   0.737024\n",
      "train loss:   1.206522\n",
      "train loss:   0.841685\n",
      "train loss:   1.012981\n",
      "train loss:   0.908924\n",
      "train loss:   1.117173\n",
      "train loss:   0.756832\n",
      "train loss:   0.928860\n",
      "train loss:   0.861469\n",
      "train loss:   1.087401\n",
      "train loss:   0.937983\n",
      "train loss:   0.826573\n",
      "train loss:   0.878138\n",
      "train loss:   0.863075\n",
      "train loss:   1.012742\n",
      "train loss:   0.924462\n",
      "train loss:   1.079022\n",
      "train loss:   1.007321\n",
      "train loss:   0.947415\n",
      "train loss:   0.914217\n",
      "train loss:   1.192438\n",
      "train loss:   1.163852\n",
      "train loss:   0.768236\n",
      "train loss:   1.198775\n",
      "train loss:   0.971875\n",
      "train loss:   0.917512\n",
      "train loss:   0.931331\n",
      "########### epoch 50 ###########\n",
      "########### loop 7000 ###########\n",
      "test loss:   0.977576   test accuracy:   0.687500\n",
      "########### loop 7000 ###########\n",
      "train loss:   1.191617\n",
      "train loss:   1.132512\n",
      "train loss:   1.047970\n",
      "train loss:   1.068439\n",
      "train loss:   1.098294\n",
      "train loss:   0.797648\n",
      "train loss:   0.930115\n",
      "train loss:   0.985640\n",
      "train loss:   1.020077\n",
      "train loss:   1.155086\n",
      "train loss:   0.709270\n",
      "train loss:   1.036565\n",
      "train loss:   0.948306\n",
      "train loss:   1.105104\n",
      "train loss:   0.885001\n",
      "train loss:   0.834805\n",
      "train loss:   1.072370\n",
      "train loss:   1.113557\n",
      "train loss:   1.088759\n",
      "train loss:   1.068757\n",
      "train loss:   0.794864\n",
      "train loss:   1.048798\n",
      "train loss:   1.270089\n",
      "train loss:   0.910424\n",
      "train loss:   1.346047\n",
      "train loss:   1.053235\n",
      "train loss:   0.770814\n",
      "train loss:   0.947901\n",
      "train loss:   0.923635\n",
      "train loss:   0.874951\n",
      "train loss:   1.269529\n",
      "train loss:   1.085128\n",
      "train loss:   1.008169\n",
      "train loss:   0.855191\n",
      "train loss:   0.688183\n",
      "train loss:   0.689526\n",
      "train loss:   0.872757\n",
      "train loss:   0.997745\n",
      "train loss:   1.041748\n",
      "train loss:   1.065871\n",
      "train loss:   1.071973\n",
      "train loss:   1.017732\n",
      "train loss:   1.252385\n",
      "train loss:   1.245257\n",
      "train loss:   1.230201\n",
      "train loss:   1.174422\n",
      "train loss:   0.857115\n",
      "train loss:   0.963539\n",
      "train loss:   1.035563\n",
      "train loss:   0.783236\n",
      "########### epoch 51 ###########\n",
      "########### loop 7050 ###########\n",
      "test loss:   0.992979   test accuracy:   0.656250\n",
      "########### loop 7050 ###########\n",
      "train loss:   0.866714\n",
      "train loss:   0.992315\n",
      "train loss:   0.987387\n",
      "train loss:   0.885168\n",
      "train loss:   1.131024\n",
      "train loss:   0.965765\n",
      "train loss:   1.076400\n",
      "train loss:   0.645167\n",
      "train loss:   0.937805\n",
      "train loss:   1.001507\n",
      "train loss:   0.926552\n",
      "train loss:   0.945673\n",
      "train loss:   0.960609\n",
      "train loss:   0.945106\n",
      "train loss:   0.880449\n",
      "train loss:   0.804936\n",
      "train loss:   0.724200\n",
      "train loss:   1.196266\n",
      "train loss:   1.088763\n",
      "train loss:   0.947852\n",
      "train loss:   0.934520\n",
      "train loss:   1.035983\n",
      "train loss:   0.867775\n",
      "train loss:   0.759158\n",
      "train loss:   0.863314\n",
      "train loss:   0.949212\n",
      "train loss:   0.762925\n",
      "train loss:   1.268939\n",
      "train loss:   1.250627\n",
      "train loss:   0.818471\n",
      "train loss:   1.071869\n",
      "train loss:   1.177163\n",
      "train loss:   1.020171\n",
      "train loss:   1.028028\n",
      "train loss:   0.909473\n",
      "train loss:   1.073999\n",
      "train loss:   0.985102\n",
      "train loss:   1.089609\n",
      "train loss:   1.206540\n",
      "train loss:   0.915964\n",
      "train loss:   1.146544\n",
      "train loss:   1.000171\n",
      "train loss:   1.017644\n",
      "train loss:   0.975226\n",
      "train loss:   1.091401\n",
      "train loss:   1.014528\n",
      "train loss:   0.854558\n",
      "train loss:   1.018430\n",
      "train loss:   1.293771\n",
      "train loss:   1.030667\n",
      "########### epoch 51 ###########\n",
      "########### loop 7100 ###########\n",
      "test loss:   1.049766   test accuracy:   0.750000\n",
      "########### loop 7100 ###########\n",
      "train loss:   1.049428\n",
      "train loss:   0.864061\n",
      "train loss:   0.867678\n",
      "train loss:   0.894397\n",
      "train loss:   1.051194\n",
      "train loss:   0.768050\n",
      "train loss:   0.975273\n",
      "train loss:   1.243528\n",
      "train loss:   1.023506\n",
      "train loss:   0.929981\n",
      "train loss:   1.125084\n",
      "train loss:   1.083791\n",
      "train loss:   1.136829\n",
      "train loss:   0.957793\n",
      "train loss:   0.744766\n",
      "train loss:   1.277821\n",
      "train loss:   0.915528\n",
      "train loss:   1.036708\n",
      "train loss:   0.930237\n",
      "train loss:   1.083851\n",
      "train loss:   0.885887\n",
      "train loss:   1.047118\n",
      "train loss:   0.901024\n",
      "train loss:   1.068807\n",
      "train loss:   0.862393\n",
      "train loss:   0.795413\n",
      "train loss:   0.861711\n",
      "train loss:   0.901354\n",
      "train loss:   1.067368\n",
      "train loss:   0.936307\n",
      "train loss:   1.083114\n",
      "train loss:   1.079813\n",
      "train loss:   0.850861\n",
      "train loss:   0.883651\n",
      "train loss:   1.222956\n",
      "train loss:   1.099799\n",
      "train loss:   0.758414\n",
      "train loss:   1.033777\n",
      "train loss:   0.941190\n",
      "train loss:   0.844298\n",
      "train loss:   0.981677\n",
      "train loss:   1.128616\n",
      "train loss:   1.081764\n",
      "train loss:   1.054049\n",
      "train loss:   1.085528\n",
      "train loss:   1.058333\n",
      "train loss:   0.701941\n",
      "train loss:   0.857021\n",
      "train loss:   1.021164\n",
      "train loss:   0.984288\n",
      "########### epoch 51 ###########\n",
      "########### loop 7150 ###########\n",
      "test loss:   1.121754   test accuracy:   0.750000\n",
      "########### loop 7150 ###########\n",
      "train loss:   1.102552\n",
      "train loss:   0.689159\n",
      "train loss:   0.999171\n",
      "train loss:   0.905481\n",
      "train loss:   0.939138\n",
      "train loss:   0.783422\n",
      "train loss:   0.845041\n",
      "train loss:   1.076505\n",
      "train loss:   1.033751\n",
      "train loss:   1.104343\n",
      "train loss:   1.041941\n",
      "train loss:   0.703247\n",
      "train loss:   1.192550\n",
      "train loss:   1.121271\n",
      "train loss:   0.931395\n",
      "train loss:   1.311298\n",
      "train loss:   1.046014\n",
      "train loss:   0.857862\n",
      "train loss:   1.063139\n",
      "train loss:   0.924539\n",
      "train loss:   0.828099\n",
      "train loss:   1.220680\n",
      "train loss:   1.248615\n",
      "train loss:   1.071353\n",
      "train loss:   0.902914\n",
      "train loss:   0.822385\n",
      "train loss:   0.717868\n",
      "train loss:   0.944833\n",
      "train loss:   0.933197\n",
      "train loss:   1.008192\n",
      "train loss:   1.151348\n",
      "train loss:   1.162117\n",
      "train loss:   1.245813\n",
      "train loss:   1.272477\n",
      "train loss:   1.268467\n",
      "train loss:   1.320515\n",
      "train loss:   1.162379\n",
      "train loss:   1.003299\n",
      "train loss:   0.977585\n",
      "train loss:   0.986035\n",
      "train loss:   0.790579\n",
      "train loss:   0.775503\n",
      "train loss:   0.959567\n",
      "train loss:   1.104595\n",
      "train loss:   0.819507\n",
      "train loss:   1.068330\n",
      "train loss:   0.896002\n",
      "train loss:   1.024909\n",
      "train loss:   0.626660\n",
      "train loss:   0.900236\n",
      "########### epoch 52 ###########\n",
      "########### loop 7200 ###########\n",
      "test loss:   1.099838   test accuracy:   0.593750\n",
      "########### loop 7200 ###########\n",
      "train loss:   0.941449\n",
      "train loss:   1.069857\n",
      "train loss:   1.063436\n",
      "train loss:   0.940463\n",
      "train loss:   0.877008\n",
      "train loss:   0.814774\n",
      "train loss:   0.821960\n",
      "train loss:   0.775305\n",
      "train loss:   1.149819\n",
      "train loss:   1.166602\n",
      "train loss:   1.006538\n",
      "train loss:   0.863631\n",
      "train loss:   1.048678\n",
      "train loss:   0.825110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.826612\n",
      "train loss:   0.857408\n",
      "train loss:   1.003613\n",
      "train loss:   0.865364\n",
      "train loss:   1.130937\n",
      "train loss:   1.165395\n",
      "train loss:   0.787108\n",
      "train loss:   1.045498\n",
      "train loss:   1.189630\n",
      "train loss:   1.016859\n",
      "train loss:   1.010429\n",
      "train loss:   0.956197\n",
      "train loss:   1.041627\n",
      "train loss:   0.981461\n",
      "train loss:   0.981180\n",
      "train loss:   1.138073\n",
      "train loss:   0.986941\n",
      "train loss:   1.184877\n",
      "train loss:   1.084213\n",
      "train loss:   1.094620\n",
      "train loss:   1.154280\n",
      "train loss:   1.230952\n",
      "train loss:   1.035317\n",
      "train loss:   0.750673\n",
      "train loss:   0.923913\n",
      "train loss:   1.226227\n",
      "train loss:   0.876455\n",
      "train loss:   0.982245\n",
      "train loss:   0.775314\n",
      "train loss:   0.794629\n",
      "train loss:   0.784384\n",
      "train loss:   1.024785\n",
      "train loss:   0.808755\n",
      "train loss:   1.025747\n",
      "train loss:   1.418372\n",
      "train loss:   1.045019\n",
      "########### epoch 52 ###########\n",
      "########### loop 7250 ###########\n",
      "test loss:   1.200732   test accuracy:   0.625000\n",
      "########### loop 7250 ###########\n",
      "train loss:   1.040558\n",
      "train loss:   1.149432\n",
      "train loss:   1.112128\n",
      "train loss:   1.114294\n",
      "train loss:   1.019657\n",
      "train loss:   0.791586\n",
      "train loss:   1.205720\n",
      "train loss:   0.823314\n",
      "train loss:   1.062526\n",
      "train loss:   0.920792\n",
      "train loss:   1.133463\n",
      "train loss:   0.771087\n",
      "train loss:   0.906416\n",
      "train loss:   0.863607\n",
      "train loss:   1.089011\n",
      "train loss:   0.857072\n",
      "train loss:   0.879102\n",
      "train loss:   0.953730\n",
      "train loss:   0.954472\n",
      "train loss:   1.052595\n",
      "train loss:   0.865586\n",
      "train loss:   1.036497\n",
      "train loss:   1.028872\n",
      "train loss:   0.899851\n",
      "train loss:   0.933939\n",
      "train loss:   1.037461\n",
      "train loss:   1.060763\n",
      "train loss:   0.724214\n",
      "train loss:   1.109880\n",
      "train loss:   0.967525\n",
      "train loss:   0.781258\n",
      "train loss:   0.851222\n",
      "train loss:   1.029219\n",
      "train loss:   1.086858\n",
      "train loss:   0.977339\n",
      "train loss:   1.112768\n",
      "train loss:   1.056697\n",
      "train loss:   0.678209\n",
      "train loss:   0.857067\n",
      "train loss:   1.058993\n",
      "train loss:   0.961997\n",
      "train loss:   1.093292\n",
      "train loss:   0.727727\n",
      "train loss:   0.982821\n",
      "train loss:   0.952175\n",
      "train loss:   0.943690\n",
      "train loss:   0.854149\n",
      "train loss:   0.996020\n",
      "train loss:   1.181158\n",
      "train loss:   1.208537\n",
      "########### epoch 52 ###########\n",
      "########### loop 7300 ###########\n",
      "test loss:   1.057967   test accuracy:   0.625000\n",
      "########### loop 7300 ###########\n",
      "train loss:   1.172502\n",
      "train loss:   1.020686\n",
      "train loss:   0.721057\n",
      "train loss:   1.139370\n",
      "train loss:   1.158933\n",
      "train loss:   0.939638\n",
      "train loss:   1.227718\n",
      "train loss:   1.123620\n",
      "train loss:   0.818973\n",
      "train loss:   0.979622\n",
      "train loss:   0.869744\n",
      "train loss:   0.896979\n",
      "train loss:   1.237038\n",
      "train loss:   1.066645\n",
      "train loss:   0.984532\n",
      "train loss:   0.762370\n",
      "train loss:   0.714320\n",
      "train loss:   0.709587\n",
      "train loss:   0.839994\n",
      "train loss:   0.886378\n",
      "train loss:   1.005878\n",
      "train loss:   1.021229\n",
      "train loss:   1.004686\n",
      "train loss:   1.025107\n",
      "train loss:   1.150896\n",
      "train loss:   1.228886\n",
      "train loss:   1.145442\n",
      "train loss:   1.015287\n",
      "train loss:   0.874055\n",
      "train loss:   0.848516\n",
      "train loss:   1.062584\n",
      "train loss:   0.853563\n",
      "train loss:   0.835515\n",
      "train loss:   0.947719\n",
      "train loss:   0.961323\n",
      "train loss:   0.877466\n",
      "train loss:   1.036786\n",
      "train loss:   0.946855\n",
      "train loss:   1.057196\n",
      "train loss:   0.680891\n",
      "train loss:   0.878873\n",
      "train loss:   0.898067\n",
      "train loss:   0.873462\n",
      "train loss:   0.913794\n",
      "train loss:   0.897434\n",
      "train loss:   0.917857\n",
      "train loss:   0.850948\n",
      "train loss:   0.811672\n",
      "train loss:   0.729159\n",
      "train loss:   1.054161\n",
      "########### epoch 53 ###########\n",
      "########### loop 7350 ###########\n",
      "test loss:   1.110496   test accuracy:   0.500000\n",
      "########### loop 7350 ###########\n",
      "train loss:   1.144484\n",
      "train loss:   1.017870\n",
      "train loss:   0.918142\n",
      "train loss:   1.059163\n",
      "train loss:   0.821655\n",
      "train loss:   0.949623\n",
      "train loss:   0.934347\n",
      "train loss:   0.983338\n",
      "train loss:   0.739404\n",
      "train loss:   1.179229\n",
      "train loss:   1.255936\n",
      "train loss:   0.779944\n",
      "train loss:   1.189100\n",
      "train loss:   1.154400\n",
      "train loss:   1.031482\n",
      "train loss:   1.113759\n",
      "train loss:   0.876336\n",
      "train loss:   1.107824\n",
      "train loss:   0.938286\n",
      "train loss:   1.040307\n",
      "train loss:   1.177473\n",
      "train loss:   0.873365\n",
      "train loss:   1.073174\n",
      "train loss:   0.875955\n",
      "train loss:   1.056857\n",
      "train loss:   1.051430\n",
      "train loss:   1.180149\n",
      "train loss:   0.968417\n",
      "train loss:   0.799326\n",
      "train loss:   0.950518\n",
      "train loss:   1.249446\n",
      "train loss:   1.022196\n",
      "train loss:   1.120483\n",
      "train loss:   0.783983\n",
      "train loss:   0.951837\n",
      "train loss:   0.850048\n",
      "train loss:   1.067097\n",
      "train loss:   0.718648\n",
      "train loss:   1.034404\n",
      "train loss:   1.266546\n",
      "train loss:   1.127826\n",
      "train loss:   0.955271\n",
      "train loss:   1.204129\n",
      "train loss:   1.026913\n",
      "train loss:   1.042415\n",
      "train loss:   0.914608\n",
      "train loss:   0.761136\n",
      "train loss:   1.237280\n",
      "train loss:   0.869254\n",
      "train loss:   1.002904\n",
      "########### epoch 53 ###########\n",
      "########### loop 7400 ###########\n",
      "test loss:   1.017410   test accuracy:   0.656250\n",
      "########### loop 7400 ###########\n",
      "train loss:   0.869101\n",
      "train loss:   1.049995\n",
      "train loss:   0.738968\n",
      "train loss:   0.896950\n",
      "train loss:   0.814199\n",
      "train loss:   1.082321\n",
      "train loss:   0.826874\n",
      "train loss:   0.896438\n",
      "train loss:   0.876475\n",
      "train loss:   0.864920\n",
      "train loss:   1.028856\n",
      "train loss:   0.963820\n",
      "train loss:   1.106037\n",
      "train loss:   1.170165\n",
      "train loss:   0.911515\n",
      "train loss:   0.896637\n",
      "train loss:   1.132487\n",
      "train loss:   1.176239\n",
      "train loss:   0.676015\n",
      "train loss:   1.159542\n",
      "train loss:   0.936790\n",
      "train loss:   0.904033\n",
      "train loss:   0.878204\n",
      "train loss:   1.059794\n",
      "train loss:   0.999490\n",
      "train loss:   1.033486\n",
      "train loss:   1.142471\n",
      "train loss:   1.079682\n",
      "train loss:   0.849567\n",
      "train loss:   0.905857\n",
      "train loss:   1.010867\n",
      "train loss:   0.949829\n",
      "train loss:   1.121810\n",
      "train loss:   0.660620\n",
      "train loss:   0.968124\n",
      "train loss:   0.868394\n",
      "train loss:   1.102213\n",
      "train loss:   0.735552\n",
      "train loss:   0.907150\n",
      "train loss:   1.092405\n",
      "train loss:   1.013567\n",
      "train loss:   1.090688\n",
      "train loss:   1.005413\n",
      "train loss:   0.754566\n",
      "train loss:   1.219915\n",
      "train loss:   1.241341\n",
      "train loss:   0.926400\n",
      "train loss:   1.265781\n",
      "train loss:   1.143273\n",
      "train loss:   0.813844\n",
      "########### epoch 53 ###########\n",
      "########### loop 7450 ###########\n",
      "test loss:   1.069648   test accuracy:   0.656250\n",
      "########### loop 7450 ###########\n",
      "train loss:   0.992165\n",
      "train loss:   0.959526\n",
      "train loss:   0.837620\n",
      "train loss:   1.213474\n",
      "train loss:   1.047628\n",
      "train loss:   1.020952\n",
      "train loss:   0.797491\n",
      "train loss:   0.703927\n",
      "train loss:   0.692302\n",
      "train loss:   1.022747\n",
      "train loss:   0.922936\n",
      "train loss:   1.050063\n",
      "train loss:   1.031750\n",
      "train loss:   1.060325\n",
      "train loss:   0.998516\n",
      "train loss:   1.208048\n",
      "train loss:   1.301119\n",
      "train loss:   1.231060\n",
      "train loss:   1.150892\n",
      "train loss:   0.917196\n",
      "train loss:   0.942825\n",
      "train loss:   1.111551\n",
      "train loss:   0.867756\n",
      "train loss:   0.925804\n",
      "train loss:   1.010768\n",
      "train loss:   1.003583\n",
      "train loss:   0.766010\n",
      "train loss:   1.068999\n",
      "train loss:   1.016026\n",
      "train loss:   0.975072\n",
      "train loss:   0.647537\n",
      "train loss:   0.840698\n",
      "train loss:   1.013063\n",
      "train loss:   0.875667\n",
      "train loss:   0.967751\n",
      "train loss:   0.976834\n",
      "train loss:   0.925961\n",
      "train loss:   0.872292\n",
      "train loss:   0.751258\n",
      "train loss:   0.665611\n",
      "train loss:   1.271850\n",
      "train loss:   1.043545\n",
      "train loss:   0.861162\n",
      "train loss:   0.862798\n",
      "train loss:   0.958064\n",
      "train loss:   0.741959\n",
      "train loss:   0.871917\n",
      "train loss:   0.990775\n",
      "train loss:   1.094794\n",
      "train loss:   0.773050\n",
      "########### epoch 54 ###########\n",
      "########### loop 7500 ###########\n",
      "test loss:   1.081708   test accuracy:   0.718750\n",
      "########### loop 7500 ###########\n",
      "train loss:   1.159102\n",
      "train loss:   1.175599\n",
      "train loss:   0.767862\n",
      "train loss:   0.976615\n",
      "train loss:   1.152095\n",
      "train loss:   1.057265\n",
      "train loss:   1.115108\n",
      "train loss:   0.822457\n",
      "train loss:   1.013819\n",
      "train loss:   0.935376\n",
      "train loss:   0.973622\n",
      "train loss:   1.190214\n",
      "train loss:   0.938228\n",
      "train loss:   1.173872\n",
      "train loss:   1.050911\n",
      "train loss:   1.070579\n",
      "train loss:   1.027191\n",
      "train loss:   1.082927\n",
      "train loss:   0.993882\n",
      "train loss:   0.847984\n",
      "train loss:   1.076169\n",
      "train loss:   1.211117\n",
      "train loss:   1.018802\n",
      "train loss:   1.001491\n",
      "train loss:   0.882635\n",
      "train loss:   0.926713\n",
      "train loss:   0.809083\n",
      "train loss:   1.090422\n",
      "train loss:   0.857769\n",
      "train loss:   1.038998\n",
      "train loss:   1.440678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.042843\n",
      "train loss:   0.948175\n",
      "train loss:   1.165258\n",
      "train loss:   0.927558\n",
      "train loss:   0.970233\n",
      "train loss:   1.006811\n",
      "train loss:   0.725706\n",
      "train loss:   1.242614\n",
      "train loss:   0.929862\n",
      "train loss:   1.022858\n",
      "train loss:   0.958551\n",
      "train loss:   1.156639\n",
      "train loss:   0.680832\n",
      "train loss:   0.991766\n",
      "train loss:   0.878515\n",
      "train loss:   1.116013\n",
      "train loss:   0.791507\n",
      "train loss:   0.876101\n",
      "train loss:   0.950586\n",
      "########### epoch 54 ###########\n",
      "########### loop 7550 ###########\n",
      "test loss:   1.036656   test accuracy:   0.718750\n",
      "########### loop 7550 ###########\n",
      "train loss:   0.829374\n",
      "train loss:   0.986197\n",
      "train loss:   0.784982\n",
      "train loss:   0.879636\n",
      "train loss:   1.018480\n",
      "train loss:   0.939312\n",
      "train loss:   1.049909\n",
      "train loss:   1.205548\n",
      "train loss:   1.227656\n",
      "train loss:   0.851101\n",
      "train loss:   1.050360\n",
      "train loss:   0.868263\n",
      "train loss:   0.850180\n",
      "train loss:   0.878789\n",
      "train loss:   1.172551\n",
      "train loss:   1.163299\n",
      "train loss:   1.007314\n",
      "train loss:   1.108100\n",
      "train loss:   1.129111\n",
      "train loss:   0.823835\n",
      "train loss:   0.833938\n",
      "train loss:   0.942276\n",
      "train loss:   1.030222\n",
      "train loss:   1.150328\n",
      "train loss:   0.640606\n",
      "train loss:   0.994468\n",
      "train loss:   0.970398\n",
      "train loss:   0.960979\n",
      "train loss:   0.837525\n",
      "train loss:   0.865903\n",
      "train loss:   1.075420\n",
      "train loss:   1.250006\n",
      "train loss:   1.180888\n",
      "train loss:   1.064805\n",
      "train loss:   0.768957\n",
      "train loss:   1.011451\n",
      "train loss:   1.052516\n",
      "train loss:   0.811569\n",
      "train loss:   1.223585\n",
      "train loss:   1.056275\n",
      "train loss:   1.011565\n",
      "train loss:   1.183075\n",
      "train loss:   0.974795\n",
      "train loss:   0.802418\n",
      "train loss:   1.103982\n",
      "train loss:   0.927187\n",
      "train loss:   0.927680\n",
      "train loss:   0.796432\n",
      "train loss:   0.724484\n",
      "train loss:   0.716664\n",
      "########### epoch 54 ###########\n",
      "########### loop 7600 ###########\n",
      "test loss:   1.426467   test accuracy:   0.468750\n",
      "########### loop 7600 ###########\n",
      "train loss:   0.984899\n",
      "train loss:   0.884290\n",
      "train loss:   0.962036\n",
      "train loss:   1.074368\n",
      "train loss:   1.024190\n",
      "train loss:   1.151420\n",
      "train loss:   1.241001\n",
      "train loss:   1.446137\n",
      "train loss:   1.404055\n",
      "train loss:   1.210609\n",
      "train loss:   1.008456\n",
      "train loss:   0.957645\n",
      "train loss:   0.967332\n",
      "train loss:   0.821812\n",
      "train loss:   0.880298\n",
      "train loss:   0.941417\n",
      "train loss:   1.107044\n",
      "train loss:   0.855150\n",
      "train loss:   1.159493\n",
      "train loss:   0.873815\n",
      "train loss:   0.956227\n",
      "train loss:   0.642274\n",
      "train loss:   0.908064\n",
      "train loss:   0.970036\n",
      "train loss:   0.968504\n",
      "train loss:   1.027656\n",
      "train loss:   0.903194\n",
      "train loss:   0.875357\n",
      "train loss:   0.813450\n",
      "train loss:   0.807655\n",
      "train loss:   0.729399\n",
      "train loss:   1.189908\n",
      "train loss:   1.155787\n",
      "train loss:   0.895913\n",
      "train loss:   0.898730\n",
      "train loss:   0.995829\n",
      "train loss:   0.804381\n",
      "train loss:   0.775844\n",
      "train loss:   0.921821\n",
      "train loss:   0.955897\n",
      "train loss:   0.781892\n",
      "train loss:   1.203078\n",
      "train loss:   1.223691\n",
      "train loss:   0.721987\n",
      "train loss:   0.996901\n",
      "train loss:   1.111010\n",
      "train loss:   1.022861\n",
      "train loss:   1.023837\n",
      "train loss:   0.862848\n",
      "train loss:   1.090274\n",
      "########### epoch 55 ###########\n",
      "########### loop 7650 ###########\n",
      "test loss:   1.135921   test accuracy:   0.656250\n",
      "########### loop 7650 ###########\n",
      "train loss:   0.923649\n",
      "train loss:   1.059106\n",
      "train loss:   1.132843\n",
      "train loss:   0.870691\n",
      "train loss:   1.083106\n",
      "train loss:   0.923832\n",
      "train loss:   0.973991\n",
      "train loss:   1.015988\n",
      "train loss:   1.069965\n",
      "train loss:   0.925118\n",
      "train loss:   0.807919\n",
      "train loss:   1.116233\n",
      "train loss:   1.213838\n",
      "train loss:   0.958950\n",
      "train loss:   1.220013\n",
      "train loss:   0.735438\n",
      "train loss:   0.785080\n",
      "train loss:   0.762463\n",
      "train loss:   1.074910\n",
      "train loss:   0.836917\n",
      "train loss:   0.970147\n",
      "train loss:   1.420792\n",
      "train loss:   1.060476\n",
      "train loss:   1.060063\n",
      "train loss:   1.204221\n",
      "train loss:   1.031340\n",
      "train loss:   1.008126\n",
      "train loss:   1.077691\n",
      "train loss:   0.739018\n",
      "train loss:   1.288644\n",
      "train loss:   0.967152\n",
      "train loss:   1.079192\n",
      "train loss:   1.018766\n",
      "train loss:   1.170673\n",
      "train loss:   0.827396\n",
      "train loss:   1.014302\n",
      "train loss:   0.928721\n",
      "train loss:   1.131709\n",
      "train loss:   0.808818\n",
      "train loss:   0.868979\n",
      "train loss:   0.957931\n",
      "train loss:   0.895280\n",
      "train loss:   0.993492\n",
      "train loss:   0.831077\n",
      "train loss:   0.871282\n",
      "train loss:   0.992347\n",
      "train loss:   0.897950\n",
      "train loss:   0.886563\n",
      "train loss:   1.124345\n",
      "train loss:   1.056924\n",
      "########### epoch 55 ###########\n",
      "########### loop 7700 ###########\n",
      "test loss:   1.068953   test accuracy:   0.593750\n",
      "########### loop 7700 ###########\n",
      "train loss:   0.683255\n",
      "train loss:   1.113432\n",
      "train loss:   0.947752\n",
      "train loss:   0.843520\n",
      "train loss:   0.882403\n",
      "train loss:   1.089394\n",
      "train loss:   1.052382\n",
      "train loss:   0.895181\n",
      "train loss:   1.032752\n",
      "train loss:   1.031593\n",
      "train loss:   0.756423\n",
      "train loss:   0.924603\n",
      "train loss:   0.929860\n",
      "train loss:   0.926255\n",
      "train loss:   1.082162\n",
      "train loss:   0.741412\n",
      "train loss:   0.909520\n",
      "train loss:   0.885871\n",
      "train loss:   1.050080\n",
      "train loss:   0.809556\n",
      "train loss:   0.949496\n",
      "train loss:   1.120091\n",
      "train loss:   1.084115\n",
      "train loss:   1.155567\n",
      "train loss:   1.001407\n",
      "train loss:   0.700768\n",
      "train loss:   1.044686\n",
      "train loss:   1.242395\n",
      "train loss:   0.906800\n",
      "train loss:   1.218567\n",
      "train loss:   1.084116\n",
      "train loss:   0.776175\n",
      "train loss:   1.002073\n",
      "train loss:   0.926696\n",
      "train loss:   0.843145\n",
      "train loss:   1.253082\n",
      "train loss:   1.040569\n",
      "train loss:   1.007151\n",
      "train loss:   0.785337\n",
      "train loss:   0.671495\n",
      "train loss:   0.712021\n",
      "train loss:   0.824402\n",
      "train loss:   0.869807\n",
      "train loss:   1.026606\n",
      "train loss:   1.047548\n",
      "train loss:   1.112962\n",
      "train loss:   1.040871\n",
      "train loss:   1.088662\n",
      "train loss:   1.227199\n",
      "train loss:   1.132023\n",
      "########### epoch 55 ###########\n",
      "########### loop 7750 ###########\n",
      "test loss:   1.073311   test accuracy:   0.718750\n",
      "########### loop 7750 ###########\n",
      "train loss:   0.944848\n",
      "train loss:   0.736588\n",
      "train loss:   0.864093\n",
      "train loss:   1.039736\n",
      "train loss:   0.766770\n",
      "train loss:   0.823111\n",
      "train loss:   0.884130\n",
      "train loss:   0.964860\n",
      "train loss:   0.805049\n",
      "train loss:   1.065648\n",
      "train loss:   0.839795\n",
      "train loss:   0.912345\n",
      "train loss:   0.650768\n",
      "train loss:   0.903839\n",
      "train loss:   0.904588\n",
      "train loss:   0.881154\n",
      "train loss:   0.917764\n",
      "train loss:   1.001068\n",
      "train loss:   0.902141\n",
      "train loss:   0.780553\n",
      "train loss:   0.813874\n",
      "train loss:   0.685803\n",
      "train loss:   1.100406\n",
      "train loss:   1.010209\n",
      "train loss:   0.940619\n",
      "train loss:   0.887202\n",
      "train loss:   0.990412\n",
      "train loss:   0.795252\n",
      "train loss:   0.743080\n",
      "train loss:   0.810186\n",
      "train loss:   0.926512\n",
      "train loss:   0.810787\n",
      "train loss:   1.239621\n",
      "train loss:   1.234540\n",
      "train loss:   0.745776\n",
      "train loss:   1.087824\n",
      "train loss:   1.139930\n",
      "train loss:   1.201658\n",
      "train loss:   1.227668\n",
      "train loss:   0.884783\n",
      "train loss:   1.028377\n",
      "train loss:   0.857800\n",
      "train loss:   1.026400\n",
      "train loss:   1.221818\n",
      "train loss:   1.011335\n",
      "train loss:   1.126586\n",
      "train loss:   0.983565\n",
      "train loss:   1.019595\n",
      "train loss:   0.969040\n",
      "train loss:   1.056845\n",
      "########### epoch 56 ###########\n",
      "########### loop 7800 ###########\n",
      "test loss:   1.173078   test accuracy:   0.656250\n",
      "########### loop 7800 ###########\n",
      "train loss:   1.055080\n",
      "train loss:   0.869631\n",
      "train loss:   0.969312\n",
      "train loss:   1.184987\n",
      "train loss:   0.901654\n",
      "train loss:   1.055282\n",
      "train loss:   0.778889\n",
      "train loss:   0.798578\n",
      "train loss:   0.749936\n",
      "train loss:   1.045894\n",
      "train loss:   0.830664\n",
      "train loss:   0.987710\n",
      "train loss:   1.169174\n",
      "train loss:   1.066486\n",
      "train loss:   0.936852\n",
      "train loss:   1.152277\n",
      "train loss:   1.019625\n",
      "train loss:   0.949817\n",
      "train loss:   1.099078\n",
      "train loss:   0.730560\n",
      "train loss:   1.121248\n",
      "train loss:   0.941352\n",
      "train loss:   1.019928\n",
      "train loss:   0.951098\n",
      "train loss:   1.077656\n",
      "train loss:   0.766630\n",
      "train loss:   0.931960\n",
      "train loss:   0.872820\n",
      "train loss:   1.117241\n",
      "train loss:   0.862624\n",
      "train loss:   0.851756\n",
      "train loss:   0.983934\n",
      "train loss:   1.001821\n",
      "train loss:   1.003985\n",
      "train loss:   0.834295\n",
      "train loss:   0.916678\n",
      "train loss:   0.996221\n",
      "train loss:   0.934388\n",
      "train loss:   0.871611\n",
      "train loss:   1.032440\n",
      "train loss:   1.088536\n",
      "train loss:   0.728219\n",
      "train loss:   1.195732\n",
      "train loss:   0.909357\n",
      "train loss:   0.860384\n",
      "train loss:   0.823136\n",
      "train loss:   1.049926\n",
      "train loss:   1.090509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.964791\n",
      "train loss:   0.981173\n",
      "########### epoch 56 ###########\n",
      "########### loop 7850 ###########\n",
      "test loss:   0.989203   test accuracy:   0.750000\n",
      "########### loop 7850 ###########\n",
      "train loss:   1.051386\n",
      "train loss:   0.672452\n",
      "train loss:   0.874624\n",
      "train loss:   0.913375\n",
      "train loss:   0.920698\n",
      "train loss:   1.153058\n",
      "train loss:   0.641057\n",
      "train loss:   0.920170\n",
      "train loss:   0.852750\n",
      "train loss:   0.998045\n",
      "train loss:   0.737762\n",
      "train loss:   0.802602\n",
      "train loss:   0.972137\n",
      "train loss:   1.052119\n",
      "train loss:   1.089970\n",
      "train loss:   1.015945\n",
      "train loss:   0.732926\n",
      "train loss:   1.078605\n",
      "train loss:   1.228469\n",
      "train loss:   0.829368\n",
      "train loss:   1.236757\n",
      "train loss:   0.964933\n",
      "train loss:   0.777077\n",
      "train loss:   1.013515\n",
      "train loss:   0.878737\n",
      "train loss:   0.734136\n",
      "train loss:   1.121369\n",
      "train loss:   1.002341\n",
      "train loss:   0.935675\n",
      "train loss:   0.794318\n",
      "train loss:   0.669010\n",
      "train loss:   0.685043\n",
      "train loss:   0.862118\n",
      "train loss:   0.895823\n",
      "train loss:   1.002664\n",
      "train loss:   1.121577\n",
      "train loss:   0.979379\n",
      "train loss:   0.979851\n",
      "train loss:   1.054368\n",
      "train loss:   1.130323\n",
      "train loss:   1.102269\n",
      "train loss:   1.017583\n",
      "train loss:   0.877098\n",
      "train loss:   0.912606\n",
      "train loss:   0.921975\n",
      "train loss:   0.790589\n",
      "train loss:   0.781400\n",
      "train loss:   0.853547\n",
      "train loss:   0.939108\n",
      "train loss:   0.879865\n",
      "########### epoch 57 ###########\n",
      "########### loop 7900 ###########\n",
      "test loss:   1.254405   test accuracy:   0.531250\n",
      "########### loop 7900 ###########\n",
      "train loss:   0.999062\n",
      "train loss:   0.883075\n",
      "train loss:   0.893138\n",
      "train loss:   0.644296\n",
      "train loss:   0.885788\n",
      "train loss:   0.886836\n",
      "train loss:   0.868657\n",
      "train loss:   0.890084\n",
      "train loss:   0.916688\n",
      "train loss:   0.872302\n",
      "train loss:   0.811697\n",
      "train loss:   0.766559\n",
      "train loss:   0.702465\n",
      "train loss:   1.076751\n",
      "train loss:   1.068114\n",
      "train loss:   0.935032\n",
      "train loss:   0.865524\n",
      "train loss:   0.960405\n",
      "train loss:   0.722718\n",
      "train loss:   0.770731\n",
      "train loss:   0.906476\n",
      "train loss:   0.913539\n",
      "train loss:   0.769906\n",
      "train loss:   1.207685\n",
      "train loss:   1.142750\n",
      "train loss:   0.811432\n",
      "train loss:   0.952684\n",
      "train loss:   1.197094\n",
      "train loss:   0.974758\n",
      "train loss:   1.172157\n",
      "train loss:   0.869694\n",
      "train loss:   1.077495\n",
      "train loss:   0.925626\n",
      "train loss:   0.982240\n",
      "train loss:   1.156679\n",
      "train loss:   0.951540\n",
      "train loss:   1.054711\n",
      "train loss:   0.940758\n",
      "train loss:   0.983798\n",
      "train loss:   1.036541\n",
      "train loss:   1.124257\n",
      "train loss:   1.044118\n",
      "train loss:   0.834294\n",
      "train loss:   1.039113\n",
      "train loss:   1.230638\n",
      "train loss:   0.883669\n",
      "train loss:   1.072917\n",
      "train loss:   0.793483\n",
      "train loss:   0.812543\n",
      "train loss:   0.811588\n",
      "########### epoch 57 ###########\n",
      "########### loop 7950 ###########\n",
      "test loss:   1.111120   test accuracy:   0.656250\n",
      "########### loop 7950 ###########\n",
      "train loss:   1.117573\n",
      "train loss:   0.747818\n",
      "train loss:   0.943941\n",
      "train loss:   1.207172\n",
      "train loss:   1.025919\n",
      "train loss:   0.988410\n",
      "train loss:   1.240278\n",
      "train loss:   1.114231\n",
      "train loss:   1.166194\n",
      "train loss:   0.968954\n",
      "train loss:   0.676403\n",
      "train loss:   1.186947\n",
      "train loss:   0.861734\n",
      "train loss:   0.948835\n",
      "train loss:   0.902440\n",
      "train loss:   1.108893\n",
      "train loss:   0.789739\n",
      "train loss:   0.939801\n",
      "train loss:   0.827021\n",
      "train loss:   1.148799\n",
      "train loss:   0.836146\n",
      "train loss:   0.833637\n",
      "train loss:   0.840020\n",
      "train loss:   1.042812\n",
      "train loss:   1.047819\n",
      "train loss:   0.937086\n",
      "train loss:   0.959736\n",
      "train loss:   0.922115\n",
      "train loss:   0.840219\n",
      "train loss:   0.896266\n",
      "train loss:   1.210980\n",
      "train loss:   1.062503\n",
      "train loss:   0.663804\n",
      "train loss:   1.147965\n",
      "train loss:   0.954757\n",
      "train loss:   0.866408\n",
      "train loss:   0.896366\n",
      "train loss:   1.018797\n",
      "train loss:   1.098278\n",
      "train loss:   0.960717\n",
      "train loss:   0.968407\n",
      "train loss:   1.049748\n",
      "train loss:   0.745515\n",
      "train loss:   0.918069\n",
      "train loss:   1.011064\n",
      "train loss:   0.966733\n",
      "train loss:   1.105251\n",
      "train loss:   0.629973\n",
      "train loss:   0.968872\n",
      "train loss:   0.870656\n",
      "########### epoch 57 ###########\n",
      "########### loop 8000 ###########\n",
      "test loss:   1.110787   test accuracy:   0.750000\n",
      "########### loop 8000 ###########\n",
      "train loss:   0.994836\n",
      "train loss:   0.812971\n",
      "train loss:   0.843118\n",
      "train loss:   1.012161\n",
      "train loss:   1.052662\n",
      "train loss:   1.166081\n",
      "train loss:   1.006322\n",
      "train loss:   0.724083\n",
      "train loss:   0.993497\n",
      "train loss:   1.179611\n",
      "train loss:   0.879294\n",
      "train loss:   1.277544\n",
      "train loss:   0.928531\n",
      "train loss:   0.760461\n",
      "train loss:   0.982126\n",
      "train loss:   0.855755\n",
      "train loss:   0.752826\n",
      "train loss:   1.073826\n",
      "train loss:   1.043751\n",
      "train loss:   1.027571\n",
      "train loss:   0.875950\n",
      "train loss:   0.695654\n",
      "train loss:   0.690236\n",
      "train loss:   0.841196\n",
      "train loss:   0.846602\n",
      "train loss:   0.999643\n",
      "train loss:   1.072743\n",
      "train loss:   0.923248\n",
      "train loss:   0.909805\n",
      "train loss:   1.080418\n",
      "train loss:   0.990225\n",
      "train loss:   1.094112\n",
      "train loss:   1.038839\n",
      "train loss:   0.788077\n",
      "train loss:   0.907133\n",
      "train loss:   0.904043\n",
      "train loss:   0.859321\n",
      "train loss:   0.820742\n",
      "train loss:   0.911765\n",
      "train loss:   0.976089\n",
      "train loss:   0.864685\n",
      "train loss:   0.993831\n",
      "train loss:   0.844019\n",
      "train loss:   0.965684\n",
      "train loss:   0.654500\n",
      "train loss:   0.910937\n",
      "train loss:   0.782138\n",
      "train loss:   0.897230\n",
      "train loss:   0.895696\n",
      "train loss:   1.035892\n",
      "########### epoch 58 ###########\n",
      "########### loop 8050 ###########\n",
      "test loss:   1.181263   test accuracy:   0.687500\n",
      "########### loop 8050 ###########\n",
      "train loss:   0.979760\n",
      "train loss:   0.882766\n",
      "train loss:   0.852091\n",
      "train loss:   0.803346\n",
      "train loss:   1.170018\n",
      "train loss:   1.064189\n",
      "train loss:   0.895629\n",
      "train loss:   0.912904\n",
      "train loss:   1.101187\n",
      "train loss:   0.889945\n",
      "train loss:   0.858747\n",
      "train loss:   0.859641\n",
      "train loss:   0.969181\n",
      "train loss:   0.798313\n",
      "train loss:   1.185778\n",
      "train loss:   1.181315\n",
      "train loss:   0.835073\n",
      "train loss:   1.100510\n",
      "train loss:   1.132722\n",
      "train loss:   1.070482\n",
      "train loss:   1.044355\n",
      "train loss:   0.882655\n",
      "train loss:   1.017423\n",
      "train loss:   0.831015\n",
      "train loss:   0.983392\n",
      "train loss:   1.203607\n",
      "train loss:   1.086081\n",
      "train loss:   1.210686\n",
      "train loss:   1.099109\n",
      "train loss:   1.101660\n",
      "train loss:   1.102073\n",
      "train loss:   1.124460\n",
      "train loss:   1.071716\n",
      "train loss:   0.762445\n",
      "train loss:   1.039027\n",
      "train loss:   1.293889\n",
      "train loss:   0.955338\n",
      "train loss:   1.140072\n",
      "train loss:   0.749124\n",
      "train loss:   0.811513\n",
      "train loss:   0.738249\n",
      "train loss:   0.985690\n",
      "train loss:   0.793726\n",
      "train loss:   0.947127\n",
      "train loss:   1.286084\n",
      "train loss:   0.992876\n",
      "train loss:   0.923099\n",
      "train loss:   1.133649\n",
      "train loss:   1.011837\n",
      "train loss:   0.949581\n",
      "########### epoch 58 ###########\n",
      "########### loop 8100 ###########\n",
      "test loss:   1.161863   test accuracy:   0.656250\n",
      "########### loop 8100 ###########\n",
      "train loss:   1.078220\n",
      "train loss:   0.782770\n",
      "train loss:   1.110348\n",
      "train loss:   0.902905\n",
      "train loss:   0.939187\n",
      "train loss:   0.877699\n",
      "train loss:   1.028324\n",
      "train loss:   0.799122\n",
      "train loss:   0.957676\n",
      "train loss:   0.853140\n",
      "train loss:   1.064134\n",
      "train loss:   0.855500\n",
      "train loss:   0.810144\n",
      "train loss:   0.949315\n",
      "train loss:   0.850118\n",
      "train loss:   0.970307\n",
      "train loss:   0.848002\n",
      "train loss:   0.902613\n",
      "train loss:   0.966441\n",
      "train loss:   0.838123\n",
      "train loss:   0.907594\n",
      "train loss:   1.168167\n",
      "train loss:   1.080024\n",
      "train loss:   0.687837\n",
      "train loss:   1.052047\n",
      "train loss:   0.930047\n",
      "train loss:   0.763739\n",
      "train loss:   0.854425\n",
      "train loss:   1.025578\n",
      "train loss:   1.074660\n",
      "train loss:   0.930484\n",
      "train loss:   1.066893\n",
      "train loss:   1.067020\n",
      "train loss:   0.810020\n",
      "train loss:   0.948564\n",
      "train loss:   0.945196\n",
      "train loss:   0.988210\n",
      "train loss:   1.101373\n",
      "train loss:   0.634598\n",
      "train loss:   0.946526\n",
      "train loss:   0.823986\n",
      "train loss:   0.972538\n",
      "train loss:   0.714458\n",
      "train loss:   0.894238\n",
      "train loss:   1.065192\n",
      "train loss:   1.056845\n",
      "train loss:   1.118119\n",
      "train loss:   1.005727\n",
      "train loss:   0.677366\n",
      "train loss:   1.073949\n",
      "########### epoch 58 ###########\n",
      "########### loop 8150 ###########\n",
      "test loss:   1.124487   test accuracy:   0.687500\n",
      "########### loop 8150 ###########\n",
      "train loss:   1.218327\n",
      "train loss:   0.843710\n",
      "train loss:   1.332328\n",
      "train loss:   1.049195\n",
      "train loss:   0.914329\n",
      "train loss:   1.035231\n",
      "train loss:   0.988592\n",
      "train loss:   0.748841\n",
      "train loss:   1.160340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.978418\n",
      "train loss:   0.930690\n",
      "train loss:   0.863390\n",
      "train loss:   0.670558\n",
      "train loss:   0.655079\n",
      "train loss:   0.816339\n",
      "train loss:   0.940170\n",
      "train loss:   1.059700\n",
      "train loss:   1.109440\n",
      "train loss:   1.023206\n",
      "train loss:   0.962804\n",
      "train loss:   1.124235\n",
      "train loss:   1.085136\n",
      "train loss:   1.045098\n",
      "train loss:   0.988787\n",
      "train loss:   0.770062\n",
      "train loss:   0.811464\n",
      "train loss:   1.110913\n",
      "train loss:   0.890780\n",
      "train loss:   0.859239\n",
      "train loss:   0.981268\n",
      "train loss:   0.841629\n",
      "train loss:   0.848705\n",
      "train loss:   1.008222\n",
      "train loss:   0.907254\n",
      "train loss:   0.940140\n",
      "train loss:   0.669318\n",
      "train loss:   0.933608\n",
      "train loss:   0.883492\n",
      "train loss:   0.851883\n",
      "train loss:   0.820867\n",
      "train loss:   0.846890\n",
      "train loss:   0.864250\n",
      "train loss:   0.826027\n",
      "train loss:   0.710130\n",
      "train loss:   0.736196\n",
      "train loss:   1.154292\n",
      "train loss:   0.973450\n",
      "train loss:   0.881768\n",
      "train loss:   0.839557\n",
      "train loss:   0.954778\n",
      "########### epoch 59 ###########\n",
      "########### loop 8200 ###########\n",
      "test loss:   1.037414   test accuracy:   0.656250\n",
      "########### loop 8200 ###########\n",
      "train loss:   0.763018\n",
      "train loss:   0.753988\n",
      "train loss:   0.821930\n",
      "train loss:   0.879396\n",
      "train loss:   0.796744\n",
      "train loss:   1.122259\n",
      "train loss:   1.140672\n",
      "train loss:   0.758800\n",
      "train loss:   0.980705\n",
      "train loss:   1.112320\n",
      "train loss:   0.914859\n",
      "train loss:   1.096589\n",
      "train loss:   0.826441\n",
      "train loss:   1.033974\n",
      "train loss:   0.882229\n",
      "train loss:   0.992088\n",
      "train loss:   1.183403\n",
      "train loss:   0.989109\n",
      "train loss:   1.168390\n",
      "train loss:   0.959458\n",
      "train loss:   1.022514\n",
      "train loss:   1.023647\n",
      "train loss:   1.099173\n",
      "train loss:   1.013811\n",
      "train loss:   0.823302\n",
      "train loss:   0.975809\n",
      "train loss:   1.155911\n",
      "train loss:   0.896645\n",
      "train loss:   1.023379\n",
      "train loss:   0.744719\n",
      "train loss:   0.823834\n",
      "train loss:   0.776124\n",
      "train loss:   0.965467\n",
      "train loss:   0.707736\n",
      "train loss:   0.986862\n",
      "train loss:   1.082253\n",
      "train loss:   1.020611\n",
      "train loss:   0.839969\n",
      "train loss:   1.058736\n",
      "train loss:   0.951604\n",
      "train loss:   1.034909\n",
      "train loss:   1.018567\n",
      "train loss:   0.762053\n",
      "train loss:   1.129624\n",
      "train loss:   0.882196\n",
      "train loss:   1.022852\n",
      "train loss:   0.940731\n",
      "train loss:   1.051852\n",
      "train loss:   0.722064\n",
      "train loss:   0.823402\n",
      "########### epoch 59 ###########\n",
      "########### loop 8250 ###########\n",
      "test loss:   1.088296   test accuracy:   0.562500\n",
      "########### loop 8250 ###########\n",
      "train loss:   0.849385\n",
      "train loss:   1.065738\n",
      "train loss:   0.830982\n",
      "train loss:   0.803683\n",
      "train loss:   0.822872\n",
      "train loss:   0.858695\n",
      "train loss:   0.977863\n",
      "train loss:   0.815818\n",
      "train loss:   0.928472\n",
      "train loss:   0.956906\n",
      "train loss:   0.831116\n",
      "train loss:   0.851926\n",
      "train loss:   1.066064\n",
      "train loss:   1.068971\n",
      "train loss:   0.704982\n",
      "train loss:   1.112705\n",
      "train loss:   0.947211\n",
      "train loss:   0.823892\n",
      "train loss:   0.868523\n",
      "train loss:   1.037675\n",
      "train loss:   1.026896\n",
      "train loss:   0.932357\n",
      "train loss:   1.021042\n",
      "train loss:   1.055427\n",
      "train loss:   0.727494\n",
      "train loss:   0.830891\n",
      "train loss:   0.863975\n",
      "train loss:   0.927372\n",
      "train loss:   1.127062\n",
      "train loss:   0.634055\n",
      "train loss:   0.855105\n",
      "train loss:   0.880749\n",
      "train loss:   0.968803\n",
      "train loss:   0.765668\n",
      "train loss:   0.829863\n",
      "train loss:   1.102010\n",
      "train loss:   1.096721\n",
      "train loss:   1.203167\n",
      "train loss:   1.063745\n",
      "train loss:   0.705532\n",
      "train loss:   1.107979\n",
      "train loss:   1.067673\n",
      "train loss:   0.871737\n",
      "train loss:   1.165576\n",
      "train loss:   0.953125\n",
      "train loss:   0.728233\n",
      "train loss:   0.982987\n",
      "train loss:   0.923996\n",
      "train loss:   0.839736\n",
      "train loss:   1.127427\n",
      "########### epoch 59 ###########\n",
      "########### loop 8300 ###########\n",
      "test loss:   0.989050   test accuracy:   0.562500\n",
      "########### loop 8300 ###########\n",
      "train loss:   1.028302\n",
      "train loss:   0.936791\n",
      "train loss:   0.858983\n",
      "train loss:   0.728783\n",
      "train loss:   0.701254\n",
      "train loss:   1.000976\n",
      "train loss:   0.952416\n",
      "train loss:   1.016604\n",
      "train loss:   1.060241\n",
      "train loss:   0.987285\n",
      "train loss:   1.035136\n",
      "train loss:   1.114787\n",
      "train loss:   1.249587\n",
      "train loss:   1.128033\n",
      "train loss:   1.074699\n",
      "train loss:   0.827579\n",
      "train loss:   0.988607\n",
      "train loss:   0.994303\n",
      "train loss:   0.820661\n",
      "train loss:   0.819026\n",
      "train loss:   0.885927\n",
      "train loss:   1.015045\n",
      "train loss:   0.751553\n",
      "train loss:   1.001187\n",
      "train loss:   0.810095\n",
      "train loss:   0.812561\n",
      "train loss:   0.648412\n",
      "train loss:   0.942188\n",
      "train loss:   0.842136\n",
      "train loss:   0.884484\n",
      "train loss:   0.825958\n",
      "train loss:   0.938060\n",
      "train loss:   0.839120\n",
      "train loss:   0.788795\n",
      "train loss:   0.750188\n",
      "train loss:   0.682523\n",
      "train loss:   1.008480\n",
      "train loss:   0.997922\n",
      "train loss:   0.915786\n",
      "train loss:   0.833585\n",
      "train loss:   1.063320\n",
      "train loss:   0.759779\n",
      "train loss:   0.914654\n",
      "train loss:   0.940420\n",
      "train loss:   0.967068\n",
      "train loss:   0.706791\n",
      "train loss:   1.173837\n",
      "train loss:   1.206714\n",
      "train loss:   0.711703\n",
      "train loss:   1.137100\n",
      "########### epoch 60 ###########\n",
      "########### loop 8350 ###########\n",
      "test loss:   1.105769   test accuracy:   0.656250\n",
      "########### loop 8350 ###########\n",
      "train loss:   1.098270\n",
      "train loss:   1.091016\n",
      "train loss:   1.104763\n",
      "train loss:   0.830484\n",
      "train loss:   1.079942\n",
      "train loss:   0.900784\n",
      "train loss:   1.009220\n",
      "train loss:   1.129171\n",
      "train loss:   0.974900\n",
      "train loss:   1.157349\n",
      "train loss:   0.956444\n",
      "train loss:   1.027371\n",
      "train loss:   1.032955\n",
      "train loss:   1.041609\n",
      "train loss:   0.980758\n",
      "train loss:   0.764575\n",
      "train loss:   0.972653\n",
      "train loss:   1.129823\n",
      "train loss:   1.031119\n",
      "train loss:   1.211671\n",
      "train loss:   0.860722\n",
      "train loss:   0.885710\n",
      "train loss:   0.838396\n",
      "train loss:   1.017596\n",
      "train loss:   0.868045\n",
      "train loss:   0.920271\n",
      "train loss:   1.281186\n",
      "train loss:   1.088049\n",
      "train loss:   0.886933\n",
      "train loss:   1.175050\n",
      "train loss:   0.911291\n",
      "train loss:   0.904365\n",
      "train loss:   1.003511\n",
      "train loss:   0.659107\n",
      "train loss:   1.148197\n",
      "train loss:   0.874322\n",
      "train loss:   0.961860\n",
      "train loss:   0.975329\n",
      "train loss:   1.178996\n",
      "train loss:   0.782371\n",
      "train loss:   0.901188\n",
      "train loss:   0.845304\n",
      "train loss:   1.057938\n",
      "train loss:   0.819518\n",
      "train loss:   0.795500\n",
      "train loss:   0.863422\n",
      "train loss:   0.865687\n",
      "train loss:   0.933857\n",
      "train loss:   0.820787\n",
      "train loss:   0.923527\n",
      "########### epoch 60 ###########\n",
      "########### loop 8400 ###########\n",
      "test loss:   1.072368   test accuracy:   0.750000\n",
      "########### loop 8400 ###########\n",
      "train loss:   0.846644\n",
      "train loss:   0.923626\n",
      "train loss:   0.857880\n",
      "train loss:   1.127343\n",
      "train loss:   1.186668\n",
      "train loss:   0.713315\n",
      "train loss:   1.128589\n",
      "train loss:   0.870364\n",
      "train loss:   0.756066\n",
      "train loss:   0.853501\n",
      "train loss:   1.018333\n",
      "train loss:   1.090839\n",
      "train loss:   0.933516\n",
      "train loss:   0.956532\n",
      "train loss:   0.994997\n",
      "train loss:   0.677025\n",
      "train loss:   0.872107\n",
      "train loss:   0.982052\n",
      "train loss:   0.951050\n",
      "train loss:   1.086571\n",
      "train loss:   0.647598\n",
      "train loss:   0.921915\n",
      "train loss:   0.842962\n",
      "train loss:   1.099717\n",
      "train loss:   0.847997\n",
      "train loss:   0.819837\n",
      "train loss:   0.939715\n",
      "train loss:   1.018864\n",
      "train loss:   1.055475\n",
      "train loss:   1.008888\n",
      "train loss:   0.667238\n",
      "train loss:   1.028052\n",
      "train loss:   1.062445\n",
      "train loss:   0.830960\n",
      "train loss:   1.182445\n",
      "train loss:   0.919127\n",
      "train loss:   0.715450\n",
      "train loss:   0.989791\n",
      "train loss:   0.890477\n",
      "train loss:   0.714819\n",
      "train loss:   1.099387\n",
      "train loss:   0.927205\n",
      "train loss:   0.934441\n",
      "train loss:   0.814681\n",
      "train loss:   0.715330\n",
      "train loss:   0.719888\n",
      "train loss:   0.800977\n",
      "train loss:   0.869265\n",
      "train loss:   0.979542\n",
      "train loss:   1.083449\n",
      "########### epoch 60 ###########\n",
      "########### loop 8450 ###########\n",
      "test loss:   1.134085   test accuracy:   0.656250\n",
      "########### loop 8450 ###########\n",
      "train loss:   0.875453\n",
      "train loss:   1.003276\n",
      "train loss:   1.154517\n",
      "train loss:   1.197236\n",
      "train loss:   1.163570\n",
      "train loss:   0.988799\n",
      "train loss:   0.807265\n",
      "train loss:   0.797320\n",
      "train loss:   1.053236\n",
      "train loss:   0.872661\n",
      "train loss:   0.841117\n",
      "train loss:   0.919142\n",
      "train loss:   0.934551\n",
      "train loss:   0.814561\n",
      "train loss:   1.031523\n",
      "train loss:   0.883636\n",
      "train loss:   1.074252\n",
      "train loss:   0.622504\n",
      "train loss:   0.854442\n",
      "train loss:   0.904591\n",
      "train loss:   0.885851\n",
      "train loss:   0.928022\n",
      "train loss:   0.873265\n",
      "train loss:   0.822910\n",
      "train loss:   0.811272\n",
      "train loss:   0.778479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.723387\n",
      "train loss:   1.085116\n",
      "train loss:   0.977557\n",
      "train loss:   0.881598\n",
      "train loss:   0.826820\n",
      "train loss:   0.930398\n",
      "train loss:   0.639795\n",
      "train loss:   0.795811\n",
      "train loss:   0.843980\n",
      "train loss:   0.895233\n",
      "train loss:   0.757177\n",
      "train loss:   1.186690\n",
      "train loss:   1.165324\n",
      "train loss:   0.705707\n",
      "train loss:   1.022492\n",
      "train loss:   1.051513\n",
      "train loss:   1.041697\n",
      "train loss:   0.963192\n",
      "train loss:   0.882872\n",
      "train loss:   1.007755\n",
      "train loss:   0.892239\n",
      "train loss:   1.002030\n",
      "train loss:   1.168705\n",
      "train loss:   0.817229\n",
      "########### epoch 61 ###########\n",
      "########### loop 8500 ###########\n",
      "test loss:   1.098026   test accuracy:   0.625000\n",
      "########### loop 8500 ###########\n",
      "train loss:   1.018816\n",
      "train loss:   0.871678\n",
      "train loss:   1.028088\n",
      "train loss:   0.975013\n",
      "train loss:   1.052685\n",
      "train loss:   1.021860\n",
      "train loss:   0.879808\n",
      "train loss:   0.974069\n",
      "train loss:   1.141433\n",
      "train loss:   0.821165\n",
      "train loss:   1.061771\n",
      "train loss:   0.840353\n",
      "train loss:   0.848655\n",
      "train loss:   0.897525\n",
      "train loss:   0.992909\n",
      "train loss:   0.778938\n",
      "train loss:   0.997952\n",
      "train loss:   1.269636\n",
      "train loss:   0.990087\n",
      "train loss:   0.890427\n",
      "train loss:   1.203749\n",
      "train loss:   0.965204\n",
      "train loss:   0.917600\n",
      "train loss:   1.002650\n",
      "train loss:   0.746936\n",
      "train loss:   0.993841\n",
      "train loss:   0.758200\n",
      "train loss:   0.947871\n",
      "train loss:   0.869346\n",
      "train loss:   1.190871\n",
      "train loss:   0.699221\n",
      "train loss:   0.843017\n",
      "train loss:   0.838615\n",
      "train loss:   1.029579\n",
      "train loss:   0.805829\n",
      "train loss:   0.805702\n",
      "train loss:   0.904979\n",
      "train loss:   0.829140\n",
      "train loss:   1.010759\n",
      "train loss:   0.849018\n",
      "train loss:   0.866139\n",
      "train loss:   0.976316\n",
      "train loss:   0.880221\n",
      "train loss:   0.940561\n",
      "train loss:   1.121314\n",
      "train loss:   1.067478\n",
      "train loss:   0.703057\n",
      "train loss:   1.064577\n",
      "train loss:   0.859052\n",
      "train loss:   0.877270\n",
      "########### epoch 61 ###########\n",
      "########### loop 8550 ###########\n",
      "test loss:   1.235039   test accuracy:   0.562500\n",
      "########### loop 8550 ###########\n",
      "train loss:   0.844779\n",
      "train loss:   1.029709\n",
      "train loss:   1.019389\n",
      "train loss:   0.980054\n",
      "train loss:   1.011121\n",
      "train loss:   1.047938\n",
      "train loss:   0.721675\n",
      "train loss:   0.909851\n",
      "train loss:   0.905332\n",
      "train loss:   1.013364\n",
      "train loss:   1.100312\n",
      "train loss:   0.623272\n",
      "train loss:   0.909942\n",
      "train loss:   0.953353\n",
      "train loss:   1.069694\n",
      "train loss:   0.830442\n",
      "train loss:   0.862753\n",
      "train loss:   0.958799\n",
      "train loss:   1.043780\n",
      "train loss:   1.162215\n",
      "train loss:   1.076915\n",
      "train loss:   0.704976\n",
      "train loss:   1.017932\n",
      "train loss:   1.059151\n",
      "train loss:   0.809845\n",
      "train loss:   1.165301\n",
      "train loss:   0.995430\n",
      "train loss:   0.779686\n",
      "train loss:   1.052701\n",
      "train loss:   0.857323\n",
      "train loss:   0.818281\n",
      "train loss:   1.078881\n",
      "train loss:   1.047255\n",
      "train loss:   1.042091\n",
      "train loss:   0.835299\n",
      "train loss:   0.798533\n",
      "train loss:   0.755370\n",
      "train loss:   0.957019\n",
      "train loss:   0.840204\n",
      "train loss:   0.967232\n",
      "train loss:   1.095957\n",
      "train loss:   0.914742\n",
      "train loss:   1.050660\n",
      "train loss:   1.129458\n",
      "train loss:   1.270713\n",
      "train loss:   1.161522\n",
      "train loss:   0.912235\n",
      "train loss:   0.753957\n",
      "train loss:   0.827433\n",
      "train loss:   1.066268\n",
      "########### epoch 61 ###########\n",
      "########### loop 8600 ###########\n",
      "test loss:   1.141035   test accuracy:   0.531250\n",
      "########### loop 8600 ###########\n",
      "train loss:   0.955752\n",
      "train loss:   1.013013\n",
      "train loss:   1.042572\n",
      "train loss:   1.008323\n",
      "train loss:   0.737856\n",
      "train loss:   0.971756\n",
      "train loss:   1.008813\n",
      "train loss:   0.893482\n",
      "train loss:   0.543934\n",
      "train loss:   0.873680\n",
      "train loss:   0.918872\n",
      "train loss:   0.975946\n",
      "train loss:   0.951479\n",
      "train loss:   0.884493\n",
      "train loss:   0.888398\n",
      "train loss:   0.910046\n",
      "train loss:   0.767977\n",
      "train loss:   0.667039\n",
      "train loss:   1.279627\n",
      "train loss:   1.077256\n",
      "train loss:   0.879098\n",
      "train loss:   0.768195\n",
      "train loss:   0.904083\n",
      "train loss:   0.673269\n",
      "train loss:   0.773903\n",
      "train loss:   0.886038\n",
      "train loss:   1.000269\n",
      "train loss:   0.824347\n",
      "train loss:   1.157593\n",
      "train loss:   1.158958\n",
      "train loss:   0.744746\n",
      "train loss:   1.147107\n",
      "train loss:   1.094033\n",
      "train loss:   0.970949\n",
      "train loss:   0.969507\n",
      "train loss:   0.825637\n",
      "train loss:   0.944181\n",
      "train loss:   0.877317\n",
      "train loss:   1.042469\n",
      "train loss:   1.102765\n",
      "train loss:   0.821211\n",
      "train loss:   1.106889\n",
      "train loss:   0.917259\n",
      "train loss:   0.979100\n",
      "train loss:   0.935266\n",
      "train loss:   1.050738\n",
      "train loss:   0.966564\n",
      "train loss:   0.807315\n",
      "train loss:   0.966352\n",
      "train loss:   1.161515\n",
      "########### epoch 62 ###########\n",
      "########### loop 8650 ###########\n",
      "test loss:   1.102754   test accuracy:   0.687500\n",
      "########### loop 8650 ###########\n",
      "train loss:   0.988294\n",
      "train loss:   1.003780\n",
      "train loss:   0.817551\n",
      "train loss:   0.808495\n",
      "train loss:   0.826601\n",
      "train loss:   0.976375\n",
      "train loss:   0.754746\n",
      "train loss:   0.895667\n",
      "train loss:   1.169222\n",
      "train loss:   1.148777\n",
      "train loss:   0.922650\n",
      "train loss:   1.198507\n",
      "train loss:   1.046834\n",
      "train loss:   1.001359\n",
      "train loss:   0.920120\n",
      "train loss:   0.679933\n",
      "train loss:   1.142714\n",
      "train loss:   0.828811\n",
      "train loss:   0.972315\n",
      "train loss:   0.923331\n",
      "train loss:   1.174178\n",
      "train loss:   0.635833\n",
      "train loss:   0.850995\n",
      "train loss:   0.791731\n",
      "train loss:   1.063714\n",
      "train loss:   0.841018\n",
      "train loss:   0.819492\n",
      "train loss:   0.894572\n",
      "train loss:   0.869259\n",
      "train loss:   0.941604\n",
      "train loss:   0.851691\n",
      "train loss:   0.931656\n",
      "train loss:   0.918005\n",
      "train loss:   0.931410\n",
      "train loss:   1.000673\n",
      "train loss:   1.158096\n",
      "train loss:   1.097026\n",
      "train loss:   0.888990\n",
      "train loss:   1.002169\n",
      "train loss:   0.851012\n",
      "train loss:   0.827912\n",
      "train loss:   0.852654\n",
      "train loss:   1.054886\n",
      "train loss:   1.057724\n",
      "train loss:   0.986018\n",
      "train loss:   1.094905\n",
      "train loss:   1.031694\n",
      "train loss:   0.668058\n",
      "train loss:   0.843247\n",
      "train loss:   0.973689\n",
      "########### epoch 62 ###########\n",
      "########### loop 8700 ###########\n",
      "test loss:   1.116342   test accuracy:   0.687500\n",
      "########### loop 8700 ###########\n",
      "train loss:   0.921434\n",
      "train loss:   1.078054\n",
      "train loss:   0.683375\n",
      "train loss:   0.994617\n",
      "train loss:   0.895565\n",
      "train loss:   0.939403\n",
      "train loss:   0.792284\n",
      "train loss:   0.859084\n",
      "train loss:   1.015020\n",
      "train loss:   0.999981\n",
      "train loss:   1.105340\n",
      "train loss:   1.080135\n",
      "train loss:   0.663842\n",
      "train loss:   1.080981\n",
      "train loss:   1.072270\n",
      "train loss:   0.796572\n",
      "train loss:   1.174300\n",
      "train loss:   0.997236\n",
      "train loss:   0.821269\n",
      "train loss:   1.084879\n",
      "train loss:   0.857465\n",
      "train loss:   0.739200\n",
      "train loss:   1.079419\n",
      "train loss:   1.054857\n",
      "train loss:   0.972932\n",
      "train loss:   0.834605\n",
      "train loss:   0.628014\n",
      "train loss:   0.658104\n",
      "train loss:   0.808031\n",
      "train loss:   0.847764\n",
      "train loss:   0.961840\n",
      "train loss:   1.064854\n",
      "train loss:   0.910499\n",
      "train loss:   0.968174\n",
      "train loss:   1.112005\n",
      "train loss:   1.280727\n",
      "train loss:   1.165854\n",
      "train loss:   1.201916\n",
      "train loss:   0.982737\n",
      "train loss:   1.070328\n",
      "train loss:   0.994055\n",
      "train loss:   1.017676\n",
      "train loss:   0.762794\n",
      "train loss:   0.888439\n",
      "train loss:   0.799253\n",
      "train loss:   0.808172\n",
      "train loss:   1.069594\n",
      "train loss:   0.870344\n",
      "train loss:   0.909582\n",
      "train loss:   0.679590\n",
      "########### epoch 63 ###########\n",
      "########### loop 8750 ###########\n",
      "test loss:   1.022076   test accuracy:   0.687500\n",
      "########### loop 8750 ###########\n",
      "train loss:   0.968367\n",
      "train loss:   0.879385\n",
      "train loss:   0.821469\n",
      "train loss:   0.867439\n",
      "train loss:   1.032175\n",
      "train loss:   0.938220\n",
      "train loss:   0.785952\n",
      "train loss:   0.787434\n",
      "train loss:   0.805253\n",
      "train loss:   1.170496\n",
      "train loss:   0.976143\n",
      "train loss:   0.904700\n",
      "train loss:   0.879136\n",
      "train loss:   1.029626\n",
      "train loss:   0.752360\n",
      "train loss:   0.888140\n",
      "train loss:   0.891292\n",
      "train loss:   0.826793\n",
      "train loss:   0.746401\n",
      "train loss:   1.244887\n",
      "train loss:   1.202114\n",
      "train loss:   0.732042\n",
      "train loss:   1.012947\n",
      "train loss:   1.121714\n",
      "train loss:   0.993422\n",
      "train loss:   1.065852\n",
      "train loss:   0.766577\n",
      "train loss:   1.033488\n",
      "train loss:   0.879821\n",
      "train loss:   0.972117\n",
      "train loss:   1.153343\n",
      "train loss:   0.873344\n",
      "train loss:   1.134453\n",
      "train loss:   0.923431\n",
      "train loss:   0.944813\n",
      "train loss:   0.948864\n",
      "train loss:   1.081871\n",
      "train loss:   0.945969\n",
      "train loss:   0.833103\n",
      "train loss:   0.963357\n",
      "train loss:   1.147659\n",
      "train loss:   0.918809\n",
      "train loss:   0.951764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.810969\n",
      "train loss:   0.834741\n",
      "train loss:   0.849640\n",
      "train loss:   0.960716\n",
      "train loss:   0.759230\n",
      "train loss:   0.953092\n",
      "train loss:   1.103980\n",
      "########### epoch 63 ###########\n",
      "########### loop 8800 ###########\n",
      "test loss:   1.026484   test accuracy:   0.625000\n",
      "########### loop 8800 ###########\n",
      "train loss:   1.054077\n",
      "train loss:   0.851992\n",
      "train loss:   1.056210\n",
      "train loss:   0.974128\n",
      "train loss:   1.101645\n",
      "train loss:   0.869418\n",
      "train loss:   0.666575\n",
      "train loss:   1.167716\n",
      "train loss:   0.885553\n",
      "train loss:   1.052570\n",
      "train loss:   0.872603\n",
      "train loss:   1.004428\n",
      "train loss:   0.716851\n",
      "train loss:   0.848799\n",
      "train loss:   0.800047\n",
      "train loss:   1.039437\n",
      "train loss:   0.844315\n",
      "train loss:   0.820099\n",
      "train loss:   0.885646\n",
      "train loss:   0.711068\n",
      "train loss:   0.923342\n",
      "train loss:   0.868988\n",
      "train loss:   1.022652\n",
      "train loss:   1.100853\n",
      "train loss:   0.921142\n",
      "train loss:   0.941431\n",
      "train loss:   1.015127\n",
      "train loss:   1.083101\n",
      "train loss:   0.732821\n",
      "train loss:   1.103481\n",
      "train loss:   0.877447\n",
      "train loss:   0.848166\n",
      "train loss:   0.793314\n",
      "train loss:   1.022977\n",
      "train loss:   1.092093\n",
      "train loss:   0.999842\n",
      "train loss:   1.044794\n",
      "train loss:   1.008335\n",
      "train loss:   0.696719\n",
      "train loss:   0.896271\n",
      "train loss:   0.874208\n",
      "train loss:   0.909746\n",
      "train loss:   1.020056\n",
      "train loss:   0.613662\n",
      "train loss:   0.871741\n",
      "train loss:   0.881940\n",
      "train loss:   1.004235\n",
      "train loss:   0.834677\n",
      "train loss:   0.864996\n",
      "train loss:   1.062663\n",
      "########### epoch 63 ###########\n",
      "########### loop 8850 ###########\n",
      "test loss:   1.324457   test accuracy:   0.562500\n",
      "########### loop 8850 ###########\n",
      "train loss:   1.112905\n",
      "train loss:   1.098247\n",
      "train loss:   1.009809\n",
      "train loss:   0.635511\n",
      "train loss:   1.037898\n",
      "train loss:   1.086866\n",
      "train loss:   0.874860\n",
      "train loss:   1.246974\n",
      "train loss:   0.980952\n",
      "train loss:   0.753855\n",
      "train loss:   0.984175\n",
      "train loss:   0.931603\n",
      "train loss:   0.833037\n",
      "train loss:   1.044971\n",
      "train loss:   0.985537\n",
      "train loss:   0.898350\n",
      "train loss:   0.811520\n",
      "train loss:   0.634232\n",
      "train loss:   0.649191\n",
      "train loss:   0.893957\n",
      "train loss:   0.971705\n",
      "train loss:   0.949973\n",
      "train loss:   0.989606\n",
      "train loss:   1.043441\n",
      "train loss:   0.982608\n",
      "train loss:   1.007727\n",
      "train loss:   1.168877\n",
      "train loss:   1.012197\n",
      "train loss:   0.888706\n",
      "train loss:   0.669065\n",
      "train loss:   0.798164\n",
      "train loss:   1.072504\n",
      "train loss:   0.748382\n",
      "train loss:   0.721838\n",
      "train loss:   0.869183\n",
      "train loss:   0.882111\n",
      "train loss:   0.846560\n",
      "train loss:   0.936985\n",
      "train loss:   0.939173\n",
      "train loss:   1.033698\n",
      "train loss:   0.683485\n",
      "train loss:   0.808286\n",
      "train loss:   0.849036\n",
      "train loss:   0.858599\n",
      "train loss:   0.907745\n",
      "train loss:   0.935623\n",
      "train loss:   0.914404\n",
      "train loss:   0.764641\n",
      "train loss:   0.770730\n",
      "train loss:   0.659788\n",
      "########### epoch 64 ###########\n",
      "########### loop 8900 ###########\n",
      "test loss:   1.139398   test accuracy:   0.625000\n",
      "########### loop 8900 ###########\n",
      "train loss:   1.175865\n",
      "train loss:   0.959292\n",
      "train loss:   0.825131\n",
      "train loss:   0.835436\n",
      "train loss:   0.983344\n",
      "train loss:   0.826230\n",
      "train loss:   0.701050\n",
      "train loss:   0.815668\n",
      "train loss:   0.904117\n",
      "train loss:   0.739543\n",
      "train loss:   1.090738\n",
      "train loss:   1.090839\n",
      "train loss:   0.755255\n",
      "train loss:   0.921213\n",
      "train loss:   1.161369\n",
      "train loss:   0.987488\n",
      "train loss:   1.004708\n",
      "train loss:   0.820977\n",
      "train loss:   0.995692\n",
      "train loss:   0.881712\n",
      "train loss:   0.984183\n",
      "train loss:   1.090352\n",
      "train loss:   0.894173\n",
      "train loss:   1.059470\n",
      "train loss:   0.934402\n",
      "train loss:   1.028386\n",
      "train loss:   0.927119\n",
      "train loss:   1.066075\n",
      "train loss:   0.903930\n",
      "train loss:   0.722041\n",
      "train loss:   0.953570\n",
      "train loss:   1.132113\n",
      "train loss:   0.897653\n",
      "train loss:   0.981727\n",
      "train loss:   0.761390\n",
      "train loss:   0.778175\n",
      "train loss:   0.886893\n",
      "train loss:   0.981010\n",
      "train loss:   0.731170\n",
      "train loss:   0.930488\n",
      "train loss:   1.274990\n",
      "train loss:   0.985183\n",
      "train loss:   0.925823\n",
      "train loss:   1.216985\n",
      "train loss:   1.057686\n",
      "train loss:   1.092549\n",
      "train loss:   0.925620\n",
      "train loss:   0.701035\n",
      "train loss:   1.242973\n",
      "train loss:   0.844506\n",
      "########### epoch 64 ###########\n",
      "########### loop 8950 ###########\n",
      "test loss:   1.145511   test accuracy:   0.625000\n",
      "########### loop 8950 ###########\n",
      "train loss:   0.958988\n",
      "train loss:   0.836895\n",
      "train loss:   1.134956\n",
      "train loss:   0.677001\n",
      "train loss:   0.844308\n",
      "train loss:   0.786588\n",
      "train loss:   1.040482\n",
      "train loss:   0.806846\n",
      "train loss:   0.810320\n",
      "train loss:   0.907457\n",
      "train loss:   0.791459\n",
      "train loss:   0.926847\n",
      "train loss:   0.822937\n",
      "train loss:   0.897723\n",
      "train loss:   0.914796\n",
      "train loss:   0.862370\n",
      "train loss:   0.854814\n",
      "train loss:   1.109765\n",
      "train loss:   1.091309\n",
      "train loss:   0.705764\n",
      "train loss:   1.017307\n",
      "train loss:   0.823302\n",
      "train loss:   0.825091\n",
      "train loss:   0.800753\n",
      "train loss:   0.989763\n",
      "train loss:   1.064840\n",
      "train loss:   0.961553\n",
      "train loss:   0.970817\n",
      "train loss:   1.012135\n",
      "train loss:   0.786480\n",
      "train loss:   0.806137\n",
      "train loss:   0.859098\n",
      "train loss:   0.929256\n",
      "train loss:   1.003951\n",
      "train loss:   0.629122\n",
      "train loss:   0.926884\n",
      "train loss:   0.813674\n",
      "train loss:   1.101268\n",
      "train loss:   0.740457\n",
      "train loss:   0.860661\n",
      "train loss:   1.015110\n",
      "train loss:   1.075213\n",
      "train loss:   1.066280\n",
      "train loss:   0.953306\n",
      "train loss:   0.738554\n",
      "train loss:   0.987158\n",
      "train loss:   1.158715\n",
      "train loss:   0.767568\n",
      "train loss:   1.184954\n",
      "train loss:   0.855794\n",
      "########### epoch 64 ###########\n",
      "########### loop 9000 ###########\n",
      "test loss:   1.258175   test accuracy:   0.687500\n",
      "########### loop 9000 ###########\n",
      "train loss:   0.792965\n",
      "train loss:   0.933407\n",
      "train loss:   0.854194\n",
      "train loss:   0.710680\n",
      "train loss:   1.118812\n",
      "train loss:   0.962734\n",
      "train loss:   0.842293\n",
      "train loss:   0.786438\n",
      "train loss:   0.618879\n",
      "train loss:   0.615545\n",
      "train loss:   0.856828\n",
      "train loss:   0.961394\n",
      "train loss:   0.977809\n",
      "train loss:   1.027558\n",
      "train loss:   1.008019\n",
      "train loss:   0.986422\n",
      "train loss:   1.284123\n",
      "train loss:   1.312309\n",
      "train loss:   1.352871\n",
      "train loss:   1.257838\n",
      "train loss:   0.849089\n",
      "train loss:   1.016715\n",
      "train loss:   0.987496\n",
      "train loss:   0.790413\n",
      "train loss:   0.740406\n",
      "train loss:   0.931878\n",
      "train loss:   1.038899\n",
      "train loss:   0.719667\n",
      "train loss:   0.976076\n",
      "train loss:   0.843365\n",
      "train loss:   0.905552\n",
      "train loss:   0.631868\n",
      "train loss:   0.865562\n",
      "train loss:   0.846303\n",
      "train loss:   0.926324\n",
      "train loss:   0.996350\n",
      "train loss:   0.976303\n",
      "train loss:   0.818486\n",
      "train loss:   0.788455\n",
      "train loss:   0.737943\n",
      "train loss:   0.636718\n",
      "train loss:   1.054683\n",
      "train loss:   1.001321\n",
      "train loss:   0.788032\n",
      "train loss:   0.755394\n",
      "train loss:   0.925113\n",
      "train loss:   0.731483\n",
      "train loss:   0.846571\n",
      "train loss:   0.886404\n",
      "train loss:   0.942266\n",
      "########### epoch 65 ###########\n",
      "########### loop 9050 ###########\n",
      "test loss:   1.078743   test accuracy:   0.562500\n",
      "########### loop 9050 ###########\n",
      "train loss:   0.780880\n",
      "train loss:   1.221364\n",
      "train loss:   1.218544\n",
      "train loss:   0.702542\n",
      "train loss:   1.115952\n",
      "train loss:   1.065428\n",
      "train loss:   0.979578\n",
      "train loss:   1.029277\n",
      "train loss:   0.776871\n",
      "train loss:   1.028796\n",
      "train loss:   0.861340\n",
      "train loss:   1.032018\n",
      "train loss:   1.086628\n",
      "train loss:   0.829340\n",
      "train loss:   1.012765\n",
      "train loss:   0.903044\n",
      "train loss:   0.946052\n",
      "train loss:   0.976180\n",
      "train loss:   1.082504\n",
      "train loss:   0.913546\n",
      "train loss:   0.706802\n",
      "train loss:   0.886015\n",
      "train loss:   1.139123\n",
      "train loss:   0.961829\n",
      "train loss:   0.942184\n",
      "train loss:   0.672442\n",
      "train loss:   0.785027\n",
      "train loss:   0.728896\n",
      "train loss:   1.082464\n",
      "train loss:   0.721749\n",
      "train loss:   0.900297\n",
      "train loss:   1.151285\n",
      "train loss:   1.009949\n",
      "train loss:   0.857736\n",
      "train loss:   1.089651\n",
      "train loss:   1.049068\n",
      "train loss:   1.046627\n",
      "train loss:   0.919578\n",
      "train loss:   0.718456\n",
      "train loss:   1.133314\n",
      "train loss:   0.780357\n",
      "train loss:   0.936961\n",
      "train loss:   0.847208\n",
      "train loss:   0.950488\n",
      "train loss:   0.743717\n",
      "train loss:   0.856441\n",
      "train loss:   0.793910\n",
      "train loss:   1.020268\n",
      "train loss:   0.815107\n",
      "train loss:   0.735565\n",
      "########### epoch 65 ###########\n",
      "########### loop 9100 ###########\n",
      "test loss:   1.024301   test accuracy:   0.687500\n",
      "########### loop 9100 ###########\n",
      "train loss:   0.763018\n",
      "train loss:   0.818442\n",
      "train loss:   0.918354\n",
      "train loss:   0.764404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.880594\n",
      "train loss:   0.911665\n",
      "train loss:   0.864976\n",
      "train loss:   0.840098\n",
      "train loss:   1.050328\n",
      "train loss:   1.038330\n",
      "train loss:   0.748418\n",
      "train loss:   1.127643\n",
      "train loss:   0.817257\n",
      "train loss:   0.807234\n",
      "train loss:   0.821548\n",
      "train loss:   0.993588\n",
      "train loss:   1.036866\n",
      "train loss:   0.881882\n",
      "train loss:   0.866761\n",
      "train loss:   1.017804\n",
      "train loss:   0.726832\n",
      "train loss:   0.716228\n",
      "train loss:   0.782531\n",
      "train loss:   0.845358\n",
      "train loss:   1.117448\n",
      "train loss:   0.594951\n",
      "train loss:   0.954531\n",
      "train loss:   0.863919\n",
      "train loss:   1.009079\n",
      "train loss:   0.789290\n",
      "train loss:   0.821911\n",
      "train loss:   0.875986\n",
      "train loss:   0.918801\n",
      "train loss:   1.032929\n",
      "train loss:   0.966279\n",
      "train loss:   0.609836\n",
      "train loss:   1.002031\n",
      "train loss:   1.088595\n",
      "train loss:   0.794618\n",
      "train loss:   1.096025\n",
      "train loss:   0.876581\n",
      "train loss:   0.767748\n",
      "train loss:   0.959658\n",
      "train loss:   0.856284\n",
      "train loss:   0.759143\n",
      "train loss:   1.059724\n",
      "train loss:   0.920035\n",
      "train loss:   0.816701\n",
      "train loss:   0.842175\n",
      "train loss:   0.677689\n",
      "########### epoch 65 ###########\n",
      "########### loop 9150 ###########\n",
      "test loss:   1.069489   test accuracy:   0.593750\n",
      "########### loop 9150 ###########\n",
      "train loss:   0.699595\n",
      "train loss:   0.960873\n",
      "train loss:   0.996527\n",
      "train loss:   0.956672\n",
      "train loss:   1.049433\n",
      "train loss:   0.795313\n",
      "train loss:   0.928203\n",
      "train loss:   1.179253\n",
      "train loss:   1.243471\n",
      "train loss:   1.203873\n",
      "train loss:   1.062452\n",
      "train loss:   0.768358\n",
      "train loss:   0.842332\n",
      "train loss:   1.027810\n",
      "train loss:   0.803572\n",
      "train loss:   0.797186\n",
      "train loss:   1.022983\n",
      "train loss:   0.955671\n",
      "train loss:   0.746327\n",
      "train loss:   0.912248\n",
      "train loss:   0.914567\n",
      "train loss:   0.935762\n",
      "train loss:   0.632397\n",
      "train loss:   0.853788\n",
      "train loss:   0.978830\n",
      "train loss:   0.831417\n",
      "train loss:   0.876969\n",
      "train loss:   0.963582\n",
      "train loss:   0.880638\n",
      "train loss:   0.784058\n",
      "train loss:   0.803000\n",
      "train loss:   0.660296\n",
      "train loss:   1.191038\n",
      "train loss:   0.987297\n",
      "train loss:   0.810818\n",
      "train loss:   0.762460\n",
      "train loss:   0.909134\n",
      "train loss:   0.780716\n",
      "train loss:   0.770589\n",
      "train loss:   0.914950\n",
      "train loss:   0.876856\n",
      "train loss:   0.753126\n",
      "train loss:   1.139832\n",
      "train loss:   1.122821\n",
      "train loss:   0.694938\n",
      "train loss:   0.991770\n",
      "train loss:   1.077237\n",
      "train loss:   1.003502\n",
      "train loss:   0.979678\n",
      "train loss:   0.855911\n",
      "########### epoch 66 ###########\n",
      "########### loop 9200 ###########\n",
      "test loss:   1.033786   test accuracy:   0.687500\n",
      "########### loop 9200 ###########\n",
      "train loss:   0.980022\n",
      "train loss:   0.899984\n",
      "train loss:   0.986651\n",
      "train loss:   0.999386\n",
      "train loss:   0.912182\n",
      "train loss:   1.126037\n",
      "train loss:   0.924380\n",
      "train loss:   1.014483\n",
      "train loss:   1.132169\n",
      "train loss:   1.162876\n",
      "train loss:   0.976853\n",
      "train loss:   0.783583\n",
      "train loss:   0.964962\n",
      "train loss:   1.256975\n",
      "train loss:   0.827919\n",
      "train loss:   0.975083\n",
      "train loss:   0.796002\n",
      "train loss:   0.796720\n",
      "train loss:   0.775872\n",
      "train loss:   0.918736\n",
      "train loss:   0.705034\n",
      "train loss:   0.919200\n",
      "train loss:   1.135005\n",
      "train loss:   1.065379\n",
      "train loss:   0.958632\n",
      "train loss:   1.135132\n",
      "train loss:   1.002813\n",
      "train loss:   0.962327\n",
      "train loss:   0.993935\n",
      "train loss:   0.682212\n",
      "train loss:   1.046235\n",
      "train loss:   0.769043\n",
      "train loss:   0.904827\n",
      "train loss:   0.877575\n",
      "train loss:   1.047361\n",
      "train loss:   0.723271\n",
      "train loss:   0.846497\n",
      "train loss:   0.802552\n",
      "train loss:   1.044197\n",
      "train loss:   0.887114\n",
      "train loss:   0.780533\n",
      "train loss:   0.816568\n",
      "train loss:   1.023859\n",
      "train loss:   0.973861\n",
      "train loss:   0.868622\n",
      "train loss:   0.922104\n",
      "train loss:   0.942776\n",
      "train loss:   0.890771\n",
      "train loss:   0.848906\n",
      "train loss:   0.961806\n",
      "########### epoch 66 ###########\n",
      "########### loop 9250 ###########\n",
      "test loss:   1.085584   test accuracy:   0.687500\n",
      "########### loop 9250 ###########\n",
      "train loss:   1.052306\n",
      "train loss:   0.673687\n",
      "train loss:   1.090348\n",
      "train loss:   0.837324\n",
      "train loss:   0.837304\n",
      "train loss:   0.802006\n",
      "train loss:   1.007069\n",
      "train loss:   0.997555\n",
      "train loss:   0.914576\n",
      "train loss:   1.059157\n",
      "train loss:   1.030778\n",
      "train loss:   0.655837\n",
      "train loss:   0.844535\n",
      "train loss:   0.913714\n",
      "train loss:   0.856954\n",
      "train loss:   1.101516\n",
      "train loss:   0.655791\n",
      "train loss:   0.903005\n",
      "train loss:   0.833893\n",
      "train loss:   0.954946\n",
      "train loss:   0.759904\n",
      "train loss:   0.872629\n",
      "train loss:   0.991177\n",
      "train loss:   0.988348\n",
      "train loss:   1.071757\n",
      "train loss:   0.949908\n",
      "train loss:   0.696042\n",
      "train loss:   1.031354\n",
      "train loss:   1.196227\n",
      "train loss:   0.770158\n",
      "train loss:   1.187353\n",
      "train loss:   0.993810\n",
      "train loss:   0.767934\n",
      "train loss:   0.897180\n",
      "train loss:   0.821116\n",
      "train loss:   0.754104\n",
      "train loss:   1.036632\n",
      "train loss:   0.942708\n",
      "train loss:   0.908853\n",
      "train loss:   0.794875\n",
      "train loss:   0.654665\n",
      "train loss:   0.648260\n",
      "train loss:   0.901867\n",
      "train loss:   0.893688\n",
      "train loss:   0.926509\n",
      "train loss:   0.996610\n",
      "train loss:   0.891310\n",
      "train loss:   1.054712\n",
      "train loss:   1.170755\n",
      "train loss:   1.261579\n",
      "########### epoch 66 ###########\n",
      "########### loop 9300 ###########\n",
      "test loss:   1.313067   test accuracy:   0.562500\n",
      "########### loop 9300 ###########\n",
      "train loss:   1.160261\n",
      "train loss:   0.959434\n",
      "train loss:   0.718476\n",
      "train loss:   0.753868\n",
      "train loss:   1.009343\n",
      "train loss:   0.749915\n",
      "train loss:   0.770927\n",
      "train loss:   0.834299\n",
      "train loss:   0.897013\n",
      "train loss:   0.812940\n",
      "train loss:   0.992775\n",
      "train loss:   0.911173\n",
      "train loss:   0.846799\n",
      "train loss:   0.675124\n",
      "train loss:   0.883070\n",
      "train loss:   0.920467\n",
      "train loss:   0.821260\n",
      "train loss:   0.859754\n",
      "train loss:   0.777588\n",
      "train loss:   0.770496\n",
      "train loss:   0.764076\n",
      "train loss:   0.698348\n",
      "train loss:   0.701521\n",
      "train loss:   1.095847\n",
      "train loss:   0.998445\n",
      "train loss:   0.804014\n",
      "train loss:   0.782905\n",
      "train loss:   0.880525\n",
      "train loss:   0.698011\n",
      "train loss:   0.729271\n",
      "train loss:   0.831469\n",
      "train loss:   0.932859\n",
      "train loss:   0.769295\n",
      "train loss:   1.134791\n",
      "train loss:   1.156154\n",
      "train loss:   0.740121\n",
      "train loss:   0.974434\n",
      "train loss:   1.099381\n",
      "train loss:   0.962652\n",
      "train loss:   0.896002\n",
      "train loss:   0.830261\n",
      "train loss:   0.905603\n",
      "train loss:   0.866907\n",
      "train loss:   0.982204\n",
      "train loss:   1.047904\n",
      "train loss:   0.773398\n",
      "train loss:   0.969202\n",
      "train loss:   0.905947\n",
      "train loss:   0.962568\n",
      "train loss:   0.930895\n",
      "########### epoch 67 ###########\n",
      "########### loop 9350 ###########\n",
      "test loss:   1.027884   test accuracy:   0.656250\n",
      "########### loop 9350 ###########\n",
      "train loss:   1.013366\n",
      "train loss:   0.940234\n",
      "train loss:   0.729858\n",
      "train loss:   0.881321\n",
      "train loss:   1.099274\n",
      "train loss:   0.868422\n",
      "train loss:   0.898571\n",
      "train loss:   0.673887\n",
      "train loss:   0.745906\n",
      "train loss:   0.675420\n",
      "train loss:   1.090041\n",
      "train loss:   0.695626\n",
      "train loss:   0.918988\n",
      "train loss:   1.167934\n",
      "train loss:   1.033952\n",
      "train loss:   0.913672\n",
      "train loss:   1.143899\n",
      "train loss:   1.060558\n",
      "train loss:   1.001413\n",
      "train loss:   0.979773\n",
      "train loss:   0.790875\n",
      "train loss:   1.218685\n",
      "train loss:   0.837749\n",
      "train loss:   0.980292\n",
      "train loss:   0.908875\n",
      "train loss:   1.053528\n",
      "train loss:   0.736157\n",
      "train loss:   0.854913\n",
      "train loss:   0.896096\n",
      "train loss:   1.053013\n",
      "train loss:   0.846744\n",
      "train loss:   0.808026\n",
      "train loss:   0.905307\n",
      "train loss:   0.915039\n",
      "train loss:   0.975822\n",
      "train loss:   0.764474\n",
      "train loss:   0.755086\n",
      "train loss:   0.887673\n",
      "train loss:   0.976064\n",
      "train loss:   0.795501\n",
      "train loss:   1.091744\n",
      "train loss:   1.031059\n",
      "train loss:   0.679862\n",
      "train loss:   1.119951\n",
      "train loss:   0.901364\n",
      "train loss:   0.805105\n",
      "train loss:   0.816162\n",
      "train loss:   1.008947\n",
      "train loss:   1.033917\n",
      "train loss:   0.884583\n",
      "########### epoch 67 ###########\n",
      "########### loop 9400 ###########\n",
      "test loss:   1.091136   test accuracy:   0.718750\n",
      "########### loop 9400 ###########\n",
      "train loss:   1.002368\n",
      "train loss:   1.014555\n",
      "train loss:   0.697608\n",
      "train loss:   0.852134\n",
      "train loss:   0.790383\n",
      "train loss:   0.924199\n",
      "train loss:   1.048086\n",
      "train loss:   0.612793\n",
      "train loss:   0.855538\n",
      "train loss:   0.854482\n",
      "train loss:   1.009301\n",
      "train loss:   0.779027\n",
      "train loss:   0.771801\n",
      "train loss:   1.006036\n",
      "train loss:   1.050484\n",
      "train loss:   1.099120\n",
      "train loss:   1.059868\n",
      "train loss:   0.641450\n",
      "train loss:   1.050754\n",
      "train loss:   1.108499\n",
      "train loss:   0.891588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.264167\n",
      "train loss:   1.017583\n",
      "train loss:   0.692741\n",
      "train loss:   0.967164\n",
      "train loss:   0.883317\n",
      "train loss:   0.745005\n",
      "train loss:   1.010966\n",
      "train loss:   1.011497\n",
      "train loss:   0.965593\n",
      "train loss:   0.821651\n",
      "train loss:   0.674212\n",
      "train loss:   0.649817\n",
      "train loss:   0.884521\n",
      "train loss:   0.824422\n",
      "train loss:   0.928266\n",
      "train loss:   0.968476\n",
      "train loss:   0.973228\n",
      "train loss:   0.923952\n",
      "train loss:   1.093138\n",
      "train loss:   1.158340\n",
      "train loss:   1.092251\n",
      "train loss:   0.983526\n",
      "train loss:   0.803652\n",
      "train loss:   0.811060\n",
      "train loss:   0.955961\n",
      "train loss:   0.808143\n",
      "train loss:   0.835727\n",
      "train loss:   0.935202\n",
      "train loss:   1.024651\n",
      "########### epoch 68 ###########\n",
      "########### loop 9450 ###########\n",
      "test loss:   1.050115   test accuracy:   0.625000\n",
      "########### loop 9450 ###########\n",
      "train loss:   0.868778\n",
      "train loss:   1.123556\n",
      "train loss:   0.891903\n",
      "train loss:   1.046118\n",
      "train loss:   0.647469\n",
      "train loss:   0.902564\n",
      "train loss:   0.929766\n",
      "train loss:   0.913830\n",
      "train loss:   0.956181\n",
      "train loss:   0.870156\n",
      "train loss:   0.819674\n",
      "train loss:   0.799632\n",
      "train loss:   0.792824\n",
      "train loss:   0.645402\n",
      "train loss:   1.166406\n",
      "train loss:   1.061031\n",
      "train loss:   0.997342\n",
      "train loss:   0.974475\n",
      "train loss:   1.159122\n",
      "train loss:   1.014315\n",
      "train loss:   0.820281\n",
      "train loss:   0.859582\n",
      "train loss:   0.842557\n",
      "train loss:   0.767434\n",
      "train loss:   1.133757\n",
      "train loss:   1.102703\n",
      "train loss:   0.741566\n",
      "train loss:   0.990179\n",
      "train loss:   1.135267\n",
      "train loss:   1.036366\n",
      "train loss:   0.922113\n",
      "train loss:   0.802837\n",
      "train loss:   0.997520\n",
      "train loss:   0.816383\n",
      "train loss:   0.985086\n",
      "train loss:   1.135070\n",
      "train loss:   0.930151\n",
      "train loss:   1.041706\n",
      "train loss:   1.030967\n",
      "train loss:   1.035018\n",
      "train loss:   0.943840\n",
      "train loss:   1.047383\n",
      "train loss:   0.873979\n",
      "train loss:   0.676716\n",
      "train loss:   0.937032\n",
      "train loss:   1.160529\n",
      "train loss:   0.882486\n",
      "train loss:   0.955067\n",
      "train loss:   0.772998\n",
      "train loss:   0.733841\n",
      "########### epoch 68 ###########\n",
      "########### loop 9500 ###########\n",
      "test loss:   1.043130   test accuracy:   0.625000\n",
      "########### loop 9500 ###########\n",
      "train loss:   0.765945\n",
      "train loss:   0.952517\n",
      "train loss:   0.724720\n",
      "train loss:   0.987922\n",
      "train loss:   1.200509\n",
      "train loss:   0.963603\n",
      "train loss:   0.853846\n",
      "train loss:   1.079756\n",
      "train loss:   0.935556\n",
      "train loss:   0.959434\n",
      "train loss:   0.891989\n",
      "train loss:   0.665289\n",
      "train loss:   1.124566\n",
      "train loss:   0.829402\n",
      "train loss:   0.978647\n",
      "train loss:   0.868366\n",
      "train loss:   1.182831\n",
      "train loss:   0.732782\n",
      "train loss:   0.857284\n",
      "train loss:   0.822258\n",
      "train loss:   1.047108\n",
      "train loss:   0.829943\n",
      "train loss:   0.770947\n",
      "train loss:   0.873721\n",
      "train loss:   0.931889\n",
      "train loss:   0.953776\n",
      "train loss:   0.777821\n",
      "train loss:   0.879624\n",
      "train loss:   0.941426\n",
      "train loss:   0.791495\n",
      "train loss:   0.783655\n",
      "train loss:   1.011241\n",
      "train loss:   1.092782\n",
      "train loss:   0.747281\n",
      "train loss:   1.028927\n",
      "train loss:   0.821789\n",
      "train loss:   0.800284\n",
      "train loss:   0.797563\n",
      "train loss:   0.984325\n",
      "train loss:   1.016015\n",
      "train loss:   0.937638\n",
      "train loss:   1.105668\n",
      "train loss:   1.071444\n",
      "train loss:   0.641416\n",
      "train loss:   0.862038\n",
      "train loss:   0.954258\n",
      "train loss:   0.846538\n",
      "train loss:   1.002189\n",
      "train loss:   0.641281\n",
      "train loss:   0.914833\n",
      "########### epoch 68 ###########\n",
      "########### loop 9550 ###########\n",
      "test loss:   1.004956   test accuracy:   0.781250\n",
      "########### loop 9550 ###########\n",
      "train loss:   0.912616\n",
      "train loss:   0.889959\n",
      "train loss:   0.801560\n",
      "train loss:   0.800994\n",
      "train loss:   1.025908\n",
      "train loss:   1.036887\n",
      "train loss:   1.177999\n",
      "train loss:   1.114187\n",
      "train loss:   0.636026\n",
      "train loss:   1.117407\n",
      "train loss:   1.022655\n",
      "train loss:   0.786645\n",
      "train loss:   1.156607\n",
      "train loss:   1.000496\n",
      "train loss:   0.808302\n",
      "train loss:   1.013343\n",
      "train loss:   0.903133\n",
      "train loss:   0.877380\n",
      "train loss:   1.033549\n",
      "train loss:   0.986017\n",
      "train loss:   0.940334\n",
      "train loss:   0.772499\n",
      "train loss:   0.733052\n",
      "train loss:   0.743726\n",
      "train loss:   1.018076\n",
      "train loss:   0.971339\n",
      "train loss:   0.935465\n",
      "train loss:   1.075175\n",
      "train loss:   1.008061\n",
      "train loss:   0.901470\n",
      "train loss:   0.985774\n",
      "train loss:   1.200599\n",
      "train loss:   1.126471\n",
      "train loss:   0.900096\n",
      "train loss:   0.763532\n",
      "train loss:   0.729177\n",
      "train loss:   1.113561\n",
      "train loss:   0.734573\n",
      "train loss:   0.722566\n",
      "train loss:   0.863275\n",
      "train loss:   0.948274\n",
      "train loss:   0.781535\n",
      "train loss:   1.065371\n",
      "train loss:   0.893499\n",
      "train loss:   0.866404\n",
      "train loss:   0.583736\n",
      "train loss:   0.781163\n",
      "train loss:   0.879500\n",
      "train loss:   0.835502\n",
      "train loss:   0.901265\n",
      "########### epoch 69 ###########\n",
      "########### loop 9600 ###########\n",
      "test loss:   1.150449   test accuracy:   0.625000\n",
      "########### loop 9600 ###########\n",
      "train loss:   0.830418\n",
      "train loss:   0.840386\n",
      "train loss:   0.810391\n",
      "train loss:   0.705412\n",
      "train loss:   0.686613\n",
      "train loss:   1.139829\n",
      "train loss:   0.987497\n",
      "train loss:   0.951006\n",
      "train loss:   0.961449\n",
      "train loss:   1.114521\n",
      "train loss:   0.874907\n",
      "train loss:   0.724708\n",
      "train loss:   0.837222\n",
      "train loss:   1.032493\n",
      "train loss:   0.834120\n",
      "train loss:   1.252069\n",
      "train loss:   1.199734\n",
      "train loss:   0.792253\n",
      "train loss:   0.946040\n",
      "train loss:   1.065595\n",
      "train loss:   0.948177\n",
      "train loss:   1.086370\n",
      "train loss:   0.801784\n",
      "train loss:   0.982365\n",
      "train loss:   0.896641\n",
      "train loss:   0.972740\n",
      "train loss:   1.037498\n",
      "train loss:   0.857422\n",
      "train loss:   1.028032\n",
      "train loss:   0.868462\n",
      "train loss:   0.931476\n",
      "train loss:   0.895775\n",
      "train loss:   1.019472\n",
      "train loss:   1.054653\n",
      "train loss:   0.904136\n",
      "train loss:   1.038067\n",
      "train loss:   1.066687\n",
      "train loss:   0.865848\n",
      "train loss:   0.981581\n",
      "train loss:   0.796161\n",
      "train loss:   0.806449\n",
      "train loss:   0.781268\n",
      "train loss:   1.012262\n",
      "train loss:   0.772437\n",
      "train loss:   0.872718\n",
      "train loss:   1.271172\n",
      "train loss:   0.939256\n",
      "train loss:   0.846819\n",
      "train loss:   1.084257\n",
      "train loss:   0.983014\n",
      "########### epoch 69 ###########\n",
      "########### loop 9650 ###########\n",
      "test loss:   1.039088   test accuracy:   0.781250\n",
      "########### loop 9650 ###########\n",
      "train loss:   0.972492\n",
      "train loss:   0.945464\n",
      "train loss:   0.745546\n",
      "train loss:   1.178313\n",
      "train loss:   0.802682\n",
      "train loss:   0.923974\n",
      "train loss:   0.808499\n",
      "train loss:   1.027873\n",
      "train loss:   0.741964\n",
      "train loss:   0.820920\n",
      "train loss:   0.807777\n",
      "train loss:   1.072673\n",
      "train loss:   0.832837\n",
      "train loss:   0.723228\n",
      "train loss:   0.828755\n",
      "train loss:   0.803094\n",
      "train loss:   0.887806\n",
      "train loss:   0.800383\n",
      "train loss:   0.882933\n",
      "train loss:   0.935572\n",
      "train loss:   0.857882\n",
      "train loss:   0.816554\n",
      "train loss:   1.090039\n",
      "train loss:   0.981945\n",
      "train loss:   0.644430\n",
      "train loss:   1.047268\n",
      "train loss:   0.810389\n",
      "train loss:   0.760709\n",
      "train loss:   0.818330\n",
      "train loss:   0.985892\n",
      "train loss:   0.973845\n",
      "train loss:   0.881519\n",
      "train loss:   0.970441\n",
      "train loss:   0.997376\n",
      "train loss:   0.746446\n",
      "train loss:   0.793676\n",
      "train loss:   0.921540\n",
      "train loss:   0.882915\n",
      "train loss:   1.010508\n",
      "train loss:   0.589338\n",
      "train loss:   0.852760\n",
      "train loss:   0.833590\n",
      "train loss:   0.996438\n",
      "train loss:   0.762579\n",
      "train loss:   0.827817\n",
      "train loss:   0.948666\n",
      "train loss:   0.955537\n",
      "train loss:   1.028690\n",
      "train loss:   0.967373\n",
      "train loss:   0.664741\n",
      "########### epoch 69 ###########\n",
      "########### loop 9700 ###########\n",
      "test loss:   1.260330   test accuracy:   0.562500\n",
      "########### loop 9700 ###########\n",
      "train loss:   1.009640\n",
      "train loss:   1.143536\n",
      "train loss:   0.745283\n",
      "train loss:   1.183837\n",
      "train loss:   0.857718\n",
      "train loss:   0.776428\n",
      "train loss:   0.966603\n",
      "train loss:   0.869221\n",
      "train loss:   0.765560\n",
      "train loss:   1.042133\n",
      "train loss:   1.004377\n",
      "train loss:   0.876968\n",
      "train loss:   0.754548\n",
      "train loss:   0.600937\n",
      "train loss:   0.629324\n",
      "train loss:   0.797265\n",
      "train loss:   0.861767\n",
      "train loss:   1.011653\n",
      "train loss:   1.011649\n",
      "train loss:   0.950801\n",
      "train loss:   0.948387\n",
      "train loss:   1.038837\n",
      "train loss:   1.132249\n",
      "train loss:   1.150947\n",
      "train loss:   1.061659\n",
      "train loss:   0.755709\n",
      "train loss:   0.794721\n",
      "train loss:   0.980498\n",
      "train loss:   0.799467\n",
      "train loss:   0.788611\n",
      "train loss:   0.855819\n",
      "train loss:   0.800576\n",
      "train loss:   0.741047\n",
      "train loss:   0.942636\n",
      "train loss:   0.928680\n",
      "train loss:   1.024497\n",
      "train loss:   0.696566\n",
      "train loss:   0.968392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.157168\n",
      "train loss:   0.884897\n",
      "train loss:   0.922638\n",
      "train loss:   0.876501\n",
      "train loss:   0.800647\n",
      "train loss:   0.833535\n",
      "train loss:   0.810872\n",
      "train loss:   0.793337\n",
      "train loss:   1.167007\n",
      "train loss:   0.933478\n",
      "train loss:   0.875718\n",
      "train loss:   0.912826\n",
      "########### epoch 70 ###########\n",
      "########### loop 9750 ###########\n",
      "test loss:   1.294817   test accuracy:   0.500000\n",
      "########### loop 9750 ###########\n",
      "train loss:   1.056417\n",
      "train loss:   0.839420\n",
      "train loss:   0.693702\n",
      "train loss:   0.848943\n",
      "train loss:   0.942432\n",
      "train loss:   0.736819\n",
      "train loss:   1.154879\n",
      "train loss:   1.160783\n",
      "train loss:   0.755750\n",
      "train loss:   0.895288\n",
      "train loss:   1.153081\n",
      "train loss:   1.036865\n",
      "train loss:   0.944223\n",
      "train loss:   0.811316\n",
      "train loss:   0.959165\n",
      "train loss:   0.830212\n",
      "train loss:   0.969952\n",
      "train loss:   1.049155\n",
      "train loss:   0.843189\n",
      "train loss:   0.979903\n",
      "train loss:   0.935091\n",
      "train loss:   0.937869\n",
      "train loss:   0.977917\n",
      "train loss:   1.030926\n",
      "train loss:   0.967026\n",
      "train loss:   0.753278\n",
      "train loss:   0.980617\n",
      "train loss:   1.170411\n",
      "train loss:   0.871985\n",
      "train loss:   0.993703\n",
      "train loss:   0.846367\n",
      "train loss:   0.740453\n",
      "train loss:   0.766901\n",
      "train loss:   1.003209\n",
      "train loss:   0.761881\n",
      "train loss:   0.902663\n",
      "train loss:   1.167405\n",
      "train loss:   0.978749\n",
      "train loss:   0.846659\n",
      "train loss:   1.034158\n",
      "train loss:   1.014727\n",
      "train loss:   1.037901\n",
      "train loss:   0.959933\n",
      "train loss:   0.687262\n",
      "train loss:   1.105937\n",
      "train loss:   0.769465\n",
      "train loss:   0.957113\n",
      "train loss:   0.834554\n",
      "train loss:   0.953098\n",
      "train loss:   0.735179\n",
      "########### epoch 70 ###########\n",
      "########### loop 9800 ###########\n",
      "test loss:   1.067847   test accuracy:   0.656250\n",
      "########### loop 9800 ###########\n",
      "train loss:   0.861929\n",
      "train loss:   0.777272\n",
      "train loss:   1.078331\n",
      "train loss:   0.728042\n",
      "train loss:   0.765228\n",
      "train loss:   0.778936\n",
      "train loss:   0.813259\n",
      "train loss:   0.893757\n",
      "train loss:   0.728489\n",
      "train loss:   0.772819\n",
      "train loss:   0.920520\n",
      "train loss:   0.821752\n",
      "train loss:   0.816032\n",
      "train loss:   1.045566\n",
      "train loss:   1.015303\n",
      "train loss:   0.700413\n",
      "train loss:   1.064150\n",
      "train loss:   0.817990\n",
      "train loss:   0.757603\n",
      "train loss:   0.845533\n",
      "train loss:   0.946884\n",
      "train loss:   1.036547\n",
      "train loss:   0.874719\n",
      "train loss:   0.895693\n",
      "train loss:   0.967228\n",
      "train loss:   0.726433\n",
      "train loss:   0.864888\n",
      "train loss:   0.893164\n",
      "train loss:   0.846954\n",
      "train loss:   1.007018\n",
      "train loss:   0.647686\n",
      "train loss:   0.811796\n",
      "train loss:   0.793334\n",
      "train loss:   1.014728\n",
      "train loss:   0.728157\n",
      "train loss:   0.834174\n",
      "train loss:   1.050385\n",
      "train loss:   1.015904\n",
      "train loss:   1.059708\n",
      "train loss:   1.094541\n",
      "train loss:   0.611545\n",
      "train loss:   1.072254\n",
      "train loss:   1.037077\n",
      "train loss:   0.807718\n",
      "train loss:   1.155630\n",
      "train loss:   0.920764\n",
      "train loss:   0.751577\n",
      "train loss:   1.005645\n",
      "train loss:   0.860910\n",
      "train loss:   0.737819\n",
      "########### epoch 70 ###########\n",
      "########### loop 9850 ###########\n",
      "test loss:   1.134077   test accuracy:   0.625000\n",
      "########### loop 9850 ###########\n",
      "train loss:   1.073073\n",
      "train loss:   0.888672\n",
      "train loss:   0.863832\n",
      "train loss:   0.762467\n",
      "train loss:   0.601958\n",
      "train loss:   0.623647\n",
      "train loss:   0.809063\n",
      "train loss:   0.928951\n",
      "train loss:   0.975268\n",
      "train loss:   1.011512\n",
      "train loss:   1.002598\n",
      "train loss:   0.980206\n",
      "train loss:   0.987797\n",
      "train loss:   1.060402\n",
      "train loss:   1.000973\n",
      "train loss:   0.983526\n",
      "train loss:   0.756325\n",
      "train loss:   0.821802\n",
      "train loss:   1.026208\n",
      "train loss:   0.735271\n",
      "train loss:   0.819953\n",
      "train loss:   0.863535\n",
      "train loss:   0.902239\n",
      "train loss:   0.699434\n",
      "train loss:   0.856613\n",
      "train loss:   0.842702\n",
      "train loss:   0.810727\n",
      "train loss:   0.647124\n",
      "train loss:   0.816678\n",
      "train loss:   0.861261\n",
      "train loss:   0.779628\n",
      "train loss:   0.811427\n",
      "train loss:   0.814776\n",
      "train loss:   0.750876\n",
      "train loss:   0.714269\n",
      "train loss:   0.723208\n",
      "train loss:   0.689718\n",
      "train loss:   1.028430\n",
      "train loss:   0.984829\n",
      "train loss:   0.787377\n",
      "train loss:   0.746589\n",
      "train loss:   0.854301\n",
      "train loss:   0.712763\n",
      "train loss:   0.770514\n",
      "train loss:   0.710720\n",
      "train loss:   0.835125\n",
      "train loss:   0.749346\n",
      "train loss:   1.029133\n",
      "train loss:   1.100471\n",
      "train loss:   0.681531\n",
      "########### epoch 71 ###########\n",
      "########### loop 9900 ###########\n",
      "test loss:   0.900862   test accuracy:   0.687500\n",
      "########### loop 9900 ###########\n",
      "train loss:   1.014397\n",
      "train loss:   1.052740\n",
      "train loss:   0.982704\n",
      "train loss:   0.902331\n",
      "train loss:   0.760131\n",
      "train loss:   0.926166\n",
      "train loss:   0.818885\n",
      "train loss:   0.881500\n",
      "train loss:   1.013439\n",
      "train loss:   0.808684\n",
      "train loss:   0.977028\n",
      "train loss:   0.905463\n",
      "train loss:   0.971059\n",
      "train loss:   0.884635\n",
      "train loss:   1.009610\n",
      "train loss:   0.920598\n",
      "train loss:   0.729499\n",
      "train loss:   0.883270\n",
      "train loss:   1.036956\n",
      "train loss:   0.806880\n",
      "train loss:   0.969436\n",
      "train loss:   0.721855\n",
      "train loss:   0.681783\n",
      "train loss:   0.721195\n",
      "train loss:   0.950884\n",
      "train loss:   0.722437\n",
      "train loss:   0.897027\n",
      "train loss:   1.206255\n",
      "train loss:   0.924345\n",
      "train loss:   0.837254\n",
      "train loss:   1.004048\n",
      "train loss:   1.037247\n",
      "train loss:   0.961297\n",
      "train loss:   0.932028\n",
      "train loss:   0.703831\n",
      "train loss:   1.070424\n",
      "train loss:   0.744229\n",
      "train loss:   0.992218\n",
      "train loss:   0.814741\n",
      "train loss:   1.034672\n",
      "train loss:   0.706066\n",
      "train loss:   0.811193\n",
      "train loss:   0.775887\n",
      "train loss:   1.135082\n",
      "train loss:   0.818999\n",
      "train loss:   0.785235\n",
      "train loss:   0.842046\n",
      "train loss:   0.860767\n",
      "train loss:   0.897771\n",
      "train loss:   0.773419\n",
      "########### epoch 71 ###########\n",
      "########### loop 9950 ###########\n",
      "test loss:   1.074154   test accuracy:   0.656250\n",
      "########### loop 9950 ###########\n",
      "train loss:   0.829697\n",
      "train loss:   0.933758\n",
      "train loss:   0.871489\n",
      "train loss:   0.893900\n",
      "train loss:   1.167113\n",
      "train loss:   0.983317\n",
      "train loss:   0.655020\n",
      "train loss:   1.023285\n",
      "train loss:   0.801756\n",
      "train loss:   0.751680\n",
      "train loss:   0.862370\n",
      "train loss:   0.957369\n",
      "train loss:   0.961564\n",
      "train loss:   0.914421\n",
      "train loss:   0.959060\n",
      "train loss:   0.994454\n",
      "train loss:   0.772298\n",
      "train loss:   0.748335\n",
      "train loss:   0.896300\n",
      "train loss:   0.881743\n",
      "train loss:   1.008715\n",
      "train loss:   0.647401\n",
      "train loss:   0.837398\n",
      "train loss:   0.782584\n",
      "train loss:   1.053942\n",
      "train loss:   0.760731\n",
      "train loss:   0.883584\n",
      "train loss:   1.024764\n",
      "train loss:   0.962131\n",
      "train loss:   1.143885\n",
      "train loss:   1.047340\n",
      "train loss:   0.655011\n",
      "train loss:   1.065402\n",
      "train loss:   1.134205\n",
      "train loss:   0.727160\n",
      "train loss:   1.104097\n",
      "train loss:   0.925360\n",
      "train loss:   0.845596\n",
      "train loss:   0.992450\n",
      "train loss:   0.937336\n",
      "train loss:   0.807024\n",
      "train loss:   1.057408\n",
      "train loss:   0.873160\n",
      "train loss:   0.858641\n",
      "train loss:   0.747401\n",
      "train loss:   0.672717\n",
      "train loss:   0.757128\n",
      "train loss:   0.748411\n",
      "train loss:   0.829858\n",
      "train loss:   1.017993\n",
      "########### epoch 71 ###########\n",
      "########### loop 10000 ###########\n",
      "test loss:   0.963455   test accuracy:   0.718750\n",
      "########### loop 10000 ###########\n",
      "train loss:   1.020528\n",
      "train loss:   0.843653\n",
      "train loss:   0.977763\n",
      "train loss:   1.093911\n",
      "train loss:   1.175500\n",
      "train loss:   1.125525\n",
      "train loss:   0.977060\n",
      "train loss:   0.705417\n",
      "train loss:   0.767881\n",
      "train loss:   0.950269\n",
      "train loss:   0.821566\n",
      "train loss:   0.714454\n",
      "train loss:   0.881478\n",
      "train loss:   0.779907\n",
      "train loss:   0.790538\n",
      "train loss:   0.934429\n",
      "train loss:   0.820200\n",
      "train loss:   0.928476\n",
      "train loss:   0.595061\n",
      "train loss:   0.806921\n",
      "train loss:   0.774193\n",
      "train loss:   0.800511\n",
      "train loss:   0.792253\n",
      "train loss:   0.826915\n",
      "train loss:   0.797162\n",
      "train loss:   0.765808\n",
      "train loss:   0.707780\n",
      "train loss:   0.660350\n",
      "train loss:   1.066052\n",
      "train loss:   0.917097\n",
      "train loss:   0.852930\n",
      "train loss:   0.828412\n",
      "train loss:   0.975405\n",
      "train loss:   0.690826\n",
      "train loss:   0.722330\n",
      "train loss:   0.797291\n",
      "train loss:   0.918456\n",
      "train loss:   0.719209\n",
      "train loss:   1.108636\n",
      "train loss:   1.139889\n",
      "train loss:   0.645680\n",
      "train loss:   0.907759\n",
      "train loss:   1.105198\n",
      "train loss:   0.892303\n",
      "train loss:   0.982429\n",
      "train loss:   0.807190\n",
      "train loss:   0.996621\n",
      "train loss:   0.858690\n",
      "train loss:   0.910642\n",
      "train loss:   1.040816\n",
      "########### epoch 72 ###########\n",
      "########### loop 10050 ###########\n",
      "test loss:   1.030359   test accuracy:   0.718750\n",
      "########### loop 10050 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.807677\n",
      "train loss:   1.017163\n",
      "train loss:   0.897399\n",
      "train loss:   0.957964\n",
      "train loss:   0.980200\n",
      "train loss:   1.037129\n",
      "train loss:   0.839889\n",
      "train loss:   0.689892\n",
      "train loss:   0.882944\n",
      "train loss:   1.168179\n",
      "train loss:   0.839312\n",
      "train loss:   0.862731\n",
      "train loss:   0.813704\n",
      "train loss:   0.778656\n",
      "train loss:   0.689621\n",
      "train loss:   0.899543\n",
      "train loss:   0.681968\n",
      "train loss:   0.910882\n",
      "train loss:   1.149149\n",
      "train loss:   0.947078\n",
      "train loss:   0.818662\n",
      "train loss:   1.042449\n",
      "train loss:   0.900294\n",
      "train loss:   0.877816\n",
      "train loss:   0.965243\n",
      "train loss:   0.700707\n",
      "train loss:   1.053772\n",
      "train loss:   0.728658\n",
      "train loss:   0.906712\n",
      "train loss:   0.844558\n",
      "train loss:   1.189750\n",
      "train loss:   0.614553\n",
      "train loss:   0.838453\n",
      "train loss:   0.808441\n",
      "train loss:   1.164809\n",
      "train loss:   0.799826\n",
      "train loss:   0.773499\n",
      "train loss:   0.831522\n",
      "train loss:   0.806474\n",
      "train loss:   1.005424\n",
      "train loss:   0.773239\n",
      "train loss:   0.768382\n",
      "train loss:   0.908778\n",
      "train loss:   0.802162\n",
      "train loss:   0.829357\n",
      "train loss:   1.013464\n",
      "train loss:   1.000727\n",
      "train loss:   0.736784\n",
      "train loss:   1.060243\n",
      "train loss:   0.900756\n",
      "########### epoch 72 ###########\n",
      "########### loop 10100 ###########\n",
      "test loss:   1.000000   test accuracy:   0.750000\n",
      "########### loop 10100 ###########\n",
      "train loss:   0.791814\n",
      "train loss:   0.829420\n",
      "train loss:   0.943324\n",
      "train loss:   1.021333\n",
      "train loss:   0.839215\n",
      "train loss:   0.953089\n",
      "train loss:   1.003198\n",
      "train loss:   0.714116\n",
      "train loss:   0.863439\n",
      "train loss:   0.899693\n",
      "train loss:   0.992877\n",
      "train loss:   1.058924\n",
      "train loss:   0.623818\n",
      "train loss:   0.864470\n",
      "train loss:   0.837889\n",
      "train loss:   0.961701\n",
      "train loss:   0.716373\n",
      "train loss:   0.906492\n",
      "train loss:   1.116808\n",
      "train loss:   1.158955\n",
      "train loss:   1.107461\n",
      "train loss:   0.973089\n",
      "train loss:   0.653696\n",
      "train loss:   1.043375\n",
      "train loss:   1.079580\n",
      "train loss:   0.789775\n",
      "train loss:   1.160493\n",
      "train loss:   0.918020\n",
      "train loss:   0.731340\n",
      "train loss:   0.928936\n",
      "train loss:   0.829036\n",
      "train loss:   0.784949\n",
      "train loss:   1.077569\n",
      "train loss:   1.004523\n",
      "train loss:   0.914823\n",
      "train loss:   0.763669\n",
      "train loss:   0.649901\n",
      "train loss:   0.674042\n",
      "train loss:   0.917684\n",
      "train loss:   0.872513\n",
      "train loss:   0.947256\n",
      "train loss:   0.943790\n",
      "train loss:   0.956313\n",
      "train loss:   1.020329\n",
      "train loss:   1.143867\n",
      "train loss:   1.330490\n",
      "train loss:   1.186959\n",
      "train loss:   1.070860\n",
      "train loss:   0.822371\n",
      "train loss:   0.853310\n",
      "########### epoch 72 ###########\n",
      "########### loop 10150 ###########\n",
      "test loss:   1.017463   test accuracy:   0.718750\n",
      "########### loop 10150 ###########\n",
      "train loss:   0.908663\n",
      "train loss:   0.744712\n",
      "train loss:   0.752697\n",
      "train loss:   0.904498\n",
      "train loss:   1.000905\n",
      "train loss:   0.748211\n",
      "train loss:   1.012289\n",
      "train loss:   0.778502\n",
      "train loss:   0.997592\n",
      "train loss:   0.636721\n",
      "train loss:   0.832457\n",
      "train loss:   0.714409\n",
      "train loss:   0.839688\n",
      "train loss:   0.864229\n",
      "train loss:   0.797097\n",
      "train loss:   0.784688\n",
      "train loss:   0.767129\n",
      "train loss:   0.759739\n",
      "train loss:   0.703200\n",
      "train loss:   1.071199\n",
      "train loss:   1.061821\n",
      "train loss:   0.894603\n",
      "train loss:   0.786742\n",
      "train loss:   0.900724\n",
      "train loss:   0.654679\n",
      "train loss:   0.769212\n",
      "train loss:   0.755002\n",
      "train loss:   0.837999\n",
      "train loss:   0.693544\n",
      "train loss:   1.163341\n",
      "train loss:   1.152596\n",
      "train loss:   0.660736\n",
      "train loss:   1.030879\n",
      "train loss:   1.034065\n",
      "train loss:   0.939066\n",
      "train loss:   0.943885\n",
      "train loss:   0.776769\n",
      "train loss:   0.949398\n",
      "train loss:   0.857739\n",
      "train loss:   0.928381\n",
      "train loss:   1.026986\n",
      "train loss:   0.826758\n",
      "train loss:   0.979479\n",
      "train loss:   0.891266\n",
      "train loss:   0.988630\n",
      "train loss:   0.894332\n",
      "train loss:   0.995392\n",
      "train loss:   0.884468\n",
      "train loss:   0.701540\n",
      "train loss:   0.960116\n",
      "########### epoch 73 ###########\n",
      "########### loop 10200 ###########\n",
      "test loss:   1.108991   test accuracy:   0.625000\n",
      "########### loop 10200 ###########\n",
      "train loss:   1.159189\n",
      "train loss:   0.856341\n",
      "train loss:   1.007153\n",
      "train loss:   0.722615\n",
      "train loss:   0.758521\n",
      "train loss:   0.727059\n",
      "train loss:   0.967334\n",
      "train loss:   0.687430\n",
      "train loss:   0.897679\n",
      "train loss:   1.144890\n",
      "train loss:   1.016940\n",
      "train loss:   0.836614\n",
      "train loss:   1.079239\n",
      "train loss:   0.966091\n",
      "train loss:   0.910671\n",
      "train loss:   0.814588\n",
      "train loss:   0.632257\n",
      "train loss:   1.122886\n",
      "train loss:   0.782006\n",
      "train loss:   0.968114\n",
      "train loss:   0.853175\n",
      "train loss:   1.097477\n",
      "train loss:   0.725791\n",
      "train loss:   0.827703\n",
      "train loss:   0.754439\n",
      "train loss:   1.073640\n",
      "train loss:   0.761643\n",
      "train loss:   0.775756\n",
      "train loss:   0.825735\n",
      "train loss:   0.768644\n",
      "train loss:   0.898396\n",
      "train loss:   0.842054\n",
      "train loss:   0.864208\n",
      "train loss:   0.928938\n",
      "train loss:   0.877501\n",
      "train loss:   0.884365\n",
      "train loss:   1.037772\n",
      "train loss:   1.005862\n",
      "train loss:   0.808160\n",
      "train loss:   1.125961\n",
      "train loss:   0.932795\n",
      "train loss:   0.845902\n",
      "train loss:   0.802952\n",
      "train loss:   0.996461\n",
      "train loss:   1.017703\n",
      "train loss:   0.920450\n",
      "train loss:   1.033010\n",
      "train loss:   0.977982\n",
      "train loss:   0.670780\n",
      "train loss:   0.809475\n",
      "########### epoch 73 ###########\n",
      "########### loop 10250 ###########\n",
      "test loss:   1.261006   test accuracy:   0.625000\n",
      "########### loop 10250 ###########\n",
      "train loss:   0.887685\n",
      "train loss:   0.908259\n",
      "train loss:   1.038220\n",
      "train loss:   0.585800\n",
      "train loss:   0.872256\n",
      "train loss:   0.823961\n",
      "train loss:   1.047743\n",
      "train loss:   0.779493\n",
      "train loss:   0.822817\n",
      "train loss:   0.861818\n",
      "train loss:   0.968526\n",
      "train loss:   1.025350\n",
      "train loss:   0.947449\n",
      "train loss:   0.619999\n",
      "train loss:   0.926048\n",
      "train loss:   1.128268\n",
      "train loss:   0.782864\n",
      "train loss:   1.262525\n",
      "train loss:   0.958614\n",
      "train loss:   0.972226\n",
      "train loss:   1.150436\n",
      "train loss:   0.925628\n",
      "train loss:   0.865902\n",
      "train loss:   1.061552\n",
      "train loss:   0.949060\n",
      "train loss:   0.856901\n",
      "train loss:   0.766813\n",
      "train loss:   0.710469\n",
      "train loss:   0.677540\n",
      "train loss:   0.961237\n",
      "train loss:   0.907781\n",
      "train loss:   0.901735\n",
      "train loss:   1.005638\n",
      "train loss:   0.972988\n",
      "train loss:   1.072192\n",
      "train loss:   1.335919\n",
      "train loss:   1.446419\n",
      "train loss:   1.341950\n",
      "train loss:   1.277555\n",
      "train loss:   0.923758\n",
      "train loss:   0.938404\n",
      "train loss:   0.932776\n",
      "train loss:   0.755969\n",
      "train loss:   0.699691\n",
      "train loss:   0.942754\n",
      "train loss:   0.912366\n",
      "train loss:   0.730263\n",
      "train loss:   0.999503\n",
      "train loss:   0.768474\n",
      "train loss:   0.806133\n",
      "########### epoch 74 ###########\n",
      "########### loop 10300 ###########\n",
      "test loss:   1.222161   test accuracy:   0.562500\n",
      "########### loop 10300 ###########\n",
      "train loss:   0.584547\n",
      "train loss:   0.923930\n",
      "train loss:   0.804233\n",
      "train loss:   0.812472\n",
      "train loss:   0.811833\n",
      "train loss:   0.815181\n",
      "train loss:   0.762091\n",
      "train loss:   0.751110\n",
      "train loss:   0.712874\n",
      "train loss:   0.657123\n",
      "train loss:   1.010647\n",
      "train loss:   0.970775\n",
      "train loss:   0.810538\n",
      "train loss:   0.721577\n",
      "train loss:   0.894689\n",
      "train loss:   0.702422\n",
      "train loss:   0.741765\n",
      "train loss:   0.861301\n",
      "train loss:   0.920416\n",
      "train loss:   0.770332\n",
      "train loss:   1.118698\n",
      "train loss:   1.083660\n",
      "train loss:   0.684332\n",
      "train loss:   0.924822\n",
      "train loss:   1.084936\n",
      "train loss:   0.902707\n",
      "train loss:   1.070340\n",
      "train loss:   0.862435\n",
      "train loss:   0.907644\n",
      "train loss:   0.833374\n",
      "train loss:   0.986268\n",
      "train loss:   1.113291\n",
      "train loss:   0.833607\n",
      "train loss:   1.042346\n",
      "train loss:   0.969745\n",
      "train loss:   0.963807\n",
      "train loss:   0.942300\n",
      "train loss:   1.080848\n",
      "train loss:   0.917441\n",
      "train loss:   0.802394\n",
      "train loss:   0.961830\n",
      "train loss:   1.119090\n",
      "train loss:   0.905462\n",
      "train loss:   0.920694\n",
      "train loss:   0.700572\n",
      "train loss:   0.722730\n",
      "train loss:   0.767327\n",
      "train loss:   0.988373\n",
      "train loss:   0.784908\n",
      "train loss:   0.872056\n",
      "########### epoch 74 ###########\n",
      "########### loop 10350 ###########\n",
      "test loss:   0.929691   test accuracy:   0.687500\n",
      "########### loop 10350 ###########\n",
      "train loss:   1.257722\n",
      "train loss:   0.921591\n",
      "train loss:   0.821864\n",
      "train loss:   0.995323\n",
      "train loss:   0.912637\n",
      "train loss:   0.967808\n",
      "train loss:   1.034136\n",
      "train loss:   0.659382\n",
      "train loss:   1.082817\n",
      "train loss:   0.757782\n",
      "train loss:   0.929186\n",
      "train loss:   0.802071\n",
      "train loss:   1.040228\n",
      "train loss:   0.642671\n",
      "train loss:   0.816108\n",
      "train loss:   0.784187\n",
      "train loss:   1.039647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.762197\n",
      "train loss:   0.734930\n",
      "train loss:   0.829382\n",
      "train loss:   0.758091\n",
      "train loss:   0.912696\n",
      "train loss:   0.735873\n",
      "train loss:   0.851864\n",
      "train loss:   0.904019\n",
      "train loss:   0.765960\n",
      "train loss:   0.773372\n",
      "train loss:   1.044412\n",
      "train loss:   1.044594\n",
      "train loss:   0.657111\n",
      "train loss:   1.036345\n",
      "train loss:   0.773393\n",
      "train loss:   0.748239\n",
      "train loss:   0.809380\n",
      "train loss:   0.954449\n",
      "train loss:   0.954829\n",
      "train loss:   0.856620\n",
      "train loss:   0.958143\n",
      "train loss:   0.965820\n",
      "train loss:   0.568955\n",
      "train loss:   0.765294\n",
      "train loss:   0.838050\n",
      "train loss:   0.866385\n",
      "train loss:   1.031144\n",
      "train loss:   0.552620\n",
      "train loss:   0.839114\n",
      "train loss:   0.830386\n",
      "train loss:   0.965019\n",
      "train loss:   0.739783\n",
      "train loss:   0.790727\n",
      "########### epoch 74 ###########\n",
      "########### loop 10400 ###########\n",
      "test loss:   1.043330   test accuracy:   0.625000\n",
      "########### loop 10400 ###########\n",
      "train loss:   1.002714\n",
      "train loss:   1.113233\n",
      "train loss:   1.031029\n",
      "train loss:   0.976564\n",
      "train loss:   0.709759\n",
      "train loss:   0.969804\n",
      "train loss:   1.081205\n",
      "train loss:   0.733697\n",
      "train loss:   1.166080\n",
      "train loss:   0.923157\n",
      "train loss:   0.745365\n",
      "train loss:   1.006747\n",
      "train loss:   0.826971\n",
      "train loss:   0.697943\n",
      "train loss:   1.041274\n",
      "train loss:   0.943252\n",
      "train loss:   0.854579\n",
      "train loss:   0.731373\n",
      "train loss:   0.568823\n",
      "train loss:   0.615681\n",
      "train loss:   0.743482\n",
      "train loss:   0.760899\n",
      "train loss:   0.923258\n",
      "train loss:   0.943919\n",
      "train loss:   0.823818\n",
      "train loss:   0.892293\n",
      "train loss:   0.947608\n",
      "train loss:   1.140210\n",
      "train loss:   1.057959\n",
      "train loss:   0.858712\n",
      "train loss:   0.716076\n",
      "train loss:   0.764974\n",
      "train loss:   0.892412\n",
      "train loss:   0.683768\n",
      "train loss:   0.772067\n",
      "train loss:   0.811909\n",
      "train loss:   0.951164\n",
      "train loss:   0.753032\n",
      "train loss:   1.002135\n",
      "train loss:   0.871236\n",
      "train loss:   0.885401\n",
      "train loss:   0.563817\n",
      "train loss:   0.811647\n",
      "train loss:   0.897802\n",
      "train loss:   0.893351\n",
      "train loss:   0.875422\n",
      "train loss:   0.884934\n",
      "train loss:   0.847507\n",
      "train loss:   0.912273\n",
      "train loss:   0.795556\n",
      "########### epoch 75 ###########\n",
      "########### loop 10450 ###########\n",
      "test loss:   1.069294   test accuracy:   0.687500\n",
      "########### loop 10450 ###########\n",
      "train loss:   0.671216\n",
      "train loss:   1.079957\n",
      "train loss:   1.041173\n",
      "train loss:   0.868667\n",
      "train loss:   0.764810\n",
      "train loss:   1.018463\n",
      "train loss:   0.794358\n",
      "train loss:   0.744205\n",
      "train loss:   0.778203\n",
      "train loss:   0.972429\n",
      "train loss:   0.744935\n",
      "train loss:   1.163814\n",
      "train loss:   1.105784\n",
      "train loss:   0.636113\n",
      "train loss:   0.852257\n",
      "train loss:   1.045873\n",
      "train loss:   0.934695\n",
      "train loss:   0.912480\n",
      "train loss:   0.790082\n",
      "train loss:   0.907912\n",
      "train loss:   0.832911\n",
      "train loss:   0.882743\n",
      "train loss:   1.029786\n",
      "train loss:   0.778631\n",
      "train loss:   1.108804\n",
      "train loss:   0.931794\n",
      "train loss:   0.967227\n",
      "train loss:   0.904321\n",
      "train loss:   0.961284\n",
      "train loss:   0.827459\n",
      "train loss:   0.664997\n",
      "train loss:   0.896803\n",
      "train loss:   1.158226\n",
      "train loss:   0.861061\n",
      "train loss:   1.000055\n",
      "train loss:   0.702896\n",
      "train loss:   0.728403\n",
      "train loss:   0.737130\n",
      "train loss:   0.890913\n",
      "train loss:   0.633794\n",
      "train loss:   0.915902\n",
      "train loss:   1.126475\n",
      "train loss:   1.034686\n",
      "train loss:   0.858181\n",
      "train loss:   1.182732\n",
      "train loss:   0.983941\n",
      "train loss:   0.869711\n",
      "train loss:   0.811641\n",
      "train loss:   0.630851\n",
      "train loss:   1.043774\n",
      "########### epoch 75 ###########\n",
      "########### loop 10500 ###########\n",
      "test loss:   1.022978   test accuracy:   0.625000\n",
      "########### loop 10500 ###########\n",
      "train loss:   0.748142\n",
      "train loss:   0.895332\n",
      "train loss:   0.856777\n",
      "train loss:   1.070567\n",
      "train loss:   0.769366\n",
      "train loss:   0.870524\n",
      "train loss:   0.778961\n",
      "train loss:   1.022722\n",
      "train loss:   0.766653\n",
      "train loss:   0.751052\n",
      "train loss:   0.807499\n",
      "train loss:   0.864733\n",
      "train loss:   0.958037\n",
      "train loss:   0.821561\n",
      "train loss:   0.871550\n",
      "train loss:   0.932612\n",
      "train loss:   0.844628\n",
      "train loss:   0.843381\n",
      "train loss:   0.956832\n",
      "train loss:   0.940236\n",
      "train loss:   0.686827\n",
      "train loss:   0.979139\n",
      "train loss:   0.859696\n",
      "train loss:   0.831847\n",
      "train loss:   0.788454\n",
      "train loss:   0.965965\n",
      "train loss:   1.039469\n",
      "train loss:   0.880262\n",
      "train loss:   0.948321\n",
      "train loss:   0.931214\n",
      "train loss:   0.571192\n",
      "train loss:   0.770151\n",
      "train loss:   0.933534\n",
      "train loss:   0.912905\n",
      "train loss:   1.092101\n",
      "train loss:   0.653641\n",
      "train loss:   0.824529\n",
      "train loss:   0.872032\n",
      "train loss:   0.985166\n",
      "train loss:   0.743834\n",
      "train loss:   0.769185\n",
      "train loss:   0.948499\n",
      "train loss:   1.009758\n",
      "train loss:   1.058581\n",
      "train loss:   0.968732\n",
      "train loss:   0.679047\n",
      "train loss:   0.908576\n",
      "train loss:   0.996176\n",
      "train loss:   0.839887\n",
      "train loss:   1.040262\n",
      "########### epoch 75 ###########\n",
      "########### loop 10550 ###########\n",
      "test loss:   1.112956   test accuracy:   0.625000\n",
      "########### loop 10550 ###########\n",
      "train loss:   0.905524\n",
      "train loss:   0.703770\n",
      "train loss:   0.928671\n",
      "train loss:   0.927824\n",
      "train loss:   0.849983\n",
      "train loss:   1.154565\n",
      "train loss:   0.918439\n",
      "train loss:   0.911305\n",
      "train loss:   0.782308\n",
      "train loss:   0.603077\n",
      "train loss:   0.616253\n",
      "train loss:   0.803492\n",
      "train loss:   0.847752\n",
      "train loss:   0.978913\n",
      "train loss:   0.956557\n",
      "train loss:   0.911791\n",
      "train loss:   1.046038\n",
      "train loss:   1.249115\n",
      "train loss:   1.347973\n",
      "train loss:   1.293187\n",
      "train loss:   1.200535\n",
      "train loss:   0.918002\n",
      "train loss:   0.964338\n",
      "train loss:   0.895278\n",
      "train loss:   0.881697\n",
      "train loss:   0.752021\n",
      "train loss:   0.822066\n",
      "train loss:   0.977873\n",
      "train loss:   0.766194\n",
      "train loss:   0.999505\n",
      "train loss:   0.766059\n",
      "train loss:   0.750668\n",
      "train loss:   0.620062\n",
      "train loss:   0.984367\n",
      "train loss:   0.799893\n",
      "train loss:   0.997509\n",
      "train loss:   0.867946\n",
      "train loss:   0.895331\n",
      "train loss:   0.804240\n",
      "train loss:   0.733668\n",
      "train loss:   0.738302\n",
      "train loss:   0.677483\n",
      "train loss:   0.976951\n",
      "train loss:   1.021586\n",
      "train loss:   0.939533\n",
      "train loss:   0.904669\n",
      "train loss:   1.040715\n",
      "train loss:   0.734522\n",
      "train loss:   0.968014\n",
      "train loss:   0.849738\n",
      "########### epoch 76 ###########\n",
      "########### loop 10600 ###########\n",
      "test loss:   1.238932   test accuracy:   0.593750\n",
      "########### loop 10600 ###########\n",
      "train loss:   0.851433\n",
      "train loss:   0.696992\n",
      "train loss:   1.242302\n",
      "train loss:   1.094388\n",
      "train loss:   0.653855\n",
      "train loss:   0.970385\n",
      "train loss:   1.019830\n",
      "train loss:   1.031228\n",
      "train loss:   0.964915\n",
      "train loss:   0.818808\n",
      "train loss:   1.006932\n",
      "train loss:   0.785086\n",
      "train loss:   0.912241\n",
      "train loss:   1.061059\n",
      "train loss:   0.858194\n",
      "train loss:   1.021045\n",
      "train loss:   0.907753\n",
      "train loss:   0.931144\n",
      "train loss:   0.969299\n",
      "train loss:   1.014031\n",
      "train loss:   0.925451\n",
      "train loss:   0.712027\n",
      "train loss:   0.891465\n",
      "train loss:   1.198519\n",
      "train loss:   0.874194\n",
      "train loss:   0.947469\n",
      "train loss:   0.805085\n",
      "train loss:   0.731196\n",
      "train loss:   0.682318\n",
      "train loss:   0.898483\n",
      "train loss:   0.782459\n",
      "train loss:   0.856359\n",
      "train loss:   1.313223\n",
      "train loss:   1.011347\n",
      "train loss:   0.903893\n",
      "train loss:   1.158053\n",
      "train loss:   1.006583\n",
      "train loss:   1.004919\n",
      "train loss:   0.946958\n",
      "train loss:   0.732719\n",
      "train loss:   1.038002\n",
      "train loss:   0.738659\n",
      "train loss:   0.891441\n",
      "train loss:   0.848890\n",
      "train loss:   1.027191\n",
      "train loss:   0.732035\n",
      "train loss:   0.870667\n",
      "train loss:   0.843921\n",
      "train loss:   0.962656\n",
      "train loss:   0.824212\n",
      "########### epoch 76 ###########\n",
      "########### loop 10650 ###########\n",
      "test loss:   1.026677   test accuracy:   0.625000\n",
      "########### loop 10650 ###########\n",
      "train loss:   0.698033\n",
      "train loss:   0.806086\n",
      "train loss:   0.750172\n",
      "train loss:   0.836150\n",
      "train loss:   0.767720\n",
      "train loss:   0.868063\n",
      "train loss:   0.906952\n",
      "train loss:   0.845008\n",
      "train loss:   0.862924\n",
      "train loss:   0.948114\n",
      "train loss:   0.936508\n",
      "train loss:   0.795765\n",
      "train loss:   1.021143\n",
      "train loss:   0.890911\n",
      "train loss:   0.769342\n",
      "train loss:   0.792569\n",
      "train loss:   0.959295\n",
      "train loss:   0.979389\n",
      "train loss:   0.825209\n",
      "train loss:   0.886580\n",
      "train loss:   1.013971\n",
      "train loss:   0.573048\n",
      "train loss:   0.777052\n",
      "train loss:   0.809968\n",
      "train loss:   0.869802\n",
      "train loss:   1.000093\n",
      "train loss:   0.606532\n",
      "train loss:   0.916687\n",
      "train loss:   0.872384\n",
      "train loss:   0.909073\n",
      "train loss:   0.846637\n",
      "train loss:   0.805520\n",
      "train loss:   0.929620\n",
      "train loss:   0.977515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.030650\n",
      "train loss:   0.932944\n",
      "train loss:   0.604647\n",
      "train loss:   0.892946\n",
      "train loss:   1.012359\n",
      "train loss:   0.758230\n",
      "train loss:   1.170386\n",
      "train loss:   0.850983\n",
      "train loss:   0.748482\n",
      "train loss:   0.929857\n",
      "train loss:   0.759088\n",
      "train loss:   0.734399\n",
      "train loss:   0.990898\n",
      "train loss:   0.917629\n",
      "train loss:   0.826673\n",
      "train loss:   0.756950\n",
      "########### epoch 76 ###########\n",
      "########### loop 10700 ###########\n",
      "test loss:   1.006575   test accuracy:   0.593750\n",
      "########### loop 10700 ###########\n",
      "train loss:   0.603796\n",
      "train loss:   0.652470\n",
      "train loss:   0.788245\n",
      "train loss:   0.770624\n",
      "train loss:   0.923392\n",
      "train loss:   0.965600\n",
      "train loss:   0.817822\n",
      "train loss:   0.873225\n",
      "train loss:   0.950914\n",
      "train loss:   1.013038\n",
      "train loss:   1.033717\n",
      "train loss:   0.841445\n",
      "train loss:   0.813343\n",
      "train loss:   0.754671\n",
      "train loss:   0.949218\n",
      "train loss:   0.825556\n",
      "train loss:   0.704908\n",
      "train loss:   0.818195\n",
      "train loss:   0.783873\n",
      "train loss:   0.743306\n",
      "train loss:   0.922190\n",
      "train loss:   0.810349\n",
      "train loss:   0.883344\n",
      "train loss:   0.618546\n",
      "train loss:   0.850074\n",
      "train loss:   1.004892\n",
      "train loss:   0.830893\n",
      "train loss:   0.803957\n",
      "train loss:   0.815464\n",
      "train loss:   0.811092\n",
      "train loss:   0.845251\n",
      "train loss:   0.745192\n",
      "train loss:   0.757280\n",
      "train loss:   1.162361\n",
      "train loss:   1.010111\n",
      "train loss:   0.873680\n",
      "train loss:   0.911405\n",
      "train loss:   1.093341\n",
      "train loss:   0.890257\n",
      "train loss:   0.878677\n",
      "train loss:   0.854402\n",
      "train loss:   0.902754\n",
      "train loss:   0.721688\n",
      "train loss:   1.134556\n",
      "train loss:   1.132306\n",
      "train loss:   0.732283\n",
      "train loss:   1.041324\n",
      "train loss:   1.103514\n",
      "train loss:   1.017399\n",
      "train loss:   1.004154\n",
      "########### epoch 77 ###########\n",
      "########### loop 10750 ###########\n",
      "test loss:   1.019468   test accuracy:   0.687500\n",
      "########### loop 10750 ###########\n",
      "train loss:   0.737800\n",
      "train loss:   0.944537\n",
      "train loss:   0.875393\n",
      "train loss:   0.987483\n",
      "train loss:   1.180380\n",
      "train loss:   0.898706\n",
      "train loss:   1.036019\n",
      "train loss:   0.909864\n",
      "train loss:   0.932553\n",
      "train loss:   0.894381\n",
      "train loss:   0.981762\n",
      "train loss:   0.945868\n",
      "train loss:   0.751121\n",
      "train loss:   0.854558\n",
      "train loss:   1.026274\n",
      "train loss:   0.840476\n",
      "train loss:   1.028671\n",
      "train loss:   0.778499\n",
      "train loss:   0.788624\n",
      "train loss:   0.748882\n",
      "train loss:   0.874562\n",
      "train loss:   0.773009\n",
      "train loss:   0.824514\n",
      "train loss:   1.215469\n",
      "train loss:   0.924172\n",
      "train loss:   0.807711\n",
      "train loss:   1.029662\n",
      "train loss:   0.968362\n",
      "train loss:   0.944459\n",
      "train loss:   0.894808\n",
      "train loss:   0.625872\n",
      "train loss:   1.003396\n",
      "train loss:   0.670669\n",
      "train loss:   0.961250\n",
      "train loss:   0.823889\n",
      "train loss:   1.000968\n",
      "train loss:   0.672662\n",
      "train loss:   0.789563\n",
      "train loss:   0.779352\n",
      "train loss:   0.991260\n",
      "train loss:   0.777056\n",
      "train loss:   0.697366\n",
      "train loss:   0.749846\n",
      "train loss:   0.799201\n",
      "train loss:   0.893854\n",
      "train loss:   0.721742\n",
      "train loss:   0.816348\n",
      "train loss:   0.837796\n",
      "train loss:   0.814043\n",
      "train loss:   0.863898\n",
      "########### epoch 77 ###########\n",
      "########### loop 10800 ###########\n",
      "test loss:   0.892231   test accuracy:   0.687500\n",
      "########### loop 10800 ###########\n",
      "train loss:   1.021419\n",
      "train loss:   0.925986\n",
      "train loss:   0.660247\n",
      "train loss:   0.969894\n",
      "train loss:   0.785448\n",
      "train loss:   0.697269\n",
      "train loss:   0.820111\n",
      "train loss:   0.933310\n",
      "train loss:   0.950190\n",
      "train loss:   0.845106\n",
      "train loss:   0.946820\n",
      "train loss:   0.908737\n",
      "train loss:   0.611824\n",
      "train loss:   0.865789\n",
      "train loss:   0.929496\n",
      "train loss:   0.957020\n",
      "train loss:   0.963291\n",
      "train loss:   0.548813\n",
      "train loss:   0.800799\n",
      "train loss:   0.789440\n",
      "train loss:   0.805133\n",
      "train loss:   0.712915\n",
      "train loss:   0.831877\n",
      "train loss:   0.984632\n",
      "train loss:   1.053215\n",
      "train loss:   1.067189\n",
      "train loss:   0.930179\n",
      "train loss:   0.596637\n",
      "train loss:   0.986186\n",
      "train loss:   1.007618\n",
      "train loss:   0.789706\n",
      "train loss:   1.113056\n",
      "train loss:   0.916591\n",
      "train loss:   0.750654\n",
      "train loss:   0.960993\n",
      "train loss:   0.861807\n",
      "train loss:   0.813653\n",
      "train loss:   1.049401\n",
      "train loss:   1.015642\n",
      "train loss:   0.916162\n",
      "train loss:   0.735995\n",
      "train loss:   0.605422\n",
      "train loss:   0.609699\n",
      "train loss:   0.826355\n",
      "train loss:   0.827738\n",
      "train loss:   0.922713\n",
      "train loss:   1.002039\n",
      "train loss:   0.864921\n",
      "train loss:   0.889253\n",
      "train loss:   1.026114\n",
      "########### epoch 77 ###########\n",
      "########### loop 10850 ###########\n",
      "test loss:   1.191466   test accuracy:   0.625000\n",
      "########### loop 10850 ###########\n",
      "train loss:   1.130105\n",
      "train loss:   1.095785\n",
      "train loss:   1.003181\n",
      "train loss:   0.823373\n",
      "train loss:   0.878985\n",
      "train loss:   0.881191\n",
      "train loss:   0.811130\n",
      "train loss:   0.693650\n",
      "train loss:   0.792779\n",
      "train loss:   0.855419\n",
      "train loss:   0.723897\n",
      "train loss:   0.917966\n",
      "train loss:   0.763357\n",
      "train loss:   0.815416\n",
      "train loss:   0.548223\n",
      "train loss:   0.779316\n",
      "train loss:   0.752816\n",
      "train loss:   0.788612\n",
      "train loss:   0.783781\n",
      "train loss:   0.825357\n",
      "train loss:   0.795753\n",
      "train loss:   0.729947\n",
      "train loss:   0.691219\n",
      "train loss:   0.614514\n",
      "train loss:   1.059542\n",
      "train loss:   0.900154\n",
      "train loss:   0.814789\n",
      "train loss:   0.766222\n",
      "train loss:   0.948885\n",
      "train loss:   0.679846\n",
      "train loss:   0.748558\n",
      "train loss:   0.801992\n",
      "train loss:   0.778068\n",
      "train loss:   0.681254\n",
      "train loss:   1.074896\n",
      "train loss:   1.084230\n",
      "train loss:   0.643114\n",
      "train loss:   0.956809\n",
      "train loss:   1.001353\n",
      "train loss:   0.916111\n",
      "train loss:   1.106255\n",
      "train loss:   0.708729\n",
      "train loss:   0.986554\n",
      "train loss:   0.845254\n",
      "train loss:   0.929863\n",
      "train loss:   0.992877\n",
      "train loss:   0.782258\n",
      "train loss:   0.914753\n",
      "train loss:   0.808594\n",
      "train loss:   0.953574\n",
      "########### epoch 78 ###########\n",
      "########### loop 10900 ###########\n",
      "test loss:   0.892522   test accuracy:   0.750000\n",
      "########### loop 10900 ###########\n",
      "train loss:   0.896147\n",
      "train loss:   0.995987\n",
      "train loss:   0.881365\n",
      "train loss:   0.709538\n",
      "train loss:   0.946412\n",
      "train loss:   1.141664\n",
      "train loss:   0.819383\n",
      "train loss:   1.000786\n",
      "train loss:   0.656479\n",
      "train loss:   0.696090\n",
      "train loss:   0.722533\n",
      "train loss:   1.049696\n",
      "train loss:   0.725305\n",
      "train loss:   0.839445\n",
      "train loss:   1.125798\n",
      "train loss:   1.012475\n",
      "train loss:   0.951808\n",
      "train loss:   1.239376\n",
      "train loss:   1.130769\n",
      "train loss:   1.022671\n",
      "train loss:   1.019390\n",
      "train loss:   0.737756\n",
      "train loss:   1.150042\n",
      "train loss:   0.806220\n",
      "train loss:   0.886179\n",
      "train loss:   0.849177\n",
      "train loss:   1.135441\n",
      "train loss:   0.704093\n",
      "train loss:   0.831405\n",
      "train loss:   0.765140\n",
      "train loss:   1.111741\n",
      "train loss:   0.785590\n",
      "train loss:   0.743680\n",
      "train loss:   0.760802\n",
      "train loss:   0.770007\n",
      "train loss:   0.900154\n",
      "train loss:   0.774129\n",
      "train loss:   0.820869\n",
      "train loss:   0.900101\n",
      "train loss:   0.840445\n",
      "train loss:   0.892263\n",
      "train loss:   1.052520\n",
      "train loss:   0.906213\n",
      "train loss:   0.730913\n",
      "train loss:   0.969784\n",
      "train loss:   0.785519\n",
      "train loss:   0.739096\n",
      "train loss:   0.856248\n",
      "train loss:   0.952567\n",
      "train loss:   0.931659\n",
      "########### epoch 78 ###########\n",
      "########### loop 10950 ###########\n",
      "test loss:   1.001730   test accuracy:   0.718750\n",
      "########### loop 10950 ###########\n",
      "train loss:   0.822163\n",
      "train loss:   0.929395\n",
      "train loss:   0.978732\n",
      "train loss:   0.562034\n",
      "train loss:   0.711849\n",
      "train loss:   0.814793\n",
      "train loss:   0.878353\n",
      "train loss:   1.019459\n",
      "train loss:   0.560817\n",
      "train loss:   0.866714\n",
      "train loss:   0.864969\n",
      "train loss:   0.839389\n",
      "train loss:   0.775120\n",
      "train loss:   0.836277\n",
      "train loss:   0.935126\n",
      "train loss:   1.027101\n",
      "train loss:   1.085989\n",
      "train loss:   0.983095\n",
      "train loss:   0.611975\n",
      "train loss:   1.134803\n",
      "train loss:   0.982380\n",
      "train loss:   0.905971\n",
      "train loss:   1.049636\n",
      "train loss:   0.773650\n",
      "train loss:   0.701928\n",
      "train loss:   0.966387\n",
      "train loss:   0.884021\n",
      "train loss:   0.846440\n",
      "train loss:   1.037761\n",
      "train loss:   0.954694\n",
      "train loss:   0.864264\n",
      "train loss:   0.741992\n",
      "train loss:   0.590104\n",
      "train loss:   0.670469\n",
      "train loss:   0.868647\n",
      "train loss:   0.956131\n",
      "train loss:   0.975094\n",
      "train loss:   0.961524\n",
      "train loss:   0.949959\n",
      "train loss:   1.006989\n",
      "train loss:   1.068165\n",
      "train loss:   1.201182\n",
      "train loss:   1.141315\n",
      "train loss:   0.985286\n",
      "train loss:   0.767464\n",
      "train loss:   0.801530\n",
      "train loss:   1.030624\n",
      "train loss:   0.796034\n",
      "train loss:   0.782834\n",
      "train loss:   0.883583\n",
      "########### epoch 79 ###########\n",
      "########### loop 11000 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   1.274692   test accuracy:   0.468750\n",
      "########### loop 11000 ###########\n",
      "train loss:   0.980884\n",
      "train loss:   0.724938\n",
      "train loss:   0.937365\n",
      "train loss:   0.790187\n",
      "train loss:   0.824559\n",
      "train loss:   0.599036\n",
      "train loss:   0.791541\n",
      "train loss:   0.886461\n",
      "train loss:   0.818410\n",
      "train loss:   0.876454\n",
      "train loss:   0.862338\n",
      "train loss:   0.785555\n",
      "train loss:   0.781652\n",
      "train loss:   0.684712\n",
      "train loss:   0.649424\n",
      "train loss:   1.046578\n",
      "train loss:   0.986498\n",
      "train loss:   0.811231\n",
      "train loss:   0.755183\n",
      "train loss:   0.905664\n",
      "train loss:   0.699598\n",
      "train loss:   0.748224\n",
      "train loss:   0.814745\n",
      "train loss:   0.871063\n",
      "train loss:   0.735372\n",
      "train loss:   1.060649\n",
      "train loss:   1.035312\n",
      "train loss:   0.761777\n",
      "train loss:   0.888908\n",
      "train loss:   1.085717\n",
      "train loss:   0.870066\n",
      "train loss:   0.909939\n",
      "train loss:   0.809362\n",
      "train loss:   0.966903\n",
      "train loss:   0.850691\n",
      "train loss:   0.870100\n",
      "train loss:   1.028085\n",
      "train loss:   0.832333\n",
      "train loss:   1.028939\n",
      "train loss:   0.864024\n",
      "train loss:   0.892775\n",
      "train loss:   0.989422\n",
      "train loss:   1.042432\n",
      "train loss:   0.840221\n",
      "train loss:   0.752470\n",
      "train loss:   1.055182\n",
      "train loss:   1.100405\n",
      "train loss:   0.832102\n",
      "train loss:   0.967641\n",
      "train loss:   0.664780\n",
      "########### epoch 79 ###########\n",
      "########### loop 11050 ###########\n",
      "test loss:   1.279338   test accuracy:   0.593750\n",
      "########### loop 11050 ###########\n",
      "train loss:   0.741564\n",
      "train loss:   0.743978\n",
      "train loss:   1.073272\n",
      "train loss:   0.704122\n",
      "train loss:   0.898774\n",
      "train loss:   1.284315\n",
      "train loss:   0.929938\n",
      "train loss:   0.847609\n",
      "train loss:   1.061290\n",
      "train loss:   0.999575\n",
      "train loss:   0.994189\n",
      "train loss:   0.955315\n",
      "train loss:   0.813094\n",
      "train loss:   1.272530\n",
      "train loss:   0.852767\n",
      "train loss:   0.926919\n",
      "train loss:   0.939098\n",
      "train loss:   1.222335\n",
      "train loss:   0.811484\n",
      "train loss:   0.921820\n",
      "train loss:   0.767231\n",
      "train loss:   1.153610\n",
      "train loss:   0.716804\n",
      "train loss:   0.874485\n",
      "train loss:   0.798708\n",
      "train loss:   0.740179\n",
      "train loss:   0.932316\n",
      "train loss:   0.820223\n",
      "train loss:   0.777967\n",
      "train loss:   0.974685\n",
      "train loss:   0.888234\n",
      "train loss:   0.845490\n",
      "train loss:   1.088549\n",
      "train loss:   0.974923\n",
      "train loss:   0.705688\n",
      "train loss:   1.005674\n",
      "train loss:   0.808697\n",
      "train loss:   0.749434\n",
      "train loss:   0.815400\n",
      "train loss:   0.957754\n",
      "train loss:   0.988227\n",
      "train loss:   0.882745\n",
      "train loss:   0.932007\n",
      "train loss:   0.956183\n",
      "train loss:   0.661452\n",
      "train loss:   0.810675\n",
      "train loss:   0.813072\n",
      "train loss:   0.889054\n",
      "train loss:   0.989179\n",
      "train loss:   0.634968\n",
      "########### epoch 79 ###########\n",
      "########### loop 11100 ###########\n",
      "test loss:   1.217482   test accuracy:   0.531250\n",
      "########### loop 11100 ###########\n",
      "train loss:   0.872527\n",
      "train loss:   0.788315\n",
      "train loss:   1.063698\n",
      "train loss:   0.707205\n",
      "train loss:   0.844341\n",
      "train loss:   1.018039\n",
      "train loss:   1.106797\n",
      "train loss:   0.981893\n",
      "train loss:   1.058294\n",
      "train loss:   0.744356\n",
      "train loss:   1.097737\n",
      "train loss:   1.201645\n",
      "train loss:   0.709155\n",
      "train loss:   1.223322\n",
      "train loss:   1.099569\n",
      "train loss:   0.866794\n",
      "train loss:   1.140994\n",
      "train loss:   0.929930\n",
      "train loss:   0.711466\n",
      "train loss:   1.061116\n",
      "train loss:   0.994851\n",
      "train loss:   0.939670\n",
      "train loss:   0.749160\n",
      "train loss:   0.659104\n",
      "train loss:   0.647071\n",
      "train loss:   0.790382\n",
      "train loss:   0.862920\n",
      "train loss:   0.900741\n",
      "train loss:   0.986936\n",
      "train loss:   0.919489\n",
      "train loss:   0.961307\n",
      "train loss:   1.058607\n",
      "train loss:   1.233473\n",
      "train loss:   1.159723\n",
      "train loss:   0.916322\n",
      "train loss:   0.829805\n",
      "train loss:   0.822155\n",
      "train loss:   0.927909\n",
      "train loss:   0.696246\n",
      "train loss:   0.710463\n",
      "train loss:   0.860106\n",
      "train loss:   0.953969\n",
      "train loss:   0.735927\n",
      "train loss:   0.980881\n",
      "train loss:   0.772693\n",
      "train loss:   0.764861\n",
      "train loss:   0.539964\n",
      "train loss:   0.790937\n",
      "train loss:   0.929239\n",
      "train loss:   0.848973\n",
      "########### epoch 80 ###########\n",
      "########### loop 11150 ###########\n",
      "test loss:   1.162066   test accuracy:   0.593750\n",
      "########### loop 11150 ###########\n",
      "train loss:   0.917870\n",
      "train loss:   0.944523\n",
      "train loss:   0.837000\n",
      "train loss:   0.768545\n",
      "train loss:   0.692181\n",
      "train loss:   0.587778\n",
      "train loss:   1.050768\n",
      "train loss:   0.971830\n",
      "train loss:   0.780887\n",
      "train loss:   0.716240\n",
      "train loss:   0.852012\n",
      "train loss:   0.640758\n",
      "train loss:   0.782802\n",
      "train loss:   0.835658\n",
      "train loss:   0.920861\n",
      "train loss:   0.707863\n",
      "train loss:   1.149926\n",
      "train loss:   1.071272\n",
      "train loss:   0.653213\n",
      "train loss:   0.894040\n",
      "train loss:   1.104301\n",
      "train loss:   0.882975\n",
      "train loss:   1.147560\n",
      "train loss:   0.844594\n",
      "train loss:   0.943428\n",
      "train loss:   0.759737\n",
      "train loss:   0.878230\n",
      "train loss:   1.044006\n",
      "train loss:   0.872645\n",
      "train loss:   0.962605\n",
      "train loss:   0.910465\n",
      "train loss:   0.904692\n",
      "train loss:   0.898032\n",
      "train loss:   0.997572\n",
      "train loss:   0.860087\n",
      "train loss:   0.736602\n",
      "train loss:   0.928672\n",
      "train loss:   1.100473\n",
      "train loss:   0.895485\n",
      "train loss:   0.969091\n",
      "train loss:   0.689928\n",
      "train loss:   0.775391\n",
      "train loss:   0.784474\n",
      "train loss:   0.937650\n",
      "train loss:   0.645418\n",
      "train loss:   0.915498\n",
      "train loss:   1.071019\n",
      "train loss:   0.988914\n",
      "train loss:   0.783067\n",
      "train loss:   0.942815\n",
      "########### epoch 80 ###########\n",
      "########### loop 11200 ###########\n",
      "test loss:   1.169877   test accuracy:   0.593750\n",
      "########### loop 11200 ###########\n",
      "train loss:   1.031234\n",
      "train loss:   1.066640\n",
      "train loss:   0.779628\n",
      "train loss:   0.660769\n",
      "train loss:   1.083595\n",
      "train loss:   0.743519\n",
      "train loss:   0.925205\n",
      "train loss:   0.862548\n",
      "train loss:   1.056989\n",
      "train loss:   0.746579\n",
      "train loss:   0.900238\n",
      "train loss:   0.722762\n",
      "train loss:   1.040296\n",
      "train loss:   0.753125\n",
      "train loss:   0.805461\n",
      "train loss:   0.768453\n",
      "train loss:   0.725561\n",
      "train loss:   0.914396\n",
      "train loss:   0.795201\n",
      "train loss:   0.911489\n",
      "train loss:   0.925627\n",
      "train loss:   0.733370\n",
      "train loss:   0.800517\n",
      "train loss:   1.039354\n",
      "train loss:   1.044310\n",
      "train loss:   0.783882\n",
      "train loss:   1.043752\n",
      "train loss:   0.825908\n",
      "train loss:   0.713803\n",
      "train loss:   0.796842\n",
      "train loss:   0.939728\n",
      "train loss:   0.937863\n",
      "train loss:   0.899943\n",
      "train loss:   1.015811\n",
      "train loss:   0.917455\n",
      "train loss:   0.627809\n",
      "train loss:   0.826792\n",
      "train loss:   0.826549\n",
      "train loss:   0.874818\n",
      "train loss:   0.930917\n",
      "train loss:   0.592836\n",
      "train loss:   0.838290\n",
      "train loss:   0.771377\n",
      "train loss:   0.868362\n",
      "train loss:   0.712507\n",
      "train loss:   0.803479\n",
      "train loss:   0.942511\n",
      "train loss:   1.023400\n",
      "train loss:   1.040113\n",
      "train loss:   0.981528\n",
      "########### epoch 80 ###########\n",
      "########### loop 11250 ###########\n",
      "test loss:   0.845470   test accuracy:   0.750000\n",
      "########### loop 11250 ###########\n",
      "train loss:   0.609578\n",
      "train loss:   1.013172\n",
      "train loss:   0.924524\n",
      "train loss:   0.817701\n",
      "train loss:   1.031817\n",
      "train loss:   0.865775\n",
      "train loss:   0.682521\n",
      "train loss:   0.848544\n",
      "train loss:   0.784484\n",
      "train loss:   0.727104\n",
      "train loss:   0.971411\n",
      "train loss:   0.927466\n",
      "train loss:   0.793704\n",
      "train loss:   0.735024\n",
      "train loss:   0.565107\n",
      "train loss:   0.560475\n",
      "train loss:   0.837377\n",
      "train loss:   0.890894\n",
      "train loss:   0.888939\n",
      "train loss:   1.021414\n",
      "train loss:   0.827212\n",
      "train loss:   0.835691\n",
      "train loss:   1.031916\n",
      "train loss:   1.102421\n",
      "train loss:   0.983238\n",
      "train loss:   0.919078\n",
      "train loss:   0.625814\n",
      "train loss:   0.740249\n",
      "train loss:   1.007775\n",
      "train loss:   0.807834\n",
      "train loss:   0.763581\n",
      "train loss:   0.871260\n",
      "train loss:   0.921812\n",
      "train loss:   0.756811\n",
      "train loss:   0.900243\n",
      "train loss:   0.806016\n",
      "train loss:   0.874895\n",
      "train loss:   0.588534\n",
      "train loss:   0.877441\n",
      "train loss:   0.846863\n",
      "train loss:   0.926308\n",
      "train loss:   0.995376\n",
      "train loss:   1.016311\n",
      "train loss:   0.843464\n",
      "train loss:   0.688884\n",
      "train loss:   0.651871\n",
      "train loss:   0.647522\n",
      "train loss:   1.054831\n",
      "train loss:   0.915682\n",
      "train loss:   0.865384\n",
      "########### epoch 81 ###########\n",
      "########### loop 11300 ###########\n",
      "test loss:   1.057105   test accuracy:   0.750000\n",
      "########### loop 11300 ###########\n",
      "train loss:   0.795951\n",
      "train loss:   0.904648\n",
      "train loss:   0.637905\n",
      "train loss:   0.734752\n",
      "train loss:   0.707539\n",
      "train loss:   0.856460\n",
      "train loss:   0.696003\n",
      "train loss:   1.078773\n",
      "train loss:   1.051134\n",
      "train loss:   0.733473\n",
      "train loss:   0.947889\n",
      "train loss:   1.053959\n",
      "train loss:   0.919364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.071120\n",
      "train loss:   0.759732\n",
      "train loss:   0.902690\n",
      "train loss:   0.802218\n",
      "train loss:   0.937153\n",
      "train loss:   1.099709\n",
      "train loss:   0.970683\n",
      "train loss:   1.052617\n",
      "train loss:   0.811224\n",
      "train loss:   0.883604\n",
      "train loss:   0.872366\n",
      "train loss:   0.966368\n",
      "train loss:   0.910973\n",
      "train loss:   0.847610\n",
      "train loss:   0.907454\n",
      "train loss:   1.021561\n",
      "train loss:   0.820485\n",
      "train loss:   0.904580\n",
      "train loss:   0.762712\n",
      "train loss:   0.763392\n",
      "train loss:   0.785562\n",
      "train loss:   0.919315\n",
      "train loss:   0.688335\n",
      "train loss:   0.814230\n",
      "train loss:   1.148626\n",
      "train loss:   1.036121\n",
      "train loss:   0.881519\n",
      "train loss:   1.182087\n",
      "train loss:   0.984337\n",
      "train loss:   0.909389\n",
      "train loss:   0.981720\n",
      "train loss:   0.632753\n",
      "train loss:   1.025196\n",
      "train loss:   0.699645\n",
      "train loss:   0.934336\n",
      "train loss:   0.792511\n",
      "train loss:   1.048641\n",
      "########### epoch 81 ###########\n",
      "########### loop 11350 ###########\n",
      "test loss:   1.176273   test accuracy:   0.593750\n",
      "########### loop 11350 ###########\n",
      "train loss:   0.687398\n",
      "train loss:   0.843271\n",
      "train loss:   0.743986\n",
      "train loss:   1.037165\n",
      "train loss:   0.820443\n",
      "train loss:   0.730135\n",
      "train loss:   0.787574\n",
      "train loss:   0.725656\n",
      "train loss:   0.829696\n",
      "train loss:   0.767480\n",
      "train loss:   0.827388\n",
      "train loss:   0.874403\n",
      "train loss:   0.819445\n",
      "train loss:   0.779023\n",
      "train loss:   0.992268\n",
      "train loss:   0.979625\n",
      "train loss:   0.693693\n",
      "train loss:   0.982774\n",
      "train loss:   0.826209\n",
      "train loss:   0.820864\n",
      "train loss:   0.752695\n",
      "train loss:   0.969861\n",
      "train loss:   1.010866\n",
      "train loss:   0.867029\n",
      "train loss:   0.953005\n",
      "train loss:   1.001423\n",
      "train loss:   0.646081\n",
      "train loss:   0.760874\n",
      "train loss:   0.774372\n",
      "train loss:   0.851981\n",
      "train loss:   0.997587\n",
      "train loss:   0.578430\n",
      "train loss:   0.812041\n",
      "train loss:   0.896302\n",
      "train loss:   0.821060\n",
      "train loss:   0.815018\n",
      "train loss:   0.783371\n",
      "train loss:   0.850075\n",
      "train loss:   0.949347\n",
      "train loss:   1.011197\n",
      "train loss:   0.919455\n",
      "train loss:   0.614161\n",
      "train loss:   0.906215\n",
      "train loss:   0.978626\n",
      "train loss:   0.785202\n",
      "train loss:   1.040177\n",
      "train loss:   0.788469\n",
      "train loss:   0.705283\n",
      "train loss:   0.855296\n",
      "train loss:   0.818006\n",
      "########### epoch 81 ###########\n",
      "########### loop 11400 ###########\n",
      "test loss:   0.983747   test accuracy:   0.750000\n",
      "########### loop 11400 ###########\n",
      "train loss:   0.714116\n",
      "train loss:   0.883932\n",
      "train loss:   0.818536\n",
      "train loss:   0.800539\n",
      "train loss:   0.753711\n",
      "train loss:   0.567177\n",
      "train loss:   0.619900\n",
      "train loss:   0.777238\n",
      "train loss:   0.747129\n",
      "train loss:   0.951116\n",
      "train loss:   0.983804\n",
      "train loss:   0.778371\n",
      "train loss:   0.823297\n",
      "train loss:   1.005332\n",
      "train loss:   0.955946\n",
      "train loss:   0.997990\n",
      "train loss:   0.888103\n",
      "train loss:   0.721217\n",
      "train loss:   0.739826\n",
      "train loss:   0.931183\n",
      "train loss:   0.804938\n",
      "train loss:   0.786564\n",
      "train loss:   0.880115\n",
      "train loss:   0.915259\n",
      "train loss:   0.732409\n",
      "train loss:   0.924850\n",
      "train loss:   0.745515\n",
      "train loss:   0.816040\n",
      "train loss:   0.538071\n",
      "train loss:   0.850296\n",
      "train loss:   0.729123\n",
      "train loss:   0.837717\n",
      "train loss:   0.846261\n",
      "train loss:   0.806941\n",
      "train loss:   0.765333\n",
      "train loss:   0.748556\n",
      "train loss:   0.677718\n",
      "train loss:   0.713675\n",
      "train loss:   0.976588\n",
      "train loss:   1.033847\n",
      "train loss:   0.868390\n",
      "train loss:   0.732973\n",
      "train loss:   0.879622\n",
      "train loss:   0.614531\n",
      "train loss:   0.769975\n",
      "train loss:   0.778699\n",
      "train loss:   0.844102\n",
      "train loss:   0.768238\n",
      "train loss:   1.097925\n",
      "train loss:   1.087365\n",
      "########### epoch 82 ###########\n",
      "########### loop 11450 ###########\n",
      "test loss:   1.132115   test accuracy:   0.656250\n",
      "########### loop 11450 ###########\n",
      "train loss:   0.690631\n",
      "train loss:   1.192742\n",
      "train loss:   1.150040\n",
      "train loss:   0.989514\n",
      "train loss:   0.987956\n",
      "train loss:   0.775373\n",
      "train loss:   0.974491\n",
      "train loss:   0.803789\n",
      "train loss:   0.933639\n",
      "train loss:   1.065382\n",
      "train loss:   0.899547\n",
      "train loss:   0.969045\n",
      "train loss:   0.912637\n",
      "train loss:   0.947251\n",
      "train loss:   0.996194\n",
      "train loss:   1.019433\n",
      "train loss:   0.899434\n",
      "train loss:   0.762112\n",
      "train loss:   0.904306\n",
      "train loss:   1.054382\n",
      "train loss:   0.827992\n",
      "train loss:   0.863614\n",
      "train loss:   0.679261\n",
      "train loss:   0.772430\n",
      "train loss:   0.774001\n",
      "train loss:   0.923634\n",
      "train loss:   0.695937\n",
      "train loss:   0.852561\n",
      "train loss:   1.090302\n",
      "train loss:   0.966204\n",
      "train loss:   0.768484\n",
      "train loss:   0.982334\n",
      "train loss:   0.867238\n",
      "train loss:   0.961031\n",
      "train loss:   1.004691\n",
      "train loss:   0.621523\n",
      "train loss:   1.085044\n",
      "train loss:   0.803943\n",
      "train loss:   0.872802\n",
      "train loss:   0.820279\n",
      "train loss:   0.979488\n",
      "train loss:   0.629497\n",
      "train loss:   0.857005\n",
      "train loss:   0.771346\n",
      "train loss:   1.052794\n",
      "train loss:   0.673690\n",
      "train loss:   0.780899\n",
      "train loss:   0.813954\n",
      "train loss:   0.769603\n",
      "train loss:   0.916492\n",
      "########### epoch 82 ###########\n",
      "########### loop 11500 ###########\n",
      "test loss:   1.229674   test accuracy:   0.562500\n",
      "########### loop 11500 ###########\n",
      "train loss:   0.725722\n",
      "train loss:   0.812303\n",
      "train loss:   0.938111\n",
      "train loss:   0.754552\n",
      "train loss:   0.846533\n",
      "train loss:   1.128271\n",
      "train loss:   1.052455\n",
      "train loss:   0.702935\n",
      "train loss:   0.933633\n",
      "train loss:   0.775674\n",
      "train loss:   0.704202\n",
      "train loss:   0.906800\n",
      "train loss:   0.951686\n",
      "train loss:   0.976485\n",
      "train loss:   0.864033\n",
      "train loss:   0.956282\n",
      "train loss:   0.952467\n",
      "train loss:   0.666761\n",
      "train loss:   0.806615\n",
      "train loss:   0.812891\n",
      "train loss:   0.946558\n",
      "train loss:   0.969381\n",
      "train loss:   0.583088\n",
      "train loss:   0.775410\n",
      "train loss:   0.754824\n",
      "train loss:   0.877391\n",
      "train loss:   0.711543\n",
      "train loss:   0.775245\n",
      "train loss:   0.918920\n",
      "train loss:   0.995644\n",
      "train loss:   1.022021\n",
      "train loss:   0.898233\n",
      "train loss:   0.610690\n",
      "train loss:   0.939523\n",
      "train loss:   0.976389\n",
      "train loss:   0.730708\n",
      "train loss:   1.135679\n",
      "train loss:   0.882642\n",
      "train loss:   0.714518\n",
      "train loss:   0.897267\n",
      "train loss:   0.757885\n",
      "train loss:   0.735591\n",
      "train loss:   1.007985\n",
      "train loss:   0.905532\n",
      "train loss:   0.812430\n",
      "train loss:   0.717035\n",
      "train loss:   0.563872\n",
      "train loss:   0.596689\n",
      "train loss:   0.718799\n",
      "train loss:   0.791410\n",
      "########### epoch 82 ###########\n",
      "########### loop 11550 ###########\n",
      "test loss:   1.184473   test accuracy:   0.531250\n",
      "########### loop 11550 ###########\n",
      "train loss:   0.894852\n",
      "train loss:   0.935964\n",
      "train loss:   0.757317\n",
      "train loss:   0.841978\n",
      "train loss:   0.990615\n",
      "train loss:   1.105248\n",
      "train loss:   1.034889\n",
      "train loss:   0.997787\n",
      "train loss:   0.668218\n",
      "train loss:   0.824078\n",
      "train loss:   0.848678\n",
      "train loss:   0.697560\n",
      "train loss:   0.757543\n",
      "train loss:   0.800024\n",
      "train loss:   0.867959\n",
      "train loss:   0.682338\n",
      "train loss:   0.821864\n",
      "train loss:   0.798332\n",
      "train loss:   0.907118\n",
      "train loss:   0.576239\n",
      "train loss:   0.760252\n",
      "train loss:   0.805344\n",
      "train loss:   0.795786\n",
      "train loss:   0.933202\n",
      "train loss:   0.937139\n",
      "train loss:   0.810080\n",
      "train loss:   0.776471\n",
      "train loss:   0.659826\n",
      "train loss:   0.646372\n",
      "train loss:   1.073859\n",
      "train loss:   0.986545\n",
      "train loss:   0.806659\n",
      "train loss:   0.710292\n",
      "train loss:   0.868225\n",
      "train loss:   0.723624\n",
      "train loss:   0.672056\n",
      "train loss:   0.799863\n",
      "train loss:   0.926633\n",
      "train loss:   0.748639\n",
      "train loss:   1.058662\n",
      "train loss:   1.109303\n",
      "train loss:   0.663376\n",
      "train loss:   0.826221\n",
      "train loss:   1.069877\n",
      "train loss:   0.829455\n",
      "train loss:   1.006428\n",
      "train loss:   0.825635\n",
      "train loss:   0.939864\n",
      "train loss:   0.858612\n",
      "train loss:   0.908743\n",
      "########### epoch 83 ###########\n",
      "########### loop 11600 ###########\n",
      "test loss:   0.961475   test accuracy:   0.812500\n",
      "########### loop 11600 ###########\n",
      "train loss:   0.998936\n",
      "train loss:   0.810168\n",
      "train loss:   1.073086\n",
      "train loss:   0.863586\n",
      "train loss:   0.946684\n",
      "train loss:   0.987603\n",
      "train loss:   1.018767\n",
      "train loss:   0.905127\n",
      "train loss:   0.756313\n",
      "train loss:   0.855331\n",
      "train loss:   1.138657\n",
      "train loss:   0.753252\n",
      "train loss:   0.831732\n",
      "train loss:   0.797216\n",
      "train loss:   0.763806\n",
      "train loss:   0.704975\n",
      "train loss:   0.929044\n",
      "train loss:   0.674513\n",
      "train loss:   0.841531\n",
      "train loss:   1.116891\n",
      "train loss:   0.893578\n",
      "train loss:   0.820805\n",
      "train loss:   1.078301\n",
      "train loss:   0.913567\n",
      "train loss:   0.919652\n",
      "train loss:   0.887173\n",
      "train loss:   0.678600\n",
      "train loss:   1.160059\n",
      "train loss:   0.816808\n",
      "train loss:   0.928594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.829307\n",
      "train loss:   1.170875\n",
      "train loss:   0.675918\n",
      "train loss:   0.839099\n",
      "train loss:   0.814189\n",
      "train loss:   1.137047\n",
      "train loss:   0.774471\n",
      "train loss:   0.702188\n",
      "train loss:   0.768131\n",
      "train loss:   0.715506\n",
      "train loss:   0.904752\n",
      "train loss:   0.733601\n",
      "train loss:   0.836312\n",
      "train loss:   0.893103\n",
      "train loss:   0.811415\n",
      "train loss:   0.893394\n",
      "train loss:   1.076890\n",
      "train loss:   0.966235\n",
      "train loss:   0.660259\n",
      "train loss:   0.953788\n",
      "########### epoch 83 ###########\n",
      "########### loop 11650 ###########\n",
      "test loss:   0.937696   test accuracy:   0.781250\n",
      "########### loop 11650 ###########\n",
      "train loss:   0.753300\n",
      "train loss:   0.695415\n",
      "train loss:   0.798778\n",
      "train loss:   0.885740\n",
      "train loss:   0.971999\n",
      "train loss:   0.912633\n",
      "train loss:   0.924181\n",
      "train loss:   0.932715\n",
      "train loss:   0.616075\n",
      "train loss:   0.664773\n",
      "train loss:   0.852685\n",
      "train loss:   0.862310\n",
      "train loss:   1.104769\n",
      "train loss:   0.651112\n",
      "train loss:   0.985511\n",
      "train loss:   0.847682\n",
      "train loss:   0.986318\n",
      "train loss:   0.708480\n",
      "train loss:   0.738182\n",
      "train loss:   0.919304\n",
      "train loss:   1.031384\n",
      "train loss:   1.054361\n",
      "train loss:   1.046836\n",
      "train loss:   0.759275\n",
      "train loss:   1.070064\n",
      "train loss:   1.051294\n",
      "train loss:   0.812563\n",
      "train loss:   1.026965\n",
      "train loss:   0.870944\n",
      "train loss:   0.609359\n",
      "train loss:   0.850227\n",
      "train loss:   0.823180\n",
      "train loss:   0.707323\n",
      "train loss:   1.018981\n",
      "train loss:   0.902605\n",
      "train loss:   0.848188\n",
      "train loss:   0.776904\n",
      "train loss:   0.592901\n",
      "train loss:   0.628681\n",
      "train loss:   0.732617\n",
      "train loss:   0.784488\n",
      "train loss:   0.874585\n",
      "train loss:   1.005758\n",
      "train loss:   0.856462\n",
      "train loss:   0.837378\n",
      "train loss:   0.975285\n",
      "train loss:   1.058966\n",
      "train loss:   1.008121\n",
      "train loss:   0.823896\n",
      "train loss:   0.688714\n",
      "########### epoch 83 ###########\n",
      "########### loop 11700 ###########\n",
      "test loss:   0.861656   test accuracy:   0.781250\n",
      "########### loop 11700 ###########\n",
      "train loss:   0.786196\n",
      "train loss:   0.860304\n",
      "train loss:   0.759412\n",
      "train loss:   0.688425\n",
      "train loss:   0.793220\n",
      "train loss:   0.927581\n",
      "train loss:   0.762227\n",
      "train loss:   0.975888\n",
      "train loss:   0.785704\n",
      "train loss:   0.800599\n",
      "train loss:   0.550194\n",
      "train loss:   0.774145\n",
      "train loss:   0.823579\n",
      "train loss:   0.779715\n",
      "train loss:   0.827330\n",
      "train loss:   0.814238\n",
      "train loss:   0.751130\n",
      "train loss:   0.752251\n",
      "train loss:   0.672330\n",
      "train loss:   0.587108\n",
      "train loss:   1.006955\n",
      "train loss:   0.939222\n",
      "train loss:   0.828004\n",
      "train loss:   0.781722\n",
      "train loss:   0.874157\n",
      "train loss:   0.633252\n",
      "train loss:   0.713683\n",
      "train loss:   0.752447\n",
      "train loss:   0.899491\n",
      "train loss:   0.715526\n",
      "train loss:   1.137099\n",
      "train loss:   1.111725\n",
      "train loss:   0.691624\n",
      "train loss:   0.980205\n",
      "train loss:   1.043779\n",
      "train loss:   0.911152\n",
      "train loss:   1.046023\n",
      "train loss:   0.756214\n",
      "train loss:   0.934586\n",
      "train loss:   0.788806\n",
      "train loss:   0.896830\n",
      "train loss:   0.976430\n",
      "train loss:   0.786187\n",
      "train loss:   1.008088\n",
      "train loss:   0.800869\n",
      "train loss:   0.942336\n",
      "train loss:   0.859999\n",
      "train loss:   0.954540\n",
      "train loss:   0.792012\n",
      "train loss:   0.682831\n",
      "########### epoch 84 ###########\n",
      "########### loop 11750 ###########\n",
      "test loss:   0.965720   test accuracy:   0.750000\n",
      "########### loop 11750 ###########\n",
      "train loss:   0.893859\n",
      "train loss:   1.045137\n",
      "train loss:   0.768326\n",
      "train loss:   0.900591\n",
      "train loss:   0.689587\n",
      "train loss:   0.742676\n",
      "train loss:   0.716679\n",
      "train loss:   0.891774\n",
      "train loss:   0.657507\n",
      "train loss:   0.857992\n",
      "train loss:   1.097478\n",
      "train loss:   0.961370\n",
      "train loss:   0.769816\n",
      "train loss:   0.960462\n",
      "train loss:   0.896962\n",
      "train loss:   0.983209\n",
      "train loss:   1.004864\n",
      "train loss:   0.671693\n",
      "train loss:   0.963924\n",
      "train loss:   0.694632\n",
      "train loss:   0.904241\n",
      "train loss:   0.784823\n",
      "train loss:   0.986468\n",
      "train loss:   0.600389\n",
      "train loss:   0.742953\n",
      "train loss:   0.747341\n",
      "train loss:   0.971772\n",
      "train loss:   0.723363\n",
      "train loss:   0.760223\n",
      "train loss:   0.780882\n",
      "train loss:   0.699051\n",
      "train loss:   0.859753\n",
      "train loss:   0.689508\n",
      "train loss:   0.777181\n",
      "train loss:   0.831720\n",
      "train loss:   0.752029\n",
      "train loss:   0.744537\n",
      "train loss:   0.965780\n",
      "train loss:   0.946541\n",
      "train loss:   0.679035\n",
      "train loss:   1.011359\n",
      "train loss:   0.778116\n",
      "train loss:   0.710688\n",
      "train loss:   0.786235\n",
      "train loss:   0.924869\n",
      "train loss:   0.991861\n",
      "train loss:   0.829224\n",
      "train loss:   0.860283\n",
      "train loss:   1.019263\n",
      "train loss:   0.677088\n",
      "########### epoch 84 ###########\n",
      "########### loop 11800 ###########\n",
      "test loss:   1.067151   test accuracy:   0.687500\n",
      "########### loop 11800 ###########\n",
      "train loss:   0.703201\n",
      "train loss:   0.777894\n",
      "train loss:   0.838545\n",
      "train loss:   1.093162\n",
      "train loss:   0.598828\n",
      "train loss:   0.851409\n",
      "train loss:   0.819115\n",
      "train loss:   0.887990\n",
      "train loss:   0.790175\n",
      "train loss:   0.738288\n",
      "train loss:   0.823966\n",
      "train loss:   0.958299\n",
      "train loss:   0.958286\n",
      "train loss:   0.903200\n",
      "train loss:   0.629817\n",
      "train loss:   0.860383\n",
      "train loss:   0.942755\n",
      "train loss:   0.759510\n",
      "train loss:   1.070647\n",
      "train loss:   0.877857\n",
      "train loss:   0.760746\n",
      "train loss:   0.992996\n",
      "train loss:   0.870762\n",
      "train loss:   0.710508\n",
      "train loss:   0.920864\n",
      "train loss:   0.808943\n",
      "train loss:   0.810379\n",
      "train loss:   0.745497\n",
      "train loss:   0.624554\n",
      "train loss:   0.616270\n",
      "train loss:   0.804527\n",
      "train loss:   0.879603\n",
      "train loss:   0.890232\n",
      "train loss:   0.957365\n",
      "train loss:   0.887227\n",
      "train loss:   0.962643\n",
      "train loss:   1.154350\n",
      "train loss:   1.222771\n",
      "train loss:   1.179024\n",
      "train loss:   1.049817\n",
      "train loss:   0.784326\n",
      "train loss:   0.826971\n",
      "train loss:   0.935156\n",
      "train loss:   0.728720\n",
      "train loss:   0.679145\n",
      "train loss:   0.845793\n",
      "train loss:   0.917220\n",
      "train loss:   0.634890\n",
      "train loss:   0.843482\n",
      "train loss:   0.736831\n",
      "########### epoch 85 ###########\n",
      "########### loop 11850 ###########\n",
      "test loss:   1.000544   test accuracy:   0.687500\n",
      "########### loop 11850 ###########\n",
      "train loss:   0.763216\n",
      "train loss:   0.551579\n",
      "train loss:   0.766325\n",
      "train loss:   0.812270\n",
      "train loss:   0.770601\n",
      "train loss:   0.849616\n",
      "train loss:   0.874537\n",
      "train loss:   0.786484\n",
      "train loss:   0.761813\n",
      "train loss:   0.670622\n",
      "train loss:   0.657058\n",
      "train loss:   1.049646\n",
      "train loss:   0.934329\n",
      "train loss:   0.851974\n",
      "train loss:   0.780749\n",
      "train loss:   0.924627\n",
      "train loss:   0.665461\n",
      "train loss:   0.728147\n",
      "train loss:   0.856450\n",
      "train loss:   0.884524\n",
      "train loss:   0.727900\n",
      "train loss:   1.009838\n",
      "train loss:   1.049527\n",
      "train loss:   0.637752\n",
      "train loss:   0.918885\n",
      "train loss:   1.024019\n",
      "train loss:   0.901816\n",
      "train loss:   0.858928\n",
      "train loss:   0.724151\n",
      "train loss:   0.886173\n",
      "train loss:   0.757453\n",
      "train loss:   0.868103\n",
      "train loss:   1.046673\n",
      "train loss:   0.745395\n",
      "train loss:   0.967046\n",
      "train loss:   0.870393\n",
      "train loss:   0.916208\n",
      "train loss:   0.877846\n",
      "train loss:   1.004273\n",
      "train loss:   0.891272\n",
      "train loss:   0.777781\n",
      "train loss:   0.909229\n",
      "train loss:   1.037597\n",
      "train loss:   0.837071\n",
      "train loss:   0.922814\n",
      "train loss:   0.754164\n",
      "train loss:   0.804708\n",
      "train loss:   0.800189\n",
      "train loss:   0.857075\n",
      "train loss:   0.638312\n",
      "########### epoch 85 ###########\n",
      "########### loop 11900 ###########\n",
      "test loss:   1.123103   test accuracy:   0.625000\n",
      "########### loop 11900 ###########\n",
      "train loss:   0.894580\n",
      "train loss:   1.089872\n",
      "train loss:   1.001092\n",
      "train loss:   0.785269\n",
      "train loss:   1.018571\n",
      "train loss:   0.892285\n",
      "train loss:   0.900724\n",
      "train loss:   0.896302\n",
      "train loss:   0.616043\n",
      "train loss:   0.997135\n",
      "train loss:   0.755136\n",
      "train loss:   0.863278\n",
      "train loss:   0.857870\n",
      "train loss:   1.078865\n",
      "train loss:   0.610649\n",
      "train loss:   0.771485\n",
      "train loss:   0.751720\n",
      "train loss:   0.966305\n",
      "train loss:   0.760682\n",
      "train loss:   0.781586\n",
      "train loss:   0.761025\n",
      "train loss:   0.722767\n",
      "train loss:   0.895358\n",
      "train loss:   0.754943\n",
      "train loss:   0.822604\n",
      "train loss:   0.867620\n",
      "train loss:   0.819234\n",
      "train loss:   0.874851\n",
      "train loss:   1.127965\n",
      "train loss:   0.933001\n",
      "train loss:   0.661652\n",
      "train loss:   0.947736\n",
      "train loss:   0.740392\n",
      "train loss:   0.688278\n",
      "train loss:   0.781262\n",
      "train loss:   0.907317\n",
      "train loss:   0.913772\n",
      "train loss:   0.859086\n",
      "train loss:   0.991907\n",
      "train loss:   0.925682\n",
      "train loss:   0.563004\n",
      "train loss:   0.704968\n",
      "train loss:   0.745621\n",
      "train loss:   0.814175\n",
      "train loss:   1.005232\n",
      "train loss:   0.524207\n",
      "train loss:   0.841030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.793362\n",
      "train loss:   0.898447\n",
      "train loss:   0.773875\n",
      "########### epoch 85 ###########\n",
      "########### loop 11950 ###########\n",
      "test loss:   0.962896   test accuracy:   0.625000\n",
      "########### loop 11950 ###########\n",
      "train loss:   0.887317\n",
      "train loss:   0.975209\n",
      "train loss:   1.038135\n",
      "train loss:   1.130071\n",
      "train loss:   0.939396\n",
      "train loss:   0.688461\n",
      "train loss:   1.028476\n",
      "train loss:   0.989843\n",
      "train loss:   0.770724\n",
      "train loss:   1.092290\n",
      "train loss:   0.904368\n",
      "train loss:   0.781705\n",
      "train loss:   0.992903\n",
      "train loss:   0.867608\n",
      "train loss:   0.830775\n",
      "train loss:   0.919094\n",
      "train loss:   0.895950\n",
      "train loss:   0.785133\n",
      "train loss:   0.770425\n",
      "train loss:   0.578754\n",
      "train loss:   0.563916\n",
      "train loss:   0.851603\n",
      "train loss:   0.796372\n",
      "train loss:   0.917919\n",
      "train loss:   1.059389\n",
      "train loss:   0.807346\n",
      "train loss:   0.938744\n",
      "train loss:   1.130975\n",
      "train loss:   1.101970\n",
      "train loss:   1.052961\n",
      "train loss:   0.909990\n",
      "train loss:   0.774398\n",
      "train loss:   0.785892\n",
      "train loss:   0.995710\n",
      "train loss:   0.852825\n",
      "train loss:   0.713776\n",
      "train loss:   0.861992\n",
      "train loss:   0.893879\n",
      "train loss:   0.766404\n",
      "train loss:   0.911799\n",
      "train loss:   0.814910\n",
      "train loss:   0.901329\n",
      "train loss:   0.569088\n",
      "train loss:   0.763688\n",
      "train loss:   0.760503\n",
      "train loss:   0.776320\n",
      "train loss:   0.775241\n",
      "train loss:   0.759472\n",
      "train loss:   0.756087\n",
      "train loss:   0.745489\n",
      "########### epoch 86 ###########\n",
      "########### loop 12000 ###########\n",
      "test loss:   0.862676   test accuracy:   0.656250\n",
      "########### loop 12000 ###########\n",
      "train loss:   0.694265\n",
      "train loss:   0.545446\n",
      "train loss:   1.046187\n",
      "train loss:   0.939119\n",
      "train loss:   0.851104\n",
      "train loss:   0.718464\n",
      "train loss:   0.897212\n",
      "train loss:   0.689771\n",
      "train loss:   0.720090\n",
      "train loss:   0.738349\n",
      "train loss:   0.843213\n",
      "train loss:   0.670746\n",
      "train loss:   1.158576\n",
      "train loss:   1.052365\n",
      "train loss:   0.612723\n",
      "train loss:   0.865744\n",
      "train loss:   0.972693\n",
      "train loss:   0.923145\n",
      "train loss:   0.872055\n",
      "train loss:   0.727836\n",
      "train loss:   0.917417\n",
      "train loss:   0.750555\n",
      "train loss:   0.863221\n",
      "train loss:   1.000683\n",
      "train loss:   0.812311\n",
      "train loss:   0.903492\n",
      "train loss:   0.769223\n",
      "train loss:   0.870520\n",
      "train loss:   0.821526\n",
      "train loss:   0.875645\n",
      "train loss:   0.905505\n",
      "train loss:   0.739166\n",
      "train loss:   0.931574\n",
      "train loss:   1.047333\n",
      "train loss:   0.792977\n",
      "train loss:   1.000376\n",
      "train loss:   0.756809\n",
      "train loss:   0.730533\n",
      "train loss:   0.727498\n",
      "train loss:   0.851455\n",
      "train loss:   0.631520\n",
      "train loss:   0.837743\n",
      "train loss:   1.079297\n",
      "train loss:   1.010335\n",
      "train loss:   0.822453\n",
      "train loss:   1.019469\n",
      "train loss:   1.072114\n",
      "train loss:   1.032790\n",
      "train loss:   0.932655\n",
      "train loss:   0.767206\n",
      "########### epoch 86 ###########\n",
      "########### loop 12050 ###########\n",
      "test loss:   1.008826   test accuracy:   0.687500\n",
      "########### loop 12050 ###########\n",
      "train loss:   1.075163\n",
      "train loss:   0.677267\n",
      "train loss:   0.904172\n",
      "train loss:   0.737676\n",
      "train loss:   1.053969\n",
      "train loss:   0.558941\n",
      "train loss:   0.749291\n",
      "train loss:   0.819972\n",
      "train loss:   1.060798\n",
      "train loss:   0.776546\n",
      "train loss:   0.770807\n",
      "train loss:   0.759042\n",
      "train loss:   0.745963\n",
      "train loss:   0.859517\n",
      "train loss:   0.717687\n",
      "train loss:   0.808555\n",
      "train loss:   0.833675\n",
      "train loss:   0.902080\n",
      "train loss:   0.829563\n",
      "train loss:   0.883531\n",
      "train loss:   0.862917\n",
      "train loss:   0.692405\n",
      "train loss:   1.040676\n",
      "train loss:   0.868297\n",
      "train loss:   0.735120\n",
      "train loss:   0.773635\n",
      "train loss:   0.895446\n",
      "train loss:   0.952803\n",
      "train loss:   0.809587\n",
      "train loss:   0.916471\n",
      "train loss:   0.893535\n",
      "train loss:   0.623637\n",
      "train loss:   0.791756\n",
      "train loss:   0.837546\n",
      "train loss:   0.834249\n",
      "train loss:   0.968432\n",
      "train loss:   0.546606\n",
      "train loss:   0.755995\n",
      "train loss:   0.792366\n",
      "train loss:   0.854327\n",
      "train loss:   0.715925\n",
      "train loss:   0.786673\n",
      "train loss:   0.993997\n",
      "train loss:   1.119125\n",
      "train loss:   1.081984\n",
      "train loss:   0.887943\n",
      "train loss:   0.677295\n",
      "train loss:   0.903274\n",
      "train loss:   0.924257\n",
      "train loss:   0.755214\n",
      "########### epoch 86 ###########\n",
      "########### loop 12100 ###########\n",
      "test loss:   1.031010   test accuracy:   0.625000\n",
      "########### loop 12100 ###########\n",
      "train loss:   1.057642\n",
      "train loss:   0.865022\n",
      "train loss:   0.677279\n",
      "train loss:   0.878389\n",
      "train loss:   0.809552\n",
      "train loss:   0.742318\n",
      "train loss:   0.967846\n",
      "train loss:   0.968976\n",
      "train loss:   0.845826\n",
      "train loss:   0.774633\n",
      "train loss:   0.518909\n",
      "train loss:   0.545478\n",
      "train loss:   0.749358\n",
      "train loss:   0.798533\n",
      "train loss:   0.910108\n",
      "train loss:   0.971954\n",
      "train loss:   0.816240\n",
      "train loss:   0.828118\n",
      "train loss:   0.983888\n",
      "train loss:   1.190215\n",
      "train loss:   1.129532\n",
      "train loss:   1.016228\n",
      "train loss:   0.800327\n",
      "train loss:   0.788244\n",
      "train loss:   0.875661\n",
      "train loss:   0.726215\n",
      "train loss:   0.623576\n",
      "train loss:   0.824867\n",
      "train loss:   0.829328\n",
      "train loss:   0.677943\n",
      "train loss:   0.864091\n",
      "train loss:   0.730916\n",
      "train loss:   0.757334\n",
      "train loss:   0.529776\n",
      "train loss:   0.761439\n",
      "train loss:   0.772223\n",
      "train loss:   0.810682\n",
      "train loss:   0.831824\n",
      "train loss:   0.834099\n",
      "train loss:   0.736978\n",
      "train loss:   0.713792\n",
      "train loss:   0.652760\n",
      "train loss:   0.629994\n",
      "train loss:   0.947864\n",
      "train loss:   0.894264\n",
      "train loss:   0.830122\n",
      "train loss:   0.782670\n",
      "train loss:   0.930164\n",
      "train loss:   0.665356\n",
      "train loss:   0.690741\n",
      "########### epoch 87 ###########\n",
      "########### loop 12150 ###########\n",
      "test loss:   0.951643   test accuracy:   0.656250\n",
      "########### loop 12150 ###########\n",
      "train loss:   0.774681\n",
      "train loss:   0.812709\n",
      "train loss:   0.688203\n",
      "train loss:   1.116031\n",
      "train loss:   1.078434\n",
      "train loss:   0.614763\n",
      "train loss:   0.908521\n",
      "train loss:   1.005112\n",
      "train loss:   0.954080\n",
      "train loss:   1.051108\n",
      "train loss:   0.752103\n",
      "train loss:   0.966946\n",
      "train loss:   0.809345\n",
      "train loss:   0.876863\n",
      "train loss:   1.034932\n",
      "train loss:   0.727623\n",
      "train loss:   0.864479\n",
      "train loss:   0.765278\n",
      "train loss:   0.977907\n",
      "train loss:   0.795903\n",
      "train loss:   0.908731\n",
      "train loss:   0.874307\n",
      "train loss:   0.754544\n",
      "train loss:   0.960442\n",
      "train loss:   1.025527\n",
      "train loss:   0.798144\n",
      "train loss:   0.945682\n",
      "train loss:   0.761925\n",
      "train loss:   0.754598\n",
      "train loss:   0.776240\n",
      "train loss:   0.895682\n",
      "train loss:   0.615878\n",
      "train loss:   0.844909\n",
      "train loss:   1.129342\n",
      "train loss:   0.934608\n",
      "train loss:   0.784727\n",
      "train loss:   1.050357\n",
      "train loss:   0.896051\n",
      "train loss:   0.860516\n",
      "train loss:   0.862914\n",
      "train loss:   0.688066\n",
      "train loss:   1.066563\n",
      "train loss:   0.732634\n",
      "train loss:   0.893667\n",
      "train loss:   0.835433\n",
      "train loss:   1.080816\n",
      "train loss:   0.646624\n",
      "train loss:   0.790963\n",
      "train loss:   0.700247\n",
      "train loss:   0.995261\n",
      "########### epoch 87 ###########\n",
      "########### loop 12200 ###########\n",
      "test loss:   1.049493   test accuracy:   0.687500\n",
      "########### loop 12200 ###########\n",
      "train loss:   0.702586\n",
      "train loss:   0.777662\n",
      "train loss:   0.814683\n",
      "train loss:   0.758026\n",
      "train loss:   0.876269\n",
      "train loss:   0.730120\n",
      "train loss:   0.736533\n",
      "train loss:   0.847865\n",
      "train loss:   0.743202\n",
      "train loss:   0.746768\n",
      "train loss:   1.014930\n",
      "train loss:   1.011246\n",
      "train loss:   0.731917\n",
      "train loss:   0.967430\n",
      "train loss:   0.745203\n",
      "train loss:   0.666562\n",
      "train loss:   0.795605\n",
      "train loss:   0.876852\n",
      "train loss:   0.918204\n",
      "train loss:   0.839464\n",
      "train loss:   0.918689\n",
      "train loss:   0.917520\n",
      "train loss:   0.589813\n",
      "train loss:   0.700897\n",
      "train loss:   0.763290\n",
      "train loss:   0.796995\n",
      "train loss:   0.968997\n",
      "train loss:   0.518752\n",
      "train loss:   0.860251\n",
      "train loss:   0.768708\n",
      "train loss:   0.893085\n",
      "train loss:   0.741830\n",
      "train loss:   0.759950\n",
      "train loss:   0.895902\n",
      "train loss:   1.012420\n",
      "train loss:   1.069114\n",
      "train loss:   0.945579\n",
      "train loss:   0.708919\n",
      "train loss:   1.021721\n",
      "train loss:   0.911366\n",
      "train loss:   0.782506\n",
      "train loss:   1.018690\n",
      "train loss:   0.814510\n",
      "train loss:   0.713957\n",
      "train loss:   0.934117\n",
      "train loss:   0.847145\n",
      "train loss:   0.667149\n",
      "train loss:   0.941403\n",
      "train loss:   0.823189\n",
      "train loss:   0.787099\n",
      "########### epoch 87 ###########\n",
      "########### loop 12250 ###########\n",
      "test loss:   1.144153   test accuracy:   0.531250\n",
      "########### loop 12250 ###########\n",
      "train loss:   0.745453\n",
      "train loss:   0.550229\n",
      "train loss:   0.599143\n",
      "train loss:   0.728520\n",
      "train loss:   0.757283\n",
      "train loss:   0.917229\n",
      "train loss:   0.969619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.829491\n",
      "train loss:   0.829933\n",
      "train loss:   1.019507\n",
      "train loss:   1.024226\n",
      "train loss:   0.965391\n",
      "train loss:   0.805934\n",
      "train loss:   0.668587\n",
      "train loss:   0.682351\n",
      "train loss:   1.039678\n",
      "train loss:   0.754958\n",
      "train loss:   0.679281\n",
      "train loss:   0.793382\n",
      "train loss:   0.777218\n",
      "train loss:   0.756297\n",
      "train loss:   0.923852\n",
      "train loss:   0.792632\n",
      "train loss:   0.896304\n",
      "train loss:   0.587122\n",
      "train loss:   0.733451\n",
      "train loss:   0.798020\n",
      "train loss:   0.747209\n",
      "train loss:   0.816553\n",
      "train loss:   0.727111\n",
      "train loss:   0.754135\n",
      "train loss:   0.761409\n",
      "train loss:   0.714434\n",
      "train loss:   0.614875\n",
      "train loss:   1.028105\n",
      "train loss:   0.873265\n",
      "train loss:   0.794390\n",
      "train loss:   0.739016\n",
      "train loss:   0.901852\n",
      "train loss:   0.723436\n",
      "train loss:   0.671210\n",
      "train loss:   0.718121\n",
      "train loss:   0.806399\n",
      "train loss:   0.737777\n",
      "train loss:   1.052100\n",
      "train loss:   1.061507\n",
      "train loss:   0.637511\n",
      "train loss:   0.889241\n",
      "train loss:   1.023939\n",
      "train loss:   0.874765\n",
      "########### epoch 88 ###########\n",
      "########### loop 12300 ###########\n",
      "test loss:   1.059955   test accuracy:   0.718750\n",
      "########### loop 12300 ###########\n",
      "train loss:   0.832504\n",
      "train loss:   0.741690\n",
      "train loss:   0.921733\n",
      "train loss:   0.755342\n",
      "train loss:   0.935961\n",
      "train loss:   1.134618\n",
      "train loss:   0.744047\n",
      "train loss:   0.969385\n",
      "train loss:   0.932774\n",
      "train loss:   0.980473\n",
      "train loss:   0.925123\n",
      "train loss:   1.005045\n",
      "train loss:   0.917246\n",
      "train loss:   0.704906\n",
      "train loss:   0.960609\n",
      "train loss:   1.041920\n",
      "train loss:   0.770944\n",
      "train loss:   1.010742\n",
      "train loss:   0.636073\n",
      "train loss:   0.704107\n",
      "train loss:   0.709103\n",
      "train loss:   1.036304\n",
      "train loss:   0.729668\n",
      "train loss:   0.902147\n",
      "train loss:   1.272413\n",
      "train loss:   0.952798\n",
      "train loss:   0.830846\n",
      "train loss:   0.971570\n",
      "train loss:   0.858875\n",
      "train loss:   0.793275\n",
      "train loss:   0.869812\n",
      "train loss:   0.606372\n",
      "train loss:   1.082967\n",
      "train loss:   0.720469\n",
      "train loss:   0.938177\n",
      "train loss:   0.781604\n",
      "train loss:   0.970180\n",
      "train loss:   0.602514\n",
      "train loss:   0.861756\n",
      "train loss:   0.774278\n",
      "train loss:   1.033687\n",
      "train loss:   0.784827\n",
      "train loss:   0.682371\n",
      "train loss:   0.756677\n",
      "train loss:   0.748479\n",
      "train loss:   0.851794\n",
      "train loss:   0.726276\n",
      "train loss:   0.838827\n",
      "train loss:   0.882968\n",
      "train loss:   0.737631\n",
      "########### epoch 88 ###########\n",
      "########### loop 12350 ###########\n",
      "test loss:   1.021067   test accuracy:   0.687500\n",
      "########### loop 12350 ###########\n",
      "train loss:   0.739057\n",
      "train loss:   1.132024\n",
      "train loss:   0.958323\n",
      "train loss:   0.651017\n",
      "train loss:   0.937049\n",
      "train loss:   0.813133\n",
      "train loss:   0.801700\n",
      "train loss:   0.870408\n",
      "train loss:   0.986842\n",
      "train loss:   0.900986\n",
      "train loss:   0.852642\n",
      "train loss:   0.968976\n",
      "train loss:   0.941955\n",
      "train loss:   0.537049\n",
      "train loss:   0.667119\n",
      "train loss:   0.748994\n",
      "train loss:   0.882441\n",
      "train loss:   1.023745\n",
      "train loss:   0.554559\n",
      "train loss:   0.839211\n",
      "train loss:   0.812014\n",
      "train loss:   0.994941\n",
      "train loss:   0.793952\n",
      "train loss:   0.826097\n",
      "train loss:   0.905378\n",
      "train loss:   1.112233\n",
      "train loss:   1.066967\n",
      "train loss:   0.836199\n",
      "train loss:   0.610324\n",
      "train loss:   0.918153\n",
      "train loss:   1.009307\n",
      "train loss:   0.751315\n",
      "train loss:   1.080149\n",
      "train loss:   0.828203\n",
      "train loss:   0.728168\n",
      "train loss:   0.825609\n",
      "train loss:   0.755589\n",
      "train loss:   0.758717\n",
      "train loss:   1.020620\n",
      "train loss:   0.863260\n",
      "train loss:   0.778074\n",
      "train loss:   0.750885\n",
      "train loss:   0.585914\n",
      "train loss:   0.631400\n",
      "train loss:   0.654817\n",
      "train loss:   0.742360\n",
      "train loss:   0.877356\n",
      "train loss:   0.961798\n",
      "train loss:   0.808149\n",
      "train loss:   0.802451\n",
      "########### epoch 88 ###########\n",
      "########### loop 12400 ###########\n",
      "test loss:   1.065704   test accuracy:   0.718750\n",
      "########### loop 12400 ###########\n",
      "train loss:   0.887608\n",
      "train loss:   1.030922\n",
      "train loss:   1.039449\n",
      "train loss:   0.886189\n",
      "train loss:   0.732815\n",
      "train loss:   0.795025\n",
      "train loss:   0.837274\n",
      "train loss:   0.730395\n",
      "train loss:   0.620801\n",
      "train loss:   0.808370\n",
      "train loss:   0.798298\n",
      "train loss:   0.677282\n",
      "train loss:   0.910125\n",
      "train loss:   0.731579\n",
      "train loss:   0.783112\n",
      "train loss:   0.563687\n",
      "train loss:   0.788949\n",
      "train loss:   0.767254\n",
      "train loss:   0.731470\n",
      "train loss:   0.751638\n",
      "train loss:   0.845447\n",
      "train loss:   0.817910\n",
      "train loss:   0.795933\n",
      "train loss:   0.728511\n",
      "train loss:   0.623271\n",
      "train loss:   0.999184\n",
      "train loss:   0.913425\n",
      "train loss:   0.787470\n",
      "train loss:   0.745554\n",
      "train loss:   0.921482\n",
      "train loss:   0.735287\n",
      "train loss:   0.801036\n",
      "train loss:   0.811382\n",
      "train loss:   0.977543\n",
      "train loss:   0.731362\n",
      "train loss:   1.223745\n",
      "train loss:   1.145313\n",
      "train loss:   0.632927\n",
      "train loss:   1.116342\n",
      "train loss:   1.060413\n",
      "train loss:   1.040161\n",
      "train loss:   0.912056\n",
      "train loss:   0.691578\n",
      "train loss:   0.972855\n",
      "train loss:   0.801384\n",
      "train loss:   0.978398\n",
      "train loss:   1.027906\n",
      "train loss:   0.780481\n",
      "train loss:   1.032117\n",
      "train loss:   0.954309\n",
      "########### epoch 89 ###########\n",
      "########### loop 12450 ###########\n",
      "test loss:   1.194601   test accuracy:   0.656250\n",
      "########### loop 12450 ###########\n",
      "train loss:   1.039224\n",
      "train loss:   0.906296\n",
      "train loss:   1.034743\n",
      "train loss:   0.932126\n",
      "train loss:   0.650984\n",
      "train loss:   0.804277\n",
      "train loss:   1.067907\n",
      "train loss:   0.765039\n",
      "train loss:   0.915638\n",
      "train loss:   0.622603\n",
      "train loss:   0.688264\n",
      "train loss:   0.744360\n",
      "train loss:   0.911819\n",
      "train loss:   0.696940\n",
      "train loss:   0.821383\n",
      "train loss:   1.232760\n",
      "train loss:   0.903447\n",
      "train loss:   0.783369\n",
      "train loss:   0.933964\n",
      "train loss:   0.863456\n",
      "train loss:   0.925069\n",
      "train loss:   0.861174\n",
      "train loss:   0.642418\n",
      "train loss:   1.060851\n",
      "train loss:   0.704349\n",
      "train loss:   0.898629\n",
      "train loss:   0.742597\n",
      "train loss:   0.985293\n",
      "train loss:   0.593907\n",
      "train loss:   0.770128\n",
      "train loss:   0.688641\n",
      "train loss:   0.993691\n",
      "train loss:   0.690791\n",
      "train loss:   0.787098\n",
      "train loss:   0.724649\n",
      "train loss:   0.714470\n",
      "train loss:   0.830752\n",
      "train loss:   0.724218\n",
      "train loss:   0.836185\n",
      "train loss:   0.848276\n",
      "train loss:   0.788347\n",
      "train loss:   0.813566\n",
      "train loss:   0.998433\n",
      "train loss:   0.941511\n",
      "train loss:   0.594220\n",
      "train loss:   0.988958\n",
      "train loss:   0.777613\n",
      "train loss:   0.733831\n",
      "train loss:   0.747984\n",
      "train loss:   0.904134\n",
      "########### epoch 89 ###########\n",
      "########### loop 12500 ###########\n",
      "test loss:   0.869851   test accuracy:   0.750000\n",
      "########### loop 12500 ###########\n",
      "train loss:   0.867157\n",
      "train loss:   0.833160\n",
      "train loss:   0.980675\n",
      "train loss:   0.932052\n",
      "train loss:   0.630496\n",
      "train loss:   0.764157\n",
      "train loss:   0.807752\n",
      "train loss:   0.873281\n",
      "train loss:   0.952022\n",
      "train loss:   0.590808\n",
      "train loss:   0.823219\n",
      "train loss:   0.896253\n",
      "train loss:   0.804693\n",
      "train loss:   0.819416\n",
      "train loss:   0.775525\n",
      "train loss:   0.915965\n",
      "train loss:   1.056249\n",
      "train loss:   1.079549\n",
      "train loss:   0.925389\n",
      "train loss:   0.625958\n",
      "train loss:   0.901546\n",
      "train loss:   0.929699\n",
      "train loss:   0.733975\n",
      "train loss:   1.010763\n",
      "train loss:   0.790761\n",
      "train loss:   0.635141\n",
      "train loss:   0.932128\n",
      "train loss:   0.875716\n",
      "train loss:   0.658912\n",
      "train loss:   0.991680\n",
      "train loss:   0.842548\n",
      "train loss:   0.846815\n",
      "train loss:   0.788535\n",
      "train loss:   0.612716\n",
      "train loss:   0.563230\n",
      "train loss:   0.800412\n",
      "train loss:   0.828354\n",
      "train loss:   0.890260\n",
      "train loss:   0.940517\n",
      "train loss:   0.824662\n",
      "train loss:   0.892963\n",
      "train loss:   1.043230\n",
      "train loss:   1.028015\n",
      "train loss:   0.980559\n",
      "train loss:   0.894165\n",
      "train loss:   0.666432\n",
      "train loss:   0.741803\n",
      "train loss:   0.883300\n",
      "train loss:   0.741351\n",
      "train loss:   0.872452\n",
      "########### epoch 90 ###########\n",
      "########### loop 12550 ###########\n",
      "test loss:   1.012984   test accuracy:   0.656250\n",
      "########### loop 12550 ###########\n",
      "train loss:   0.877803\n",
      "train loss:   1.005804\n",
      "train loss:   0.655568\n",
      "train loss:   0.776006\n",
      "train loss:   0.789932\n",
      "train loss:   0.819384\n",
      "train loss:   0.544853\n",
      "train loss:   0.758308\n",
      "train loss:   0.760139\n",
      "train loss:   0.823483\n",
      "train loss:   0.879659\n",
      "train loss:   0.938991\n",
      "train loss:   0.781551\n",
      "train loss:   0.826571\n",
      "train loss:   0.695555\n",
      "train loss:   0.605718\n",
      "train loss:   1.011880\n",
      "train loss:   1.016713\n",
      "train loss:   0.788823\n",
      "train loss:   0.700657\n",
      "train loss:   0.958773\n",
      "train loss:   0.652214\n",
      "train loss:   0.693543\n",
      "train loss:   0.765217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.833263\n",
      "train loss:   0.671910\n",
      "train loss:   0.979138\n",
      "train loss:   1.075457\n",
      "train loss:   0.647659\n",
      "train loss:   0.916961\n",
      "train loss:   1.018179\n",
      "train loss:   0.924337\n",
      "train loss:   0.970736\n",
      "train loss:   0.688175\n",
      "train loss:   0.887796\n",
      "train loss:   0.710184\n",
      "train loss:   0.863547\n",
      "train loss:   1.068855\n",
      "train loss:   0.765688\n",
      "train loss:   1.016616\n",
      "train loss:   0.816044\n",
      "train loss:   0.891372\n",
      "train loss:   0.858094\n",
      "train loss:   1.001485\n",
      "train loss:   0.936107\n",
      "train loss:   0.736645\n",
      "train loss:   0.917469\n",
      "train loss:   1.037493\n",
      "train loss:   0.776014\n",
      "train loss:   0.925695\n",
      "########### epoch 90 ###########\n",
      "########### loop 12600 ###########\n",
      "test loss:   1.075247   test accuracy:   0.687500\n",
      "########### loop 12600 ###########\n",
      "train loss:   0.764920\n",
      "train loss:   0.663758\n",
      "train loss:   0.836730\n",
      "train loss:   0.955207\n",
      "train loss:   0.763048\n",
      "train loss:   0.806202\n",
      "train loss:   1.237902\n",
      "train loss:   0.894196\n",
      "train loss:   0.828032\n",
      "train loss:   1.049451\n",
      "train loss:   0.974918\n",
      "train loss:   1.208576\n",
      "train loss:   1.003096\n",
      "train loss:   0.586009\n",
      "train loss:   1.138044\n",
      "train loss:   0.822146\n",
      "train loss:   0.915619\n",
      "train loss:   0.733548\n",
      "train loss:   0.915028\n",
      "train loss:   0.612240\n",
      "train loss:   0.746750\n",
      "train loss:   0.707962\n",
      "train loss:   0.996312\n",
      "train loss:   0.709442\n",
      "train loss:   0.768335\n",
      "train loss:   0.835389\n",
      "train loss:   0.752355\n",
      "train loss:   0.819394\n",
      "train loss:   0.730748\n",
      "train loss:   0.808803\n",
      "train loss:   0.931752\n",
      "train loss:   0.682996\n",
      "train loss:   0.800765\n",
      "train loss:   1.050461\n",
      "train loss:   1.068617\n",
      "train loss:   0.676412\n",
      "train loss:   0.912251\n",
      "train loss:   0.748554\n",
      "train loss:   0.692119\n",
      "train loss:   0.777147\n",
      "train loss:   0.889315\n",
      "train loss:   1.003000\n",
      "train loss:   0.964712\n",
      "train loss:   1.018787\n",
      "train loss:   1.031862\n",
      "train loss:   0.601466\n",
      "train loss:   0.633362\n",
      "train loss:   0.725613\n",
      "train loss:   0.852752\n",
      "train loss:   0.963767\n",
      "########### epoch 90 ###########\n",
      "########### loop 12650 ###########\n",
      "test loss:   1.023809   test accuracy:   0.718750\n",
      "########### loop 12650 ###########\n",
      "train loss:   0.544128\n",
      "train loss:   0.843833\n",
      "train loss:   0.765901\n",
      "train loss:   0.938322\n",
      "train loss:   0.731689\n",
      "train loss:   0.773179\n",
      "train loss:   0.904593\n",
      "train loss:   0.976049\n",
      "train loss:   1.065036\n",
      "train loss:   0.879959\n",
      "train loss:   0.546640\n",
      "train loss:   0.888837\n",
      "train loss:   0.967244\n",
      "train loss:   0.760109\n",
      "train loss:   1.117753\n",
      "train loss:   0.785410\n",
      "train loss:   0.682147\n",
      "train loss:   0.906394\n",
      "train loss:   0.774330\n",
      "train loss:   0.733377\n",
      "train loss:   0.939069\n",
      "train loss:   0.924286\n",
      "train loss:   0.825204\n",
      "train loss:   0.701796\n",
      "train loss:   0.602403\n",
      "train loss:   0.627088\n",
      "train loss:   0.803285\n",
      "train loss:   0.865847\n",
      "train loss:   0.963681\n",
      "train loss:   0.960977\n",
      "train loss:   0.828281\n",
      "train loss:   0.924796\n",
      "train loss:   1.001944\n",
      "train loss:   1.149021\n",
      "train loss:   1.187806\n",
      "train loss:   1.052907\n",
      "train loss:   0.929673\n",
      "train loss:   0.835073\n",
      "train loss:   0.915347\n",
      "train loss:   0.774177\n",
      "train loss:   0.649817\n",
      "train loss:   0.804690\n",
      "train loss:   0.902480\n",
      "train loss:   0.716377\n",
      "train loss:   0.893617\n",
      "train loss:   0.778198\n",
      "train loss:   0.727948\n",
      "train loss:   0.543534\n",
      "train loss:   0.849407\n",
      "train loss:   0.790363\n",
      "########### epoch 91 ###########\n",
      "########### loop 12700 ###########\n",
      "test loss:   1.286698   test accuracy:   0.625000\n",
      "########### loop 12700 ###########\n",
      "train loss:   0.897032\n",
      "train loss:   0.843451\n",
      "train loss:   0.774952\n",
      "train loss:   0.713523\n",
      "train loss:   0.704055\n",
      "train loss:   0.629296\n",
      "train loss:   0.560133\n",
      "train loss:   1.127298\n",
      "train loss:   0.927897\n",
      "train loss:   0.771896\n",
      "train loss:   0.732151\n",
      "train loss:   0.813888\n",
      "train loss:   0.638373\n",
      "train loss:   0.692176\n",
      "train loss:   0.725425\n",
      "train loss:   0.822644\n",
      "train loss:   0.700961\n",
      "train loss:   1.045588\n",
      "train loss:   1.017639\n",
      "train loss:   0.652381\n",
      "train loss:   0.881907\n",
      "train loss:   1.085019\n",
      "train loss:   0.941616\n",
      "train loss:   1.040919\n",
      "train loss:   0.700450\n",
      "train loss:   0.954767\n",
      "train loss:   0.772737\n",
      "train loss:   0.905548\n",
      "train loss:   0.975421\n",
      "train loss:   0.826102\n",
      "train loss:   1.032662\n",
      "train loss:   0.891738\n",
      "train loss:   1.059987\n",
      "train loss:   0.958607\n",
      "train loss:   0.978468\n",
      "train loss:   0.892706\n",
      "train loss:   0.685460\n",
      "train loss:   0.808040\n",
      "train loss:   1.057366\n",
      "train loss:   0.899877\n",
      "train loss:   1.146552\n",
      "train loss:   0.625706\n",
      "train loss:   0.698590\n",
      "train loss:   0.694593\n",
      "train loss:   0.992944\n",
      "train loss:   0.673199\n",
      "train loss:   0.845321\n",
      "train loss:   1.024240\n",
      "train loss:   1.017282\n",
      "train loss:   0.837917\n",
      "########### epoch 91 ###########\n",
      "########### loop 12750 ###########\n",
      "test loss:   1.061730   test accuracy:   0.656250\n",
      "########### loop 12750 ###########\n",
      "train loss:   1.063432\n",
      "train loss:   1.042859\n",
      "train loss:   1.031987\n",
      "train loss:   1.015803\n",
      "train loss:   0.737810\n",
      "train loss:   0.966060\n",
      "train loss:   0.701786\n",
      "train loss:   0.904666\n",
      "train loss:   0.777670\n",
      "train loss:   1.027740\n",
      "train loss:   0.571647\n",
      "train loss:   0.715368\n",
      "train loss:   0.759722\n",
      "train loss:   0.984425\n",
      "train loss:   0.729381\n",
      "train loss:   0.693908\n",
      "train loss:   0.789497\n",
      "train loss:   0.748706\n",
      "train loss:   0.885487\n",
      "train loss:   0.692453\n",
      "train loss:   0.735099\n",
      "train loss:   0.846317\n",
      "train loss:   0.764312\n",
      "train loss:   0.803231\n",
      "train loss:   1.024902\n",
      "train loss:   0.874150\n",
      "train loss:   0.668263\n",
      "train loss:   0.954656\n",
      "train loss:   0.765106\n",
      "train loss:   0.719591\n",
      "train loss:   0.837121\n",
      "train loss:   0.886265\n",
      "train loss:   0.880562\n",
      "train loss:   0.785585\n",
      "train loss:   0.857094\n",
      "train loss:   0.964212\n",
      "train loss:   0.578939\n",
      "train loss:   0.737366\n",
      "train loss:   0.807198\n",
      "train loss:   0.815793\n",
      "train loss:   0.991859\n",
      "train loss:   0.565879\n",
      "train loss:   0.806524\n",
      "train loss:   0.812029\n",
      "train loss:   0.852558\n",
      "train loss:   0.666990\n",
      "train loss:   0.814689\n",
      "train loss:   0.898447\n",
      "train loss:   0.959186\n",
      "train loss:   1.024663\n",
      "########### epoch 91 ###########\n",
      "########### loop 12800 ###########\n",
      "test loss:   1.027378   test accuracy:   0.625000\n",
      "########### loop 12800 ###########\n",
      "train loss:   0.928957\n",
      "train loss:   0.625822\n",
      "train loss:   0.990111\n",
      "train loss:   1.051318\n",
      "train loss:   0.787248\n",
      "train loss:   1.024715\n",
      "train loss:   0.839491\n",
      "train loss:   0.617715\n",
      "train loss:   0.854017\n",
      "train loss:   0.832225\n",
      "train loss:   0.645515\n",
      "train loss:   0.983133\n",
      "train loss:   0.850942\n",
      "train loss:   0.853466\n",
      "train loss:   0.840326\n",
      "train loss:   0.599674\n",
      "train loss:   0.602455\n",
      "train loss:   0.755953\n",
      "train loss:   0.790803\n",
      "train loss:   0.861708\n",
      "train loss:   1.053239\n",
      "train loss:   0.835182\n",
      "train loss:   0.861753\n",
      "train loss:   0.954224\n",
      "train loss:   0.964727\n",
      "train loss:   0.997160\n",
      "train loss:   0.871471\n",
      "train loss:   0.615877\n",
      "train loss:   0.740924\n",
      "train loss:   0.868840\n",
      "train loss:   0.684955\n",
      "train loss:   0.672547\n",
      "train loss:   0.762869\n",
      "train loss:   0.866201\n",
      "train loss:   0.720706\n",
      "train loss:   0.861737\n",
      "train loss:   0.791648\n",
      "train loss:   0.864866\n",
      "train loss:   0.593232\n",
      "train loss:   0.759916\n",
      "train loss:   0.858587\n",
      "train loss:   0.757403\n",
      "train loss:   0.848362\n",
      "train loss:   0.792591\n",
      "train loss:   0.778937\n",
      "train loss:   0.723448\n",
      "train loss:   0.651310\n",
      "train loss:   0.570293\n",
      "train loss:   1.016379\n",
      "train loss:   0.961514\n",
      "########### epoch 92 ###########\n",
      "########### loop 12850 ###########\n",
      "test loss:   0.967777   test accuracy:   0.750000\n",
      "########### loop 12850 ###########\n",
      "train loss:   0.779827\n",
      "train loss:   0.694542\n",
      "train loss:   0.876748\n",
      "train loss:   0.697316\n",
      "train loss:   0.710821\n",
      "train loss:   0.793195\n",
      "train loss:   0.963445\n",
      "train loss:   0.716913\n",
      "train loss:   1.037238\n",
      "train loss:   1.109596\n",
      "train loss:   0.637410\n",
      "train loss:   0.924104\n",
      "train loss:   0.985261\n",
      "train loss:   0.799681\n",
      "train loss:   0.854500\n",
      "train loss:   0.808629\n",
      "train loss:   0.889661\n",
      "train loss:   0.797207\n",
      "train loss:   0.878931\n",
      "train loss:   0.976984\n",
      "train loss:   0.753658\n",
      "train loss:   0.999516\n",
      "train loss:   0.888577\n",
      "train loss:   0.906445\n",
      "train loss:   1.048624\n",
      "train loss:   1.077260\n",
      "train loss:   0.865708\n",
      "train loss:   0.778509\n",
      "train loss:   0.906507\n",
      "train loss:   1.032750\n",
      "train loss:   0.810109\n",
      "train loss:   0.835764\n",
      "train loss:   0.680041\n",
      "train loss:   0.665015\n",
      "train loss:   0.736270\n",
      "train loss:   0.934045\n",
      "train loss:   0.625462\n",
      "train loss:   0.869232\n",
      "train loss:   1.160940\n",
      "train loss:   0.910373\n",
      "train loss:   0.729186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.909694\n",
      "train loss:   0.898664\n",
      "train loss:   0.875248\n",
      "train loss:   0.862255\n",
      "train loss:   0.704239\n",
      "train loss:   1.086246\n",
      "train loss:   0.710809\n",
      "train loss:   0.891441\n",
      "train loss:   0.764549\n",
      "########### epoch 92 ###########\n",
      "########### loop 12900 ###########\n",
      "test loss:   0.992355   test accuracy:   0.718750\n",
      "########### loop 12900 ###########\n",
      "train loss:   0.959140\n",
      "train loss:   0.663508\n",
      "train loss:   0.801085\n",
      "train loss:   0.800201\n",
      "train loss:   0.979393\n",
      "train loss:   0.701811\n",
      "train loss:   0.713571\n",
      "train loss:   0.757094\n",
      "train loss:   0.746813\n",
      "train loss:   0.878533\n",
      "train loss:   0.703780\n",
      "train loss:   0.787327\n",
      "train loss:   0.829838\n",
      "train loss:   0.729720\n",
      "train loss:   0.757391\n",
      "train loss:   0.887697\n",
      "train loss:   0.923988\n",
      "train loss:   0.611472\n",
      "train loss:   0.998790\n",
      "train loss:   0.750497\n",
      "train loss:   0.658039\n",
      "train loss:   0.763397\n",
      "train loss:   0.872304\n",
      "train loss:   0.945134\n",
      "train loss:   0.804786\n",
      "train loss:   0.842463\n",
      "train loss:   0.929865\n",
      "train loss:   0.593829\n",
      "train loss:   0.669314\n",
      "train loss:   0.832288\n",
      "train loss:   0.839079\n",
      "train loss:   0.988959\n",
      "train loss:   0.587161\n",
      "train loss:   0.779531\n",
      "train loss:   0.718449\n",
      "train loss:   0.814663\n",
      "train loss:   0.717739\n",
      "train loss:   0.969772\n",
      "train loss:   1.091057\n",
      "train loss:   1.016337\n",
      "train loss:   0.999886\n",
      "train loss:   0.905732\n",
      "train loss:   0.567140\n",
      "train loss:   1.121527\n",
      "train loss:   1.015570\n",
      "train loss:   0.859047\n",
      "train loss:   1.086869\n",
      "train loss:   0.837373\n",
      "train loss:   0.680067\n",
      "train loss:   0.917990\n",
      "########### epoch 92 ###########\n",
      "########### loop 12950 ###########\n",
      "test loss:   1.013747   test accuracy:   0.781250\n",
      "########### loop 12950 ###########\n",
      "train loss:   0.858662\n",
      "train loss:   0.792899\n",
      "train loss:   1.068441\n",
      "train loss:   1.048737\n",
      "train loss:   0.881664\n",
      "train loss:   0.816602\n",
      "train loss:   0.555426\n",
      "train loss:   0.550250\n",
      "train loss:   0.759260\n",
      "train loss:   0.809916\n",
      "train loss:   0.846089\n",
      "train loss:   0.984756\n",
      "train loss:   0.758134\n",
      "train loss:   0.806864\n",
      "train loss:   0.931382\n",
      "train loss:   0.969792\n",
      "train loss:   0.961719\n",
      "train loss:   0.808614\n",
      "train loss:   0.679583\n",
      "train loss:   0.683407\n",
      "train loss:   0.904171\n",
      "train loss:   0.709451\n",
      "train loss:   0.657016\n",
      "train loss:   0.793275\n",
      "train loss:   0.926217\n",
      "train loss:   0.742178\n",
      "train loss:   0.884753\n",
      "train loss:   0.774384\n",
      "train loss:   0.883778\n",
      "train loss:   0.564231\n",
      "train loss:   0.741578\n",
      "train loss:   0.744449\n",
      "train loss:   0.820031\n",
      "train loss:   1.008167\n",
      "train loss:   0.866128\n",
      "train loss:   0.780508\n",
      "train loss:   0.723437\n",
      "train loss:   0.653291\n",
      "train loss:   0.551904\n",
      "train loss:   1.053155\n",
      "train loss:   0.982039\n",
      "train loss:   0.737047\n",
      "train loss:   0.683748\n",
      "train loss:   0.840318\n",
      "train loss:   0.646403\n",
      "train loss:   0.662814\n",
      "train loss:   0.770985\n",
      "train loss:   0.802585\n",
      "train loss:   0.702172\n",
      "train loss:   1.084848\n",
      "########### epoch 93 ###########\n",
      "########### loop 13000 ###########\n",
      "test loss:   1.209421   test accuracy:   0.593750\n",
      "########### loop 13000 ###########\n",
      "train loss:   1.125042\n",
      "train loss:   0.602789\n",
      "train loss:   0.933380\n",
      "train loss:   0.991867\n",
      "train loss:   0.838939\n",
      "train loss:   0.812485\n",
      "train loss:   0.708823\n",
      "train loss:   0.885955\n",
      "train loss:   0.707802\n",
      "train loss:   0.816588\n",
      "train loss:   1.007706\n",
      "train loss:   0.731795\n",
      "train loss:   1.037894\n",
      "train loss:   0.833947\n",
      "train loss:   0.908276\n",
      "train loss:   0.817627\n",
      "train loss:   0.897022\n",
      "train loss:   0.819354\n",
      "train loss:   0.611093\n",
      "train loss:   0.801163\n",
      "train loss:   1.052698\n",
      "train loss:   0.775015\n",
      "train loss:   0.859442\n",
      "train loss:   0.710409\n",
      "train loss:   0.685782\n",
      "train loss:   0.619520\n",
      "train loss:   0.884526\n",
      "train loss:   0.642953\n",
      "train loss:   0.840247\n",
      "train loss:   1.130049\n",
      "train loss:   0.979138\n",
      "train loss:   0.829184\n",
      "train loss:   1.011393\n",
      "train loss:   0.917172\n",
      "train loss:   0.814478\n",
      "train loss:   0.929590\n",
      "train loss:   0.697830\n",
      "train loss:   1.120650\n",
      "train loss:   0.742884\n",
      "train loss:   0.834638\n",
      "train loss:   0.786083\n",
      "train loss:   1.015380\n",
      "train loss:   0.605044\n",
      "train loss:   0.755374\n",
      "train loss:   0.747764\n",
      "train loss:   1.098132\n",
      "train loss:   0.713291\n",
      "train loss:   0.823633\n",
      "train loss:   0.802647\n",
      "train loss:   0.706570\n",
      "########### epoch 93 ###########\n",
      "########### loop 13050 ###########\n",
      "test loss:   0.978284   test accuracy:   0.656250\n",
      "########### loop 13050 ###########\n",
      "train loss:   0.887858\n",
      "train loss:   0.691771\n",
      "train loss:   0.680095\n",
      "train loss:   0.848228\n",
      "train loss:   0.752764\n",
      "train loss:   0.780384\n",
      "train loss:   0.997262\n",
      "train loss:   0.930956\n",
      "train loss:   0.672566\n",
      "train loss:   0.956543\n",
      "train loss:   0.788518\n",
      "train loss:   0.702939\n",
      "train loss:   0.809985\n",
      "train loss:   0.898734\n",
      "train loss:   0.948493\n",
      "train loss:   0.849598\n",
      "train loss:   0.842067\n",
      "train loss:   0.942834\n",
      "train loss:   0.629745\n",
      "train loss:   0.750901\n",
      "train loss:   0.805112\n",
      "train loss:   0.801000\n",
      "train loss:   0.937488\n",
      "train loss:   0.548182\n",
      "train loss:   0.790793\n",
      "train loss:   0.766752\n",
      "train loss:   0.863555\n",
      "train loss:   0.682006\n",
      "train loss:   0.660151\n",
      "train loss:   0.819994\n",
      "train loss:   0.929695\n",
      "train loss:   0.966282\n",
      "train loss:   0.922789\n",
      "train loss:   0.641072\n",
      "train loss:   0.914852\n",
      "train loss:   0.982192\n",
      "train loss:   0.676294\n",
      "train loss:   1.019645\n",
      "train loss:   0.765316\n",
      "train loss:   0.623374\n",
      "train loss:   0.841202\n",
      "train loss:   0.782066\n",
      "train loss:   0.674290\n",
      "train loss:   0.951223\n",
      "train loss:   0.805830\n",
      "train loss:   0.783735\n",
      "train loss:   0.697429\n",
      "train loss:   0.545797\n",
      "train loss:   0.615261\n",
      "train loss:   0.724828\n",
      "########### epoch 93 ###########\n",
      "########### loop 13100 ###########\n",
      "test loss:   1.029353   test accuracy:   0.625000\n",
      "########### loop 13100 ###########\n",
      "train loss:   0.749002\n",
      "train loss:   0.855120\n",
      "train loss:   0.887459\n",
      "train loss:   0.836290\n",
      "train loss:   0.848744\n",
      "train loss:   0.871486\n",
      "train loss:   1.143612\n",
      "train loss:   0.993931\n",
      "train loss:   0.817018\n",
      "train loss:   0.654936\n",
      "train loss:   0.717546\n",
      "train loss:   0.862354\n",
      "train loss:   0.690572\n",
      "train loss:   0.685388\n",
      "train loss:   0.742789\n",
      "train loss:   0.934028\n",
      "train loss:   0.764449\n",
      "train loss:   0.914485\n",
      "train loss:   0.744597\n",
      "train loss:   0.838107\n",
      "train loss:   0.549697\n",
      "train loss:   0.736457\n",
      "train loss:   0.789382\n",
      "train loss:   0.758223\n",
      "train loss:   0.879268\n",
      "train loss:   0.773652\n",
      "train loss:   0.741703\n",
      "train loss:   0.723720\n",
      "train loss:   0.668690\n",
      "train loss:   0.613508\n",
      "train loss:   1.038277\n",
      "train loss:   0.904141\n",
      "train loss:   0.771700\n",
      "train loss:   0.748445\n",
      "train loss:   0.881675\n",
      "train loss:   0.736834\n",
      "train loss:   0.661164\n",
      "train loss:   0.686840\n",
      "train loss:   0.792047\n",
      "train loss:   0.746450\n",
      "train loss:   1.037717\n",
      "train loss:   1.013778\n",
      "train loss:   0.690026\n",
      "train loss:   0.901836\n",
      "train loss:   0.991528\n",
      "train loss:   0.797499\n",
      "train loss:   0.818045\n",
      "train loss:   0.742116\n",
      "train loss:   0.857895\n",
      "train loss:   0.751175\n",
      "########### epoch 94 ###########\n",
      "########### loop 13150 ###########\n",
      "test loss:   1.203245   test accuracy:   0.562500\n",
      "########### loop 13150 ###########\n",
      "train loss:   0.901542\n",
      "train loss:   1.031856\n",
      "train loss:   0.782262\n",
      "train loss:   0.896674\n",
      "train loss:   0.888139\n",
      "train loss:   0.987290\n",
      "train loss:   0.857441\n",
      "train loss:   0.968916\n",
      "train loss:   0.792746\n",
      "train loss:   0.668963\n",
      "train loss:   0.838604\n",
      "train loss:   1.005166\n",
      "train loss:   0.873666\n",
      "train loss:   0.980177\n",
      "train loss:   0.622582\n",
      "train loss:   0.715421\n",
      "train loss:   0.713503\n",
      "train loss:   0.877064\n",
      "train loss:   0.645255\n",
      "train loss:   0.799082\n",
      "train loss:   1.225784\n",
      "train loss:   0.893370\n",
      "train loss:   0.829045\n",
      "train loss:   0.992628\n",
      "train loss:   0.874945\n",
      "train loss:   0.872203\n",
      "train loss:   0.846236\n",
      "train loss:   0.598414\n",
      "train loss:   1.084785\n",
      "train loss:   0.730984\n",
      "train loss:   0.874790\n",
      "train loss:   0.763292\n",
      "train loss:   0.927608\n",
      "train loss:   0.645503\n",
      "train loss:   0.820840\n",
      "train loss:   0.706912\n",
      "train loss:   1.001646\n",
      "train loss:   0.677402\n",
      "train loss:   0.816688\n",
      "train loss:   0.754305\n",
      "train loss:   0.741929\n",
      "train loss:   0.920893\n",
      "train loss:   0.680352\n",
      "train loss:   0.776192\n",
      "train loss:   0.820836\n",
      "train loss:   0.695985\n",
      "train loss:   0.792624\n",
      "train loss:   0.898350\n",
      "train loss:   0.902929\n",
      "train loss:   0.587055\n",
      "########### epoch 94 ###########\n",
      "########### loop 13200 ###########\n",
      "test loss:   1.047456   test accuracy:   0.687500\n",
      "########### loop 13200 ###########\n",
      "train loss:   0.940128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.807653\n",
      "train loss:   0.675535\n",
      "train loss:   0.752820\n",
      "train loss:   0.885379\n",
      "train loss:   0.925384\n",
      "train loss:   0.851331\n",
      "train loss:   0.959152\n",
      "train loss:   0.915245\n",
      "train loss:   0.534908\n",
      "train loss:   0.746237\n",
      "train loss:   0.799546\n",
      "train loss:   0.813680\n",
      "train loss:   0.892035\n",
      "train loss:   0.505484\n",
      "train loss:   0.795403\n",
      "train loss:   0.748797\n",
      "train loss:   0.918008\n",
      "train loss:   0.673452\n",
      "train loss:   0.764942\n",
      "train loss:   0.844600\n",
      "train loss:   0.933982\n",
      "train loss:   1.034437\n",
      "train loss:   0.877009\n",
      "train loss:   0.639627\n",
      "train loss:   0.902506\n",
      "train loss:   1.013747\n",
      "train loss:   0.672384\n",
      "train loss:   1.090391\n",
      "train loss:   0.865808\n",
      "train loss:   0.734631\n",
      "train loss:   0.880699\n",
      "train loss:   0.829412\n",
      "train loss:   0.679255\n",
      "train loss:   0.939772\n",
      "train loss:   0.858034\n",
      "train loss:   0.788312\n",
      "train loss:   0.756759\n",
      "train loss:   0.578433\n",
      "train loss:   0.573434\n",
      "train loss:   0.807617\n",
      "train loss:   0.787691\n",
      "train loss:   0.866290\n",
      "train loss:   1.026126\n",
      "train loss:   0.807863\n",
      "train loss:   0.824222\n",
      "train loss:   0.941055\n",
      "train loss:   0.948800\n",
      "train loss:   0.991861\n",
      "train loss:   0.794482\n",
      "########### epoch 94 ###########\n",
      "########### loop 13250 ###########\n",
      "test loss:   1.035038   test accuracy:   0.625000\n",
      "########### loop 13250 ###########\n",
      "train loss:   0.673810\n",
      "train loss:   0.779556\n",
      "train loss:   0.957286\n",
      "train loss:   0.789251\n",
      "train loss:   0.686491\n",
      "train loss:   0.838523\n",
      "train loss:   0.751700\n",
      "train loss:   0.643129\n",
      "train loss:   0.875347\n",
      "train loss:   0.783873\n",
      "train loss:   0.727817\n",
      "train loss:   0.483237\n",
      "train loss:   0.749503\n",
      "train loss:   0.721219\n",
      "train loss:   0.768405\n",
      "train loss:   0.788388\n",
      "train loss:   0.868472\n",
      "train loss:   0.803870\n",
      "train loss:   0.769007\n",
      "train loss:   0.674087\n",
      "train loss:   0.532851\n",
      "train loss:   1.071003\n",
      "train loss:   0.933589\n",
      "train loss:   0.767006\n",
      "train loss:   0.769056\n",
      "train loss:   0.913256\n",
      "train loss:   0.704555\n",
      "train loss:   0.699937\n",
      "train loss:   0.717959\n",
      "train loss:   0.794954\n",
      "train loss:   0.667078\n",
      "train loss:   1.109302\n",
      "train loss:   1.118739\n",
      "train loss:   0.603619\n",
      "train loss:   0.965097\n",
      "train loss:   0.973864\n",
      "train loss:   0.872411\n",
      "train loss:   0.858230\n",
      "train loss:   0.719865\n",
      "train loss:   0.899680\n",
      "train loss:   0.735830\n",
      "train loss:   0.939338\n",
      "train loss:   1.020854\n",
      "train loss:   0.745372\n",
      "train loss:   1.002282\n",
      "train loss:   0.866688\n",
      "train loss:   1.062096\n",
      "train loss:   0.919443\n",
      "train loss:   1.007040\n",
      "train loss:   0.962309\n",
      "########### epoch 95 ###########\n",
      "########### loop 13300 ###########\n",
      "test loss:   1.044146   test accuracy:   0.687500\n",
      "########### loop 13300 ###########\n",
      "train loss:   0.702777\n",
      "train loss:   0.804532\n",
      "train loss:   0.933151\n",
      "train loss:   0.826517\n",
      "train loss:   0.972935\n",
      "train loss:   0.600636\n",
      "train loss:   0.660583\n",
      "train loss:   0.685479\n",
      "train loss:   0.918818\n",
      "train loss:   0.630863\n",
      "train loss:   0.788885\n",
      "train loss:   1.068054\n",
      "train loss:   0.908813\n",
      "train loss:   0.758744\n",
      "train loss:   0.946376\n",
      "train loss:   0.888275\n",
      "train loss:   0.861046\n",
      "train loss:   0.850727\n",
      "train loss:   0.551811\n",
      "train loss:   0.909062\n",
      "train loss:   0.657929\n",
      "train loss:   0.827183\n",
      "train loss:   0.786544\n",
      "train loss:   0.916127\n",
      "train loss:   0.622233\n",
      "train loss:   0.746583\n",
      "train loss:   0.662647\n",
      "train loss:   1.016270\n",
      "train loss:   0.680385\n",
      "train loss:   0.731520\n",
      "train loss:   0.673470\n",
      "train loss:   0.754729\n",
      "train loss:   0.848715\n",
      "train loss:   0.747391\n",
      "train loss:   0.794453\n",
      "train loss:   0.837570\n",
      "train loss:   0.812255\n",
      "train loss:   0.811810\n",
      "train loss:   0.918444\n",
      "train loss:   0.888790\n",
      "train loss:   0.560322\n",
      "train loss:   0.958478\n",
      "train loss:   0.801354\n",
      "train loss:   0.723614\n",
      "train loss:   0.747301\n",
      "train loss:   0.882219\n",
      "train loss:   0.885310\n",
      "train loss:   0.790706\n",
      "train loss:   0.876197\n",
      "train loss:   0.874115\n",
      "########### epoch 95 ###########\n",
      "########### loop 13350 ###########\n",
      "test loss:   0.919275   test accuracy:   0.718750\n",
      "########### loop 13350 ###########\n",
      "train loss:   0.612171\n",
      "train loss:   0.711171\n",
      "train loss:   0.801582\n",
      "train loss:   0.842100\n",
      "train loss:   0.952563\n",
      "train loss:   0.560480\n",
      "train loss:   0.822141\n",
      "train loss:   0.813893\n",
      "train loss:   0.843008\n",
      "train loss:   0.707374\n",
      "train loss:   0.713671\n",
      "train loss:   0.867026\n",
      "train loss:   1.021626\n",
      "train loss:   1.044184\n",
      "train loss:   0.988491\n",
      "train loss:   0.651211\n",
      "train loss:   0.861722\n",
      "train loss:   0.843742\n",
      "train loss:   0.693127\n",
      "train loss:   0.986122\n",
      "train loss:   0.792934\n",
      "train loss:   0.695789\n",
      "train loss:   0.895484\n",
      "train loss:   0.858230\n",
      "train loss:   0.660627\n",
      "train loss:   0.881396\n",
      "train loss:   0.777466\n",
      "train loss:   0.771538\n",
      "train loss:   0.765050\n",
      "train loss:   0.549345\n",
      "train loss:   0.544564\n",
      "train loss:   0.692958\n",
      "train loss:   0.760945\n",
      "train loss:   0.885055\n",
      "train loss:   0.905291\n",
      "train loss:   0.777447\n",
      "train loss:   0.819689\n",
      "train loss:   1.053972\n",
      "train loss:   1.032798\n",
      "train loss:   1.008277\n",
      "train loss:   0.987395\n",
      "train loss:   0.682695\n",
      "train loss:   0.724625\n",
      "train loss:   0.876652\n",
      "train loss:   0.711472\n",
      "train loss:   0.739991\n",
      "train loss:   0.857306\n",
      "train loss:   0.774193\n",
      "train loss:   0.665942\n",
      "train loss:   0.809528\n",
      "########### epoch 96 ###########\n",
      "########### loop 13400 ###########\n",
      "test loss:   1.010617   test accuracy:   0.625000\n",
      "########### loop 13400 ###########\n",
      "train loss:   0.795768\n",
      "train loss:   0.824994\n",
      "train loss:   0.540172\n",
      "train loss:   0.775415\n",
      "train loss:   0.787331\n",
      "train loss:   0.759369\n",
      "train loss:   0.788865\n",
      "train loss:   0.795345\n",
      "train loss:   0.736066\n",
      "train loss:   0.674318\n",
      "train loss:   0.643570\n",
      "train loss:   0.636261\n",
      "train loss:   1.017977\n",
      "train loss:   0.914671\n",
      "train loss:   0.769540\n",
      "train loss:   0.772298\n",
      "train loss:   0.857880\n",
      "train loss:   0.636626\n",
      "train loss:   0.677596\n",
      "train loss:   0.722085\n",
      "train loss:   0.789592\n",
      "train loss:   0.690211\n",
      "train loss:   0.989858\n",
      "train loss:   1.038964\n",
      "train loss:   0.674229\n",
      "train loss:   0.876553\n",
      "train loss:   1.027305\n",
      "train loss:   0.829655\n",
      "train loss:   0.828839\n",
      "train loss:   0.682412\n",
      "train loss:   0.851543\n",
      "train loss:   0.730848\n",
      "train loss:   0.861561\n",
      "train loss:   0.940933\n",
      "train loss:   0.756679\n",
      "train loss:   0.949354\n",
      "train loss:   0.827210\n",
      "train loss:   0.924156\n",
      "train loss:   0.854005\n",
      "train loss:   0.982047\n",
      "train loss:   0.875817\n",
      "train loss:   0.692919\n",
      "train loss:   0.842192\n",
      "train loss:   1.024667\n",
      "train loss:   0.819919\n",
      "train loss:   0.940059\n",
      "train loss:   0.645069\n",
      "train loss:   0.682306\n",
      "train loss:   0.760417\n",
      "train loss:   0.842023\n",
      "########### epoch 96 ###########\n",
      "########### loop 13450 ###########\n",
      "test loss:   1.015254   test accuracy:   0.625000\n",
      "########### loop 13450 ###########\n",
      "train loss:   0.664320\n",
      "train loss:   0.767781\n",
      "train loss:   1.105658\n",
      "train loss:   0.944345\n",
      "train loss:   0.749049\n",
      "train loss:   1.005341\n",
      "train loss:   0.876957\n",
      "train loss:   0.842134\n",
      "train loss:   0.861193\n",
      "train loss:   0.572413\n",
      "train loss:   0.978653\n",
      "train loss:   0.605141\n",
      "train loss:   0.837945\n",
      "train loss:   0.745670\n",
      "train loss:   1.022994\n",
      "train loss:   0.661288\n",
      "train loss:   0.882098\n",
      "train loss:   0.767699\n",
      "train loss:   0.967385\n",
      "train loss:   0.719105\n",
      "train loss:   0.692133\n",
      "train loss:   0.671880\n",
      "train loss:   0.680212\n",
      "train loss:   0.785874\n",
      "train loss:   0.745325\n",
      "train loss:   0.821346\n",
      "train loss:   0.868206\n",
      "train loss:   0.780889\n",
      "train loss:   0.737729\n",
      "train loss:   0.946493\n",
      "train loss:   0.884428\n",
      "train loss:   0.582928\n",
      "train loss:   0.907322\n",
      "train loss:   0.728643\n",
      "train loss:   0.677472\n",
      "train loss:   0.782673\n",
      "train loss:   0.906334\n",
      "train loss:   0.895133\n",
      "train loss:   0.848688\n",
      "train loss:   0.934291\n",
      "train loss:   0.926513\n",
      "train loss:   0.547535\n",
      "train loss:   0.743343\n",
      "train loss:   0.761964\n",
      "train loss:   0.846947\n",
      "train loss:   0.946001\n",
      "train loss:   0.551943\n",
      "train loss:   0.765814\n",
      "train loss:   0.788231\n",
      "train loss:   0.780065\n",
      "########### epoch 96 ###########\n",
      "########### loop 13500 ###########\n",
      "test loss:   1.249746   test accuracy:   0.625000\n",
      "########### loop 13500 ###########\n",
      "train loss:   0.669042\n",
      "train loss:   0.759991\n",
      "train loss:   0.954235\n",
      "train loss:   1.108407\n",
      "train loss:   1.050123\n",
      "train loss:   0.945078\n",
      "train loss:   0.661744\n",
      "train loss:   0.894192\n",
      "train loss:   0.900467\n",
      "train loss:   0.709521\n",
      "train loss:   1.102829\n",
      "train loss:   0.922948\n",
      "train loss:   0.736153\n",
      "train loss:   0.943217\n",
      "train loss:   0.801859\n",
      "train loss:   0.702528\n",
      "train loss:   0.906813\n",
      "train loss:   0.909789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.775122\n",
      "train loss:   0.715544\n",
      "train loss:   0.525004\n",
      "train loss:   0.534121\n",
      "train loss:   0.717647\n",
      "train loss:   0.756962\n",
      "train loss:   0.831072\n",
      "train loss:   0.912913\n",
      "train loss:   0.768660\n",
      "train loss:   0.803724\n",
      "train loss:   0.977458\n",
      "train loss:   1.105895\n",
      "train loss:   1.061079\n",
      "train loss:   1.067332\n",
      "train loss:   0.774289\n",
      "train loss:   0.881088\n",
      "train loss:   0.878086\n",
      "train loss:   0.769823\n",
      "train loss:   0.645890\n",
      "train loss:   0.743107\n",
      "train loss:   0.773221\n",
      "train loss:   0.696291\n",
      "train loss:   0.920272\n",
      "train loss:   0.721011\n",
      "train loss:   0.782108\n",
      "train loss:   0.634266\n",
      "train loss:   0.889846\n",
      "train loss:   0.839816\n",
      "train loss:   0.820718\n",
      "train loss:   0.930561\n",
      "train loss:   0.830510\n",
      "train loss:   0.757863\n",
      "########### epoch 97 ###########\n",
      "########### loop 13550 ###########\n",
      "test loss:   1.288317   test accuracy:   0.625000\n",
      "########### loop 13550 ###########\n",
      "train loss:   0.753659\n",
      "train loss:   0.679012\n",
      "train loss:   0.641622\n",
      "train loss:   1.009590\n",
      "train loss:   0.949099\n",
      "train loss:   0.755090\n",
      "train loss:   0.741497\n",
      "train loss:   0.818015\n",
      "train loss:   0.571927\n",
      "train loss:   0.718455\n",
      "train loss:   0.742166\n",
      "train loss:   0.867273\n",
      "train loss:   0.661551\n",
      "train loss:   1.069976\n",
      "train loss:   1.078404\n",
      "train loss:   0.647816\n",
      "train loss:   0.986398\n",
      "train loss:   0.985486\n",
      "train loss:   0.941590\n",
      "train loss:   0.770925\n",
      "train loss:   0.639734\n",
      "train loss:   0.926376\n",
      "train loss:   0.752933\n",
      "train loss:   0.952783\n",
      "train loss:   1.043731\n",
      "train loss:   0.754389\n",
      "train loss:   0.875000\n",
      "train loss:   0.806461\n",
      "train loss:   0.864549\n",
      "train loss:   0.772624\n",
      "train loss:   0.890180\n",
      "train loss:   0.909713\n",
      "train loss:   0.730813\n",
      "train loss:   0.953104\n",
      "train loss:   1.074169\n",
      "train loss:   0.776337\n",
      "train loss:   0.977211\n",
      "train loss:   0.716542\n",
      "train loss:   0.654159\n",
      "train loss:   0.670067\n",
      "train loss:   0.839212\n",
      "train loss:   0.713552\n",
      "train loss:   0.821442\n",
      "train loss:   1.216285\n",
      "train loss:   0.930521\n",
      "train loss:   0.775446\n",
      "train loss:   0.916588\n",
      "train loss:   0.822380\n",
      "train loss:   0.864859\n",
      "train loss:   0.949500\n",
      "########### epoch 97 ###########\n",
      "########### loop 13600 ###########\n",
      "test loss:   0.984583   test accuracy:   0.687500\n",
      "########### loop 13600 ###########\n",
      "train loss:   0.635009\n",
      "train loss:   1.009581\n",
      "train loss:   0.724602\n",
      "train loss:   0.845109\n",
      "train loss:   0.722789\n",
      "train loss:   0.959750\n",
      "train loss:   0.563369\n",
      "train loss:   0.745712\n",
      "train loss:   0.738510\n",
      "train loss:   0.955353\n",
      "train loss:   0.675075\n",
      "train loss:   0.720398\n",
      "train loss:   0.691799\n",
      "train loss:   0.712501\n",
      "train loss:   0.821210\n",
      "train loss:   0.709585\n",
      "train loss:   0.748742\n",
      "train loss:   0.799310\n",
      "train loss:   0.813784\n",
      "train loss:   0.796018\n",
      "train loss:   0.905743\n",
      "train loss:   0.887127\n",
      "train loss:   0.636565\n",
      "train loss:   0.863876\n",
      "train loss:   0.730414\n",
      "train loss:   0.702637\n",
      "train loss:   0.767439\n",
      "train loss:   0.904930\n",
      "train loss:   0.906877\n",
      "train loss:   0.794600\n",
      "train loss:   0.801467\n",
      "train loss:   0.914876\n",
      "train loss:   0.577132\n",
      "train loss:   0.789875\n",
      "train loss:   0.811413\n",
      "train loss:   0.820642\n",
      "train loss:   0.915615\n",
      "train loss:   0.551692\n",
      "train loss:   0.760761\n",
      "train loss:   0.868991\n",
      "train loss:   0.868771\n",
      "train loss:   0.792833\n",
      "train loss:   0.755699\n",
      "train loss:   0.848143\n",
      "train loss:   0.975976\n",
      "train loss:   1.014981\n",
      "train loss:   0.882034\n",
      "train loss:   0.628546\n",
      "train loss:   0.902235\n",
      "train loss:   1.033528\n",
      "########### epoch 97 ###########\n",
      "########### loop 13650 ###########\n",
      "test loss:   1.098202   test accuracy:   0.656250\n",
      "########### loop 13650 ###########\n",
      "train loss:   0.690332\n",
      "train loss:   0.969551\n",
      "train loss:   0.722723\n",
      "train loss:   0.605730\n",
      "train loss:   0.767064\n",
      "train loss:   0.716084\n",
      "train loss:   0.699796\n",
      "train loss:   0.945992\n",
      "train loss:   0.809796\n",
      "train loss:   0.707566\n",
      "train loss:   0.698506\n",
      "train loss:   0.533930\n",
      "train loss:   0.572647\n",
      "train loss:   0.712572\n",
      "train loss:   0.699098\n",
      "train loss:   0.833828\n",
      "train loss:   0.951347\n",
      "train loss:   0.777988\n",
      "train loss:   0.924790\n",
      "train loss:   0.990395\n",
      "train loss:   1.017830\n",
      "train loss:   1.029284\n",
      "train loss:   0.881358\n",
      "train loss:   0.685607\n",
      "train loss:   0.725099\n",
      "train loss:   0.862785\n",
      "train loss:   0.793459\n",
      "train loss:   0.711091\n",
      "train loss:   0.869770\n",
      "train loss:   0.836667\n",
      "train loss:   0.715440\n",
      "train loss:   0.873046\n",
      "train loss:   0.750456\n",
      "train loss:   0.888893\n",
      "train loss:   0.546192\n",
      "train loss:   0.810229\n",
      "train loss:   0.680804\n",
      "train loss:   0.796552\n",
      "train loss:   0.789602\n",
      "train loss:   0.786353\n",
      "train loss:   0.732686\n",
      "train loss:   0.706367\n",
      "train loss:   0.629082\n",
      "train loss:   0.630180\n",
      "train loss:   0.969942\n",
      "train loss:   0.915980\n",
      "train loss:   0.754857\n",
      "train loss:   0.669826\n",
      "train loss:   0.805744\n",
      "train loss:   0.668826\n",
      "########### epoch 98 ###########\n",
      "########### loop 13700 ###########\n",
      "test loss:   0.938972   test accuracy:   0.687500\n",
      "########### loop 13700 ###########\n",
      "train loss:   0.661732\n",
      "train loss:   0.694624\n",
      "train loss:   0.722282\n",
      "train loss:   0.672845\n",
      "train loss:   1.038869\n",
      "train loss:   1.046426\n",
      "train loss:   0.543334\n",
      "train loss:   0.836474\n",
      "train loss:   0.968559\n",
      "train loss:   0.849962\n",
      "train loss:   1.016305\n",
      "train loss:   0.750623\n",
      "train loss:   0.924716\n",
      "train loss:   0.741191\n",
      "train loss:   0.808714\n",
      "train loss:   1.009313\n",
      "train loss:   0.708951\n",
      "train loss:   0.931074\n",
      "train loss:   0.860826\n",
      "train loss:   0.901846\n",
      "train loss:   0.829541\n",
      "train loss:   0.955175\n",
      "train loss:   0.804365\n",
      "train loss:   0.660776\n",
      "train loss:   0.865829\n",
      "train loss:   1.079178\n",
      "train loss:   0.753229\n",
      "train loss:   0.823696\n",
      "train loss:   0.688388\n",
      "train loss:   0.654198\n",
      "train loss:   0.700588\n",
      "train loss:   0.869231\n",
      "train loss:   0.652755\n",
      "train loss:   0.841109\n",
      "train loss:   1.189710\n",
      "train loss:   0.900073\n",
      "train loss:   0.790611\n",
      "train loss:   1.018265\n",
      "train loss:   0.923449\n",
      "train loss:   1.050275\n",
      "train loss:   0.837548\n",
      "train loss:   0.597106\n",
      "train loss:   1.101157\n",
      "train loss:   0.674940\n",
      "train loss:   0.876792\n",
      "train loss:   0.757971\n",
      "train loss:   0.972439\n",
      "train loss:   0.601300\n",
      "train loss:   0.770262\n",
      "train loss:   0.707565\n",
      "########### epoch 98 ###########\n",
      "########### loop 13750 ###########\n",
      "test loss:   0.863560   test accuracy:   0.812500\n",
      "########### loop 13750 ###########\n",
      "train loss:   0.986951\n",
      "train loss:   0.682789\n",
      "train loss:   0.756352\n",
      "train loss:   0.762829\n",
      "train loss:   0.717292\n",
      "train loss:   0.889524\n",
      "train loss:   0.817844\n",
      "train loss:   0.910673\n",
      "train loss:   0.889112\n",
      "train loss:   0.859296\n",
      "train loss:   0.939167\n",
      "train loss:   1.019141\n",
      "train loss:   0.929685\n",
      "train loss:   0.623183\n",
      "train loss:   0.842585\n",
      "train loss:   0.733268\n",
      "train loss:   0.699860\n",
      "train loss:   0.789749\n",
      "train loss:   0.948434\n",
      "train loss:   0.906548\n",
      "train loss:   0.837233\n",
      "train loss:   0.975134\n",
      "train loss:   0.897284\n",
      "train loss:   0.520440\n",
      "train loss:   0.606914\n",
      "train loss:   0.734788\n",
      "train loss:   0.810197\n",
      "train loss:   0.905740\n",
      "train loss:   0.563804\n",
      "train loss:   0.736375\n",
      "train loss:   0.763316\n",
      "train loss:   0.835591\n",
      "train loss:   0.635016\n",
      "train loss:   0.823521\n",
      "train loss:   0.925617\n",
      "train loss:   0.976068\n",
      "train loss:   0.967692\n",
      "train loss:   0.924092\n",
      "train loss:   0.603643\n",
      "train loss:   0.983936\n",
      "train loss:   0.978016\n",
      "train loss:   0.709178\n",
      "train loss:   1.022017\n",
      "train loss:   0.837500\n",
      "train loss:   0.717958\n",
      "train loss:   0.917205\n",
      "train loss:   0.840182\n",
      "train loss:   0.662657\n",
      "train loss:   1.000548\n",
      "train loss:   0.820325\n",
      "########### epoch 98 ###########\n",
      "########### loop 13800 ###########\n",
      "test loss:   1.025490   test accuracy:   0.687500\n",
      "########### loop 13800 ###########\n",
      "train loss:   0.744498\n",
      "train loss:   0.695302\n",
      "train loss:   0.529791\n",
      "train loss:   0.558276\n",
      "train loss:   0.756876\n",
      "train loss:   0.803788\n",
      "train loss:   0.904416\n",
      "train loss:   0.944984\n",
      "train loss:   0.813225\n",
      "train loss:   0.834285\n",
      "train loss:   1.008682\n",
      "train loss:   1.121910\n",
      "train loss:   1.105266\n",
      "train loss:   1.065747\n",
      "train loss:   0.799030\n",
      "train loss:   0.848666\n",
      "train loss:   0.871311\n",
      "train loss:   0.755668\n",
      "train loss:   0.720128\n",
      "train loss:   0.813476\n",
      "train loss:   0.824633\n",
      "train loss:   0.668986\n",
      "train loss:   0.876667\n",
      "train loss:   0.771551\n",
      "train loss:   0.748466\n",
      "train loss:   0.553325\n",
      "train loss:   0.813868\n",
      "train loss:   0.628634\n",
      "train loss:   0.762879\n",
      "train loss:   0.737985\n",
      "train loss:   0.871424\n",
      "train loss:   0.755803\n",
      "train loss:   0.668128\n",
      "train loss:   0.669735\n",
      "train loss:   0.636011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.990360\n",
      "train loss:   0.876682\n",
      "train loss:   0.775984\n",
      "train loss:   0.708000\n",
      "train loss:   0.904588\n",
      "train loss:   0.683431\n",
      "train loss:   0.739449\n",
      "train loss:   0.707449\n",
      "train loss:   0.741410\n",
      "train loss:   0.671711\n",
      "train loss:   1.037302\n",
      "train loss:   1.099668\n",
      "train loss:   0.633414\n",
      "train loss:   0.983492\n",
      "train loss:   0.984274\n",
      "########### epoch 99 ###########\n",
      "########### loop 13850 ###########\n",
      "test loss:   0.968486   test accuracy:   0.750000\n",
      "########### loop 13850 ###########\n",
      "train loss:   0.852460\n",
      "train loss:   0.950079\n",
      "train loss:   0.768620\n",
      "train loss:   0.901880\n",
      "train loss:   0.754170\n",
      "train loss:   0.941260\n",
      "train loss:   0.995813\n",
      "train loss:   0.704735\n",
      "train loss:   0.949708\n",
      "train loss:   0.857927\n",
      "train loss:   0.987699\n",
      "train loss:   0.789526\n",
      "train loss:   0.998022\n",
      "train loss:   0.869377\n",
      "train loss:   0.689916\n",
      "train loss:   0.976279\n",
      "train loss:   1.061355\n",
      "train loss:   0.796331\n",
      "train loss:   0.882178\n",
      "train loss:   0.636997\n",
      "train loss:   0.617916\n",
      "train loss:   0.628446\n",
      "train loss:   0.966886\n",
      "train loss:   0.654447\n",
      "train loss:   0.793587\n",
      "train loss:   1.150280\n",
      "train loss:   0.934991\n",
      "train loss:   0.743270\n",
      "train loss:   1.021297\n",
      "train loss:   0.907171\n",
      "train loss:   0.905526\n",
      "train loss:   0.792849\n",
      "train loss:   0.620064\n",
      "train loss:   1.117795\n",
      "train loss:   0.770605\n",
      "train loss:   0.909396\n",
      "train loss:   0.744129\n",
      "train loss:   0.910512\n",
      "train loss:   0.593794\n",
      "train loss:   0.752247\n",
      "train loss:   0.680696\n",
      "train loss:   0.962475\n",
      "train loss:   0.688999\n",
      "train loss:   0.711038\n",
      "train loss:   0.683366\n",
      "train loss:   0.675130\n",
      "train loss:   0.795279\n",
      "train loss:   0.658226\n",
      "train loss:   0.729498\n",
      "train loss:   0.804701\n",
      "########### epoch 99 ###########\n",
      "########### loop 13900 ###########\n",
      "test loss:   0.997343   test accuracy:   0.781250\n",
      "########### loop 13900 ###########\n",
      "train loss:   0.787799\n",
      "train loss:   0.800882\n",
      "train loss:   0.941605\n",
      "train loss:   0.870881\n",
      "train loss:   0.677986\n",
      "train loss:   0.937196\n",
      "train loss:   0.808086\n",
      "train loss:   0.677256\n",
      "train loss:   0.702937\n",
      "train loss:   0.840853\n",
      "train loss:   0.847581\n",
      "train loss:   0.814796\n",
      "train loss:   0.897177\n",
      "train loss:   0.854737\n",
      "train loss:   0.565333\n",
      "train loss:   0.722029\n",
      "train loss:   0.802305\n",
      "train loss:   0.801128\n",
      "train loss:   0.939995\n",
      "train loss:   0.492085\n",
      "train loss:   0.773052\n",
      "train loss:   0.811148\n",
      "train loss:   0.950517\n",
      "train loss:   0.711115\n",
      "train loss:   0.762489\n",
      "train loss:   0.919335\n",
      "train loss:   1.031194\n",
      "train loss:   1.010294\n",
      "train loss:   0.878912\n",
      "train loss:   0.592139\n",
      "train loss:   0.851739\n",
      "train loss:   0.964931\n",
      "train loss:   0.685521\n",
      "train loss:   1.037190\n",
      "train loss:   0.785697\n",
      "train loss:   0.681246\n",
      "train loss:   0.824947\n",
      "train loss:   0.753497\n",
      "train loss:   0.673556\n",
      "train loss:   0.904927\n",
      "train loss:   0.830710\n",
      "train loss:   0.789061\n",
      "train loss:   0.706665\n",
      "train loss:   0.573744\n",
      "train loss:   0.612531\n",
      "train loss:   0.681426\n",
      "train loss:   0.739518\n",
      "train loss:   0.840659\n",
      "train loss:   0.865703\n",
      "train loss:   0.840572\n",
      "########### epoch 99 ###########\n",
      "########### loop 13950 ###########\n",
      "test loss:   0.928088   test accuracy:   0.781250\n",
      "########### loop 13950 ###########\n",
      "train loss:   0.803613\n",
      "train loss:   0.912841\n",
      "train loss:   0.947031\n",
      "train loss:   1.058402\n",
      "train loss:   0.834493\n",
      "train loss:   0.718575\n",
      "train loss:   0.697289\n",
      "train loss:   0.865524\n",
      "train loss:   0.702335\n",
      "train loss:   0.609701\n",
      "train loss:   0.742006\n",
      "train loss:   0.677883\n",
      "train loss:   0.681937\n",
      "train loss:   0.869259\n",
      "train loss:   0.714941\n",
      "train loss:   0.807559\n",
      "train loss:   0.571276\n",
      "train loss:   0.760890\n",
      "train loss:   0.749150\n",
      "train loss:   0.735356\n",
      "train loss:   0.785535\n",
      "train loss:   0.856241\n",
      "train loss:   0.780941\n",
      "train loss:   0.705061\n",
      "train loss:   0.691977\n",
      "train loss:   0.612204\n",
      "train loss:   0.985138\n",
      "train loss:   0.911237\n",
      "train loss:   0.704004\n",
      "train loss:   0.716978\n",
      "train loss:   0.900168\n",
      "train loss:   0.722846\n",
      "train loss:   0.749810\n",
      "train loss:   0.757316\n",
      "train loss:   0.646040\n",
      "train loss:   0.698108\n",
      "train loss:   0.959269\n",
      "train loss:   1.016171\n",
      "train loss:   0.652481\n",
      "train loss:   0.962979\n",
      "train loss:   1.001081\n",
      "train loss:   0.789029\n",
      "train loss:   0.910712\n",
      "train loss:   0.686851\n",
      "train loss:   0.877331\n",
      "train loss:   0.772545\n",
      "train loss:   0.885441\n",
      "train loss:   1.132553\n",
      "train loss:   0.794780\n",
      "train loss:   0.997811\n",
      "########### epoch 100 ###########\n",
      "########### loop 14000 ###########\n",
      "test loss:   1.155418   test accuracy:   0.718750\n",
      "########### loop 14000 ###########\n",
      "train loss:   0.891581\n",
      "train loss:   0.927027\n",
      "train loss:   0.817696\n",
      "train loss:   1.060547\n",
      "train loss:   0.942090\n",
      "train loss:   0.636951\n",
      "train loss:   0.865698\n",
      "train loss:   1.043388\n",
      "train loss:   0.772188\n",
      "train loss:   0.814689\n",
      "train loss:   0.627737\n",
      "train loss:   0.656852\n",
      "train loss:   0.674745\n",
      "train loss:   0.790927\n",
      "train loss:   0.621037\n",
      "train loss:   0.819056\n",
      "train loss:   1.016306\n",
      "train loss:   1.024042\n",
      "train loss:   0.837186\n",
      "train loss:   1.009981\n",
      "train loss:   0.975740\n",
      "train loss:   0.914890\n",
      "train loss:   0.845156\n",
      "train loss:   0.631923\n",
      "train loss:   0.990069\n",
      "train loss:   0.684444\n",
      "train loss:   0.933497\n",
      "train loss:   0.744513\n",
      "train loss:   0.988472\n",
      "train loss:   0.523876\n",
      "train loss:   0.743811\n",
      "train loss:   0.736597\n",
      "train loss:   0.955872\n",
      "train loss:   0.714927\n",
      "train loss:   0.707759\n",
      "train loss:   0.731332\n",
      "train loss:   0.689147\n",
      "train loss:   0.829286\n",
      "train loss:   0.658798\n",
      "train loss:   0.756196\n",
      "train loss:   0.780021\n",
      "train loss:   0.764534\n",
      "train loss:   0.776322\n",
      "train loss:   0.958948\n",
      "train loss:   0.977592\n",
      "train loss:   0.626743\n",
      "train loss:   1.029093\n",
      "train loss:   0.768096\n",
      "train loss:   0.658509\n",
      "train loss:   0.771527\n",
      "########### epoch 100 ###########\n",
      "########### loop 14050 ###########\n",
      "test loss:   1.061221   test accuracy:   0.593750\n",
      "########### loop 14050 ###########\n",
      "train loss:   0.860962\n",
      "train loss:   0.916784\n",
      "train loss:   0.812050\n",
      "train loss:   0.882097\n",
      "train loss:   0.866248\n",
      "train loss:   0.629748\n",
      "train loss:   0.675193\n",
      "train loss:   0.770064\n",
      "train loss:   0.795260\n",
      "train loss:   0.934487\n",
      "train loss:   0.536249\n",
      "train loss:   0.771758\n",
      "train loss:   0.752443\n",
      "train loss:   0.794867\n",
      "train loss:   0.652325\n",
      "train loss:   0.702341\n",
      "train loss:   0.862538\n",
      "train loss:   0.999055\n",
      "train loss:   1.003699\n",
      "train loss:   0.887822\n",
      "train loss:   0.561647\n",
      "train loss:   0.863666\n",
      "train loss:   0.878038\n",
      "train loss:   0.676059\n",
      "train loss:   1.004772\n",
      "train loss:   0.745799\n",
      "train loss:   0.674027\n",
      "train loss:   0.851499\n",
      "train loss:   0.792952\n",
      "train loss:   0.681712\n",
      "train loss:   0.905055\n",
      "train loss:   0.840867\n",
      "train loss:   0.720154\n",
      "train loss:   0.726041\n",
      "train loss:   0.527125\n",
      "train loss:   0.578721\n",
      "train loss:   0.661789\n",
      "train loss:   0.705019\n",
      "train loss:   0.806827\n",
      "train loss:   0.883591\n",
      "train loss:   0.755306\n",
      "train loss:   0.807401\n",
      "train loss:   0.913657\n",
      "train loss:   0.965269\n",
      "train loss:   0.932348\n",
      "train loss:   0.915797\n",
      "train loss:   0.644371\n",
      "train loss:   0.744816\n",
      "train loss:   0.903370\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "# build input\n",
    "imgs = tf.placeholder(tf.float32, shape = [None, image_size, image_size, 3], name = \"input_x\")\n",
    "targets = tf.placeholder(tf.int32, shape = [None, n_classes], name = \"input_y\")\n",
    "keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "\n",
    "# build cal graph\n",
    "bilinear_cnn = vgg_bilinear_model(imgs, keep_prob, pretrained_weights = weights_file, sess = sess, finetune = False)\n",
    "\n",
    "# build cal node parameter\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = targets, logits = bilinear_cnn.fc3l)\n",
    "cost = tf.reduce_mean(cross_entropy, name = \"cost\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.9, momentum=0.9).minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(bilinear_cnn.fc3l, name = \"predicted\")\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name = \"accuracy\")\n",
    "\n",
    "# start train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "bilinear_cnn.load_vgg_weights(sess)\n",
    "\n",
    "for v in tf.trainable_variables():\n",
    "    print(\"Trainable variables\", v)\n",
    "print('Starting training')\n",
    "\n",
    "\n",
    "###################################\n",
    "train_img_batch, train_label_batch = get_batchs(tfrecord_filename = \"train.tfrecords\", \n",
    "                                                    image_size = image_size, classes = n_classes,\n",
    "                                                    batch_size = batch_size, min_after_dequeue = 500)\n",
    "test_img_batch, test_label_batch = get_batchs(tfrecord_filename = \"test.tfrecords\", \n",
    "                                                  image_size = image_size, classes = n_classes,\n",
    "                                                  batch_size = batch_size, min_after_dequeue = 500)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "for i in range(int((4512 / batch_size) * epoch)):\n",
    "    # get data\n",
    "    train_imgs, train_labels= sess.run([train_img_batch, train_label_batch])\n",
    "    test_imgs, test_labels= sess.run([test_img_batch, test_label_batch])\n",
    "\n",
    "    # train single loop\n",
    "    sess.run(optimizer, feed_dict={imgs: train_imgs, targets: train_labels, keep_prob: 0.5})\n",
    "    train_loss = sess.run(cost, feed_dict={imgs: train_imgs, targets: train_labels, keep_prob: 1.})\n",
    "    print(\"train loss: %10f\"%(train_loss))\n",
    "            \n",
    "    # print test stat\n",
    "    if (i % 50 == 0):\n",
    "        print(\"########### epoch %d ###########\"%((i / (4512 / batch_size)) + 1))\n",
    "        print(\"########### loop %d ###########\"%(i))\n",
    "        test_loss = sess.run(cost, feed_dict={imgs: test_imgs, targets: test_labels, keep_prob: 1.})\n",
    "        test_acc = sess.run(accuracy, feed_dict={imgs: test_imgs, targets: test_labels, keep_prob: 1.})\n",
    "        print(\"test loss: %10f   test accuracy: %10f\"%(test_loss, test_acc))\n",
    "        print(\"########### loop %d ###########\"%(i))\n",
    "\n",
    "# close data queue\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable test1/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"D:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"D:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2628, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3f35a2b3b9b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m           use_resource=use_resource)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    662\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 664\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    665\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable test1/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"D:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"D:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2628, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('test1'):\n",
    "    weights = tf.get_variable('W', [1], trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-42-da1db3f03447>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-42-da1db3f03447>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    np.savez('own_last_layer_weights.npz', 'xxx' = [sess.run(var)])\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    with tf.variable_scope('test1', reuse = True):\n",
    "        var = tf.get_variable('W')\n",
    "        print(var.name)\n",
    "        print(var.eval())\n",
    "        sess.run(var.assign([111]))\n",
    "        print(var.eval())\n",
    "        print(sess.run(var))\n",
    "        np.savez('own_last_layer_weights.npz', xxx = [sess.run(var)])\n",
    "        \n",
    "        \n",
    "#   sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict_test = np.load('own_last_layer_weights.npz', encoding = 'bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxx']\n",
      "[[111.]]\n"
     ]
    }
   ],
   "source": [
    "print(weights_dict_test.keys())\n",
    "print(weights_dict_test['xxx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
