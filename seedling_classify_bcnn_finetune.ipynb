{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-B FineTune 训练\n",
    "\n",
    "### 1.加载VGG预训练参数，加载FC层参数\n",
    "\n",
    "### 2.训练CONV层和FC层\n",
    "\n",
    "### 2.保存CONV层和FC层参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from data_preprocess import *\n",
    "from image_preprocess import *\n",
    "from bcnn_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_file = \"vgg16_weights.npz\"\n",
    "weights_file_last_layer = \"bcnn_last_weights.npz\"\n",
    "weights_file_conv = \"bcnn_conv_weights.npz\"\n",
    "\n",
    "batch_size = 24\n",
    "lr = 0.001\n",
    "finetune_step = -1\n",
    "\n",
    "image_size = 224\n",
    "n_classes = 12\n",
    "\n",
    "epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Data Augmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\XX\\Anaconda2\\envs\\gpu-env-tf\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of z (?, 262144)\n",
      "Adding weights to conv1_1/b:0\n",
      "Adding weights to conv1_1/W:0\n",
      "Adding weights to conv1_2/b:0\n",
      "Adding weights to conv1_2/W:0\n",
      "Adding weights to conv2_1/b:0\n",
      "Adding weights to conv2_1/W:0\n",
      "Adding weights to conv2_2/b:0\n",
      "Adding weights to conv2_2/W:0\n",
      "Adding weights to conv3_1/b:0\n",
      "Adding weights to conv3_1/W:0\n",
      "Adding weights to conv3_2/b:0\n",
      "Adding weights to conv3_2/W:0\n",
      "Adding weights to conv3_3/b:0\n",
      "Adding weights to conv3_3/W:0\n",
      "Adding weights to conv4_1/b:0\n",
      "Adding weights to conv4_1/W:0\n",
      "Adding weights to conv4_2/b:0\n",
      "Adding weights to conv4_2/W:0\n",
      "Adding weights to conv4_3/b:0\n",
      "Adding weights to conv4_3/W:0\n",
      "Adding weights to conv5_1/b:0\n",
      "Adding weights to conv5_1/W:0\n",
      "Adding weights to conv5_2/b:0\n",
      "Adding weights to conv5_2/W:0\n",
      "Adding weights to conv5_3/b:0\n",
      "Adding weights to conv5_3/W:0\n",
      "last_layer [[ 9.5026232e-02  7.2930790e-02 -3.6813654e-02 ... -3.4357134e-02\n",
      "  -8.7767147e-02  5.0819542e-02]\n",
      " [ 1.3950452e-03 -1.4508091e-03 -3.7143724e-03 ...  4.9801045e-03\n",
      "  -6.6605175e-04 -8.1326594e-05]\n",
      " [ 5.5598471e-02  4.8295103e-02 -4.3294623e-02 ... -5.8805291e-02\n",
      "  -2.5589159e-02  5.1891845e-02]\n",
      " ...\n",
      " [-2.8579900e-04  7.9469429e-03 -5.1174024e-03 ... -1.3578797e-03\n",
      "  -7.5880910e-04 -4.8587504e-03]\n",
      " [ 8.6970087e-03  4.8119997e-05 -3.0925651e-03 ... -6.2323422e-03\n",
      "  -6.1725550e-03 -2.7743427e-03]\n",
      " [ 7.4410692e-02  5.7588834e-02  5.7855304e-02 ... -5.0982535e-02\n",
      "   4.5402646e-02 -7.4410759e-02]]\n",
      "Adding weights to fc-new/W:0\n",
      "Adding weights to fc-new/b:0\n",
      "Trainable variables <tf.Variable 'conv1_1/W:0' shape=(3, 3, 3, 64) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv1_1/b:0' shape=(64,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv1_2/W:0' shape=(3, 3, 64, 64) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv1_2/b:0' shape=(64,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv2_1/W:0' shape=(3, 3, 64, 128) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv2_1/b:0' shape=(128,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv2_2/W:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv2_2/b:0' shape=(128,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv3_1/W:0' shape=(3, 3, 128, 256) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv3_1/b:0' shape=(256,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv3_2/W:0' shape=(3, 3, 256, 256) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv3_2/b:0' shape=(256,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv3_3/W:0' shape=(3, 3, 256, 256) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv3_3/b:0' shape=(256,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv4_1/W:0' shape=(3, 3, 256, 512) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv4_1/b:0' shape=(512,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv4_2/W:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv4_2/b:0' shape=(512,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv4_3/W:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv4_3/b:0' shape=(512,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv5_1/W:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv5_1/b:0' shape=(512,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv5_2/W:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv5_2/b:0' shape=(512,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv5_3/W:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'conv5_3/b:0' shape=(512,) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'fc-new/W:0' shape=(262144, 12) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'fc-new/b:0' shape=(12,) dtype=float32_ref>\n",
      "Starting training\n",
      "train loss:   1.446690\n",
      "########### epoch 1 ###########\n",
      "########### loop 0 ###########\n",
      "test loss:   0.956768   test accuracy:   0.791667\n",
      "########### loop 0 ###########\n",
      "train loss:   1.206481\n",
      "train loss:   1.348149\n",
      "train loss:   1.495503\n",
      "train loss:   1.152087\n",
      "train loss:   1.413761\n",
      "train loss:   1.151150\n",
      "train loss:   1.522206\n",
      "train loss:   1.524835\n",
      "train loss:   1.554299\n",
      "train loss:   1.413249\n",
      "train loss:   1.570132\n",
      "train loss:   1.438315\n",
      "train loss:   1.502350\n",
      "train loss:   1.524733\n",
      "train loss:   1.622660\n",
      "train loss:   1.194467\n",
      "train loss:   1.552259\n",
      "train loss:   1.275622\n",
      "train loss:   1.358264\n",
      "train loss:   1.184700\n",
      "train loss:   1.589165\n",
      "train loss:   1.367151\n",
      "train loss:   1.447926\n",
      "train loss:   1.485324\n",
      "train loss:   1.195764\n",
      "train loss:   1.500261\n",
      "train loss:   1.488414\n",
      "train loss:   1.466113\n",
      "train loss:   1.625227\n",
      "train loss:   1.676327\n",
      "train loss:   1.378700\n",
      "train loss:   1.169537\n",
      "train loss:   1.282290\n",
      "train loss:   1.861921\n",
      "train loss:   1.578070\n",
      "train loss:   1.371134\n",
      "train loss:   1.273144\n",
      "train loss:   1.462535\n",
      "train loss:   1.301816\n",
      "train loss:   1.185384\n",
      "train loss:   1.125744\n",
      "train loss:   1.567873\n",
      "train loss:   1.205323\n",
      "train loss:   1.772277\n",
      "train loss:   1.377218\n",
      "train loss:   1.395926\n",
      "train loss:   1.506386\n",
      "train loss:   1.650220\n",
      "train loss:   1.600468\n",
      "train loss:   1.550635\n",
      "########### epoch 1 ###########\n",
      "########### loop 50 ###########\n",
      "test loss:   0.815367   test accuracy:   0.833333\n",
      "########### loop 50 ###########\n",
      "train loss:   1.393303\n",
      "train loss:   1.524603\n",
      "train loss:   1.543792\n",
      "train loss:   1.503240\n",
      "train loss:   0.828612\n",
      "train loss:   1.317875\n",
      "train loss:   1.648261\n",
      "train loss:   1.118619\n",
      "train loss:   1.178461\n",
      "train loss:   1.105265\n",
      "train loss:   1.246282\n",
      "train loss:   1.535214\n",
      "train loss:   1.478973\n",
      "train loss:   1.633020\n",
      "train loss:   1.121049\n",
      "train loss:   1.287938\n",
      "train loss:   1.440660\n",
      "train loss:   1.747358\n",
      "train loss:   1.363735\n",
      "train loss:   1.775877\n",
      "train loss:   1.523775\n",
      "train loss:   1.322572\n",
      "train loss:   1.505403\n",
      "train loss:   0.981140\n",
      "train loss:   1.610277\n",
      "train loss:   1.557089\n",
      "train loss:   1.461792\n",
      "train loss:   1.620989\n",
      "train loss:   1.320069\n",
      "train loss:   1.595224\n",
      "train loss:   1.058620\n",
      "train loss:   1.378592\n",
      "train loss:   1.851907\n",
      "train loss:   1.392311\n",
      "train loss:   1.112091\n",
      "train loss:   1.205959\n",
      "train loss:   1.865329\n",
      "train loss:   1.445959\n",
      "train loss:   1.901015\n",
      "train loss:   1.490806\n",
      "train loss:   1.189607\n",
      "train loss:   1.195216\n",
      "train loss:   1.399206\n",
      "train loss:   1.715975\n",
      "train loss:   1.374667\n",
      "train loss:   1.500565\n",
      "train loss:   1.890350\n",
      "train loss:   1.270571\n",
      "train loss:   1.221819\n",
      "train loss:   0.785088\n",
      "########### epoch 1 ###########\n",
      "########### loop 100 ###########\n",
      "test loss:   0.818008   test accuracy:   0.833333\n",
      "########### loop 100 ###########\n",
      "train loss:   1.592734\n",
      "train loss:   1.429827\n",
      "train loss:   1.448892\n",
      "train loss:   1.207644\n",
      "train loss:   1.418338\n",
      "train loss:   1.469598\n",
      "train loss:   1.282269\n",
      "train loss:   1.153586\n",
      "train loss:   1.544310\n",
      "train loss:   1.448752\n",
      "train loss:   1.722546\n",
      "train loss:   1.207709\n",
      "train loss:   1.154609\n",
      "train loss:   1.236778\n",
      "train loss:   1.318097\n",
      "train loss:   1.392075\n",
      "train loss:   1.281829\n",
      "train loss:   1.551128\n",
      "train loss:   1.055620\n",
      "train loss:   1.419536\n",
      "train loss:   1.450387\n",
      "train loss:   1.490475\n",
      "train loss:   1.133050\n",
      "train loss:   1.282069\n",
      "train loss:   1.315989\n",
      "train loss:   1.143034\n",
      "train loss:   1.027888\n",
      "train loss:   1.212506\n",
      "train loss:   1.663018\n",
      "train loss:   0.985792\n",
      "train loss:   1.442873\n",
      "train loss:   1.161452\n",
      "train loss:   0.936065\n",
      "train loss:   1.682420\n",
      "train loss:   1.282760\n",
      "train loss:   1.568987\n",
      "train loss:   1.394839\n",
      "train loss:   1.077842\n",
      "train loss:   1.609365\n",
      "train loss:   1.148272\n",
      "train loss:   1.197083\n",
      "train loss:   1.609809\n",
      "train loss:   1.410264\n",
      "train loss:   1.150260\n",
      "train loss:   0.953179\n",
      "train loss:   1.385407\n",
      "train loss:   1.321076\n",
      "train loss:   1.177219\n",
      "train loss:   1.203794\n",
      "train loss:   1.182156\n",
      "########### epoch 1 ###########\n",
      "########### loop 150 ###########\n",
      "test loss:   0.549079   test accuracy:   0.958333\n",
      "########### loop 150 ###########\n",
      "train loss:   1.659503\n",
      "train loss:   1.611468\n",
      "train loss:   1.684411\n",
      "train loss:   1.275063\n",
      "train loss:   1.351895\n",
      "train loss:   1.381715\n",
      "train loss:   1.347229\n",
      "train loss:   1.235537\n",
      "train loss:   1.348985\n",
      "train loss:   1.221763\n",
      "train loss:   1.376377\n",
      "train loss:   1.627591\n",
      "train loss:   1.635383\n",
      "train loss:   1.446357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.305809\n",
      "train loss:   1.304635\n",
      "train loss:   1.154440\n",
      "train loss:   0.971507\n",
      "train loss:   1.066664\n",
      "train loss:   1.235158\n",
      "train loss:   1.633465\n",
      "train loss:   1.286285\n",
      "train loss:   1.748744\n",
      "train loss:   1.336265\n",
      "train loss:   1.263767\n",
      "train loss:   1.392297\n",
      "train loss:   1.183750\n",
      "train loss:   1.431417\n",
      "train loss:   1.565588\n",
      "train loss:   1.072053\n",
      "train loss:   1.339392\n",
      "train loss:   1.523391\n",
      "train loss:   1.172826\n",
      "train loss:   1.765412\n",
      "train loss:   1.287030\n",
      "train loss:   1.120016\n",
      "train loss:   1.016456\n",
      "train loss:   1.376207\n",
      "train loss:   1.350168\n",
      "train loss:   0.815626\n",
      "train loss:   1.181934\n",
      "train loss:   1.210267\n",
      "train loss:   1.253849\n",
      "train loss:   1.107709\n",
      "train loss:   1.355977\n",
      "train loss:   1.191433\n",
      "train loss:   1.270161\n",
      "train loss:   1.547381\n",
      "train loss:   1.595149\n",
      "train loss:   1.358863\n",
      "########### epoch 2 ###########\n",
      "########### loop 200 ###########\n",
      "test loss:   0.573254   test accuracy:   1.000000\n",
      "########### loop 200 ###########\n",
      "train loss:   1.174574\n",
      "train loss:   1.546985\n",
      "train loss:   1.288688\n",
      "train loss:   1.248023\n",
      "train loss:   1.552372\n",
      "train loss:   1.127195\n",
      "train loss:   1.302088\n",
      "train loss:   1.151060\n",
      "train loss:   1.282926\n",
      "train loss:   1.756208\n",
      "train loss:   1.415100\n",
      "train loss:   1.398751\n",
      "train loss:   1.385369\n",
      "train loss:   1.236395\n",
      "train loss:   1.065956\n",
      "train loss:   1.145792\n",
      "train loss:   1.368948\n",
      "train loss:   1.354216\n",
      "train loss:   0.814616\n",
      "train loss:   1.267039\n",
      "train loss:   1.407548\n",
      "train loss:   1.086870\n",
      "train loss:   1.275002\n",
      "train loss:   0.911784\n",
      "train loss:   0.940589\n",
      "train loss:   1.342913\n",
      "train loss:   1.276770\n",
      "train loss:   1.089689\n",
      "train loss:   1.469249\n",
      "train loss:   1.285058\n",
      "train loss:   1.186782\n",
      "train loss:   1.072508\n",
      "train loss:   1.467313\n",
      "train loss:   1.330345\n",
      "train loss:   1.621362\n",
      "train loss:   1.025852\n",
      "train loss:   1.150069\n",
      "train loss:   1.202208\n",
      "train loss:   1.277155\n",
      "train loss:   1.261886\n",
      "train loss:   1.327069\n",
      "train loss:   1.044222\n",
      "train loss:   0.910724\n",
      "train loss:   1.170089\n",
      "train loss:   1.008036\n",
      "train loss:   1.394969\n",
      "train loss:   1.115520\n",
      "train loss:   0.830175\n",
      "train loss:   1.651407\n",
      "train loss:   1.436802\n",
      "########### epoch 2 ###########\n",
      "########### loop 250 ###########\n",
      "test loss:   0.558572   test accuracy:   1.000000\n",
      "########### loop 250 ###########\n",
      "train loss:   1.018153\n",
      "train loss:   0.985751\n",
      "train loss:   1.351753\n",
      "train loss:   1.565383\n",
      "train loss:   1.152944\n",
      "train loss:   1.121493\n",
      "train loss:   1.283740\n",
      "train loss:   1.429005\n",
      "train loss:   1.259387\n",
      "train loss:   1.071137\n",
      "train loss:   1.622571\n",
      "train loss:   1.521205\n",
      "train loss:   1.415179\n",
      "train loss:   1.326970\n",
      "train loss:   0.950711\n",
      "train loss:   1.422968\n",
      "train loss:   1.039104\n",
      "train loss:   1.376712\n",
      "train loss:   1.347020\n",
      "train loss:   1.125460\n",
      "train loss:   1.351279\n",
      "train loss:   1.405715\n",
      "train loss:   1.265846\n",
      "train loss:   1.589999\n",
      "train loss:   1.589131\n",
      "train loss:   1.557139\n",
      "train loss:   1.598314\n",
      "train loss:   1.258387\n",
      "train loss:   0.790511\n",
      "train loss:   1.082313\n",
      "train loss:   1.352865\n",
      "train loss:   1.328233\n",
      "train loss:   1.658617\n",
      "train loss:   1.079521\n",
      "train loss:   1.314188\n",
      "train loss:   1.355098\n",
      "train loss:   1.601637\n",
      "train loss:   1.554009\n",
      "train loss:   1.308011\n",
      "train loss:   1.075804\n",
      "train loss:   1.534753\n",
      "train loss:   1.574682\n",
      "train loss:   1.406523\n",
      "train loss:   1.066677\n",
      "train loss:   1.671019\n",
      "train loss:   1.423786\n",
      "train loss:   1.637951\n",
      "train loss:   1.249649\n",
      "train loss:   1.269464\n",
      "train loss:   0.862517\n",
      "########### epoch 2 ###########\n",
      "########### loop 300 ###########\n",
      "test loss:   0.745602   test accuracy:   0.875000\n",
      "########### loop 300 ###########\n",
      "train loss:   1.505094\n",
      "train loss:   0.983522\n",
      "train loss:   1.158937\n",
      "train loss:   0.903860\n",
      "train loss:   1.071308\n",
      "train loss:   1.084570\n",
      "train loss:   1.223070\n",
      "train loss:   1.283526\n",
      "train loss:   1.183180\n",
      "train loss:   1.506149\n",
      "train loss:   1.242587\n",
      "train loss:   1.140033\n",
      "train loss:   1.196063\n",
      "train loss:   1.647077\n",
      "train loss:   1.487849\n",
      "train loss:   1.332507\n",
      "train loss:   1.126426\n",
      "train loss:   1.334290\n",
      "train loss:   1.261486\n",
      "train loss:   1.358954\n",
      "train loss:   1.029952\n",
      "train loss:   1.139757\n",
      "train loss:   1.302585\n",
      "train loss:   1.538057\n",
      "train loss:   0.957552\n",
      "train loss:   1.069720\n",
      "train loss:   1.454965\n",
      "train loss:   0.838269\n",
      "train loss:   1.188882\n",
      "train loss:   1.171372\n",
      "train loss:   1.150902\n",
      "train loss:   1.493057\n",
      "train loss:   0.738693\n",
      "train loss:   0.807514\n",
      "train loss:   1.484717\n",
      "train loss:   1.280708\n",
      "train loss:   1.160843\n",
      "train loss:   1.585290\n",
      "train loss:   1.145058\n",
      "train loss:   1.454574\n",
      "train loss:   1.124552\n",
      "train loss:   1.421516\n",
      "train loss:   1.018581\n",
      "train loss:   1.124402\n",
      "train loss:   1.537779\n",
      "train loss:   1.201817\n",
      "train loss:   1.102334\n",
      "train loss:   1.503005\n",
      "train loss:   1.143654\n",
      "train loss:   1.133976\n",
      "########### epoch 2 ###########\n",
      "########### loop 350 ###########\n",
      "test loss:   0.731162   test accuracy:   0.833333\n",
      "########### loop 350 ###########\n",
      "train loss:   1.627370\n",
      "train loss:   1.127468\n",
      "train loss:   1.039414\n",
      "train loss:   1.342547\n",
      "train loss:   1.121057\n",
      "train loss:   1.107924\n",
      "train loss:   0.983279\n",
      "train loss:   1.153068\n",
      "train loss:   1.162130\n",
      "train loss:   1.426941\n",
      "train loss:   1.223904\n",
      "train loss:   0.840915\n",
      "train loss:   1.111798\n",
      "train loss:   1.603113\n",
      "train loss:   1.201318\n",
      "train loss:   1.230554\n",
      "train loss:   1.374113\n",
      "train loss:   1.371756\n",
      "train loss:   1.477401\n",
      "train loss:   1.166037\n",
      "train loss:   0.914752\n",
      "train loss:   1.296819\n",
      "train loss:   1.371754\n",
      "train loss:   1.186673\n",
      "train loss:   1.305721\n",
      "train loss:   1.250002\n",
      "train loss:   1.097680\n",
      "train loss:   1.437827\n",
      "train loss:   1.424588\n",
      "train loss:   1.390049\n",
      "train loss:   1.318192\n",
      "train loss:   1.156165\n",
      "train loss:   1.379994\n",
      "train loss:   1.035075\n",
      "train loss:   0.976453\n",
      "train loss:   1.125591\n",
      "train loss:   1.038668\n",
      "train loss:   1.441074\n",
      "train loss:   1.470311\n",
      "train loss:   1.293358\n",
      "train loss:   1.322114\n",
      "train loss:   1.084656\n",
      "train loss:   1.327322\n",
      "train loss:   1.267098\n",
      "train loss:   1.529678\n",
      "train loss:   1.297784\n",
      "train loss:   1.211059\n",
      "train loss:   1.602552\n",
      "train loss:   1.047596\n",
      "train loss:   1.050600\n",
      "########### epoch 3 ###########\n",
      "########### loop 400 ###########\n",
      "test loss:   0.828109   test accuracy:   0.833333\n",
      "########### loop 400 ###########\n",
      "train loss:   1.007138\n",
      "train loss:   1.312752\n",
      "train loss:   1.024783\n",
      "train loss:   1.191021\n",
      "train loss:   1.180456\n",
      "train loss:   1.330612\n",
      "train loss:   1.436247\n",
      "train loss:   1.168433\n",
      "train loss:   0.766503\n",
      "train loss:   1.016033\n",
      "train loss:   1.164317\n",
      "train loss:   0.722979\n",
      "train loss:   1.168515\n",
      "train loss:   1.704071\n",
      "train loss:   1.211402\n",
      "train loss:   1.202455\n",
      "train loss:   1.438979\n",
      "train loss:   1.037411\n",
      "train loss:   0.866192\n",
      "train loss:   1.761197\n",
      "train loss:   1.370980\n",
      "train loss:   1.128040\n",
      "train loss:   1.420541\n",
      "train loss:   1.308192\n",
      "train loss:   1.240057\n",
      "train loss:   1.210663\n",
      "train loss:   1.370763\n",
      "train loss:   1.458809\n",
      "train loss:   0.998114\n",
      "train loss:   1.156195\n",
      "train loss:   1.167434\n",
      "train loss:   1.100565\n",
      "train loss:   1.100282\n",
      "train loss:   1.199630\n",
      "train loss:   1.144166\n",
      "train loss:   1.228219\n",
      "train loss:   1.552823\n",
      "train loss:   1.242890\n",
      "train loss:   0.943949\n",
      "train loss:   1.105891\n",
      "train loss:   1.193153\n",
      "train loss:   1.536093\n",
      "train loss:   1.257380\n",
      "train loss:   1.254826\n",
      "train loss:   1.151172\n",
      "train loss:   0.998424\n",
      "train loss:   0.970862\n",
      "train loss:   1.185649\n",
      "train loss:   0.977898\n",
      "train loss:   1.197911\n",
      "########### epoch 3 ###########\n",
      "########### loop 450 ###########\n",
      "test loss:   0.697606   test accuracy:   0.791667\n",
      "########### loop 450 ###########\n",
      "train loss:   1.570920\n",
      "train loss:   1.473050\n",
      "train loss:   1.514397\n",
      "train loss:   1.230075\n",
      "train loss:   1.000465\n",
      "train loss:   1.076561\n",
      "train loss:   1.324732\n",
      "train loss:   1.357627\n",
      "train loss:   1.314976\n",
      "train loss:   1.011059\n",
      "train loss:   1.325999\n",
      "train loss:   1.548347\n",
      "train loss:   1.249427\n",
      "train loss:   1.342876\n",
      "train loss:   1.159228\n",
      "train loss:   1.183719\n",
      "train loss:   1.633024\n",
      "train loss:   1.616329\n",
      "train loss:   0.759243\n",
      "train loss:   1.431506\n",
      "train loss:   1.248870\n",
      "train loss:   0.790731\n",
      "train loss:   1.287913\n",
      "train loss:   1.336673\n",
      "train loss:   1.027015\n",
      "train loss:   1.040167\n",
      "train loss:   1.344646\n",
      "train loss:   1.413792\n",
      "train loss:   1.240191\n",
      "train loss:   1.288218\n",
      "train loss:   0.915624\n",
      "train loss:   1.171581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.488551\n",
      "train loss:   1.029090\n",
      "train loss:   1.191346\n",
      "train loss:   0.893130\n",
      "train loss:   1.129353\n",
      "train loss:   1.196782\n",
      "train loss:   1.102425\n",
      "train loss:   1.467123\n",
      "train loss:   1.106547\n",
      "train loss:   1.368871\n",
      "train loss:   1.342979\n",
      "train loss:   1.243079\n",
      "train loss:   1.436270\n",
      "train loss:   0.938568\n",
      "train loss:   1.122082\n",
      "train loss:   1.026681\n",
      "train loss:   0.598622\n",
      "train loss:   1.023836\n",
      "########### epoch 3 ###########\n",
      "########### loop 500 ###########\n",
      "test loss:   0.479470   test accuracy:   0.958333\n",
      "########### loop 500 ###########\n",
      "train loss:   1.238457\n",
      "train loss:   1.310407\n",
      "train loss:   1.219466\n",
      "train loss:   0.992456\n",
      "train loss:   0.942778\n",
      "train loss:   1.274207\n",
      "train loss:   1.176272\n",
      "train loss:   1.642425\n",
      "train loss:   1.233101\n",
      "train loss:   1.443221\n",
      "train loss:   1.088242\n",
      "train loss:   1.049178\n",
      "train loss:   1.347713\n",
      "train loss:   1.145346\n",
      "train loss:   1.320932\n",
      "train loss:   1.292358\n",
      "train loss:   1.116151\n",
      "train loss:   1.119900\n",
      "train loss:   0.956854\n",
      "train loss:   0.751812\n",
      "train loss:   1.196006\n",
      "train loss:   0.776417\n",
      "train loss:   1.264891\n",
      "train loss:   1.325032\n",
      "train loss:   1.162286\n",
      "train loss:   1.401962\n",
      "train loss:   1.183577\n",
      "train loss:   1.584249\n",
      "train loss:   1.194063\n",
      "train loss:   1.434568\n",
      "train loss:   1.151090\n",
      "train loss:   1.050004\n",
      "train loss:   1.541450\n",
      "train loss:   1.223813\n",
      "train loss:   1.193814\n",
      "train loss:   0.930935\n",
      "train loss:   1.262721\n",
      "train loss:   1.398128\n",
      "train loss:   1.433205\n",
      "train loss:   1.057101\n",
      "train loss:   1.104683\n",
      "train loss:   1.315007\n",
      "train loss:   1.081716\n",
      "train loss:   1.053708\n",
      "train loss:   1.608187\n",
      "train loss:   0.999344\n",
      "train loss:   1.288292\n",
      "train loss:   1.379869\n",
      "train loss:   0.969204\n",
      "train loss:   1.479324\n",
      "########### epoch 3 ###########\n",
      "########### loop 550 ###########\n",
      "test loss:   0.862199   test accuracy:   0.791667\n",
      "########### loop 550 ###########\n",
      "train loss:   1.383415\n",
      "train loss:   1.554738\n",
      "train loss:   1.453568\n",
      "train loss:   1.100838\n",
      "train loss:   1.078162\n",
      "train loss:   1.051410\n",
      "train loss:   1.466830\n",
      "train loss:   1.304470\n",
      "train loss:   1.529650\n",
      "train loss:   1.338328\n",
      "train loss:   1.539584\n",
      "train loss:   1.065995\n",
      "train loss:   1.272382\n",
      "train loss:   1.016110\n",
      "train loss:   1.208711\n",
      "train loss:   1.099860\n",
      "train loss:   1.157080\n",
      "train loss:   1.198522\n",
      "train loss:   0.922364\n",
      "train loss:   1.167016\n",
      "train loss:   1.087292\n",
      "train loss:   1.157115\n",
      "train loss:   1.741738\n",
      "train loss:   1.045564\n",
      "train loss:   1.275491\n",
      "train loss:   0.816122\n",
      "train loss:   1.137765\n",
      "train loss:   1.095182\n",
      "train loss:   1.153135\n",
      "train loss:   1.206262\n",
      "train loss:   1.188509\n",
      "train loss:   1.156328\n",
      "train loss:   1.292778\n",
      "train loss:   0.991341\n",
      "train loss:   1.097115\n",
      "train loss:   1.121488\n",
      "train loss:   1.012988\n",
      "train loss:   1.443155\n",
      "train loss:   1.203049\n",
      "train loss:   1.081233\n",
      "train loss:   1.473034\n",
      "train loss:   0.780243\n",
      "train loss:   1.104871\n",
      "train loss:   1.535794\n",
      "train loss:   1.553450\n",
      "train loss:   1.196806\n",
      "train loss:   1.157292\n",
      "train loss:   1.052235\n",
      "train loss:   1.304924\n",
      "train loss:   0.783664\n",
      "########### epoch 4 ###########\n",
      "########### loop 600 ###########\n",
      "test loss:   0.470477   test accuracy:   1.000000\n",
      "########### loop 600 ###########\n",
      "train loss:   1.240914\n",
      "train loss:   1.281294\n",
      "train loss:   0.904865\n",
      "train loss:   1.199411\n",
      "train loss:   0.970310\n",
      "train loss:   1.465621\n",
      "train loss:   1.335948\n",
      "train loss:   1.099674\n",
      "train loss:   1.197068\n",
      "train loss:   1.251897\n",
      "train loss:   1.368726\n",
      "train loss:   1.270976\n",
      "train loss:   1.426696\n",
      "train loss:   1.339727\n",
      "train loss:   1.071833\n",
      "train loss:   1.357353\n",
      "train loss:   1.050485\n",
      "train loss:   1.117201\n",
      "train loss:   1.293522\n",
      "train loss:   1.274290\n",
      "train loss:   1.539571\n",
      "train loss:   1.670437\n",
      "train loss:   0.951197\n",
      "train loss:   0.997776\n",
      "train loss:   1.297217\n",
      "train loss:   1.147761\n",
      "train loss:   1.187267\n",
      "train loss:   1.211365\n",
      "train loss:   1.560871\n",
      "train loss:   1.042118\n",
      "train loss:   1.060699\n",
      "train loss:   1.495957\n",
      "train loss:   1.254863\n",
      "train loss:   1.368016\n",
      "train loss:   1.149603\n",
      "train loss:   1.235579\n",
      "train loss:   1.381647\n",
      "train loss:   1.159935\n",
      "train loss:   1.292952\n",
      "train loss:   1.451781\n",
      "train loss:   1.010059\n",
      "train loss:   1.346781\n",
      "train loss:   1.297544\n",
      "train loss:   0.976383\n",
      "train loss:   1.303714\n",
      "train loss:   0.869036\n",
      "train loss:   1.253812\n",
      "train loss:   0.853016\n",
      "train loss:   1.214228\n",
      "train loss:   1.173674\n",
      "########### epoch 4 ###########\n",
      "########### loop 650 ###########\n",
      "test loss:   0.463043   test accuracy:   0.916667\n",
      "########### loop 650 ###########\n",
      "train loss:   1.408215\n",
      "train loss:   1.229961\n",
      "train loss:   1.087159\n",
      "train loss:   1.549036\n",
      "train loss:   1.064114\n",
      "train loss:   1.461716\n",
      "train loss:   1.252368\n",
      "train loss:   1.204888\n",
      "train loss:   1.221550\n",
      "train loss:   1.597249\n",
      "train loss:   1.472661\n",
      "train loss:   1.383371\n",
      "train loss:   0.871715\n",
      "train loss:   1.148450\n",
      "train loss:   1.531699\n",
      "train loss:   0.785007\n",
      "train loss:   1.296887\n",
      "train loss:   1.263925\n",
      "train loss:   1.464716\n",
      "train loss:   1.285493\n",
      "train loss:   1.118239\n",
      "train loss:   1.122244\n",
      "train loss:   1.278300\n",
      "train loss:   1.381331\n",
      "train loss:   1.351276\n",
      "train loss:   0.978459\n",
      "train loss:   1.337680\n",
      "train loss:   1.273326\n",
      "train loss:   1.172161\n",
      "train loss:   1.411855\n",
      "train loss:   1.326558\n",
      "train loss:   1.131049\n",
      "train loss:   1.003190\n",
      "train loss:   1.456292\n",
      "train loss:   1.104042\n",
      "train loss:   1.564776\n",
      "train loss:   0.874366\n",
      "train loss:   1.266186\n",
      "train loss:   0.890937\n",
      "train loss:   0.950106\n",
      "train loss:   1.202839\n",
      "train loss:   0.898250\n",
      "train loss:   1.253138\n",
      "train loss:   1.076024\n",
      "train loss:   1.094878\n",
      "train loss:   1.095466\n",
      "train loss:   1.726241\n",
      "train loss:   1.151911\n",
      "train loss:   1.268224\n",
      "train loss:   1.285734\n",
      "########### epoch 4 ###########\n",
      "########### loop 700 ###########\n",
      "test loss:   0.465569   test accuracy:   0.958333\n",
      "########### loop 700 ###########\n",
      "train loss:   1.248969\n",
      "train loss:   0.921505\n",
      "train loss:   1.021489\n",
      "train loss:   0.737161\n",
      "train loss:   1.244671\n",
      "train loss:   0.484317\n",
      "train loss:   1.346753\n",
      "train loss:   0.994894\n",
      "train loss:   0.687346\n",
      "train loss:   1.324512\n",
      "train loss:   1.394922\n",
      "train loss:   1.090782\n",
      "train loss:   1.243109\n",
      "train loss:   1.343052\n",
      "train loss:   1.239085\n",
      "train loss:   1.210354\n",
      "train loss:   1.065211\n",
      "train loss:   1.109134\n",
      "train loss:   1.401885\n",
      "train loss:   1.104263\n",
      "train loss:   1.131553\n",
      "train loss:   1.330270\n",
      "train loss:   1.080998\n",
      "train loss:   1.002249\n",
      "train loss:   0.868497\n",
      "train loss:   1.319979\n",
      "train loss:   1.836236\n",
      "train loss:   1.404986\n",
      "train loss:   1.134791\n",
      "train loss:   1.459040\n",
      "train loss:   1.078876\n",
      "train loss:   1.125780\n",
      "train loss:   1.280268\n",
      "train loss:   0.814198\n",
      "train loss:   0.902467\n",
      "train loss:   1.396556\n",
      "train loss:   1.157443\n",
      "train loss:   1.497440\n",
      "train loss:   0.977267\n",
      "train loss:   1.411533\n",
      "train loss:   1.286265\n",
      "train loss:   0.973447\n",
      "train loss:   1.054062\n",
      "train loss:   1.122759\n",
      "train loss:   0.890863\n",
      "train loss:   1.753285\n",
      "train loss:   1.119627\n",
      "train loss:   1.464834\n",
      "train loss:   0.877102\n",
      "train loss:   1.065064\n",
      "########### epoch 4 ###########\n",
      "########### loop 750 ###########\n",
      "test loss:   0.724606   test accuracy:   0.875000\n",
      "########### loop 750 ###########\n",
      "train loss:   1.261058\n",
      "train loss:   1.188774\n",
      "train loss:   0.888927\n",
      "train loss:   1.204220\n",
      "train loss:   1.372912\n",
      "train loss:   1.266600\n",
      "train loss:   1.342472\n",
      "train loss:   1.164179\n",
      "train loss:   1.445883\n",
      "train loss:   0.936818\n",
      "train loss:   0.895970\n",
      "train loss:   1.049308\n",
      "train loss:   1.184842\n",
      "train loss:   1.585598\n",
      "train loss:   1.061893\n",
      "train loss:   1.231778\n",
      "train loss:   1.273099\n",
      "train loss:   1.202630\n",
      "train loss:   1.569036\n",
      "train loss:   1.356002\n",
      "train loss:   0.884992\n",
      "train loss:   1.255212\n",
      "train loss:   1.364151\n",
      "train loss:   1.516580\n",
      "train loss:   0.722253\n",
      "train loss:   1.429933\n",
      "train loss:   0.699033\n",
      "train loss:   1.017782\n",
      "train loss:   1.584619\n",
      "train loss:   1.222916\n",
      "train loss:   1.371678\n",
      "train loss:   1.060875\n",
      "train loss:   1.230369\n",
      "train loss:   0.833173\n",
      "train loss:   1.219714\n",
      "train loss:   1.438332\n",
      "train loss:   1.077367\n",
      "train loss:   1.302868\n",
      "train loss:   1.113001\n",
      "train loss:   0.975490\n",
      "train loss:   1.508813\n",
      "train loss:   1.117349\n",
      "train loss:   1.329740\n",
      "train loss:   1.419012\n",
      "train loss:   0.681829\n",
      "train loss:   1.497408\n",
      "train loss:   1.144515\n",
      "train loss:   1.123076\n",
      "train loss:   0.902590\n",
      "train loss:   1.340458\n",
      "########### epoch 5 ###########\n",
      "########### loop 800 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.746583   test accuracy:   0.791667\n",
      "########### loop 800 ###########\n",
      "train loss:   1.254417\n",
      "train loss:   1.392872\n",
      "train loss:   1.360669\n",
      "train loss:   1.431822\n",
      "train loss:   1.293302\n",
      "train loss:   0.898535\n",
      "train loss:   1.406948\n",
      "train loss:   1.133807\n",
      "train loss:   1.194562\n",
      "train loss:   1.267136\n",
      "train loss:   1.617858\n",
      "train loss:   1.131206\n",
      "train loss:   1.242165\n",
      "train loss:   1.343644\n",
      "train loss:   1.362128\n",
      "train loss:   1.413802\n",
      "train loss:   1.554616\n",
      "train loss:   1.155139\n",
      "train loss:   1.332316\n",
      "train loss:   1.303252\n",
      "train loss:   1.209154\n",
      "train loss:   1.296989\n",
      "train loss:   1.431611\n",
      "train loss:   1.232925\n",
      "train loss:   1.463853\n",
      "train loss:   0.927439\n",
      "train loss:   1.118241\n",
      "train loss:   1.349376\n",
      "train loss:   1.352282\n",
      "train loss:   0.952528\n",
      "train loss:   0.824470\n",
      "train loss:   1.405099\n",
      "train loss:   1.213409\n",
      "train loss:   0.939123\n",
      "train loss:   1.441260\n",
      "train loss:   1.257957\n",
      "train loss:   1.521539\n",
      "train loss:   1.354513\n",
      "train loss:   0.993088\n",
      "train loss:   0.911422\n",
      "train loss:   1.468669\n",
      "train loss:   1.284887\n",
      "train loss:   1.356339\n",
      "train loss:   1.027326\n",
      "train loss:   0.954295\n",
      "train loss:   1.218779\n",
      "train loss:   1.068514\n",
      "train loss:   1.440012\n",
      "train loss:   1.088174\n",
      "train loss:   1.658394\n",
      "########### epoch 5 ###########\n",
      "########### loop 850 ###########\n",
      "test loss:   0.461193   test accuracy:   0.958333\n",
      "########### loop 850 ###########\n",
      "train loss:   1.231817\n",
      "train loss:   1.099038\n",
      "train loss:   1.175945\n",
      "train loss:   0.954343\n",
      "train loss:   1.531443\n",
      "train loss:   1.020836\n",
      "train loss:   1.269158\n",
      "train loss:   1.689428\n",
      "train loss:   1.401287\n",
      "train loss:   0.779093\n",
      "train loss:   0.923652\n",
      "train loss:   1.200422\n",
      "train loss:   1.001634\n",
      "train loss:   1.194553\n",
      "train loss:   1.119923\n",
      "train loss:   1.401891\n",
      "train loss:   1.136971\n",
      "train loss:   1.034574\n",
      "train loss:   1.308888\n",
      "train loss:   1.016773\n",
      "train loss:   0.966201\n",
      "train loss:   1.269238\n",
      "train loss:   1.085824\n",
      "train loss:   0.987997\n",
      "train loss:   1.756067\n",
      "train loss:   1.126864\n",
      "train loss:   1.330153\n",
      "train loss:   1.147860\n",
      "train loss:   1.373152\n",
      "train loss:   1.352713\n",
      "train loss:   1.051711\n",
      "train loss:   1.111371\n",
      "train loss:   1.017844\n",
      "train loss:   0.730816\n",
      "train loss:   1.425073\n",
      "train loss:   1.368922\n",
      "train loss:   1.363485\n",
      "train loss:   1.338423\n",
      "train loss:   1.338975\n",
      "train loss:   1.413515\n",
      "train loss:   0.905900\n",
      "train loss:   1.060391\n",
      "train loss:   1.310992\n",
      "train loss:   0.820114\n",
      "train loss:   1.109077\n",
      "train loss:   1.106466\n",
      "train loss:   1.340634\n",
      "train loss:   1.240484\n",
      "train loss:   1.115175\n",
      "train loss:   1.484261\n",
      "########### epoch 5 ###########\n",
      "########### loop 900 ###########\n",
      "test loss:   0.649212   test accuracy:   0.875000\n",
      "########### loop 900 ###########\n",
      "train loss:   0.975703\n",
      "train loss:   0.947500\n",
      "train loss:   1.035395\n",
      "train loss:   0.926904\n",
      "train loss:   1.280775\n",
      "train loss:   1.458389\n",
      "train loss:   0.916928\n",
      "train loss:   1.419937\n",
      "train loss:   1.404940\n",
      "train loss:   1.065879\n",
      "train loss:   0.926998\n",
      "train loss:   1.221078\n",
      "train loss:   1.086361\n",
      "train loss:   1.044187\n",
      "train loss:   1.212498\n",
      "train loss:   1.436975\n",
      "train loss:   1.380287\n",
      "train loss:   0.934175\n",
      "train loss:   1.319737\n",
      "train loss:   1.784444\n",
      "train loss:   1.569040\n",
      "train loss:   1.408651\n",
      "train loss:   1.514266\n",
      "train loss:   1.079508\n",
      "train loss:   1.150815\n",
      "train loss:   0.990222\n",
      "train loss:   1.345769\n",
      "train loss:   1.247308\n",
      "train loss:   1.139652\n",
      "train loss:   0.889740\n",
      "train loss:   0.792305\n",
      "train loss:   1.306563\n",
      "train loss:   1.116825\n",
      "train loss:   1.123877\n",
      "train loss:   1.155177\n",
      "train loss:   1.315725\n",
      "train loss:   1.151038\n",
      "train loss:   1.095241\n",
      "train loss:   1.168036\n",
      "train loss:   0.932726\n",
      "train loss:   1.160431\n",
      "train loss:   1.147139\n",
      "train loss:   1.142904\n",
      "train loss:   1.317697\n",
      "train loss:   1.692674\n",
      "train loss:   1.626478\n",
      "train loss:   0.954711\n",
      "train loss:   1.083489\n",
      "train loss:   1.235047\n",
      "train loss:   1.227320\n",
      "########### epoch 6 ###########\n",
      "########### loop 950 ###########\n",
      "test loss:   0.610550   test accuracy:   0.875000\n",
      "########### loop 950 ###########\n",
      "train loss:   1.051455\n",
      "train loss:   1.503823\n",
      "train loss:   1.567495\n",
      "train loss:   1.195678\n",
      "train loss:   1.465873\n",
      "train loss:   1.235115\n",
      "train loss:   1.413209\n",
      "train loss:   1.150639\n",
      "train loss:   1.005482\n",
      "train loss:   1.314486\n",
      "train loss:   1.166454\n",
      "train loss:   0.976922\n",
      "train loss:   0.939662\n",
      "train loss:   1.023546\n",
      "train loss:   1.186960\n",
      "train loss:   1.344112\n",
      "train loss:   1.304063\n",
      "train loss:   0.873942\n",
      "train loss:   1.104783\n",
      "train loss:   1.354179\n",
      "train loss:   1.157218\n",
      "train loss:   1.342035\n",
      "train loss:   1.296580\n",
      "train loss:   1.087291\n",
      "train loss:   1.159163\n",
      "train loss:   1.280737\n",
      "train loss:   0.859008\n",
      "train loss:   0.706824\n",
      "train loss:   1.503332\n",
      "train loss:   0.942683\n",
      "train loss:   1.377516\n",
      "train loss:   1.248723\n",
      "train loss:   1.074799\n",
      "train loss:   1.255019\n",
      "train loss:   1.205432\n",
      "train loss:   1.152335\n",
      "train loss:   1.365795\n",
      "train loss:   1.199753\n",
      "train loss:   1.251813\n",
      "train loss:   1.062845\n",
      "train loss:   1.032768\n",
      "train loss:   1.263512\n",
      "train loss:   1.005385\n",
      "train loss:   0.865425\n",
      "train loss:   1.279534\n",
      "train loss:   1.228737\n",
      "train loss:   1.314140\n",
      "train loss:   1.230721\n",
      "train loss:   1.658022\n",
      "train loss:   1.302310\n",
      "########### epoch 6 ###########\n",
      "########### loop 1000 ###########\n",
      "test loss:   0.465104   test accuracy:   0.958333\n",
      "########### loop 1000 ###########\n",
      "train loss:   1.129774\n",
      "train loss:   1.403912\n",
      "train loss:   1.369855\n",
      "train loss:   1.375389\n",
      "train loss:   1.540256\n",
      "train loss:   1.197800\n",
      "train loss:   1.026677\n",
      "train loss:   1.559597\n",
      "train loss:   1.355227\n",
      "train loss:   1.249820\n",
      "train loss:   1.247745\n",
      "train loss:   1.464936\n",
      "train loss:   1.075702\n",
      "train loss:   0.681575\n",
      "train loss:   1.140974\n",
      "train loss:   1.156447\n",
      "train loss:   1.107752\n",
      "train loss:   1.023143\n",
      "train loss:   1.340621\n",
      "train loss:   1.187599\n",
      "train loss:   0.827240\n",
      "train loss:   1.281456\n",
      "train loss:   1.420550\n",
      "train loss:   1.390703\n",
      "train loss:   1.210423\n",
      "train loss:   1.130154\n",
      "train loss:   1.095188\n",
      "train loss:   1.492918\n",
      "train loss:   0.914967\n",
      "train loss:   1.183270\n",
      "train loss:   1.224285\n",
      "train loss:   1.151571\n",
      "train loss:   1.122747\n",
      "train loss:   0.874230\n",
      "train loss:   0.926150\n",
      "train loss:   0.919863\n",
      "train loss:   0.758920\n",
      "train loss:   1.003027\n",
      "train loss:   1.520291\n",
      "train loss:   1.487388\n",
      "train loss:   1.004489\n",
      "train loss:   1.513757\n",
      "train loss:   0.864677\n",
      "train loss:   0.819910\n",
      "train loss:   0.936880\n",
      "train loss:   0.533663\n",
      "train loss:   1.112871\n",
      "train loss:   1.173205\n",
      "train loss:   1.495581\n",
      "train loss:   1.114067\n",
      "########### epoch 6 ###########\n",
      "########### loop 1050 ###########\n",
      "test loss:   0.600283   test accuracy:   0.916667\n",
      "########### loop 1050 ###########\n",
      "train loss:   1.334888\n",
      "train loss:   1.238556\n",
      "train loss:   1.277161\n",
      "train loss:   0.899531\n",
      "train loss:   1.196518\n",
      "train loss:   1.334237\n",
      "train loss:   1.338552\n",
      "train loss:   1.155933\n",
      "train loss:   1.074556\n",
      "train loss:   0.989550\n",
      "train loss:   0.987230\n",
      "train loss:   1.581536\n",
      "train loss:   1.023778\n",
      "train loss:   1.438269\n",
      "train loss:   1.001709\n",
      "train loss:   1.109766\n",
      "train loss:   1.297930\n",
      "train loss:   0.888911\n",
      "train loss:   1.248726\n",
      "train loss:   1.036025\n",
      "train loss:   1.809545\n",
      "train loss:   1.480107\n",
      "train loss:   1.181356\n",
      "train loss:   1.281072\n",
      "train loss:   1.295107\n",
      "train loss:   1.081750\n",
      "train loss:   1.196267\n",
      "train loss:   0.967736\n",
      "train loss:   1.387150\n",
      "train loss:   1.163466\n",
      "train loss:   1.175518\n",
      "train loss:   1.480205\n",
      "train loss:   1.060319\n",
      "train loss:   1.182930\n",
      "train loss:   1.049186\n",
      "train loss:   1.213699\n",
      "train loss:   0.833429\n",
      "train loss:   1.210027\n",
      "train loss:   1.057481\n",
      "train loss:   1.062662\n",
      "train loss:   0.906544\n",
      "train loss:   1.208601\n",
      "train loss:   1.656060\n",
      "train loss:   1.527602\n",
      "train loss:   1.001097\n",
      "train loss:   1.075475\n",
      "train loss:   1.247068\n",
      "train loss:   1.132995\n",
      "train loss:   1.400006\n",
      "train loss:   1.324693\n",
      "########### epoch 6 ###########\n",
      "########### loop 1100 ###########\n",
      "test loss:   0.672711   test accuracy:   0.833333\n",
      "########### loop 1100 ###########\n",
      "train loss:   1.208530\n",
      "train loss:   1.405470\n",
      "train loss:   0.964708\n",
      "train loss:   0.880943\n",
      "train loss:   1.055840\n",
      "train loss:   1.273873\n",
      "train loss:   1.339683\n",
      "train loss:   1.111129\n",
      "train loss:   1.141299\n",
      "train loss:   1.100627\n",
      "train loss:   1.412083\n",
      "train loss:   1.470219\n",
      "train loss:   1.057545\n",
      "train loss:   1.136215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.865906\n",
      "train loss:   0.834156\n",
      "train loss:   1.533885\n",
      "train loss:   1.555041\n",
      "train loss:   1.067367\n",
      "train loss:   0.855136\n",
      "train loss:   1.220739\n",
      "train loss:   1.220376\n",
      "train loss:   1.160497\n",
      "train loss:   1.085449\n",
      "train loss:   1.350641\n",
      "train loss:   1.367209\n",
      "train loss:   0.941025\n",
      "train loss:   1.324860\n",
      "train loss:   1.327168\n",
      "train loss:   1.204369\n",
      "train loss:   1.247572\n",
      "train loss:   1.016339\n",
      "train loss:   1.186400\n",
      "train loss:   1.543716\n",
      "train loss:   1.270001\n",
      "train loss:   1.357722\n",
      "train loss:   1.014909\n",
      "train loss:   1.124493\n",
      "train loss:   1.185362\n",
      "train loss:   1.111366\n",
      "train loss:   1.419289\n",
      "train loss:   0.808679\n",
      "train loss:   1.152358\n",
      "train loss:   1.214495\n",
      "train loss:   1.103584\n",
      "train loss:   1.014069\n",
      "train loss:   1.307702\n",
      "train loss:   1.312172\n",
      "train loss:   1.478911\n",
      "train loss:   1.668099\n",
      "########### epoch 7 ###########\n",
      "########### loop 1150 ###########\n",
      "test loss:   0.412278   test accuracy:   0.958333\n",
      "########### loop 1150 ###########\n",
      "train loss:   1.178519\n",
      "train loss:   1.317335\n",
      "train loss:   1.420269\n",
      "train loss:   1.373850\n",
      "train loss:   1.237928\n",
      "train loss:   1.224125\n",
      "train loss:   1.009426\n",
      "train loss:   0.809620\n",
      "train loss:   1.144547\n",
      "train loss:   1.002559\n",
      "train loss:   1.173486\n",
      "train loss:   1.113919\n",
      "train loss:   1.011906\n",
      "train loss:   1.473259\n",
      "train loss:   1.490788\n",
      "train loss:   1.353558\n",
      "train loss:   1.393761\n",
      "train loss:   1.010843\n",
      "train loss:   1.159534\n",
      "train loss:   0.939246\n",
      "train loss:   1.283687\n",
      "train loss:   1.157725\n",
      "train loss:   1.167419\n",
      "train loss:   1.365116\n",
      "train loss:   1.483146\n",
      "train loss:   1.212091\n",
      "train loss:   1.312007\n",
      "train loss:   1.475044\n",
      "train loss:   0.961058\n",
      "train loss:   1.030382\n",
      "train loss:   1.075488\n",
      "train loss:   1.379511\n",
      "train loss:   0.880324\n",
      "train loss:   1.270862\n",
      "train loss:   1.587973\n",
      "train loss:   1.189778\n",
      "train loss:   1.589512\n",
      "train loss:   1.483890\n",
      "train loss:   1.027595\n",
      "train loss:   0.984368\n",
      "train loss:   1.184301\n",
      "train loss:   1.058120\n",
      "train loss:   1.516320\n",
      "train loss:   1.475356\n",
      "train loss:   0.922506\n",
      "train loss:   0.857347\n",
      "train loss:   1.184540\n",
      "train loss:   1.294687\n",
      "train loss:   1.077474\n",
      "train loss:   1.088371\n",
      "########### epoch 7 ###########\n",
      "########### loop 1200 ###########\n",
      "test loss:   0.517920   test accuracy:   0.875000\n",
      "########### loop 1200 ###########\n",
      "train loss:   0.915175\n",
      "train loss:   1.638020\n",
      "train loss:   1.184026\n",
      "train loss:   1.284396\n",
      "train loss:   0.824098\n",
      "train loss:   0.975505\n",
      "train loss:   1.016576\n",
      "train loss:   0.924844\n",
      "train loss:   1.168222\n",
      "train loss:   1.095294\n",
      "train loss:   1.223491\n",
      "train loss:   1.142349\n",
      "train loss:   0.963509\n",
      "train loss:   1.124016\n",
      "train loss:   0.976470\n",
      "train loss:   1.053442\n",
      "train loss:   1.137371\n",
      "train loss:   0.652091\n",
      "train loss:   1.590994\n",
      "train loss:   1.445944\n",
      "train loss:   1.518561\n",
      "train loss:   1.415575\n",
      "train loss:   1.018296\n",
      "train loss:   1.383079\n",
      "train loss:   1.718322\n",
      "train loss:   1.050020\n",
      "train loss:   0.857380\n",
      "train loss:   0.704586\n",
      "train loss:   1.145746\n",
      "train loss:   1.118015\n",
      "train loss:   1.081754\n",
      "train loss:   1.201305\n",
      "train loss:   0.773886\n",
      "train loss:   1.877529\n",
      "train loss:   0.910896\n",
      "train loss:   1.063172\n",
      "train loss:   1.051419\n",
      "train loss:   1.150980\n",
      "train loss:   0.779603\n",
      "train loss:   1.116695\n",
      "train loss:   1.438993\n",
      "train loss:   1.257756\n",
      "train loss:   0.760501\n",
      "train loss:   1.056654\n",
      "train loss:   1.198938\n",
      "train loss:   1.139167\n",
      "train loss:   0.978383\n",
      "train loss:   0.910866\n",
      "train loss:   1.180749\n",
      "train loss:   1.014549\n",
      "########### epoch 7 ###########\n",
      "########### loop 1250 ###########\n",
      "test loss:   0.570945   test accuracy:   0.916667\n",
      "########### loop 1250 ###########\n",
      "train loss:   1.076012\n",
      "train loss:   1.400818\n",
      "train loss:   1.110836\n",
      "train loss:   1.067980\n",
      "train loss:   1.371650\n",
      "train loss:   0.888942\n",
      "train loss:   1.052260\n",
      "train loss:   1.226371\n",
      "train loss:   1.551785\n",
      "train loss:   1.396092\n",
      "train loss:   1.012150\n",
      "train loss:   1.235860\n",
      "train loss:   0.796246\n",
      "train loss:   1.243250\n",
      "train loss:   0.916219\n",
      "train loss:   1.152657\n",
      "train loss:   1.389164\n",
      "train loss:   0.984918\n",
      "train loss:   1.052461\n",
      "train loss:   1.313150\n",
      "train loss:   1.376452\n",
      "train loss:   1.498739\n",
      "train loss:   1.317071\n",
      "train loss:   1.065142\n",
      "train loss:   1.125115\n",
      "train loss:   1.398274\n",
      "train loss:   1.103039\n",
      "train loss:   1.030335\n",
      "train loss:   1.605596\n",
      "train loss:   0.908746\n",
      "train loss:   1.405596\n",
      "train loss:   1.049612\n",
      "train loss:   1.254913\n",
      "train loss:   1.268398\n",
      "train loss:   0.872270\n",
      "train loss:   0.795397\n",
      "train loss:   1.383280\n",
      "train loss:   1.669330\n",
      "train loss:   1.196357\n",
      "train loss:   1.533658\n",
      "train loss:   1.207348\n",
      "train loss:   0.818262\n",
      "train loss:   0.815539\n",
      "train loss:   1.186197\n",
      "train loss:   0.962944\n",
      "train loss:   0.879436\n",
      "train loss:   1.162055\n",
      "train loss:   1.401135\n",
      "train loss:   1.090583\n",
      "train loss:   1.175866\n",
      "########### epoch 7 ###########\n",
      "########### loop 1300 ###########\n",
      "test loss:   0.421381   test accuracy:   0.958333\n",
      "########### loop 1300 ###########\n",
      "train loss:   0.989683\n",
      "train loss:   1.274582\n",
      "train loss:   1.324377\n",
      "train loss:   1.172268\n",
      "train loss:   1.162434\n",
      "train loss:   1.519549\n",
      "train loss:   1.180235\n",
      "train loss:   1.423885\n",
      "train loss:   1.245436\n",
      "train loss:   1.385134\n",
      "train loss:   1.136921\n",
      "train loss:   1.345589\n",
      "train loss:   1.016418\n",
      "train loss:   0.848483\n",
      "train loss:   1.134879\n",
      "train loss:   1.266816\n",
      "train loss:   1.328358\n",
      "train loss:   1.066735\n",
      "train loss:   1.106825\n",
      "train loss:   1.044252\n",
      "train loss:   1.332492\n",
      "train loss:   0.671616\n",
      "train loss:   1.307297\n",
      "train loss:   1.220029\n",
      "train loss:   0.927052\n",
      "train loss:   0.871941\n",
      "train loss:   1.332000\n",
      "train loss:   1.010703\n",
      "train loss:   0.914219\n",
      "train loss:   0.932532\n",
      "train loss:   0.878662\n",
      "train loss:   1.410829\n",
      "train loss:   1.127493\n",
      "train loss:   1.135880\n",
      "train loss:   1.581537\n",
      "train loss:   1.267891\n",
      "train loss:   1.179647\n",
      "train loss:   0.945920\n",
      "train loss:   1.012231\n",
      "train loss:   1.435991\n",
      "train loss:   1.037202\n",
      "train loss:   1.406927\n",
      "train loss:   1.182376\n",
      "train loss:   1.305708\n",
      "train loss:   1.118700\n",
      "train loss:   1.141910\n",
      "train loss:   0.877809\n",
      "train loss:   1.449269\n",
      "train loss:   1.441489\n",
      "train loss:   0.950479\n",
      "########### epoch 8 ###########\n",
      "########### loop 1350 ###########\n",
      "test loss:   0.323392   test accuracy:   1.000000\n",
      "########### loop 1350 ###########\n",
      "train loss:   1.173096\n",
      "train loss:   0.917265\n",
      "train loss:   0.745066\n",
      "train loss:   1.065075\n",
      "train loss:   1.225606\n",
      "train loss:   1.138437\n",
      "train loss:   1.362932\n",
      "train loss:   0.841147\n",
      "train loss:   1.028919\n",
      "train loss:   1.624167\n",
      "train loss:   1.333574\n",
      "train loss:   1.008547\n",
      "train loss:   1.312101\n",
      "train loss:   1.680381\n",
      "train loss:   1.137836\n",
      "train loss:   0.971508\n",
      "train loss:   0.973109\n",
      "train loss:   1.305714\n",
      "train loss:   1.193959\n",
      "train loss:   1.216064\n",
      "train loss:   1.115713\n",
      "train loss:   1.276627\n",
      "train loss:   0.988362\n",
      "train loss:   1.528858\n",
      "train loss:   0.911090\n",
      "train loss:   1.024034\n",
      "train loss:   1.448976\n",
      "train loss:   1.107599\n",
      "train loss:   1.000208\n",
      "train loss:   0.990509\n",
      "train loss:   1.141823\n",
      "train loss:   1.053693\n",
      "train loss:   1.438727\n",
      "train loss:   0.864263\n",
      "train loss:   0.976528\n",
      "train loss:   1.205122\n",
      "train loss:   1.082196\n",
      "train loss:   1.571892\n",
      "train loss:   0.933963\n",
      "train loss:   1.358736\n",
      "train loss:   1.459608\n",
      "train loss:   1.045756\n",
      "train loss:   1.071354\n",
      "train loss:   1.259271\n",
      "train loss:   1.083822\n",
      "train loss:   0.971983\n",
      "train loss:   1.202371\n",
      "train loss:   0.964702\n",
      "train loss:   1.571112\n",
      "train loss:   0.851987\n",
      "########### epoch 8 ###########\n",
      "########### loop 1400 ###########\n",
      "test loss:   0.719981   test accuracy:   0.875000\n",
      "########### loop 1400 ###########\n",
      "train loss:   1.210969\n",
      "train loss:   1.274194\n",
      "train loss:   1.047102\n",
      "train loss:   0.851026\n",
      "train loss:   1.316653\n",
      "train loss:   1.700500\n",
      "train loss:   1.171990\n",
      "train loss:   1.340950\n",
      "train loss:   1.371780\n",
      "train loss:   1.509038\n",
      "train loss:   0.907007\n",
      "train loss:   0.911338\n",
      "train loss:   1.235559\n",
      "train loss:   0.829036\n",
      "train loss:   1.065018\n",
      "train loss:   1.303303\n",
      "train loss:   0.956791\n",
      "train loss:   0.863807\n",
      "train loss:   1.105496\n",
      "train loss:   1.164836\n",
      "train loss:   1.170316\n",
      "train loss:   1.410359\n",
      "train loss:   1.094171\n",
      "train loss:   1.197990\n",
      "train loss:   1.004343\n",
      "train loss:   1.015872\n",
      "train loss:   1.201857\n",
      "train loss:   1.018342\n",
      "train loss:   1.398674\n",
      "train loss:   1.161764\n",
      "train loss:   1.228420\n",
      "train loss:   0.747935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.250367\n",
      "train loss:   1.242071\n",
      "train loss:   1.328498\n",
      "train loss:   1.062279\n",
      "train loss:   1.051570\n",
      "train loss:   1.063971\n",
      "train loss:   1.364101\n",
      "train loss:   1.172938\n",
      "train loss:   1.161680\n",
      "train loss:   0.810286\n",
      "train loss:   1.063633\n",
      "train loss:   1.203961\n",
      "train loss:   0.912811\n",
      "train loss:   1.451449\n",
      "train loss:   1.209063\n",
      "train loss:   0.799680\n",
      "train loss:   0.651325\n",
      "train loss:   1.283284\n",
      "########### epoch 8 ###########\n",
      "########### loop 1450 ###########\n",
      "test loss:   0.589697   test accuracy:   0.916667\n",
      "########### loop 1450 ###########\n",
      "train loss:   1.252832\n",
      "train loss:   1.517755\n",
      "train loss:   1.478642\n",
      "train loss:   1.191978\n",
      "train loss:   1.411323\n",
      "train loss:   0.896461\n",
      "train loss:   1.165606\n",
      "train loss:   1.240286\n",
      "train loss:   1.534172\n",
      "train loss:   0.888256\n",
      "train loss:   1.407699\n",
      "train loss:   1.352654\n",
      "train loss:   0.926100\n",
      "train loss:   1.574030\n",
      "train loss:   1.273149\n",
      "train loss:   1.050149\n",
      "train loss:   1.351521\n",
      "train loss:   0.963188\n",
      "train loss:   0.986388\n",
      "train loss:   1.073937\n",
      "train loss:   1.042763\n",
      "train loss:   1.025384\n",
      "train loss:   1.160754\n",
      "train loss:   1.332360\n",
      "train loss:   1.250114\n",
      "train loss:   1.040549\n",
      "train loss:   1.396758\n",
      "train loss:   1.183282\n",
      "train loss:   1.655623\n",
      "train loss:   1.111689\n",
      "train loss:   0.901718\n",
      "train loss:   1.060794\n",
      "train loss:   1.451836\n",
      "train loss:   1.102126\n",
      "train loss:   1.516385\n",
      "train loss:   1.062397\n",
      "train loss:   1.268137\n",
      "train loss:   0.843564\n",
      "train loss:   1.105603\n",
      "train loss:   0.900487\n",
      "train loss:   1.499087\n",
      "train loss:   1.107831\n",
      "train loss:   1.310482\n",
      "train loss:   1.114944\n",
      "train loss:   1.442639\n",
      "train loss:   1.208978\n",
      "train loss:   1.397830\n",
      "train loss:   0.901263\n",
      "train loss:   1.472507\n",
      "train loss:   0.895310\n",
      "########### epoch 8 ###########\n",
      "########### loop 1500 ###########\n",
      "test loss:   0.681262   test accuracy:   0.875000\n",
      "########### loop 1500 ###########\n",
      "train loss:   1.000953\n",
      "train loss:   1.505180\n",
      "train loss:   1.089662\n",
      "train loss:   1.191656\n",
      "train loss:   1.187484\n",
      "train loss:   1.009170\n",
      "train loss:   1.352849\n",
      "train loss:   1.259746\n",
      "train loss:   1.055808\n",
      "train loss:   1.103651\n",
      "train loss:   1.110664\n",
      "train loss:   0.734226\n",
      "train loss:   1.163817\n",
      "train loss:   1.375659\n",
      "train loss:   1.305076\n",
      "train loss:   0.879475\n",
      "train loss:   0.949785\n",
      "train loss:   1.250870\n",
      "train loss:   1.189505\n",
      "train loss:   0.994085\n",
      "train loss:   1.171213\n",
      "train loss:   1.272426\n",
      "train loss:   1.350716\n",
      "train loss:   1.205287\n",
      "train loss:   1.527587\n",
      "train loss:   1.301225\n",
      "train loss:   1.223226\n",
      "train loss:   1.157822\n",
      "train loss:   1.101110\n",
      "train loss:   1.038114\n",
      "train loss:   1.134400\n",
      "train loss:   1.422129\n",
      "train loss:   1.334755\n",
      "train loss:   1.274326\n",
      "train loss:   1.310771\n",
      "train loss:   0.879410\n",
      "train loss:   0.958609\n",
      "train loss:   1.286014\n",
      "train loss:   1.100198\n",
      "train loss:   1.273298\n",
      "train loss:   0.876469\n",
      "train loss:   0.853836\n",
      "train loss:   0.866227\n",
      "train loss:   1.320695\n",
      "train loss:   1.237189\n",
      "train loss:   1.092007\n",
      "train loss:   0.708949\n",
      "train loss:   1.432347\n",
      "train loss:   1.089111\n",
      "train loss:   0.752319\n",
      "########### epoch 9 ###########\n",
      "########### loop 1550 ###########\n",
      "test loss:   0.498205   test accuracy:   0.958333\n",
      "########### loop 1550 ###########\n",
      "train loss:   1.570239\n",
      "train loss:   1.526292\n",
      "train loss:   1.267284\n",
      "train loss:   0.765664\n",
      "train loss:   0.977808\n",
      "train loss:   1.179854\n",
      "train loss:   0.853142\n",
      "train loss:   1.208625\n",
      "train loss:   1.219636\n",
      "train loss:   1.248524\n",
      "train loss:   1.186718\n",
      "train loss:   1.579858\n",
      "train loss:   0.787418\n",
      "train loss:   0.753675\n",
      "train loss:   0.882535\n",
      "train loss:   1.303838\n",
      "train loss:   1.245022\n",
      "train loss:   1.324172\n",
      "train loss:   1.292321\n",
      "train loss:   0.906658\n",
      "train loss:   0.854711\n",
      "train loss:   1.053303\n",
      "train loss:   1.509003\n",
      "train loss:   0.966730\n",
      "train loss:   1.425959\n",
      "train loss:   1.423661\n",
      "train loss:   1.457457\n",
      "train loss:   1.031388\n",
      "train loss:   1.381621\n",
      "train loss:   0.950561\n",
      "train loss:   0.816890\n",
      "train loss:   0.986836\n",
      "train loss:   1.233381\n",
      "train loss:   1.238522\n",
      "train loss:   1.191855\n",
      "train loss:   1.136500\n",
      "train loss:   0.969668\n",
      "train loss:   1.110927\n",
      "train loss:   1.499673\n",
      "train loss:   1.425435\n",
      "train loss:   0.845984\n",
      "train loss:   1.051767\n",
      "train loss:   0.862360\n",
      "train loss:   1.170464\n",
      "train loss:   1.478814\n",
      "train loss:   1.069620\n",
      "train loss:   0.915596\n",
      "train loss:   1.046509\n",
      "train loss:   1.211734\n",
      "train loss:   1.278412\n",
      "########### epoch 9 ###########\n",
      "########### loop 1600 ###########\n",
      "test loss:   0.551860   test accuracy:   0.875000\n",
      "########### loop 1600 ###########\n",
      "train loss:   1.166448\n",
      "train loss:   1.199000\n",
      "train loss:   1.114159\n",
      "train loss:   1.229750\n",
      "train loss:   1.240790\n",
      "train loss:   1.388428\n",
      "train loss:   1.195544\n",
      "train loss:   1.328860\n",
      "train loss:   1.188201\n",
      "train loss:   1.243011\n",
      "train loss:   0.839343\n",
      "train loss:   0.895122\n",
      "train loss:   0.908856\n",
      "train loss:   0.784035\n",
      "train loss:   1.176536\n",
      "train loss:   1.190852\n",
      "train loss:   1.079406\n",
      "train loss:   1.083448\n",
      "train loss:   0.910743\n",
      "train loss:   1.278779\n",
      "train loss:   1.270574\n",
      "train loss:   1.044363\n",
      "train loss:   1.132840\n",
      "train loss:   1.128446\n",
      "train loss:   1.259097\n",
      "train loss:   1.228000\n",
      "train loss:   0.982022\n",
      "train loss:   1.036205\n",
      "train loss:   0.954084\n",
      "train loss:   0.809761\n",
      "train loss:   1.071437\n",
      "train loss:   1.050005\n",
      "train loss:   0.810475\n",
      "train loss:   0.667432\n",
      "train loss:   1.219883\n",
      "train loss:   0.930651\n",
      "train loss:   1.211803\n",
      "train loss:   1.374909\n",
      "train loss:   1.127628\n",
      "train loss:   1.030070\n",
      "train loss:   1.013758\n",
      "train loss:   1.174675\n",
      "train loss:   1.040039\n",
      "train loss:   0.899484\n",
      "train loss:   1.070969\n",
      "train loss:   1.375465\n",
      "train loss:   1.134608\n",
      "train loss:   0.962810\n",
      "train loss:   1.144271\n",
      "train loss:   1.318238\n",
      "########### epoch 9 ###########\n",
      "########### loop 1650 ###########\n",
      "test loss:   0.342315   test accuracy:   0.958333\n",
      "########### loop 1650 ###########\n",
      "train loss:   0.532735\n",
      "train loss:   0.914200\n",
      "train loss:   1.231521\n",
      "train loss:   1.096938\n",
      "train loss:   1.431026\n",
      "train loss:   1.111580\n",
      "train loss:   1.116854\n",
      "train loss:   0.886796\n",
      "train loss:   0.848824\n",
      "train loss:   0.926609\n",
      "train loss:   1.101253\n",
      "train loss:   1.337122\n",
      "train loss:   0.944352\n",
      "train loss:   1.370803\n",
      "train loss:   1.119088\n",
      "train loss:   1.381014\n",
      "train loss:   1.198768\n",
      "train loss:   1.215706\n",
      "train loss:   1.205815\n",
      "train loss:   1.099801\n",
      "train loss:   0.695004\n",
      "train loss:   1.433443\n",
      "train loss:   1.303147\n",
      "train loss:   1.272211\n",
      "train loss:   1.355046\n",
      "train loss:   1.476544\n",
      "train loss:   1.055629\n",
      "train loss:   1.104299\n",
      "train loss:   0.821999\n",
      "train loss:   1.120315\n",
      "train loss:   1.304427\n",
      "train loss:   1.193922\n",
      "train loss:   1.290812\n",
      "train loss:   1.130322\n",
      "train loss:   1.064163\n",
      "train loss:   1.090739\n",
      "train loss:   1.254821\n",
      "train loss:   0.837915\n",
      "train loss:   1.047235\n",
      "train loss:   1.694161\n",
      "train loss:   1.204414\n",
      "train loss:   0.807977\n",
      "train loss:   0.912228\n",
      "train loss:   0.949606\n",
      "train loss:   1.088906\n",
      "train loss:   1.441437\n",
      "train loss:   1.344465\n",
      "train loss:   1.301628\n",
      "train loss:   1.191329\n",
      "train loss:   1.116416\n",
      "########### epoch 10 ###########\n",
      "########### loop 1700 ###########\n",
      "test loss:   0.513955   test accuracy:   0.791667\n",
      "########### loop 1700 ###########\n",
      "train loss:   0.707599\n",
      "train loss:   1.428653\n",
      "train loss:   1.347057\n",
      "train loss:   1.187574\n",
      "train loss:   1.393269\n",
      "train loss:   1.385023\n",
      "train loss:   1.526221\n",
      "train loss:   1.070059\n",
      "train loss:   1.243899\n",
      "train loss:   0.907894\n",
      "train loss:   1.584718\n",
      "train loss:   1.037269\n",
      "train loss:   0.873530\n",
      "train loss:   1.213500\n",
      "train loss:   1.238501\n",
      "train loss:   1.190556\n",
      "train loss:   0.990656\n",
      "train loss:   1.368295\n",
      "train loss:   1.572149\n",
      "train loss:   1.135671\n",
      "train loss:   1.219612\n",
      "train loss:   1.114251\n",
      "train loss:   1.365956\n",
      "train loss:   0.963173\n",
      "train loss:   1.370418\n",
      "train loss:   1.065348\n",
      "train loss:   1.057614\n",
      "train loss:   1.127724\n",
      "train loss:   1.273385\n",
      "train loss:   1.304927\n",
      "train loss:   0.971123\n",
      "train loss:   1.234558\n",
      "train loss:   1.101112\n",
      "train loss:   1.425866\n",
      "train loss:   1.086853\n",
      "train loss:   1.486347\n",
      "train loss:   0.993210\n",
      "train loss:   1.057262\n",
      "train loss:   0.956738\n",
      "train loss:   1.416157\n",
      "train loss:   0.960544\n",
      "train loss:   1.027929\n",
      "train loss:   0.945740\n",
      "train loss:   1.039435\n",
      "train loss:   1.221298\n",
      "train loss:   1.443854\n",
      "train loss:   1.167671\n",
      "train loss:   1.335294\n",
      "train loss:   1.394306\n",
      "train loss:   1.253064\n",
      "########### epoch 10 ###########\n",
      "########### loop 1750 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.452129   test accuracy:   0.916667\n",
      "########### loop 1750 ###########\n",
      "train loss:   0.866192\n",
      "train loss:   1.556743\n",
      "train loss:   1.065940\n",
      "train loss:   0.929031\n",
      "train loss:   1.010533\n",
      "train loss:   1.222757\n",
      "train loss:   0.901911\n",
      "train loss:   1.006108\n",
      "train loss:   0.886089\n",
      "train loss:   0.976965\n",
      "train loss:   1.179294\n",
      "train loss:   1.037935\n",
      "train loss:   1.277733\n",
      "train loss:   0.748164\n",
      "train loss:   0.994815\n",
      "train loss:   1.032472\n",
      "train loss:   1.153292\n",
      "train loss:   0.983968\n",
      "train loss:   1.067496\n",
      "train loss:   1.025733\n",
      "train loss:   1.098965\n",
      "train loss:   1.165089\n",
      "train loss:   1.112933\n",
      "train loss:   1.520650\n",
      "train loss:   1.244308\n",
      "train loss:   1.164880\n",
      "train loss:   1.374586\n",
      "train loss:   1.181934\n",
      "train loss:   1.375635\n",
      "train loss:   1.148132\n",
      "train loss:   0.927473\n",
      "train loss:   1.270297\n",
      "train loss:   1.048817\n",
      "train loss:   1.285951\n",
      "train loss:   0.698668\n",
      "train loss:   1.103110\n",
      "train loss:   1.141493\n",
      "train loss:   1.114195\n",
      "train loss:   1.267586\n",
      "train loss:   1.131395\n",
      "train loss:   0.999867\n",
      "train loss:   1.446257\n",
      "train loss:   1.251530\n",
      "train loss:   1.290292\n",
      "train loss:   0.964676\n",
      "train loss:   1.151354\n",
      "train loss:   1.215025\n",
      "train loss:   1.004442\n",
      "train loss:   1.365548\n",
      "train loss:   1.005281\n",
      "########### epoch 10 ###########\n",
      "########### loop 1800 ###########\n",
      "test loss:   0.641863   test accuracy:   0.875000\n",
      "########### loop 1800 ###########\n",
      "train loss:   1.314363\n",
      "train loss:   1.317253\n",
      "train loss:   1.205230\n",
      "train loss:   0.992117\n",
      "train loss:   0.872897\n",
      "train loss:   1.385389\n",
      "train loss:   1.525669\n",
      "train loss:   1.672237\n",
      "train loss:   1.174167\n",
      "train loss:   1.117404\n",
      "train loss:   0.928836\n",
      "train loss:   1.060220\n",
      "train loss:   1.332246\n",
      "train loss:   0.856898\n",
      "train loss:   0.902000\n",
      "train loss:   1.195470\n",
      "train loss:   1.032245\n",
      "train loss:   1.039361\n",
      "train loss:   1.303047\n",
      "train loss:   0.862142\n",
      "train loss:   1.137636\n",
      "train loss:   1.333143\n",
      "train loss:   1.110984\n",
      "train loss:   1.620229\n",
      "train loss:   0.776371\n",
      "train loss:   1.203965\n",
      "train loss:   1.333914\n",
      "train loss:   1.364343\n",
      "train loss:   1.092716\n",
      "train loss:   1.315556\n",
      "train loss:   1.136685\n",
      "train loss:   1.118308\n",
      "train loss:   0.890822\n",
      "train loss:   1.374880\n",
      "train loss:   1.179015\n",
      "train loss:   1.115213\n",
      "train loss:   1.156669\n",
      "train loss:   1.107930\n",
      "train loss:   1.113016\n",
      "train loss:   1.016155\n",
      "train loss:   1.295216\n",
      "train loss:   1.395264\n",
      "train loss:   1.234424\n",
      "train loss:   1.127911\n",
      "train loss:   0.944019\n",
      "train loss:   0.862121\n",
      "train loss:   0.869571\n",
      "train loss:   1.384983\n",
      "train loss:   1.266919\n",
      "train loss:   1.104929\n",
      "########### epoch 10 ###########\n",
      "########### loop 1850 ###########\n",
      "test loss:   0.552045   test accuracy:   0.875000\n",
      "########### loop 1850 ###########\n",
      "train loss:   0.925354\n",
      "train loss:   0.959476\n",
      "train loss:   1.046978\n",
      "train loss:   1.222489\n",
      "train loss:   1.017480\n",
      "train loss:   0.950820\n",
      "train loss:   0.851957\n",
      "train loss:   1.128218\n",
      "train loss:   0.983434\n",
      "train loss:   1.289941\n",
      "train loss:   1.431603\n",
      "train loss:   1.082794\n",
      "train loss:   1.181247\n",
      "train loss:   1.321083\n",
      "train loss:   0.819450\n",
      "train loss:   1.043917\n",
      "train loss:   0.854117\n",
      "train loss:   1.524009\n",
      "train loss:   1.241505\n",
      "train loss:   1.230452\n",
      "train loss:   1.295745\n",
      "train loss:   1.176557\n",
      "train loss:   1.188436\n",
      "train loss:   1.244828\n",
      "train loss:   1.443013\n",
      "train loss:   0.873172\n",
      "train loss:   1.376272\n",
      "train loss:   0.994485\n",
      "train loss:   1.292711\n",
      "train loss:   1.156158\n",
      "train loss:   1.224478\n",
      "train loss:   1.039980\n",
      "train loss:   0.883105\n",
      "train loss:   1.357964\n",
      "train loss:   1.100163\n",
      "train loss:   0.927629\n",
      "train loss:   1.044267\n",
      "train loss:   1.482729\n",
      "train loss:   1.068964\n",
      "train loss:   0.983571\n",
      "train loss:   1.211855\n",
      "train loss:   1.283510\n",
      "train loss:   1.452241\n",
      "train loss:   1.130731\n",
      "train loss:   1.449802\n",
      "train loss:   1.039157\n",
      "train loss:   1.053730\n",
      "train loss:   1.181470\n",
      "train loss:   0.961630\n",
      "train loss:   1.331322\n",
      "########### epoch 11 ###########\n",
      "########### loop 1900 ###########\n",
      "test loss:   0.271730   test accuracy:   0.958333\n",
      "########### loop 1900 ###########\n",
      "train loss:   0.754854\n",
      "train loss:   1.046312\n",
      "train loss:   0.911365\n",
      "train loss:   1.101976\n",
      "train loss:   1.300858\n",
      "train loss:   1.340374\n",
      "train loss:   1.310804\n",
      "train loss:   1.285473\n",
      "train loss:   1.034948\n",
      "train loss:   1.043963\n",
      "train loss:   1.083026\n",
      "train loss:   1.295738\n",
      "train loss:   1.048192\n",
      "train loss:   1.330487\n",
      "train loss:   1.742375\n",
      "train loss:   1.134424\n",
      "train loss:   1.074005\n",
      "train loss:   1.552972\n",
      "train loss:   0.872791\n",
      "train loss:   1.473925\n",
      "train loss:   1.315110\n",
      "train loss:   0.814017\n",
      "train loss:   1.060560\n",
      "train loss:   0.922691\n",
      "train loss:   1.460785\n",
      "train loss:   1.313285\n",
      "train loss:   1.091063\n",
      "train loss:   1.369365\n",
      "train loss:   1.748205\n",
      "train loss:   0.962218\n",
      "train loss:   1.300611\n",
      "train loss:   0.997058\n",
      "train loss:   1.200763\n",
      "train loss:   1.209833\n",
      "train loss:   1.212502\n",
      "train loss:   0.909998\n",
      "train loss:   1.327935\n",
      "train loss:   0.905100\n",
      "train loss:   1.163391\n",
      "train loss:   0.981974\n",
      "train loss:   1.070835\n",
      "train loss:   1.050566\n",
      "train loss:   1.132602\n",
      "train loss:   0.960864\n",
      "train loss:   0.886560\n",
      "train loss:   0.891726\n",
      "train loss:   0.969177\n",
      "train loss:   1.204738\n",
      "train loss:   1.056787\n",
      "train loss:   1.136917\n",
      "########### epoch 11 ###########\n",
      "########### loop 1950 ###########\n",
      "test loss:   0.410338   test accuracy:   0.958333\n",
      "########### loop 1950 ###########\n",
      "train loss:   1.228554\n",
      "train loss:   0.865702\n",
      "train loss:   0.959238\n",
      "train loss:   0.976708\n",
      "train loss:   0.856217\n",
      "train loss:   1.126065\n",
      "train loss:   1.081218\n",
      "train loss:   1.262553\n",
      "train loss:   1.161965\n",
      "train loss:   0.926155\n",
      "train loss:   0.764675\n",
      "train loss:   0.611148\n",
      "train loss:   1.400885\n",
      "train loss:   1.239801\n",
      "train loss:   0.936003\n",
      "train loss:   1.258229\n",
      "train loss:   0.875112\n",
      "train loss:   1.069423\n",
      "train loss:   0.916602\n",
      "train loss:   0.717989\n",
      "train loss:   0.847765\n",
      "train loss:   0.938641\n",
      "train loss:   1.272514\n",
      "train loss:   1.255574\n",
      "train loss:   1.036841\n",
      "train loss:   1.482428\n",
      "train loss:   1.322530\n",
      "train loss:   1.155657\n",
      "train loss:   1.377155\n",
      "train loss:   1.449459\n",
      "train loss:   1.067466\n",
      "train loss:   1.231470\n",
      "train loss:   1.148009\n",
      "train loss:   1.422080\n",
      "train loss:   1.130654\n",
      "train loss:   1.236025\n",
      "train loss:   1.083162\n",
      "train loss:   1.262949\n",
      "train loss:   1.105945\n",
      "train loss:   1.338494\n",
      "train loss:   1.380374\n",
      "train loss:   0.958537\n",
      "train loss:   0.942657\n",
      "train loss:   1.113893\n",
      "train loss:   1.306425\n",
      "train loss:   0.862085\n",
      "train loss:   1.098373\n",
      "train loss:   1.019110\n",
      "train loss:   1.226439\n",
      "train loss:   1.266856\n",
      "########### epoch 11 ###########\n",
      "########### loop 2000 ###########\n",
      "test loss:   0.399879   test accuracy:   0.916667\n",
      "########### loop 2000 ###########\n",
      "train loss:   1.231973\n",
      "train loss:   1.005365\n",
      "train loss:   1.224032\n",
      "train loss:   1.489571\n",
      "train loss:   0.880004\n",
      "train loss:   0.967600\n",
      "train loss:   1.029601\n",
      "train loss:   1.070982\n",
      "train loss:   1.127659\n",
      "train loss:   1.155349\n",
      "train loss:   0.942856\n",
      "train loss:   1.246699\n",
      "train loss:   1.158462\n",
      "train loss:   1.340967\n",
      "train loss:   1.220526\n",
      "train loss:   1.131081\n",
      "train loss:   0.910083\n",
      "train loss:   1.217591\n",
      "train loss:   1.187533\n",
      "train loss:   1.258498\n",
      "train loss:   0.989582\n",
      "train loss:   1.290029\n",
      "train loss:   1.431286\n",
      "train loss:   1.468373\n",
      "train loss:   0.971552\n",
      "train loss:   0.628446\n",
      "train loss:   1.049952\n",
      "train loss:   1.142048\n",
      "train loss:   1.126661\n",
      "train loss:   0.941210\n",
      "train loss:   1.196570\n",
      "train loss:   1.663924\n",
      "train loss:   0.903705\n",
      "train loss:   1.519276\n",
      "train loss:   1.359393\n",
      "train loss:   1.225313\n",
      "train loss:   0.835798\n",
      "train loss:   1.102869\n",
      "train loss:   1.347443\n",
      "train loss:   0.985637\n",
      "train loss:   0.931703\n",
      "train loss:   1.265538\n",
      "train loss:   1.238297\n",
      "train loss:   0.852381\n",
      "train loss:   1.037380\n",
      "train loss:   1.061895\n",
      "train loss:   1.131460\n",
      "train loss:   0.887333\n",
      "train loss:   1.137150\n",
      "train loss:   0.910990\n",
      "########### epoch 11 ###########\n",
      "########### loop 2050 ###########\n",
      "test loss:   0.578893   test accuracy:   0.958333\n",
      "########### loop 2050 ###########\n",
      "train loss:   0.991612\n",
      "train loss:   1.179783\n",
      "train loss:   1.240500\n",
      "train loss:   1.310178\n",
      "train loss:   0.749278\n",
      "train loss:   1.085827\n",
      "train loss:   1.262640\n",
      "train loss:   1.197910\n",
      "train loss:   1.572873\n",
      "train loss:   1.194965\n",
      "train loss:   1.002785\n",
      "train loss:   0.989752\n",
      "train loss:   1.260176\n",
      "train loss:   1.181090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.150254\n",
      "train loss:   1.302884\n",
      "train loss:   1.111841\n",
      "train loss:   1.278872\n",
      "train loss:   1.438495\n",
      "train loss:   1.453103\n",
      "train loss:   1.086457\n",
      "train loss:   0.985949\n",
      "train loss:   1.137451\n",
      "train loss:   1.111045\n",
      "train loss:   1.064210\n",
      "train loss:   0.864727\n",
      "train loss:   1.077776\n",
      "train loss:   1.479899\n",
      "train loss:   1.253905\n",
      "train loss:   1.312574\n",
      "train loss:   1.196788\n",
      "train loss:   1.307474\n",
      "train loss:   1.085701\n",
      "train loss:   1.439715\n",
      "train loss:   0.958189\n",
      "train loss:   1.272014\n",
      "train loss:   1.096970\n",
      "train loss:   0.797487\n",
      "train loss:   1.458997\n",
      "train loss:   0.699663\n",
      "train loss:   1.101739\n",
      "train loss:   1.365334\n",
      "train loss:   1.128123\n",
      "train loss:   0.942627\n",
      "train loss:   1.033941\n",
      "train loss:   1.303018\n",
      "train loss:   0.990177\n",
      "train loss:   0.876656\n",
      "train loss:   1.225766\n",
      "train loss:   0.976072\n",
      "########### epoch 12 ###########\n",
      "########### loop 2100 ###########\n",
      "test loss:   0.646264   test accuracy:   0.833333\n",
      "########### loop 2100 ###########\n",
      "train loss:   1.026881\n",
      "train loss:   1.108265\n",
      "train loss:   1.247206\n",
      "train loss:   0.642908\n",
      "train loss:   1.096582\n",
      "train loss:   0.842662\n",
      "train loss:   1.308065\n",
      "train loss:   1.057182\n",
      "train loss:   1.064128\n",
      "train loss:   1.240549\n",
      "train loss:   1.544872\n",
      "train loss:   0.901219\n",
      "train loss:   1.494814\n",
      "train loss:   0.842116\n",
      "train loss:   1.075847\n",
      "train loss:   0.986256\n",
      "train loss:   1.021473\n",
      "train loss:   1.154324\n",
      "train loss:   0.941528\n",
      "train loss:   0.847175\n",
      "train loss:   1.235514\n",
      "train loss:   1.204057\n",
      "train loss:   1.098559\n",
      "train loss:   0.831297\n",
      "train loss:   1.106761\n",
      "train loss:   1.241132\n",
      "train loss:   1.103709\n",
      "train loss:   1.641931\n",
      "train loss:   1.312436\n",
      "train loss:   1.034361\n",
      "train loss:   1.166390\n",
      "train loss:   0.981544\n",
      "train loss:   1.290852\n",
      "train loss:   1.204867\n",
      "train loss:   1.649424\n",
      "train loss:   0.851720\n",
      "train loss:   1.297117\n",
      "train loss:   1.270521\n",
      "train loss:   0.591426\n",
      "train loss:   1.287432\n",
      "train loss:   0.915304\n",
      "train loss:   1.185875\n",
      "train loss:   1.133753\n",
      "train loss:   1.157555\n",
      "train loss:   1.039127\n",
      "train loss:   0.850783\n",
      "train loss:   1.119353\n",
      "train loss:   1.020275\n",
      "train loss:   1.128711\n",
      "train loss:   1.352420\n",
      "########### epoch 12 ###########\n",
      "########### loop 2150 ###########\n",
      "test loss:   0.323495   test accuracy:   1.000000\n",
      "########### loop 2150 ###########\n",
      "train loss:   1.527285\n",
      "train loss:   1.077175\n",
      "train loss:   1.203217\n",
      "train loss:   0.910421\n",
      "train loss:   1.399785\n",
      "train loss:   1.125000\n",
      "train loss:   0.889180\n",
      "train loss:   1.455869\n",
      "train loss:   0.895247\n",
      "train loss:   1.362425\n",
      "train loss:   1.428355\n",
      "train loss:   0.819260\n",
      "train loss:   1.213286\n",
      "train loss:   1.250115\n",
      "train loss:   1.221183\n",
      "train loss:   1.071447\n",
      "train loss:   1.365733\n",
      "train loss:   1.186732\n",
      "train loss:   0.988456\n",
      "train loss:   1.183971\n",
      "train loss:   1.099287\n",
      "train loss:   1.256038\n",
      "train loss:   1.229995\n",
      "train loss:   1.211579\n",
      "train loss:   1.280285\n",
      "train loss:   1.138545\n",
      "train loss:   1.326877\n",
      "train loss:   0.949470\n",
      "train loss:   1.266251\n",
      "train loss:   1.045865\n",
      "train loss:   1.073553\n",
      "train loss:   1.229204\n",
      "train loss:   1.188041\n",
      "train loss:   1.036895\n",
      "train loss:   0.726506\n",
      "train loss:   0.805469\n",
      "train loss:   0.982258\n",
      "train loss:   1.245788\n",
      "train loss:   1.236711\n",
      "train loss:   1.561976\n",
      "train loss:   1.175400\n",
      "train loss:   1.161467\n",
      "train loss:   1.157502\n",
      "train loss:   0.913743\n",
      "train loss:   1.157961\n",
      "train loss:   1.053913\n",
      "train loss:   0.886578\n",
      "train loss:   1.006107\n",
      "train loss:   1.457246\n",
      "train loss:   0.972279\n",
      "########### epoch 12 ###########\n",
      "########### loop 2200 ###########\n",
      "test loss:   0.644673   test accuracy:   0.791667\n",
      "########### loop 2200 ###########\n",
      "train loss:   0.882523\n",
      "train loss:   1.222897\n",
      "train loss:   1.382074\n",
      "train loss:   1.098387\n",
      "train loss:   1.154925\n",
      "train loss:   0.680147\n",
      "train loss:   1.067743\n",
      "train loss:   1.394670\n",
      "train loss:   0.942808\n",
      "train loss:   1.116382\n",
      "train loss:   1.108095\n",
      "train loss:   1.325219\n",
      "train loss:   0.921914\n",
      "train loss:   0.469249\n",
      "train loss:   1.448485\n",
      "train loss:   0.799812\n",
      "train loss:   1.441897\n",
      "train loss:   1.244462\n",
      "train loss:   0.830308\n",
      "train loss:   1.314854\n",
      "train loss:   1.040966\n",
      "train loss:   1.166873\n",
      "train loss:   1.128291\n",
      "train loss:   0.938280\n",
      "train loss:   1.304173\n",
      "train loss:   1.263406\n",
      "train loss:   1.687540\n",
      "train loss:   1.315785\n",
      "train loss:   1.431656\n",
      "train loss:   1.093459\n",
      "train loss:   1.065782\n",
      "train loss:   1.163644\n",
      "train loss:   1.357419\n",
      "train loss:   1.240541\n",
      "train loss:   0.959077\n",
      "train loss:   1.302064\n",
      "train loss:   0.633725\n",
      "train loss:   0.920972\n",
      "train loss:   1.228001\n",
      "train loss:   1.338515\n",
      "train loss:   1.088032\n",
      "train loss:   1.448406\n",
      "train loss:   0.893467\n",
      "train loss:   1.203089\n",
      "train loss:   0.985142\n",
      "train loss:   1.036965\n",
      "train loss:   1.183160\n",
      "train loss:   1.224203\n",
      "train loss:   0.855843\n",
      "train loss:   1.481473\n",
      "########### epoch 12 ###########\n",
      "########### loop 2250 ###########\n",
      "test loss:   0.374606   test accuracy:   1.000000\n",
      "########### loop 2250 ###########\n",
      "train loss:   1.143939\n",
      "train loss:   1.400373\n",
      "train loss:   1.179704\n",
      "train loss:   1.191341\n",
      "train loss:   0.645493\n",
      "train loss:   1.294998\n",
      "train loss:   0.555716\n",
      "train loss:   1.496508\n",
      "train loss:   1.017317\n",
      "train loss:   0.834158\n",
      "train loss:   1.268155\n",
      "train loss:   1.437239\n",
      "train loss:   1.190479\n",
      "train loss:   1.234287\n",
      "train loss:   1.104691\n",
      "train loss:   0.982669\n",
      "train loss:   1.204428\n",
      "train loss:   0.889045\n",
      "train loss:   1.412719\n",
      "train loss:   1.251804\n",
      "train loss:   1.099632\n",
      "train loss:   1.036587\n",
      "train loss:   1.101846\n",
      "train loss:   0.957970\n",
      "train loss:   0.899470\n",
      "train loss:   1.179750\n",
      "train loss:   0.778495\n",
      "train loss:   1.139348\n",
      "train loss:   1.391728\n",
      "train loss:   1.305770\n",
      "train loss:   1.166411\n",
      "train loss:   0.929460\n",
      "train loss:   1.113144\n",
      "train loss:   1.086761\n",
      "train loss:   0.890830\n",
      "train loss:   0.950747\n",
      "train loss:   1.089829\n",
      "train loss:   0.905662\n",
      "train loss:   1.363764\n",
      "train loss:   1.067582\n",
      "train loss:   1.214882\n",
      "train loss:   1.087159\n",
      "train loss:   1.315233\n",
      "train loss:   1.539050\n",
      "train loss:   1.031714\n",
      "train loss:   1.040363\n",
      "train loss:   1.051657\n",
      "train loss:   1.060499\n",
      "train loss:   0.686066\n",
      "train loss:   1.631547\n",
      "########### epoch 13 ###########\n",
      "########### loop 2300 ###########\n",
      "test loss:   0.388526   test accuracy:   1.000000\n",
      "########### loop 2300 ###########\n",
      "train loss:   0.979811\n",
      "train loss:   1.419453\n",
      "train loss:   1.492222\n",
      "train loss:   0.938882\n",
      "train loss:   1.401927\n",
      "train loss:   0.989351\n",
      "train loss:   1.118077\n",
      "train loss:   1.054963\n",
      "train loss:   0.966244\n",
      "train loss:   0.938663\n",
      "train loss:   1.372624\n",
      "train loss:   1.093314\n",
      "train loss:   1.100667\n",
      "train loss:   1.219676\n",
      "train loss:   1.300019\n",
      "train loss:   1.081579\n",
      "train loss:   1.062657\n",
      "train loss:   1.044553\n",
      "train loss:   0.885171\n",
      "train loss:   0.651471\n",
      "train loss:   1.049141\n",
      "train loss:   1.051930\n",
      "train loss:   1.436213\n",
      "train loss:   0.903937\n",
      "train loss:   1.191582\n",
      "train loss:   1.059072\n",
      "train loss:   1.124447\n",
      "train loss:   1.330894\n",
      "train loss:   1.140463\n",
      "train loss:   1.235356\n",
      "train loss:   0.991435\n",
      "train loss:   0.888341\n",
      "train loss:   0.902617\n",
      "train loss:   0.603552\n",
      "train loss:   0.882998\n",
      "train loss:   1.156109\n",
      "train loss:   1.161787\n",
      "train loss:   0.663194\n",
      "train loss:   1.062992\n",
      "train loss:   1.099758\n",
      "train loss:   1.355071\n",
      "train loss:   1.077941\n",
      "train loss:   1.310069\n",
      "train loss:   1.430591\n",
      "train loss:   1.233462\n",
      "train loss:   1.049011\n",
      "train loss:   1.320584\n",
      "train loss:   1.048857\n",
      "train loss:   1.289886\n",
      "train loss:   1.388166\n",
      "########### epoch 13 ###########\n",
      "########### loop 2350 ###########\n",
      "test loss:   0.368417   test accuracy:   1.000000\n",
      "########### loop 2350 ###########\n",
      "train loss:   1.152924\n",
      "train loss:   1.376067\n",
      "train loss:   1.202709\n",
      "train loss:   0.696053\n",
      "train loss:   1.278590\n",
      "train loss:   1.193316\n",
      "train loss:   1.124888\n",
      "train loss:   1.096265\n",
      "train loss:   1.071149\n",
      "train loss:   1.405071\n",
      "train loss:   1.145041\n",
      "train loss:   0.988622\n",
      "train loss:   0.913239\n",
      "train loss:   0.841274\n",
      "train loss:   1.094568\n",
      "train loss:   1.061183\n",
      "train loss:   0.955621\n",
      "train loss:   0.959964\n",
      "train loss:   1.397009\n",
      "train loss:   1.110581\n",
      "train loss:   0.917099\n",
      "train loss:   1.121571\n",
      "train loss:   1.580796\n",
      "train loss:   1.262316\n",
      "train loss:   1.267058\n",
      "train loss:   1.212819\n",
      "train loss:   1.056395\n",
      "train loss:   1.208116\n",
      "train loss:   0.974839\n",
      "train loss:   1.183314\n",
      "train loss:   1.017289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.012838\n",
      "train loss:   1.077891\n",
      "train loss:   0.815719\n",
      "train loss:   1.516235\n",
      "train loss:   0.754526\n",
      "train loss:   1.157523\n",
      "train loss:   1.405818\n",
      "train loss:   0.900382\n",
      "train loss:   0.833630\n",
      "train loss:   1.178022\n",
      "train loss:   1.213526\n",
      "train loss:   1.130572\n",
      "train loss:   0.995225\n",
      "train loss:   1.205614\n",
      "train loss:   0.817135\n",
      "train loss:   0.843845\n",
      "train loss:   0.856162\n",
      "train loss:   1.138327\n",
      "train loss:   1.230399\n",
      "########### epoch 13 ###########\n",
      "########### loop 2400 ###########\n",
      "test loss:   0.581387   test accuracy:   0.875000\n",
      "########### loop 2400 ###########\n",
      "train loss:   1.015991\n",
      "train loss:   1.200768\n",
      "train loss:   0.948750\n",
      "train loss:   1.431577\n",
      "train loss:   1.097731\n",
      "train loss:   1.117906\n",
      "train loss:   0.841431\n",
      "train loss:   1.404291\n",
      "train loss:   1.250179\n",
      "train loss:   1.323063\n",
      "train loss:   0.810437\n",
      "train loss:   0.933232\n",
      "train loss:   0.794666\n",
      "train loss:   0.955489\n",
      "train loss:   0.975536\n",
      "train loss:   1.203480\n",
      "train loss:   1.189952\n",
      "train loss:   0.809142\n",
      "train loss:   1.184336\n",
      "train loss:   1.100476\n",
      "train loss:   1.211041\n",
      "train loss:   0.927409\n",
      "train loss:   1.355380\n",
      "train loss:   1.245736\n",
      "train loss:   1.050893\n",
      "train loss:   1.496485\n",
      "train loss:   1.209340\n",
      "train loss:   1.327398\n",
      "train loss:   1.254700\n",
      "train loss:   1.129175\n",
      "train loss:   1.370438\n",
      "train loss:   1.134800\n",
      "train loss:   0.828020\n",
      "train loss:   1.343169\n",
      "train loss:   1.170091\n",
      "train loss:   1.242602\n",
      "train loss:   1.196410\n",
      "train loss:   1.128916\n",
      "train loss:   0.968584\n",
      "train loss:   1.463344\n",
      "train loss:   1.238965\n",
      "train loss:   1.183081\n",
      "train loss:   1.283604\n",
      "train loss:   1.482181\n",
      "train loss:   1.138122\n",
      "train loss:   1.263497\n",
      "train loss:   1.403016\n",
      "train loss:   1.167750\n",
      "train loss:   1.126830\n",
      "train loss:   1.270172\n",
      "########### epoch 14 ###########\n",
      "########### loop 2450 ###########\n",
      "test loss:   0.509507   test accuracy:   0.958333\n",
      "########### loop 2450 ###########\n",
      "train loss:   1.100732\n",
      "train loss:   1.140213\n",
      "train loss:   0.867701\n",
      "train loss:   1.253857\n",
      "train loss:   1.104926\n",
      "train loss:   0.754727\n",
      "train loss:   1.191683\n",
      "train loss:   1.519746\n",
      "train loss:   1.688997\n",
      "train loss:   1.250654\n",
      "train loss:   0.876384\n",
      "train loss:   0.869383\n",
      "train loss:   1.459314\n",
      "train loss:   0.718751\n",
      "train loss:   1.267624\n",
      "train loss:   0.770186\n",
      "train loss:   0.984267\n",
      "train loss:   1.401576\n",
      "train loss:   1.514564\n",
      "train loss:   0.992455\n",
      "train loss:   0.719565\n",
      "train loss:   0.975155\n",
      "train loss:   0.757266\n",
      "train loss:   1.538587\n",
      "train loss:   0.788551\n",
      "train loss:   1.138538\n",
      "train loss:   1.424775\n",
      "train loss:   1.399574\n",
      "train loss:   0.638092\n",
      "train loss:   0.938443\n",
      "train loss:   1.307893\n",
      "train loss:   1.164751\n",
      "train loss:   0.842115\n",
      "train loss:   1.007300\n",
      "train loss:   1.160594\n",
      "train loss:   1.180007\n",
      "train loss:   0.814606\n",
      "train loss:   1.280307\n",
      "train loss:   0.973850\n",
      "train loss:   1.586630\n",
      "train loss:   1.129916\n",
      "train loss:   0.795418\n",
      "train loss:   1.180882\n",
      "train loss:   1.356265\n",
      "train loss:   0.980159\n",
      "train loss:   0.851607\n",
      "train loss:   1.159713\n",
      "train loss:   1.386326\n",
      "train loss:   1.321951\n",
      "train loss:   1.136828\n",
      "########### epoch 14 ###########\n",
      "########### loop 2500 ###########\n",
      "test loss:   0.594057   test accuracy:   0.833333\n",
      "########### loop 2500 ###########\n",
      "train loss:   0.954780\n",
      "train loss:   1.179710\n",
      "train loss:   1.063129\n",
      "train loss:   1.278375\n",
      "train loss:   1.008416\n",
      "train loss:   0.879822\n",
      "train loss:   1.346859\n",
      "train loss:   1.059360\n",
      "train loss:   1.389125\n",
      "train loss:   0.879052\n",
      "train loss:   0.842249\n",
      "train loss:   1.484736\n",
      "train loss:   1.120615\n",
      "train loss:   1.302091\n",
      "train loss:   1.032947\n",
      "train loss:   1.036322\n",
      "train loss:   1.216688\n",
      "train loss:   1.565907\n",
      "train loss:   0.928226\n",
      "train loss:   1.223018\n",
      "train loss:   1.508634\n",
      "train loss:   1.102387\n",
      "train loss:   0.962005\n",
      "train loss:   0.826220\n",
      "train loss:   0.928122\n",
      "train loss:   0.919819\n",
      "train loss:   1.157854\n",
      "train loss:   1.287296\n",
      "train loss:   1.450076\n",
      "train loss:   0.894493\n",
      "train loss:   1.357275\n",
      "train loss:   0.943841\n",
      "train loss:   1.148446\n",
      "train loss:   1.165190\n",
      "train loss:   0.984201\n",
      "train loss:   1.160805\n",
      "train loss:   1.242241\n",
      "train loss:   0.935504\n",
      "train loss:   1.126341\n",
      "train loss:   0.901790\n",
      "train loss:   0.733940\n",
      "train loss:   1.091895\n",
      "train loss:   1.091715\n",
      "train loss:   1.132053\n",
      "train loss:   0.912530\n",
      "train loss:   0.926570\n",
      "train loss:   1.389681\n",
      "train loss:   0.892820\n",
      "train loss:   0.678286\n",
      "train loss:   1.103728\n",
      "########### epoch 14 ###########\n",
      "########### loop 2550 ###########\n",
      "test loss:   0.345458   test accuracy:   1.000000\n",
      "########### loop 2550 ###########\n",
      "train loss:   1.007718\n",
      "train loss:   1.519251\n",
      "train loss:   1.096097\n",
      "train loss:   0.936789\n",
      "train loss:   0.721838\n",
      "train loss:   1.107121\n",
      "train loss:   1.279848\n",
      "train loss:   1.088597\n",
      "train loss:   1.355911\n",
      "train loss:   0.901838\n",
      "train loss:   0.779691\n",
      "train loss:   0.875779\n",
      "train loss:   1.287915\n",
      "train loss:   1.612917\n",
      "train loss:   1.182303\n",
      "train loss:   0.857562\n",
      "train loss:   0.542356\n",
      "train loss:   1.230033\n",
      "train loss:   0.975635\n",
      "train loss:   0.731282\n",
      "train loss:   1.155154\n",
      "train loss:   1.103550\n",
      "train loss:   1.083116\n",
      "train loss:   1.006612\n",
      "train loss:   1.029843\n",
      "train loss:   1.113053\n",
      "train loss:   1.120390\n",
      "train loss:   1.188071\n",
      "train loss:   0.954541\n",
      "train loss:   1.435427\n",
      "train loss:   1.373742\n",
      "train loss:   1.073637\n",
      "train loss:   1.063273\n",
      "train loss:   0.954014\n",
      "train loss:   1.099145\n",
      "train loss:   1.186464\n",
      "train loss:   1.247306\n",
      "train loss:   1.423600\n",
      "train loss:   1.251458\n",
      "train loss:   0.917666\n",
      "train loss:   0.952836\n",
      "train loss:   0.991969\n",
      "train loss:   1.433117\n",
      "train loss:   1.119712\n",
      "train loss:   1.248583\n",
      "train loss:   1.028898\n",
      "train loss:   1.352036\n",
      "train loss:   0.754276\n",
      "train loss:   1.222438\n",
      "train loss:   1.138730\n",
      "########### epoch 14 ###########\n",
      "########### loop 2600 ###########\n",
      "test loss:   0.457329   test accuracy:   0.916667\n",
      "########### loop 2600 ###########\n",
      "train loss:   1.125600\n",
      "train loss:   0.897793\n",
      "train loss:   1.148853\n",
      "train loss:   1.475332\n",
      "train loss:   1.486563\n",
      "train loss:   0.827580\n",
      "train loss:   0.895912\n",
      "train loss:   0.885198\n",
      "train loss:   0.840067\n",
      "train loss:   1.005328\n",
      "train loss:   1.477833\n",
      "train loss:   1.397197\n",
      "train loss:   1.433116\n",
      "train loss:   1.098442\n",
      "train loss:   1.181813\n",
      "train loss:   1.154927\n",
      "train loss:   1.140876\n",
      "train loss:   0.906727\n",
      "train loss:   1.080623\n",
      "train loss:   1.333459\n",
      "train loss:   1.091206\n",
      "train loss:   1.136771\n",
      "train loss:   1.167570\n",
      "train loss:   0.977371\n",
      "train loss:   1.085481\n",
      "train loss:   1.139291\n",
      "train loss:   0.696391\n",
      "train loss:   1.205208\n",
      "train loss:   1.150317\n",
      "train loss:   0.877018\n",
      "train loss:   1.185619\n",
      "train loss:   1.290916\n",
      "train loss:   1.192367\n",
      "train loss:   1.272939\n",
      "train loss:   1.290190\n",
      "train loss:   0.855085\n",
      "train loss:   1.080102\n",
      "train loss:   0.970646\n",
      "train loss:   1.049809\n",
      "train loss:   1.067677\n",
      "train loss:   1.677102\n",
      "train loss:   1.231423\n",
      "train loss:   0.807091\n",
      "train loss:   1.293392\n",
      "train loss:   1.057586\n",
      "train loss:   1.218316\n",
      "train loss:   1.148418\n",
      "train loss:   1.217874\n",
      "train loss:   0.957571\n",
      "train loss:   1.177588\n",
      "########### epoch 15 ###########\n",
      "########### loop 2650 ###########\n",
      "test loss:   0.438756   test accuracy:   0.916667\n",
      "########### loop 2650 ###########\n",
      "train loss:   1.109709\n",
      "train loss:   1.340217\n",
      "train loss:   0.757877\n",
      "train loss:   1.066472\n",
      "train loss:   1.417771\n",
      "train loss:   1.180426\n",
      "train loss:   1.014464\n",
      "train loss:   1.041330\n",
      "train loss:   1.116844\n",
      "train loss:   1.147974\n",
      "train loss:   0.816236\n",
      "train loss:   1.139342\n",
      "train loss:   0.807187\n",
      "train loss:   1.095256\n",
      "train loss:   1.020920\n",
      "train loss:   0.949154\n",
      "train loss:   1.057814\n",
      "train loss:   1.201148\n",
      "train loss:   0.852290\n",
      "train loss:   1.190577\n",
      "train loss:   1.332006\n",
      "train loss:   1.349138\n",
      "train loss:   1.194273\n",
      "train loss:   1.226247\n",
      "train loss:   0.973523\n",
      "train loss:   0.849575\n",
      "train loss:   0.984800\n",
      "train loss:   1.209909\n",
      "train loss:   0.945728\n",
      "train loss:   1.047486\n",
      "train loss:   0.984155\n",
      "train loss:   1.509547\n",
      "train loss:   1.547904\n",
      "train loss:   1.068190\n",
      "train loss:   0.960848\n",
      "train loss:   1.023951\n",
      "train loss:   1.074589\n",
      "train loss:   1.276973\n",
      "train loss:   1.530752\n",
      "train loss:   1.012972\n",
      "train loss:   0.889761\n",
      "train loss:   1.156101\n",
      "train loss:   1.120648\n",
      "train loss:   1.370334\n",
      "train loss:   1.147345\n",
      "train loss:   1.248888\n",
      "train loss:   0.668725\n",
      "train loss:   1.092776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.720078\n",
      "train loss:   0.853645\n",
      "########### epoch 15 ###########\n",
      "########### loop 2700 ###########\n",
      "test loss:   0.488679   test accuracy:   0.916667\n",
      "########### loop 2700 ###########\n",
      "train loss:   1.036876\n",
      "train loss:   1.525339\n",
      "train loss:   0.941725\n",
      "train loss:   1.382469\n",
      "train loss:   0.991028\n",
      "train loss:   0.995343\n",
      "train loss:   1.161144\n",
      "train loss:   0.949183\n",
      "train loss:   0.770258\n",
      "train loss:   0.718371\n",
      "train loss:   1.043128\n",
      "train loss:   1.292418\n",
      "train loss:   1.082641\n",
      "train loss:   1.424527\n",
      "train loss:   0.700344\n",
      "train loss:   1.348273\n",
      "train loss:   1.288840\n",
      "train loss:   1.007323\n",
      "train loss:   1.109935\n",
      "train loss:   1.094867\n",
      "train loss:   1.183806\n",
      "train loss:   1.207489\n",
      "train loss:   1.287729\n",
      "train loss:   0.883733\n",
      "train loss:   1.159400\n",
      "train loss:   0.922747\n",
      "train loss:   1.194484\n",
      "train loss:   0.977979\n",
      "train loss:   1.239050\n",
      "train loss:   1.179721\n",
      "train loss:   1.170146\n",
      "train loss:   1.134080\n",
      "train loss:   0.734634\n",
      "train loss:   1.047416\n",
      "train loss:   1.237012\n",
      "train loss:   1.136141\n",
      "train loss:   0.986514\n",
      "train loss:   0.999907\n",
      "train loss:   1.402374\n",
      "train loss:   0.697883\n",
      "train loss:   1.256606\n",
      "train loss:   1.187766\n",
      "train loss:   1.144833\n",
      "train loss:   0.691870\n",
      "train loss:   1.233661\n",
      "train loss:   1.264670\n",
      "train loss:   0.754399\n",
      "train loss:   1.019939\n",
      "train loss:   0.942550\n",
      "train loss:   1.052289\n",
      "########### epoch 15 ###########\n",
      "########### loop 2750 ###########\n",
      "test loss:   0.409169   test accuracy:   0.958333\n",
      "########### loop 2750 ###########\n",
      "train loss:   0.826908\n",
      "train loss:   1.618967\n",
      "train loss:   0.903506\n",
      "train loss:   1.048033\n",
      "train loss:   0.997696\n",
      "train loss:   1.097219\n",
      "train loss:   1.002038\n",
      "train loss:   1.408551\n",
      "train loss:   0.876841\n",
      "train loss:   1.098418\n",
      "train loss:   0.967549\n",
      "train loss:   0.898357\n",
      "train loss:   1.233585\n",
      "train loss:   1.112799\n",
      "train loss:   1.058916\n",
      "train loss:   1.330877\n",
      "train loss:   1.318526\n",
      "train loss:   1.090062\n",
      "train loss:   1.397136\n",
      "train loss:   1.096338\n",
      "train loss:   1.095415\n",
      "train loss:   1.425750\n",
      "train loss:   1.206970\n",
      "train loss:   0.915175\n",
      "train loss:   1.337840\n",
      "train loss:   1.114430\n",
      "train loss:   0.843590\n",
      "train loss:   0.851250\n",
      "train loss:   0.840046\n",
      "train loss:   1.038540\n",
      "train loss:   1.233169\n",
      "train loss:   1.305344\n",
      "train loss:   1.098831\n",
      "train loss:   1.132938\n",
      "train loss:   0.932697\n",
      "train loss:   1.063345\n",
      "train loss:   1.034935\n",
      "train loss:   1.388063\n",
      "train loss:   1.083862\n",
      "train loss:   1.174392\n",
      "train loss:   0.995873\n",
      "train loss:   1.171764\n",
      "train loss:   1.613256\n",
      "train loss:   1.311066\n",
      "train loss:   0.668221\n",
      "train loss:   1.444483\n",
      "train loss:   1.092675\n",
      "train loss:   0.997800\n",
      "train loss:   1.145525\n",
      "train loss:   0.900261\n",
      "########### epoch 15 ###########\n",
      "########### loop 2800 ###########\n",
      "test loss:   0.505979   test accuracy:   0.875000\n",
      "########### loop 2800 ###########\n",
      "train loss:   1.418833\n",
      "train loss:   0.748627\n",
      "train loss:   0.917072\n",
      "train loss:   1.404861\n",
      "train loss:   1.015217\n",
      "train loss:   1.379802\n",
      "train loss:   1.230783\n",
      "train loss:   1.500693\n",
      "train loss:   1.309988\n",
      "train loss:   1.099519\n",
      "train loss:   0.868032\n",
      "train loss:   1.001103\n",
      "train loss:   0.943190\n",
      "train loss:   1.007449\n",
      "train loss:   1.559374\n",
      "train loss:   0.894674\n",
      "train loss:   1.154368\n",
      "train loss:   1.420711\n",
      "train loss:   1.247696\n",
      "train loss:   1.016112\n",
      "train loss:   1.534807\n",
      "train loss:   0.949958\n",
      "train loss:   0.971067\n",
      "train loss:   0.826155\n",
      "train loss:   0.902696\n",
      "train loss:   0.939447\n",
      "train loss:   1.280189\n",
      "train loss:   1.192286\n",
      "train loss:   0.972678\n",
      "train loss:   0.944119\n",
      "train loss:   1.092892\n",
      "train loss:   0.805206\n",
      "train loss:   1.231290\n",
      "train loss:   1.175744\n",
      "train loss:   1.237647\n",
      "train loss:   1.109545\n",
      "train loss:   0.758625\n",
      "train loss:   1.221276\n",
      "train loss:   1.197879\n",
      "train loss:   1.090441\n",
      "train loss:   0.901084\n",
      "train loss:   0.950885\n",
      "train loss:   0.976217\n",
      "train loss:   1.249966\n",
      "train loss:   1.166957\n",
      "train loss:   0.879491\n",
      "train loss:   1.444704\n",
      "train loss:   1.172232\n",
      "train loss:   1.115669\n",
      "train loss:   0.841590\n",
      "########### epoch 16 ###########\n",
      "########### loop 2850 ###########\n",
      "test loss:   0.493173   test accuracy:   0.916667\n",
      "########### loop 2850 ###########\n",
      "train loss:   0.662182\n",
      "train loss:   1.005523\n",
      "train loss:   1.124562\n",
      "train loss:   1.023321\n",
      "train loss:   1.241793\n",
      "train loss:   1.037046\n",
      "train loss:   0.907384\n",
      "train loss:   1.213756\n",
      "train loss:   0.987424\n",
      "train loss:   1.520937\n",
      "train loss:   1.435141\n",
      "train loss:   0.987944\n",
      "train loss:   1.019030\n",
      "train loss:   1.025575\n",
      "train loss:   1.226030\n",
      "train loss:   0.740626\n",
      "train loss:   1.056204\n",
      "train loss:   1.407381\n",
      "train loss:   1.225558\n",
      "train loss:   1.142035\n",
      "train loss:   1.625843\n",
      "train loss:   0.931800\n",
      "train loss:   1.187768\n",
      "train loss:   1.357726\n",
      "train loss:   1.116941\n",
      "train loss:   1.180758\n",
      "train loss:   1.723979\n",
      "train loss:   1.327336\n",
      "train loss:   0.774753\n",
      "train loss:   1.086094\n",
      "train loss:   0.787387\n",
      "train loss:   1.029885\n",
      "train loss:   1.420595\n",
      "train loss:   1.354291\n",
      "train loss:   1.235730\n",
      "train loss:   0.963267\n",
      "train loss:   0.794876\n",
      "train loss:   1.577067\n",
      "train loss:   1.178519\n",
      "train loss:   1.164861\n",
      "train loss:   1.443336\n",
      "train loss:   1.147534\n",
      "train loss:   0.846863\n",
      "train loss:   1.034724\n",
      "train loss:   0.952812\n",
      "train loss:   0.818600\n",
      "train loss:   1.268438\n",
      "train loss:   0.886658\n",
      "train loss:   0.978986\n",
      "train loss:   0.901876\n",
      "########### epoch 16 ###########\n",
      "########### loop 2900 ###########\n",
      "test loss:   0.464875   test accuracy:   0.875000\n",
      "########### loop 2900 ###########\n",
      "train loss:   1.101427\n",
      "train loss:   1.220500\n",
      "train loss:   1.303837\n",
      "train loss:   1.139569\n",
      "train loss:   1.411792\n",
      "train loss:   1.471670\n",
      "train loss:   0.647945\n",
      "train loss:   1.005235\n",
      "train loss:   0.950335\n",
      "train loss:   1.399774\n",
      "train loss:   0.950878\n",
      "train loss:   1.136427\n",
      "train loss:   1.162063\n",
      "train loss:   1.370091\n",
      "train loss:   1.219905\n",
      "train loss:   1.115295\n",
      "train loss:   1.191697\n",
      "train loss:   0.734688\n",
      "train loss:   1.256362\n",
      "train loss:   1.132864\n",
      "train loss:   1.069209\n",
      "train loss:   1.216705\n",
      "train loss:   1.141555\n",
      "train loss:   1.265694\n",
      "train loss:   1.060752\n",
      "train loss:   1.333277\n",
      "train loss:   1.163576\n",
      "train loss:   0.691149\n",
      "train loss:   0.980982\n",
      "train loss:   0.897407\n",
      "train loss:   1.049213\n",
      "train loss:   1.385285\n",
      "train loss:   1.148011\n",
      "train loss:   1.069051\n",
      "train loss:   0.989647\n",
      "train loss:   0.934968\n",
      "train loss:   1.443138\n",
      "train loss:   1.063575\n",
      "train loss:   0.930825\n",
      "train loss:   0.971244\n",
      "train loss:   0.940958\n",
      "train loss:   0.934581\n",
      "train loss:   0.717355\n",
      "train loss:   1.240582\n",
      "train loss:   1.163139\n",
      "train loss:   0.844268\n",
      "train loss:   1.424709\n",
      "train loss:   1.193972\n",
      "train loss:   1.052639\n",
      "train loss:   0.882051\n",
      "########### epoch 16 ###########\n",
      "########### loop 2950 ###########\n",
      "test loss:   0.553559   test accuracy:   0.958333\n",
      "########### loop 2950 ###########\n",
      "train loss:   0.850576\n",
      "train loss:   1.130723\n",
      "train loss:   1.282153\n",
      "train loss:   1.738298\n",
      "train loss:   1.116510\n",
      "train loss:   1.354334\n",
      "train loss:   1.059206\n",
      "train loss:   0.976832\n",
      "train loss:   1.206418\n",
      "train loss:   0.714337\n",
      "train loss:   1.042726\n",
      "train loss:   1.359554\n",
      "train loss:   1.281293\n",
      "train loss:   1.290123\n",
      "train loss:   1.013450\n",
      "train loss:   1.377370\n",
      "train loss:   1.300561\n",
      "train loss:   0.985888\n",
      "train loss:   1.140420\n",
      "train loss:   1.380792\n",
      "train loss:   0.893316\n",
      "train loss:   0.828353\n",
      "train loss:   1.011388\n",
      "train loss:   1.021847\n",
      "train loss:   1.575373\n",
      "train loss:   0.931644\n",
      "train loss:   0.921816\n",
      "train loss:   1.336216\n",
      "train loss:   1.342648\n",
      "train loss:   1.166409\n",
      "train loss:   0.991728\n",
      "train loss:   1.271127\n",
      "train loss:   1.347664\n",
      "train loss:   1.108788\n",
      "train loss:   0.799852\n",
      "train loss:   0.602197\n",
      "train loss:   1.007737\n",
      "train loss:   1.197921\n",
      "train loss:   1.010194\n",
      "train loss:   1.242998\n",
      "train loss:   1.333681\n",
      "train loss:   0.852598\n",
      "train loss:   1.013063\n",
      "train loss:   0.990395\n",
      "train loss:   1.212771\n",
      "train loss:   1.245373\n",
      "train loss:   0.775349\n",
      "train loss:   1.452552\n",
      "train loss:   1.326516\n",
      "train loss:   1.027212\n",
      "########### epoch 16 ###########\n",
      "########### loop 3000 ###########\n",
      "test loss:   0.347585   test accuracy:   1.000000\n",
      "########### loop 3000 ###########\n",
      "train loss:   1.274714\n",
      "train loss:   1.484871\n",
      "train loss:   0.780713\n",
      "train loss:   1.126407\n",
      "train loss:   1.508181\n",
      "train loss:   1.346766\n",
      "train loss:   0.784760\n",
      "train loss:   0.972149\n",
      "train loss:   1.244737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.620956\n",
      "train loss:   1.202876\n",
      "train loss:   0.809645\n",
      "train loss:   0.909059\n",
      "train loss:   1.175102\n",
      "train loss:   1.260329\n",
      "train loss:   1.015740\n",
      "train loss:   1.296186\n",
      "train loss:   0.769999\n",
      "train loss:   1.043656\n",
      "train loss:   1.150481\n",
      "train loss:   1.138782\n",
      "train loss:   1.295116\n",
      "train loss:   1.443341\n",
      "train loss:   1.222057\n",
      "train loss:   1.414359\n",
      "train loss:   1.184833\n",
      "train loss:   1.177152\n",
      "train loss:   1.351076\n",
      "train loss:   0.828775\n",
      "train loss:   1.221905\n",
      "train loss:   1.250590\n",
      "train loss:   1.046309\n",
      "train loss:   1.183647\n",
      "train loss:   1.051851\n",
      "train loss:   1.334347\n",
      "train loss:   1.547245\n",
      "train loss:   1.408614\n",
      "train loss:   1.405972\n",
      "train loss:   1.275910\n",
      "train loss:   0.968593\n",
      "train loss:   1.037025\n",
      "train loss:   1.208800\n",
      "train loss:   1.247955\n",
      "train loss:   1.087831\n",
      "train loss:   1.138672\n",
      "train loss:   0.829976\n",
      "train loss:   1.476436\n",
      "train loss:   1.107845\n",
      "train loss:   0.943588\n",
      "train loss:   1.179760\n",
      "########### epoch 17 ###########\n",
      "########### loop 3050 ###########\n",
      "test loss:   0.727619   test accuracy:   0.833333\n",
      "########### loop 3050 ###########\n",
      "train loss:   1.396100\n",
      "train loss:   0.868685\n",
      "train loss:   1.304138\n",
      "train loss:   1.155675\n",
      "train loss:   0.938903\n",
      "train loss:   0.845784\n",
      "train loss:   0.848109\n",
      "train loss:   1.040033\n",
      "train loss:   1.415950\n",
      "train loss:   1.044208\n",
      "train loss:   1.041921\n",
      "train loss:   1.064819\n",
      "train loss:   1.149721\n",
      "train loss:   0.674319\n",
      "train loss:   0.878053\n",
      "train loss:   1.126363\n",
      "train loss:   0.926511\n",
      "train loss:   0.892531\n",
      "train loss:   1.058411\n",
      "train loss:   1.175833\n",
      "train loss:   1.189773\n",
      "train loss:   1.010351\n",
      "train loss:   0.882441\n",
      "train loss:   1.276962\n",
      "train loss:   1.051831\n",
      "train loss:   1.118376\n",
      "train loss:   0.956413\n",
      "train loss:   1.085351\n",
      "train loss:   0.847374\n",
      "train loss:   0.872868\n",
      "train loss:   0.843611\n",
      "train loss:   1.057124\n",
      "train loss:   0.992982\n",
      "train loss:   1.020511\n",
      "train loss:   1.337838\n",
      "train loss:   1.376909\n",
      "train loss:   1.199142\n",
      "train loss:   1.035304\n",
      "train loss:   0.831385\n",
      "train loss:   1.208989\n",
      "train loss:   1.217969\n",
      "train loss:   1.276895\n",
      "train loss:   1.284666\n",
      "train loss:   1.155996\n",
      "train loss:   0.896262\n",
      "train loss:   1.187543\n",
      "train loss:   0.986561\n",
      "train loss:   0.940232\n",
      "train loss:   1.054086\n",
      "train loss:   1.454289\n",
      "########### epoch 17 ###########\n",
      "########### loop 3100 ###########\n",
      "test loss:   0.535455   test accuracy:   0.916667\n",
      "########### loop 3100 ###########\n",
      "train loss:   0.959553\n",
      "train loss:   1.105269\n",
      "train loss:   1.283522\n",
      "train loss:   0.963721\n",
      "train loss:   1.389939\n",
      "train loss:   1.240952\n",
      "train loss:   1.160635\n",
      "train loss:   1.187671\n",
      "train loss:   1.077282\n",
      "train loss:   1.062170\n",
      "train loss:   1.267218\n",
      "train loss:   1.088813\n",
      "train loss:   1.409840\n",
      "train loss:   1.082697\n",
      "train loss:   1.261444\n",
      "train loss:   1.355495\n",
      "train loss:   1.058409\n",
      "train loss:   1.120152\n",
      "train loss:   0.998223\n",
      "train loss:   1.335541\n",
      "train loss:   0.883812\n",
      "train loss:   1.390906\n",
      "train loss:   0.570009\n",
      "train loss:   1.273894\n",
      "train loss:   1.191704\n",
      "train loss:   1.190190\n",
      "train loss:   1.523866\n",
      "train loss:   1.413636\n",
      "train loss:   1.127359\n",
      "train loss:   1.133858\n",
      "train loss:   1.118943\n",
      "train loss:   1.335634\n",
      "train loss:   1.298972\n",
      "train loss:   0.916471\n",
      "train loss:   1.119895\n",
      "train loss:   1.073023\n",
      "train loss:   1.035091\n",
      "train loss:   1.221556\n",
      "train loss:   1.111332\n",
      "train loss:   0.852974\n",
      "train loss:   0.969453\n",
      "train loss:   0.945852\n",
      "train loss:   1.168377\n",
      "train loss:   1.043097\n",
      "train loss:   1.034650\n",
      "train loss:   0.669549\n",
      "train loss:   0.982124\n",
      "train loss:   1.093210\n",
      "train loss:   0.932006\n",
      "train loss:   1.168456\n",
      "########### epoch 17 ###########\n",
      "########### loop 3150 ###########\n",
      "test loss:   0.315653   test accuracy:   0.958333\n",
      "########### loop 3150 ###########\n",
      "train loss:   0.992432\n",
      "train loss:   1.268381\n",
      "train loss:   0.845985\n",
      "train loss:   0.971341\n",
      "train loss:   0.878034\n",
      "train loss:   1.088405\n",
      "train loss:   1.535931\n",
      "train loss:   1.146260\n",
      "train loss:   1.381593\n",
      "train loss:   1.067288\n",
      "train loss:   1.250349\n",
      "train loss:   1.103379\n",
      "train loss:   1.157795\n",
      "train loss:   0.953823\n",
      "train loss:   0.988934\n",
      "train loss:   0.802232\n",
      "train loss:   0.913864\n",
      "train loss:   1.080066\n",
      "train loss:   1.091640\n",
      "train loss:   1.110423\n",
      "train loss:   1.184544\n",
      "train loss:   0.950143\n",
      "train loss:   1.110232\n",
      "train loss:   1.481219\n",
      "train loss:   0.981750\n",
      "train loss:   1.414932\n",
      "train loss:   1.157433\n",
      "train loss:   1.253587\n",
      "train loss:   1.056030\n",
      "train loss:   1.121496\n",
      "train loss:   0.831603\n",
      "train loss:   0.900292\n",
      "train loss:   1.117951\n",
      "train loss:   1.215031\n",
      "train loss:   0.666129\n",
      "train loss:   1.392603\n",
      "train loss:   0.887868\n",
      "train loss:   1.495266\n",
      "train loss:   1.095439\n",
      "train loss:   1.046974\n",
      "train loss:   1.290111\n",
      "train loss:   1.100412\n",
      "train loss:   0.909145\n",
      "train loss:   0.983564\n",
      "train loss:   0.777166\n",
      "train loss:   1.304971\n",
      "train loss:   0.969465\n",
      "train loss:   1.102018\n",
      "train loss:   0.899293\n",
      "train loss:   1.069566\n",
      "########### epoch 18 ###########\n",
      "########### loop 3200 ###########\n",
      "test loss:   0.338738   test accuracy:   0.958333\n",
      "########### loop 3200 ###########\n",
      "train loss:   1.110903\n",
      "train loss:   1.315238\n",
      "train loss:   1.147267\n",
      "train loss:   1.037347\n",
      "train loss:   1.099934\n",
      "train loss:   1.256375\n",
      "train loss:   1.110979\n",
      "train loss:   1.182735\n",
      "train loss:   1.058520\n",
      "train loss:   1.135205\n",
      "train loss:   0.850947\n",
      "train loss:   1.250626\n",
      "train loss:   1.228444\n",
      "train loss:   0.857540\n",
      "train loss:   1.068734\n",
      "train loss:   1.445663\n",
      "train loss:   1.587556\n",
      "train loss:   0.867872\n",
      "train loss:   1.474237\n",
      "train loss:   1.017771\n",
      "train loss:   1.037926\n",
      "train loss:   0.876761\n",
      "train loss:   1.022892\n",
      "train loss:   1.297007\n",
      "train loss:   1.107728\n",
      "train loss:   1.175406\n",
      "train loss:   1.256833\n",
      "train loss:   1.000193\n",
      "train loss:   1.634158\n",
      "train loss:   1.004899\n",
      "train loss:   1.035438\n",
      "train loss:   0.812061\n",
      "train loss:   0.832725\n",
      "train loss:   1.104717\n",
      "train loss:   0.969018\n",
      "train loss:   0.828063\n",
      "train loss:   1.414362\n",
      "train loss:   1.312103\n",
      "train loss:   1.129894\n",
      "train loss:   1.178951\n",
      "train loss:   1.470688\n",
      "train loss:   0.643447\n",
      "train loss:   1.096618\n",
      "train loss:   1.240319\n",
      "train loss:   1.409561\n",
      "train loss:   1.021683\n",
      "train loss:   1.101268\n",
      "train loss:   0.931367\n",
      "train loss:   0.917337\n",
      "train loss:   1.320530\n",
      "########### epoch 18 ###########\n",
      "########### loop 3250 ###########\n",
      "test loss:   0.468379   test accuracy:   0.958333\n",
      "########### loop 3250 ###########\n",
      "train loss:   1.178659\n",
      "train loss:   1.029211\n",
      "train loss:   0.846299\n",
      "train loss:   1.168805\n",
      "train loss:   0.543550\n",
      "train loss:   1.166567\n",
      "train loss:   0.864581\n",
      "train loss:   0.925658\n",
      "train loss:   1.064772\n",
      "train loss:   0.682581\n",
      "train loss:   1.050023\n",
      "train loss:   0.597046\n",
      "train loss:   1.237645\n",
      "train loss:   0.539869\n",
      "train loss:   0.991370\n",
      "train loss:   1.264417\n",
      "train loss:   0.882557\n",
      "train loss:   0.754548\n",
      "train loss:   1.678514\n",
      "train loss:   1.077744\n",
      "train loss:   0.813559\n",
      "train loss:   1.198449\n",
      "train loss:   0.691917\n",
      "train loss:   1.109692\n",
      "train loss:   1.556480\n",
      "train loss:   1.149318\n",
      "train loss:   1.016438\n",
      "train loss:   1.291049\n",
      "train loss:   1.136938\n",
      "train loss:   0.755718\n",
      "train loss:   1.104051\n",
      "train loss:   0.952631\n",
      "train loss:   1.502405\n",
      "train loss:   1.299759\n",
      "train loss:   0.866508\n",
      "train loss:   1.288219\n",
      "train loss:   1.264095\n",
      "train loss:   0.905390\n",
      "train loss:   1.014645\n",
      "train loss:   0.711395\n",
      "train loss:   1.404079\n",
      "train loss:   0.823408\n",
      "train loss:   1.520163\n",
      "train loss:   1.399948\n",
      "train loss:   1.072942\n",
      "train loss:   1.080667\n",
      "train loss:   1.279895\n",
      "train loss:   0.778900\n",
      "train loss:   1.216969\n",
      "train loss:   1.126758\n",
      "########### epoch 18 ###########\n",
      "########### loop 3300 ###########\n",
      "test loss:   0.459680   test accuracy:   0.875000\n",
      "########### loop 3300 ###########\n",
      "train loss:   1.480872\n",
      "train loss:   1.267259\n",
      "train loss:   1.341974\n",
      "train loss:   1.140753\n",
      "train loss:   1.283487\n",
      "train loss:   1.174643\n",
      "train loss:   0.760893\n",
      "train loss:   1.099838\n",
      "train loss:   1.236760\n",
      "train loss:   1.236559\n",
      "train loss:   1.132473\n",
      "train loss:   0.977615\n",
      "train loss:   1.186114\n",
      "train loss:   1.334266\n",
      "train loss:   1.078781\n",
      "train loss:   1.197223\n",
      "train loss:   1.178963\n",
      "train loss:   1.307704\n",
      "train loss:   1.173529\n",
      "train loss:   1.000530\n",
      "train loss:   0.812936\n",
      "train loss:   1.582257\n",
      "train loss:   1.333793\n",
      "train loss:   0.862996\n",
      "train loss:   0.979114\n",
      "train loss:   0.946665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.346142\n",
      "train loss:   1.009923\n",
      "train loss:   0.906037\n",
      "train loss:   1.143731\n",
      "train loss:   0.721858\n",
      "train loss:   1.132722\n",
      "train loss:   0.887992\n",
      "train loss:   0.939598\n",
      "train loss:   1.474154\n",
      "train loss:   1.444652\n",
      "train loss:   0.969598\n",
      "train loss:   1.248585\n",
      "train loss:   1.598400\n",
      "train loss:   0.655590\n",
      "train loss:   0.994100\n",
      "train loss:   1.416996\n",
      "train loss:   1.036964\n",
      "train loss:   0.964164\n",
      "train loss:   1.039779\n",
      "train loss:   0.985484\n",
      "train loss:   1.192516\n",
      "train loss:   0.948110\n",
      "train loss:   1.080994\n",
      "train loss:   0.799612\n",
      "########### epoch 18 ###########\n",
      "########### loop 3350 ###########\n",
      "test loss:   0.345557   test accuracy:   1.000000\n",
      "########### loop 3350 ###########\n",
      "train loss:   1.311105\n",
      "train loss:   1.046901\n",
      "train loss:   1.402504\n",
      "train loss:   0.952699\n",
      "train loss:   0.973805\n",
      "train loss:   1.025589\n",
      "train loss:   1.118459\n",
      "train loss:   1.217105\n",
      "train loss:   0.862655\n",
      "train loss:   1.220094\n",
      "train loss:   1.108004\n",
      "train loss:   1.381750\n",
      "train loss:   1.367657\n",
      "train loss:   1.200490\n",
      "train loss:   1.254538\n",
      "train loss:   1.145956\n",
      "train loss:   0.779766\n",
      "train loss:   1.183874\n",
      "train loss:   1.120124\n",
      "train loss:   0.987712\n",
      "train loss:   1.484593\n",
      "train loss:   1.313917\n",
      "train loss:   1.006754\n",
      "train loss:   0.942291\n",
      "train loss:   1.039742\n",
      "train loss:   1.348683\n",
      "train loss:   0.844402\n",
      "train loss:   1.196489\n",
      "train loss:   1.008254\n",
      "train loss:   1.089122\n",
      "train loss:   1.126397\n",
      "train loss:   1.343803\n",
      "train loss:   1.223980\n",
      "train loss:   0.810730\n",
      "train loss:   0.792707\n",
      "train loss:   0.954049\n",
      "train loss:   0.998401\n",
      "train loss:   1.157363\n",
      "train loss:   1.394267\n",
      "train loss:   1.432268\n",
      "train loss:   0.995471\n",
      "train loss:   1.380860\n",
      "train loss:   1.429028\n",
      "train loss:   1.021747\n",
      "train loss:   1.051210\n",
      "train loss:   1.249289\n",
      "train loss:   1.213693\n",
      "train loss:   0.854642\n",
      "train loss:   1.287313\n",
      "train loss:   1.586287\n",
      "########### epoch 19 ###########\n",
      "########### loop 3400 ###########\n",
      "test loss:   0.299674   test accuracy:   1.000000\n",
      "########### loop 3400 ###########\n",
      "train loss:   0.834049\n",
      "train loss:   1.145450\n",
      "train loss:   0.933524\n",
      "train loss:   1.088097\n",
      "train loss:   1.242520\n",
      "train loss:   1.123999\n",
      "train loss:   1.241199\n",
      "train loss:   1.205569\n",
      "train loss:   1.178980\n",
      "train loss:   1.108696\n",
      "train loss:   1.080935\n",
      "train loss:   1.356160\n",
      "train loss:   1.160245\n",
      "train loss:   0.875350\n",
      "train loss:   1.428740\n",
      "train loss:   0.740181\n",
      "train loss:   1.078874\n",
      "train loss:   0.922808\n",
      "train loss:   0.788082\n",
      "train loss:   1.074604\n",
      "train loss:   1.053469\n",
      "train loss:   1.301167\n",
      "train loss:   1.175537\n",
      "train loss:   0.915082\n",
      "train loss:   1.200490\n",
      "train loss:   1.170495\n",
      "train loss:   0.768976\n",
      "train loss:   1.362452\n",
      "train loss:   1.164092\n",
      "train loss:   1.163417\n",
      "train loss:   0.855006\n",
      "train loss:   0.941410\n",
      "train loss:   1.295558\n",
      "train loss:   0.922011\n",
      "train loss:   1.168752\n",
      "train loss:   1.069858\n",
      "train loss:   1.006900\n",
      "train loss:   1.033474\n",
      "train loss:   1.119282\n",
      "train loss:   0.822621\n",
      "train loss:   1.283769\n",
      "train loss:   1.048662\n",
      "train loss:   1.212967\n",
      "train loss:   0.757793\n",
      "train loss:   0.924082\n",
      "train loss:   1.302737\n",
      "train loss:   0.917024\n",
      "train loss:   1.260805\n",
      "train loss:   0.899266\n",
      "train loss:   1.116295\n",
      "########### epoch 19 ###########\n",
      "########### loop 3450 ###########\n",
      "test loss:   0.405696   test accuracy:   0.916667\n",
      "########### loop 3450 ###########\n",
      "train loss:   0.893361\n",
      "train loss:   1.246752\n",
      "train loss:   1.464643\n",
      "train loss:   1.298025\n",
      "train loss:   1.300570\n",
      "train loss:   1.122520\n",
      "train loss:   1.490107\n",
      "train loss:   1.059551\n",
      "train loss:   1.348226\n",
      "train loss:   0.953956\n",
      "train loss:   1.007483\n",
      "train loss:   0.874196\n",
      "train loss:   1.169339\n",
      "train loss:   0.612414\n",
      "train loss:   1.202635\n",
      "train loss:   0.980863\n",
      "train loss:   1.264411\n",
      "train loss:   1.420661\n",
      "train loss:   0.975177\n",
      "train loss:   1.206286\n",
      "train loss:   1.373578\n",
      "train loss:   1.045347\n",
      "train loss:   1.293183\n",
      "train loss:   0.910039\n",
      "train loss:   1.424937\n",
      "train loss:   1.089592\n",
      "train loss:   0.960222\n",
      "train loss:   1.176000\n",
      "train loss:   1.451711\n",
      "train loss:   1.049108\n",
      "train loss:   1.277111\n",
      "train loss:   1.173740\n",
      "train loss:   1.142217\n",
      "train loss:   1.090356\n",
      "train loss:   1.144985\n",
      "train loss:   1.607691\n",
      "train loss:   1.240731\n",
      "train loss:   1.235106\n",
      "train loss:   1.109466\n",
      "train loss:   0.983384\n",
      "train loss:   1.080991\n",
      "train loss:   0.693329\n",
      "train loss:   0.773061\n",
      "train loss:   1.060554\n",
      "train loss:   0.753628\n",
      "train loss:   1.299275\n",
      "train loss:   0.988707\n",
      "train loss:   1.047519\n",
      "train loss:   1.112193\n",
      "train loss:   1.258009\n",
      "########### epoch 19 ###########\n",
      "########### loop 3500 ###########\n",
      "test loss:   0.399865   test accuracy:   0.958333\n",
      "########### loop 3500 ###########\n",
      "train loss:   0.740893\n",
      "train loss:   0.798104\n",
      "train loss:   1.240505\n",
      "train loss:   1.241125\n",
      "train loss:   0.817883\n",
      "train loss:   1.251795\n",
      "train loss:   1.209109\n",
      "train loss:   1.397767\n",
      "train loss:   1.011973\n",
      "train loss:   1.233032\n",
      "train loss:   1.331531\n",
      "train loss:   1.216903\n",
      "train loss:   0.803635\n",
      "train loss:   1.191003\n",
      "train loss:   1.180994\n",
      "train loss:   0.954717\n",
      "train loss:   1.199663\n",
      "train loss:   0.773152\n",
      "train loss:   1.033013\n",
      "train loss:   0.946802\n",
      "train loss:   0.569430\n",
      "train loss:   1.162188\n",
      "train loss:   1.017679\n",
      "train loss:   1.272333\n",
      "train loss:   1.048676\n",
      "train loss:   1.477568\n",
      "train loss:   1.187166\n",
      "train loss:   0.734623\n",
      "train loss:   0.787513\n",
      "train loss:   0.970588\n",
      "train loss:   1.163426\n",
      "train loss:   1.185178\n",
      "train loss:   1.160838\n",
      "train loss:   1.494303\n",
      "train loss:   1.113160\n",
      "train loss:   1.185920\n",
      "train loss:   1.219173\n",
      "train loss:   1.074045\n",
      "train loss:   1.020576\n",
      "train loss:   0.816309\n",
      "train loss:   0.991399\n",
      "train loss:   1.112607\n",
      "train loss:   1.084479\n",
      "train loss:   1.034846\n",
      "train loss:   1.254571\n",
      "train loss:   0.877391\n",
      "train loss:   1.043444\n",
      "train loss:   1.341153\n",
      "train loss:   1.143543\n",
      "train loss:   1.167877\n",
      "########### epoch 19 ###########\n",
      "########### loop 3550 ###########\n",
      "test loss:   0.391843   test accuracy:   0.958333\n",
      "########### loop 3550 ###########\n",
      "train loss:   1.274802\n",
      "train loss:   1.229399\n",
      "train loss:   0.627292\n",
      "train loss:   0.916510\n",
      "train loss:   0.937161\n",
      "train loss:   0.974752\n",
      "train loss:   1.091294\n",
      "train loss:   0.789142\n",
      "train loss:   1.581696\n",
      "train loss:   1.174446\n",
      "train loss:   1.078358\n",
      "train loss:   1.003782\n",
      "train loss:   0.955320\n",
      "train loss:   0.960262\n",
      "train loss:   1.410987\n",
      "train loss:   1.190683\n",
      "train loss:   1.006895\n",
      "train loss:   0.955395\n",
      "train loss:   0.631022\n",
      "train loss:   0.805250\n",
      "train loss:   0.824372\n",
      "train loss:   0.826133\n",
      "train loss:   0.936049\n",
      "train loss:   0.943606\n",
      "train loss:   1.190503\n",
      "train loss:   1.177993\n",
      "train loss:   0.974314\n",
      "train loss:   1.396763\n",
      "train loss:   1.178622\n",
      "train loss:   1.614355\n",
      "train loss:   1.373863\n",
      "train loss:   1.159426\n",
      "train loss:   0.929498\n",
      "train loss:   0.695162\n",
      "train loss:   1.236803\n",
      "train loss:   1.289546\n",
      "train loss:   1.048425\n",
      "train loss:   1.295026\n",
      "train loss:   1.380114\n",
      "train loss:   1.321770\n",
      "train loss:   0.748864\n",
      "train loss:   0.973672\n",
      "train loss:   1.060988\n",
      "train loss:   1.480099\n",
      "train loss:   0.972270\n",
      "train loss:   1.393477\n",
      "train loss:   0.984168\n",
      "train loss:   1.089717\n",
      "train loss:   1.038800\n",
      "train loss:   1.418463\n",
      "########### epoch 20 ###########\n",
      "########### loop 3600 ###########\n",
      "test loss:   0.653805   test accuracy:   0.833333\n",
      "########### loop 3600 ###########\n",
      "train loss:   1.245987\n",
      "train loss:   1.142757\n",
      "train loss:   0.957630\n",
      "train loss:   1.210962\n",
      "train loss:   1.019753\n",
      "train loss:   1.058476\n",
      "train loss:   1.276176\n",
      "train loss:   1.417845\n",
      "train loss:   0.814604\n",
      "train loss:   1.328302\n",
      "train loss:   0.672313\n",
      "train loss:   1.188215\n",
      "train loss:   1.293046\n",
      "train loss:   0.858000\n",
      "train loss:   0.931779\n",
      "train loss:   1.051229\n",
      "train loss:   1.315758\n",
      "train loss:   0.764887\n",
      "train loss:   0.889920\n",
      "train loss:   1.010827\n",
      "train loss:   1.096870\n",
      "train loss:   1.052770\n",
      "train loss:   0.958411\n",
      "train loss:   0.972976\n",
      "train loss:   1.204422\n",
      "train loss:   1.339023\n",
      "train loss:   1.417807\n",
      "train loss:   0.981721\n",
      "train loss:   1.161112\n",
      "train loss:   1.020308\n",
      "train loss:   1.333844\n",
      "train loss:   1.106309\n",
      "train loss:   1.170462\n",
      "train loss:   1.020313\n",
      "train loss:   1.201892\n",
      "train loss:   1.355487\n",
      "train loss:   1.291017\n",
      "train loss:   1.540782\n",
      "train loss:   1.499796\n",
      "train loss:   0.693177\n",
      "train loss:   0.993603\n",
      "train loss:   0.580768\n",
      "train loss:   1.413475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.081052\n",
      "train loss:   1.384627\n",
      "train loss:   0.891486\n",
      "train loss:   1.176603\n",
      "train loss:   1.795640\n",
      "train loss:   1.259257\n",
      "train loss:   1.513474\n",
      "########### epoch 20 ###########\n",
      "########### loop 3650 ###########\n",
      "test loss:   0.505136   test accuracy:   0.916667\n",
      "########### loop 3650 ###########\n",
      "train loss:   0.891970\n",
      "train loss:   1.196433\n",
      "train loss:   0.688800\n",
      "train loss:   1.372989\n",
      "train loss:   1.227289\n",
      "train loss:   1.077703\n",
      "train loss:   0.881757\n",
      "train loss:   1.130136\n",
      "train loss:   1.341499\n",
      "train loss:   0.992898\n",
      "train loss:   1.187899\n",
      "train loss:   0.839762\n",
      "train loss:   0.928390\n",
      "train loss:   0.989920\n",
      "train loss:   0.835778\n",
      "train loss:   0.767137\n",
      "train loss:   0.848842\n",
      "train loss:   1.038964\n",
      "train loss:   1.646449\n",
      "train loss:   0.998387\n",
      "train loss:   0.663726\n",
      "train loss:   1.216530\n",
      "train loss:   0.931974\n",
      "train loss:   1.000508\n",
      "train loss:   1.178891\n",
      "train loss:   1.056846\n",
      "train loss:   1.142992\n",
      "train loss:   0.844391\n",
      "train loss:   0.967613\n",
      "train loss:   1.071252\n",
      "train loss:   1.041578\n",
      "train loss:   1.068121\n",
      "train loss:   1.106695\n",
      "train loss:   1.024343\n",
      "train loss:   1.203942\n",
      "train loss:   1.176182\n",
      "train loss:   0.956789\n",
      "train loss:   1.338623\n",
      "train loss:   1.135532\n",
      "train loss:   1.223336\n",
      "train loss:   1.122102\n",
      "train loss:   1.125778\n",
      "train loss:   1.325823\n",
      "train loss:   0.964932\n",
      "train loss:   0.758461\n",
      "train loss:   0.926666\n",
      "train loss:   0.782828\n",
      "train loss:   1.170127\n",
      "train loss:   1.291390\n",
      "train loss:   1.221842\n",
      "########### epoch 20 ###########\n",
      "########### loop 3700 ###########\n",
      "test loss:   0.315851   test accuracy:   0.958333\n",
      "########### loop 3700 ###########\n",
      "train loss:   1.186200\n",
      "train loss:   1.172602\n",
      "train loss:   1.422554\n",
      "train loss:   0.759756\n",
      "train loss:   1.193858\n",
      "train loss:   1.042992\n",
      "train loss:   1.159228\n",
      "train loss:   1.120945\n",
      "train loss:   1.204588\n",
      "train loss:   1.162265\n",
      "train loss:   0.641879\n",
      "train loss:   0.699083\n",
      "train loss:   1.159854\n",
      "train loss:   0.689267\n",
      "train loss:   1.260041\n",
      "train loss:   1.265435\n",
      "train loss:   1.419482\n",
      "train loss:   0.503654\n",
      "train loss:   1.095044\n",
      "train loss:   1.208306\n",
      "train loss:   1.199553\n",
      "train loss:   1.051360\n",
      "train loss:   1.196885\n",
      "train loss:   0.764568\n",
      "train loss:   1.207437\n",
      "train loss:   1.025641\n",
      "train loss:   1.622134\n",
      "train loss:   1.100895\n",
      "train loss:   0.922421\n",
      "train loss:   1.199492\n",
      "train loss:   1.321976\n",
      "train loss:   0.968101\n",
      "train loss:   1.022835\n",
      "train loss:   0.827460\n",
      "train loss:   1.039303\n",
      "train loss:   1.087132\n",
      "train loss:   1.433622\n",
      "train loss:   1.285636\n",
      "train loss:   0.715415\n",
      "train loss:   0.835458\n",
      "train loss:   1.524328\n",
      "train loss:   1.230186\n",
      "train loss:   0.876393\n",
      "train loss:   1.239423\n",
      "train loss:   1.132071\n",
      "train loss:   1.014690\n",
      "train loss:   0.998695\n",
      "train loss:   0.848875\n",
      "train loss:   1.033718\n",
      "train loss:   1.017580\n",
      "########### epoch 20 ###########\n",
      "########### loop 3750 ###########\n",
      "test loss:   0.328459   test accuracy:   0.958333\n",
      "########### loop 3750 ###########\n",
      "train loss:   1.350779\n",
      "train loss:   1.282142\n",
      "train loss:   0.945348\n",
      "train loss:   0.870979\n",
      "train loss:   1.344059\n",
      "train loss:   0.952435\n",
      "train loss:   1.324254\n",
      "train loss:   0.784829\n",
      "train loss:   0.827348\n",
      "train loss:   1.357785\n",
      "train loss:   0.932802\n",
      "train loss:   0.836292\n",
      "train loss:   0.885190\n",
      "train loss:   1.056474\n",
      "train loss:   1.013545\n",
      "train loss:   1.014399\n",
      "train loss:   1.096322\n",
      "train loss:   1.298940\n",
      "train loss:   1.480193\n",
      "train loss:   1.047880\n",
      "train loss:   1.124678\n",
      "train loss:   1.036566\n",
      "train loss:   1.473334\n",
      "train loss:   1.361768\n",
      "train loss:   1.131599\n",
      "train loss:   0.920766\n",
      "train loss:   0.795026\n",
      "train loss:   1.221574\n",
      "train loss:   1.206449\n",
      "train loss:   1.115483\n",
      "train loss:   1.477081\n",
      "train loss:   1.228906\n",
      "train loss:   1.399758\n",
      "train loss:   0.906423\n",
      "train loss:   1.350990\n",
      "train loss:   0.976949\n",
      "train loss:   0.950013\n",
      "train loss:   1.094767\n",
      "train loss:   1.261166\n",
      "train loss:   1.019215\n",
      "train loss:   1.072256\n",
      "train loss:   1.241173\n",
      "train loss:   0.820186\n",
      "train loss:   1.062720\n",
      "train loss:   0.778329\n",
      "train loss:   1.028078\n",
      "train loss:   1.069888\n",
      "train loss:   1.177032\n",
      "train loss:   0.748001\n",
      "train loss:   0.947139\n",
      "########### epoch 21 ###########\n",
      "########### loop 3800 ###########\n",
      "test loss:   0.345485   test accuracy:   0.916667\n",
      "########### loop 3800 ###########\n",
      "train loss:   1.100935\n",
      "train loss:   1.271172\n",
      "train loss:   1.313507\n",
      "train loss:   1.045108\n",
      "train loss:   1.341887\n",
      "train loss:   1.077868\n",
      "train loss:   1.547340\n",
      "train loss:   1.234371\n",
      "train loss:   1.093988\n",
      "train loss:   1.139709\n",
      "train loss:   0.995527\n",
      "train loss:   1.166096\n",
      "train loss:   0.956295\n",
      "train loss:   1.457666\n",
      "train loss:   0.994575\n",
      "train loss:   0.837044\n",
      "train loss:   1.484172\n",
      "train loss:   1.087846\n",
      "train loss:   0.950998\n",
      "train loss:   1.058990\n",
      "train loss:   1.106143\n",
      "train loss:   1.312841\n",
      "train loss:   0.949220\n",
      "train loss:   1.021180\n",
      "train loss:   0.870231\n",
      "train loss:   0.744054\n",
      "train loss:   0.938280\n",
      "train loss:   1.142766\n",
      "train loss:   1.098091\n",
      "train loss:   1.323974\n",
      "train loss:   0.935409\n",
      "train loss:   1.192215\n",
      "train loss:   1.150798\n",
      "train loss:   1.372430\n",
      "train loss:   1.213108\n",
      "train loss:   1.361861\n",
      "train loss:   1.284085\n",
      "train loss:   0.881609\n",
      "train loss:   1.047759\n",
      "train loss:   1.388851\n",
      "train loss:   0.938279\n",
      "train loss:   1.055238\n",
      "train loss:   1.131121\n",
      "train loss:   1.139340\n",
      "train loss:   1.277254\n",
      "train loss:   1.054673\n",
      "train loss:   1.294688\n",
      "train loss:   1.218948\n",
      "train loss:   0.726910\n",
      "train loss:   1.183803\n",
      "########### epoch 21 ###########\n",
      "########### loop 3850 ###########\n",
      "test loss:   0.334485   test accuracy:   0.958333\n",
      "########### loop 3850 ###########\n",
      "train loss:   0.915694\n",
      "train loss:   0.800138\n",
      "train loss:   0.733734\n",
      "train loss:   0.962882\n",
      "train loss:   1.191787\n",
      "train loss:   1.021260\n",
      "train loss:   1.131119\n",
      "train loss:   1.354234\n",
      "train loss:   1.436299\n",
      "train loss:   1.395553\n",
      "train loss:   1.135501\n",
      "train loss:   1.011180\n",
      "train loss:   0.859377\n",
      "train loss:   1.283486\n",
      "train loss:   1.375677\n",
      "train loss:   0.954151\n",
      "train loss:   0.649012\n",
      "train loss:   1.175378\n",
      "train loss:   1.159545\n",
      "train loss:   1.037658\n",
      "train loss:   1.193284\n",
      "train loss:   1.237086\n",
      "train loss:   0.838353\n",
      "train loss:   1.332488\n",
      "train loss:   0.870306\n",
      "train loss:   1.189543\n",
      "train loss:   0.705296\n",
      "train loss:   1.008547\n",
      "train loss:   0.931281\n",
      "train loss:   1.153537\n",
      "train loss:   1.168693\n",
      "train loss:   0.898747\n",
      "train loss:   0.902191\n",
      "train loss:   1.098796\n",
      "train loss:   1.452109\n",
      "train loss:   1.165083\n",
      "train loss:   1.009343\n",
      "train loss:   1.038750\n",
      "train loss:   1.422821\n",
      "train loss:   1.117806\n",
      "train loss:   0.842629\n",
      "train loss:   0.860073\n",
      "train loss:   1.545728\n",
      "train loss:   0.821713\n",
      "train loss:   1.606096\n",
      "train loss:   1.261067\n",
      "train loss:   1.478153\n",
      "train loss:   1.145762\n",
      "train loss:   1.042531\n",
      "train loss:   1.372856\n",
      "########### epoch 21 ###########\n",
      "########### loop 3900 ###########\n",
      "test loss:   0.219798   test accuracy:   1.000000\n",
      "########### loop 3900 ###########\n",
      "train loss:   1.259082\n",
      "train loss:   1.491122\n",
      "train loss:   1.146279\n",
      "train loss:   0.854215\n",
      "train loss:   1.180205\n",
      "train loss:   1.104579\n",
      "train loss:   1.144168\n",
      "train loss:   1.352239\n",
      "train loss:   0.916352\n",
      "train loss:   1.260576\n",
      "train loss:   1.077330\n",
      "train loss:   0.971599\n",
      "train loss:   1.293967\n",
      "train loss:   0.974217\n",
      "train loss:   0.819774\n",
      "train loss:   0.858068\n",
      "train loss:   1.359915\n",
      "train loss:   1.050263\n",
      "train loss:   0.724243\n",
      "train loss:   0.792656\n",
      "train loss:   1.458401\n",
      "train loss:   1.217308\n",
      "train loss:   1.420012\n",
      "train loss:   0.788628\n",
      "train loss:   0.896468\n",
      "train loss:   1.177079\n",
      "train loss:   1.489823\n",
      "train loss:   1.064995\n",
      "train loss:   1.400132\n",
      "train loss:   0.892552\n",
      "train loss:   1.008680\n",
      "train loss:   0.726306\n",
      "train loss:   1.095576\n",
      "train loss:   1.022155\n",
      "train loss:   1.401823\n",
      "train loss:   1.002136\n",
      "train loss:   1.317809\n",
      "train loss:   1.167348\n",
      "train loss:   1.009243\n",
      "train loss:   0.889651\n",
      "train loss:   0.835656\n",
      "train loss:   1.006857\n",
      "train loss:   0.856480\n",
      "train loss:   1.394889\n",
      "train loss:   1.023138\n",
      "train loss:   1.213218\n",
      "train loss:   1.104837\n",
      "train loss:   0.700448\n",
      "train loss:   0.923453\n",
      "train loss:   1.149702\n",
      "########### epoch 22 ###########\n",
      "########### loop 3950 ###########\n",
      "test loss:   0.369517   test accuracy:   0.916667\n",
      "########### loop 3950 ###########\n",
      "train loss:   1.237713\n",
      "train loss:   1.144490\n",
      "train loss:   1.270995\n",
      "train loss:   0.484696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.962865\n",
      "train loss:   0.980324\n",
      "train loss:   1.430119\n",
      "train loss:   1.348671\n",
      "train loss:   1.020383\n",
      "train loss:   1.149929\n",
      "train loss:   1.041618\n",
      "train loss:   1.277151\n",
      "train loss:   1.224586\n",
      "train loss:   0.925467\n",
      "train loss:   1.287769\n",
      "train loss:   0.794300\n",
      "train loss:   1.147283\n",
      "train loss:   0.705320\n",
      "train loss:   1.005182\n",
      "train loss:   0.920864\n",
      "train loss:   1.155592\n",
      "train loss:   1.227952\n",
      "train loss:   0.959780\n",
      "train loss:   0.890892\n",
      "train loss:   1.358622\n",
      "train loss:   1.084751\n",
      "train loss:   1.228656\n",
      "train loss:   1.155771\n",
      "train loss:   1.177484\n",
      "train loss:   0.934632\n",
      "train loss:   1.354479\n",
      "train loss:   1.116761\n",
      "train loss:   0.823437\n",
      "train loss:   0.912789\n",
      "train loss:   1.110295\n",
      "train loss:   1.022258\n",
      "train loss:   0.637869\n",
      "train loss:   1.264815\n",
      "train loss:   1.286183\n",
      "train loss:   1.197186\n",
      "train loss:   0.978360\n",
      "train loss:   1.218604\n",
      "train loss:   1.244160\n",
      "train loss:   0.964372\n",
      "train loss:   0.859825\n",
      "train loss:   1.157975\n",
      "train loss:   1.101899\n",
      "train loss:   1.003592\n",
      "train loss:   1.105419\n",
      "train loss:   1.051580\n",
      "########### epoch 22 ###########\n",
      "########### loop 4000 ###########\n",
      "test loss:   0.638034   test accuracy:   0.875000\n",
      "########### loop 4000 ###########\n",
      "train loss:   1.025567\n",
      "train loss:   0.915317\n",
      "train loss:   0.533323\n",
      "train loss:   0.960396\n",
      "train loss:   0.798104\n",
      "train loss:   1.183815\n",
      "train loss:   0.734244\n",
      "train loss:   1.099414\n",
      "train loss:   1.085719\n",
      "train loss:   0.673899\n",
      "train loss:   1.150747\n",
      "train loss:   1.060735\n",
      "train loss:   1.043278\n",
      "train loss:   0.986131\n",
      "train loss:   1.037776\n",
      "train loss:   0.674908\n",
      "train loss:   0.894784\n",
      "train loss:   1.390993\n",
      "train loss:   1.078680\n",
      "train loss:   1.264707\n",
      "train loss:   1.071532\n",
      "train loss:   1.115312\n",
      "train loss:   0.946889\n",
      "train loss:   1.071287\n",
      "train loss:   0.950854\n",
      "train loss:   0.926842\n",
      "train loss:   1.305542\n",
      "train loss:   1.212136\n",
      "train loss:   0.982689\n",
      "train loss:   0.919059\n",
      "train loss:   1.054162\n",
      "train loss:   0.973340\n",
      "train loss:   1.526577\n",
      "train loss:   1.059064\n",
      "train loss:   0.981057\n",
      "train loss:   1.104813\n",
      "train loss:   1.378014\n",
      "train loss:   0.921116\n",
      "train loss:   0.962154\n",
      "train loss:   1.100740\n",
      "train loss:   0.941581\n",
      "train loss:   0.835392\n",
      "train loss:   1.032560\n",
      "train loss:   1.007726\n",
      "train loss:   0.954663\n",
      "train loss:   1.193814\n",
      "train loss:   0.968190\n",
      "train loss:   1.481122\n",
      "train loss:   0.793574\n",
      "train loss:   0.986203\n",
      "########### epoch 22 ###########\n",
      "########### loop 4050 ###########\n",
      "test loss:   0.373364   test accuracy:   1.000000\n",
      "########### loop 4050 ###########\n",
      "train loss:   0.572840\n",
      "train loss:   0.939774\n",
      "train loss:   0.857428\n",
      "train loss:   0.944276\n",
      "train loss:   0.926211\n",
      "train loss:   0.973027\n",
      "train loss:   1.189255\n",
      "train loss:   0.888367\n",
      "train loss:   1.233330\n",
      "train loss:   1.300402\n",
      "train loss:   1.038211\n",
      "train loss:   1.013032\n",
      "train loss:   0.902373\n",
      "train loss:   1.134313\n",
      "train loss:   1.052862\n",
      "train loss:   0.653873\n",
      "train loss:   0.971417\n",
      "train loss:   1.529402\n",
      "train loss:   0.818358\n",
      "train loss:   1.215264\n",
      "train loss:   0.945760\n",
      "train loss:   0.971520\n",
      "train loss:   1.125483\n",
      "train loss:   1.271884\n",
      "train loss:   1.073125\n",
      "train loss:   1.224222\n",
      "train loss:   1.236448\n",
      "train loss:   0.876849\n",
      "train loss:   1.034679\n",
      "train loss:   1.178296\n",
      "train loss:   0.454144\n",
      "train loss:   1.390460\n",
      "train loss:   1.267426\n",
      "train loss:   1.285361\n",
      "train loss:   1.066205\n",
      "train loss:   1.213170\n",
      "train loss:   1.314765\n",
      "train loss:   1.180502\n",
      "train loss:   0.905408\n",
      "train loss:   1.155264\n",
      "train loss:   1.022431\n",
      "train loss:   1.259732\n",
      "train loss:   1.021651\n",
      "train loss:   1.210508\n",
      "train loss:   1.133395\n",
      "train loss:   1.082283\n",
      "train loss:   1.082386\n",
      "train loss:   1.144688\n",
      "train loss:   1.013639\n",
      "train loss:   0.798660\n",
      "########### epoch 22 ###########\n",
      "########### loop 4100 ###########\n",
      "test loss:   0.295007   test accuracy:   0.958333\n",
      "########### loop 4100 ###########\n",
      "train loss:   1.092609\n",
      "train loss:   0.925659\n",
      "train loss:   1.305419\n",
      "train loss:   0.980417\n",
      "train loss:   1.176655\n",
      "train loss:   1.048794\n",
      "train loss:   1.209757\n",
      "train loss:   1.063443\n",
      "train loss:   0.902117\n",
      "train loss:   1.058973\n",
      "train loss:   1.274204\n",
      "train loss:   1.193499\n",
      "train loss:   1.103878\n",
      "train loss:   1.045574\n",
      "train loss:   1.041684\n",
      "train loss:   0.937843\n",
      "train loss:   0.991677\n",
      "train loss:   1.000620\n",
      "train loss:   1.331707\n",
      "train loss:   1.016517\n",
      "train loss:   1.300361\n",
      "train loss:   1.359704\n",
      "train loss:   0.826537\n",
      "train loss:   1.139941\n",
      "train loss:   0.875339\n",
      "train loss:   0.988443\n",
      "train loss:   0.805593\n",
      "train loss:   0.980695\n",
      "train loss:   1.375876\n",
      "train loss:   0.847146\n",
      "train loss:   1.265650\n",
      "train loss:   1.312594\n",
      "train loss:   1.389361\n",
      "train loss:   0.941994\n",
      "train loss:   0.715341\n",
      "train loss:   1.348436\n",
      "train loss:   1.122460\n",
      "train loss:   1.378060\n",
      "train loss:   1.236615\n",
      "train loss:   1.279438\n",
      "train loss:   1.034345\n",
      "train loss:   1.164903\n",
      "train loss:   1.031505\n",
      "train loss:   0.925383\n",
      "train loss:   1.258420\n",
      "train loss:   0.768349\n",
      "train loss:   0.750975\n",
      "train loss:   0.950214\n",
      "train loss:   1.162079\n",
      "train loss:   1.017317\n",
      "########### epoch 23 ###########\n",
      "########### loop 4150 ###########\n",
      "test loss:   0.186683   test accuracy:   1.000000\n",
      "########### loop 4150 ###########\n",
      "train loss:   1.226679\n",
      "train loss:   1.240235\n",
      "train loss:   1.490610\n",
      "train loss:   0.748376\n",
      "train loss:   1.055266\n",
      "train loss:   1.178817\n",
      "train loss:   0.971215\n",
      "train loss:   0.967516\n",
      "train loss:   1.383251\n",
      "train loss:   0.980841\n",
      "train loss:   1.229808\n",
      "train loss:   0.990097\n",
      "train loss:   1.295172\n",
      "train loss:   1.406478\n",
      "train loss:   0.863052\n",
      "train loss:   0.943982\n",
      "train loss:   1.133578\n",
      "train loss:   1.135571\n",
      "train loss:   1.405087\n",
      "train loss:   0.850504\n",
      "train loss:   1.224695\n",
      "train loss:   1.184414\n",
      "train loss:   0.975497\n",
      "train loss:   0.816705\n",
      "train loss:   1.417335\n",
      "train loss:   0.630775\n",
      "train loss:   1.035819\n",
      "train loss:   1.018159\n",
      "train loss:   1.037683\n",
      "train loss:   1.027060\n",
      "train loss:   1.218466\n",
      "train loss:   0.849372\n",
      "train loss:   1.032958\n",
      "train loss:   0.793529\n",
      "train loss:   1.381265\n",
      "train loss:   0.832562\n",
      "train loss:   0.648887\n",
      "train loss:   0.935549\n",
      "train loss:   1.086484\n",
      "train loss:   1.331547\n",
      "train loss:   1.100731\n",
      "train loss:   1.169879\n",
      "train loss:   0.940639\n",
      "train loss:   1.362402\n",
      "train loss:   0.920436\n",
      "train loss:   1.439548\n",
      "train loss:   1.021710\n",
      "train loss:   0.747994\n",
      "train loss:   0.686893\n",
      "train loss:   1.014114\n",
      "########### epoch 23 ###########\n",
      "########### loop 4200 ###########\n",
      "test loss:   0.255304   test accuracy:   0.958333\n",
      "########### loop 4200 ###########\n",
      "train loss:   1.418038\n",
      "train loss:   1.124080\n",
      "train loss:   1.185868\n",
      "train loss:   1.112049\n",
      "train loss:   0.966393\n",
      "train loss:   0.737870\n",
      "train loss:   1.073796\n",
      "train loss:   1.182375\n",
      "train loss:   0.875024\n",
      "train loss:   0.810253\n",
      "train loss:   0.676598\n",
      "train loss:   1.254824\n",
      "train loss:   1.363759\n",
      "train loss:   0.872985\n",
      "train loss:   0.911757\n",
      "train loss:   1.150798\n",
      "train loss:   0.980449\n",
      "train loss:   1.088825\n",
      "train loss:   1.350781\n",
      "train loss:   0.867687\n",
      "train loss:   0.942307\n",
      "train loss:   1.188671\n",
      "train loss:   1.396411\n",
      "train loss:   0.989062\n",
      "train loss:   1.084718\n",
      "train loss:   0.993531\n",
      "train loss:   0.927600\n",
      "train loss:   1.035500\n",
      "train loss:   1.135710\n",
      "train loss:   1.038502\n",
      "train loss:   1.250229\n",
      "train loss:   1.240840\n",
      "train loss:   0.867681\n",
      "train loss:   1.121963\n",
      "train loss:   0.993983\n",
      "train loss:   0.720212\n",
      "train loss:   1.289090\n",
      "train loss:   0.986176\n",
      "train loss:   1.198103\n",
      "train loss:   1.228550\n",
      "train loss:   1.194075\n",
      "train loss:   0.912919\n",
      "train loss:   1.222326\n",
      "train loss:   1.106881\n",
      "train loss:   1.096969\n",
      "train loss:   1.085935\n",
      "train loss:   0.779961\n",
      "train loss:   0.741013\n",
      "train loss:   1.230330\n",
      "train loss:   1.243677\n",
      "########### epoch 23 ###########\n",
      "########### loop 4250 ###########\n",
      "test loss:   0.369243   test accuracy:   0.958333\n",
      "########### loop 4250 ###########\n",
      "train loss:   1.190421\n",
      "train loss:   0.991218\n",
      "train loss:   1.073467\n",
      "train loss:   1.244666\n",
      "train loss:   1.231845\n",
      "train loss:   1.190097\n",
      "train loss:   1.018746\n",
      "train loss:   0.977442\n",
      "train loss:   1.259566\n",
      "train loss:   0.913723\n",
      "train loss:   1.372003\n",
      "train loss:   0.959302\n",
      "train loss:   1.062410\n",
      "train loss:   1.075358\n",
      "train loss:   1.076947\n",
      "train loss:   1.180638\n",
      "train loss:   1.005192\n",
      "train loss:   0.830116\n",
      "train loss:   0.999827\n",
      "train loss:   1.308211\n",
      "train loss:   1.126831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.960778\n",
      "train loss:   0.983381\n",
      "train loss:   0.775685\n",
      "train loss:   0.922483\n",
      "train loss:   1.204159\n",
      "train loss:   1.178888\n",
      "train loss:   0.851937\n",
      "train loss:   1.144937\n",
      "train loss:   0.945047\n",
      "train loss:   0.863508\n",
      "train loss:   0.727051\n",
      "train loss:   1.227951\n",
      "train loss:   1.087819\n",
      "train loss:   1.095171\n",
      "train loss:   1.094245\n",
      "train loss:   0.799652\n",
      "train loss:   1.097208\n",
      "train loss:   0.897729\n",
      "train loss:   0.874516\n",
      "train loss:   0.798954\n",
      "train loss:   1.526158\n",
      "train loss:   0.958104\n",
      "train loss:   0.849534\n",
      "train loss:   1.431839\n",
      "train loss:   1.049027\n",
      "train loss:   1.050311\n",
      "train loss:   0.840859\n",
      "train loss:   1.200760\n",
      "train loss:   1.111360\n",
      "########### epoch 23 ###########\n",
      "########### loop 4300 ###########\n",
      "test loss:   0.426013   test accuracy:   0.958333\n",
      "########### loop 4300 ###########\n",
      "train loss:   0.925057\n",
      "train loss:   1.307474\n",
      "train loss:   1.113896\n",
      "train loss:   1.292184\n",
      "train loss:   1.188555\n",
      "train loss:   0.693134\n",
      "train loss:   0.754866\n",
      "train loss:   1.080656\n",
      "train loss:   0.927200\n",
      "train loss:   1.085738\n",
      "train loss:   0.968883\n",
      "train loss:   1.260792\n",
      "train loss:   0.716523\n",
      "train loss:   1.007106\n",
      "train loss:   0.538727\n",
      "train loss:   1.263381\n",
      "train loss:   1.152730\n",
      "train loss:   1.296431\n",
      "train loss:   1.278052\n",
      "train loss:   1.168995\n",
      "train loss:   1.081496\n",
      "train loss:   1.046958\n",
      "train loss:   0.903453\n",
      "train loss:   1.197429\n",
      "train loss:   1.024279\n",
      "train loss:   1.402275\n",
      "train loss:   0.716755\n",
      "train loss:   1.045905\n",
      "train loss:   1.019437\n",
      "train loss:   1.156817\n",
      "train loss:   1.147899\n",
      "train loss:   0.934422\n",
      "train loss:   1.266728\n",
      "train loss:   1.099026\n",
      "train loss:   1.016010\n",
      "train loss:   0.998205\n",
      "train loss:   1.142694\n",
      "train loss:   1.174744\n",
      "train loss:   1.441322\n",
      "train loss:   1.365434\n",
      "train loss:   1.005119\n",
      "train loss:   1.144021\n",
      "train loss:   1.289760\n",
      "train loss:   1.100227\n",
      "train loss:   0.984014\n",
      "train loss:   1.774675\n",
      "train loss:   1.155588\n",
      "train loss:   1.120830\n",
      "train loss:   1.084092\n",
      "train loss:   0.962978\n",
      "########### epoch 24 ###########\n",
      "########### loop 4350 ###########\n",
      "test loss:   0.301296   test accuracy:   0.958333\n",
      "########### loop 4350 ###########\n",
      "train loss:   1.332041\n",
      "train loss:   1.227736\n",
      "train loss:   1.034948\n",
      "train loss:   0.907762\n",
      "train loss:   1.413120\n",
      "train loss:   1.312637\n",
      "train loss:   1.169806\n",
      "train loss:   1.494893\n",
      "train loss:   1.026660\n",
      "train loss:   1.434974\n",
      "train loss:   1.284389\n",
      "train loss:   0.756620\n",
      "train loss:   1.222199\n",
      "train loss:   1.436220\n",
      "train loss:   1.219615\n",
      "train loss:   1.403773\n",
      "train loss:   0.801018\n",
      "train loss:   1.493478\n",
      "train loss:   1.245139\n",
      "train loss:   0.997640\n",
      "train loss:   1.374746\n",
      "train loss:   0.892302\n",
      "train loss:   1.076770\n",
      "train loss:   0.661589\n",
      "train loss:   0.948372\n",
      "train loss:   0.841448\n",
      "train loss:   1.435144\n",
      "train loss:   1.054002\n",
      "train loss:   1.168182\n",
      "train loss:   1.214854\n",
      "train loss:   0.652592\n",
      "train loss:   1.041552\n",
      "train loss:   1.374730\n",
      "train loss:   1.476887\n",
      "train loss:   1.326215\n",
      "train loss:   1.025457\n",
      "train loss:   0.848120\n",
      "train loss:   1.269734\n",
      "train loss:   0.809786\n",
      "train loss:   0.868615\n",
      "train loss:   1.066993\n",
      "train loss:   1.033538\n",
      "train loss:   0.972577\n",
      "train loss:   0.754323\n",
      "train loss:   1.156000\n",
      "train loss:   0.747302\n",
      "train loss:   1.019983\n",
      "train loss:   1.095377\n",
      "train loss:   0.937780\n",
      "train loss:   1.391389\n",
      "########### epoch 24 ###########\n",
      "########### loop 4400 ###########\n",
      "test loss:   0.209966   test accuracy:   1.000000\n",
      "########### loop 4400 ###########\n",
      "train loss:   1.322271\n",
      "train loss:   0.869973\n",
      "train loss:   1.142635\n",
      "train loss:   1.309320\n",
      "train loss:   1.231362\n",
      "train loss:   0.914813\n",
      "train loss:   1.094024\n",
      "train loss:   0.947574\n",
      "train loss:   0.876624\n",
      "train loss:   0.958739\n",
      "train loss:   0.735657\n",
      "train loss:   0.898361\n",
      "train loss:   0.963892\n",
      "train loss:   1.204261\n",
      "train loss:   0.982647\n",
      "train loss:   1.051312\n",
      "train loss:   1.084903\n",
      "train loss:   1.064909\n",
      "train loss:   0.905145\n",
      "train loss:   0.847956\n",
      "train loss:   0.861057\n",
      "train loss:   1.003527\n",
      "train loss:   0.891647\n",
      "train loss:   0.877470\n",
      "train loss:   0.871933\n",
      "train loss:   0.821142\n",
      "train loss:   1.210768\n",
      "train loss:   1.217240\n",
      "train loss:   0.669934\n",
      "train loss:   1.043305\n",
      "train loss:   1.240007\n",
      "train loss:   0.998301\n",
      "train loss:   1.451962\n",
      "train loss:   1.051088\n",
      "train loss:   0.942310\n",
      "train loss:   1.250485\n",
      "train loss:   1.134274\n",
      "train loss:   1.233600\n",
      "train loss:   0.999343\n",
      "train loss:   0.848822\n",
      "train loss:   0.880142\n",
      "train loss:   0.942202\n",
      "train loss:   1.268331\n",
      "train loss:   1.169094\n",
      "train loss:   0.897828\n",
      "train loss:   1.341238\n",
      "train loss:   1.297147\n",
      "train loss:   1.222905\n",
      "train loss:   1.259297\n",
      "train loss:   0.930092\n",
      "########### epoch 24 ###########\n",
      "########### loop 4450 ###########\n",
      "test loss:   0.280780   test accuracy:   0.958333\n",
      "########### loop 4450 ###########\n",
      "train loss:   0.831226\n",
      "train loss:   1.298040\n",
      "train loss:   0.974882\n",
      "train loss:   1.380744\n",
      "train loss:   0.964006\n",
      "train loss:   1.270974\n",
      "train loss:   0.960594\n",
      "train loss:   1.012841\n",
      "train loss:   1.060825\n",
      "train loss:   1.244500\n",
      "train loss:   0.901136\n",
      "train loss:   1.396020\n",
      "train loss:   0.982601\n",
      "train loss:   1.121942\n",
      "train loss:   1.022296\n",
      "train loss:   1.132752\n",
      "train loss:   0.956457\n",
      "train loss:   1.391251\n",
      "train loss:   1.170343\n",
      "train loss:   1.047055\n",
      "train loss:   0.865640\n",
      "train loss:   0.942203\n",
      "train loss:   1.240213\n",
      "train loss:   1.105641\n",
      "train loss:   0.973590\n",
      "train loss:   0.664280\n",
      "train loss:   1.001057\n",
      "train loss:   1.068154\n",
      "train loss:   1.148852\n",
      "train loss:   0.972395\n",
      "train loss:   1.085830\n",
      "train loss:   0.667955\n",
      "train loss:   1.188797\n",
      "train loss:   1.127319\n",
      "train loss:   1.186931\n",
      "train loss:   1.223644\n",
      "train loss:   0.934112\n",
      "train loss:   0.897729\n",
      "train loss:   0.824263\n",
      "train loss:   1.619466\n",
      "train loss:   0.954573\n",
      "train loss:   0.921292\n",
      "train loss:   1.570065\n",
      "train loss:   1.239012\n",
      "train loss:   1.229391\n",
      "train loss:   0.856831\n",
      "train loss:   0.952777\n",
      "train loss:   1.040366\n",
      "train loss:   1.100492\n",
      "train loss:   0.999741\n",
      "########### epoch 24 ###########\n",
      "########### loop 4500 ###########\n",
      "test loss:   0.276509   test accuracy:   0.958333\n",
      "########### loop 4500 ###########\n",
      "train loss:   1.369337\n",
      "train loss:   1.124549\n",
      "train loss:   0.864185\n",
      "train loss:   1.072445\n",
      "train loss:   1.446014\n",
      "train loss:   1.138579\n",
      "train loss:   0.978773\n",
      "train loss:   1.395151\n",
      "train loss:   1.082544\n",
      "train loss:   0.972548\n",
      "train loss:   1.000299\n",
      "train loss:   1.175920\n",
      "train loss:   1.223708\n",
      "train loss:   1.116436\n",
      "train loss:   1.443811\n",
      "train loss:   1.334073\n",
      "train loss:   1.077038\n",
      "train loss:   1.053531\n",
      "train loss:   1.122437\n",
      "train loss:   1.433661\n",
      "train loss:   1.015533\n",
      "train loss:   1.044899\n",
      "train loss:   0.979385\n",
      "train loss:   0.998927\n",
      "train loss:   0.876140\n",
      "train loss:   0.939892\n",
      "train loss:   1.309018\n",
      "train loss:   1.161859\n",
      "train loss:   1.086149\n",
      "train loss:   1.003182\n",
      "train loss:   1.180864\n",
      "train loss:   1.218495\n",
      "train loss:   0.975146\n",
      "train loss:   1.263927\n",
      "train loss:   0.969888\n",
      "train loss:   0.981436\n",
      "train loss:   1.204936\n",
      "train loss:   0.796655\n",
      "train loss:   0.914632\n",
      "train loss:   1.088767\n",
      "train loss:   0.692167\n",
      "train loss:   0.673059\n",
      "train loss:   0.932991\n",
      "train loss:   0.866811\n",
      "train loss:   0.959530\n",
      "train loss:   0.844000\n",
      "train loss:   1.015849\n",
      "train loss:   1.300924\n",
      "train loss:   0.758660\n",
      "train loss:   1.349039\n",
      "########### epoch 25 ###########\n",
      "########### loop 4550 ###########\n",
      "test loss:   0.327289   test accuracy:   0.916667\n",
      "########### loop 4550 ###########\n",
      "train loss:   1.043443\n",
      "train loss:   1.267632\n",
      "train loss:   1.262727\n",
      "train loss:   1.244976\n",
      "train loss:   1.046183\n",
      "train loss:   1.234684\n",
      "train loss:   1.273355\n",
      "train loss:   1.029618\n",
      "train loss:   0.995596\n",
      "train loss:   0.857241\n",
      "train loss:   1.053269\n",
      "train loss:   1.229537\n",
      "train loss:   1.067325\n",
      "train loss:   1.360079\n",
      "train loss:   1.009825\n",
      "train loss:   1.030951\n",
      "train loss:   1.060261\n",
      "train loss:   1.151295\n",
      "train loss:   1.141444\n",
      "train loss:   0.991555\n",
      "train loss:   0.943434\n",
      "train loss:   0.968408\n",
      "train loss:   0.915067\n",
      "train loss:   0.876498\n",
      "train loss:   0.904178\n",
      "train loss:   0.957572\n",
      "train loss:   1.357241\n",
      "train loss:   0.965598\n",
      "train loss:   0.469171\n",
      "train loss:   0.807140\n",
      "train loss:   0.925392\n",
      "train loss:   0.769357\n",
      "train loss:   1.340865\n",
      "train loss:   1.355448\n",
      "train loss:   0.840798\n",
      "train loss:   1.429862\n",
      "train loss:   0.845728\n",
      "train loss:   1.190817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.878069\n",
      "train loss:   1.371142\n",
      "train loss:   1.030530\n",
      "train loss:   1.127231\n",
      "train loss:   0.965464\n",
      "train loss:   0.769945\n",
      "train loss:   0.876795\n",
      "train loss:   1.111686\n",
      "train loss:   0.924104\n",
      "train loss:   1.019575\n",
      "train loss:   1.149498\n",
      "train loss:   0.983634\n",
      "########### epoch 25 ###########\n",
      "########### loop 4600 ###########\n",
      "test loss:   0.390225   test accuracy:   0.958333\n",
      "########### loop 4600 ###########\n",
      "train loss:   0.860170\n",
      "train loss:   1.078058\n",
      "train loss:   1.292065\n",
      "train loss:   1.079609\n",
      "train loss:   1.320079\n",
      "train loss:   1.133827\n",
      "train loss:   1.059300\n",
      "train loss:   1.125665\n",
      "train loss:   0.965530\n",
      "train loss:   0.974984\n",
      "train loss:   1.274042\n",
      "train loss:   1.165833\n",
      "train loss:   1.121507\n",
      "train loss:   1.087717\n",
      "train loss:   0.837120\n",
      "train loss:   0.958433\n",
      "train loss:   1.002204\n",
      "train loss:   1.067294\n",
      "train loss:   1.184256\n",
      "train loss:   1.337332\n",
      "train loss:   1.193517\n",
      "train loss:   1.012173\n",
      "train loss:   1.516895\n",
      "train loss:   1.314363\n",
      "train loss:   1.232310\n",
      "train loss:   0.782523\n",
      "train loss:   0.998998\n",
      "train loss:   1.464986\n",
      "train loss:   0.992079\n",
      "train loss:   1.068607\n",
      "train loss:   1.128960\n",
      "train loss:   1.006521\n",
      "train loss:   1.175565\n",
      "train loss:   1.220193\n",
      "train loss:   1.007296\n",
      "train loss:   0.718564\n",
      "train loss:   1.229890\n",
      "train loss:   0.888178\n",
      "train loss:   1.459063\n",
      "train loss:   1.080230\n",
      "train loss:   0.759978\n",
      "train loss:   1.145789\n",
      "train loss:   1.064774\n",
      "train loss:   0.882296\n",
      "train loss:   1.492720\n",
      "train loss:   1.127982\n",
      "train loss:   1.024089\n",
      "train loss:   1.138066\n",
      "train loss:   0.915015\n",
      "train loss:   0.760094\n",
      "########### epoch 25 ###########\n",
      "########### loop 4650 ###########\n",
      "test loss:   0.472823   test accuracy:   0.875000\n",
      "########### loop 4650 ###########\n",
      "train loss:   0.942225\n",
      "train loss:   1.353538\n",
      "train loss:   1.310530\n",
      "train loss:   1.061306\n",
      "train loss:   1.486709\n",
      "train loss:   1.182489\n",
      "train loss:   0.936321\n",
      "train loss:   1.202867\n",
      "train loss:   1.521722\n",
      "train loss:   1.194699\n",
      "train loss:   1.015347\n",
      "train loss:   0.992089\n",
      "train loss:   0.838478\n",
      "train loss:   0.952520\n",
      "train loss:   0.832339\n",
      "train loss:   0.817711\n",
      "train loss:   1.128791\n",
      "train loss:   1.150881\n",
      "train loss:   0.935572\n",
      "train loss:   0.940355\n",
      "train loss:   0.662684\n",
      "train loss:   1.049898\n",
      "train loss:   1.103397\n",
      "train loss:   0.705220\n",
      "train loss:   1.195262\n",
      "train loss:   1.167067\n",
      "train loss:   1.154718\n",
      "train loss:   1.126351\n",
      "train loss:   1.151952\n",
      "train loss:   1.453472\n",
      "train loss:   1.223730\n",
      "train loss:   1.163734\n",
      "train loss:   0.715780\n",
      "train loss:   1.183992\n",
      "train loss:   1.254861\n",
      "train loss:   1.013482\n",
      "train loss:   1.490948\n",
      "train loss:   1.242720\n",
      "train loss:   0.970791\n",
      "train loss:   1.037178\n",
      "train loss:   1.306859\n",
      "train loss:   1.217084\n",
      "train loss:   1.147264\n",
      "train loss:   1.355400\n",
      "train loss:   0.843483\n",
      "train loss:   0.958364\n",
      "train loss:   0.918858\n",
      "train loss:   1.226748\n",
      "train loss:   0.995541\n",
      "train loss:   1.070813\n",
      "########### epoch 26 ###########\n",
      "########### loop 4700 ###########\n",
      "test loss:   0.376227   test accuracy:   0.875000\n",
      "########### loop 4700 ###########\n",
      "train loss:   1.019330\n",
      "train loss:   1.202519\n",
      "train loss:   1.116849\n",
      "train loss:   1.029895\n",
      "train loss:   1.033795\n",
      "train loss:   1.104146\n",
      "train loss:   0.921663\n",
      "train loss:   0.971792\n",
      "train loss:   0.968684\n",
      "train loss:   1.337538\n",
      "train loss:   0.929493\n",
      "train loss:   0.906518\n",
      "train loss:   0.977250\n",
      "train loss:   1.192717\n",
      "train loss:   0.845294\n",
      "train loss:   1.342962\n",
      "train loss:   1.235237\n",
      "train loss:   0.583050\n",
      "train loss:   1.160684\n",
      "train loss:   1.003015\n",
      "train loss:   0.929029\n",
      "train loss:   1.158446\n",
      "train loss:   1.011464\n",
      "train loss:   0.697056\n",
      "train loss:   1.000703\n",
      "train loss:   0.937663\n",
      "train loss:   0.509779\n",
      "train loss:   1.071486\n",
      "train loss:   0.548894\n",
      "train loss:   0.615941\n",
      "train loss:   1.024862\n",
      "train loss:   0.951143\n",
      "train loss:   1.248008\n",
      "train loss:   1.445785\n",
      "train loss:   0.969830\n",
      "train loss:   1.219437\n",
      "train loss:   0.826559\n",
      "train loss:   1.085373\n",
      "train loss:   1.094804\n",
      "train loss:   0.704682\n",
      "train loss:   1.081931\n",
      "train loss:   0.961584\n",
      "train loss:   1.037787\n",
      "train loss:   1.496374\n",
      "train loss:   1.042696\n",
      "train loss:   0.993670\n",
      "train loss:   0.755498\n",
      "train loss:   0.922377\n",
      "train loss:   1.067977\n",
      "train loss:   1.139294\n",
      "########### epoch 26 ###########\n",
      "########### loop 4750 ###########\n",
      "test loss:   0.248847   test accuracy:   1.000000\n",
      "########### loop 4750 ###########\n",
      "train loss:   1.146444\n",
      "train loss:   1.252126\n",
      "train loss:   1.143508\n",
      "train loss:   1.113366\n",
      "train loss:   1.562661\n",
      "train loss:   1.520929\n",
      "train loss:   1.161001\n",
      "train loss:   0.793357\n",
      "train loss:   1.048909\n",
      "train loss:   1.022350\n",
      "train loss:   1.031708\n",
      "train loss:   1.324331\n",
      "train loss:   0.851927\n",
      "train loss:   1.085505\n",
      "train loss:   0.701942\n",
      "train loss:   1.192319\n",
      "train loss:   1.124953\n",
      "train loss:   1.058360\n",
      "train loss:   0.993781\n",
      "train loss:   1.213133\n",
      "train loss:   0.786336\n",
      "train loss:   1.563357\n",
      "train loss:   0.898333\n",
      "train loss:   1.237595\n",
      "train loss:   0.949384\n",
      "train loss:   1.120814\n",
      "train loss:   1.371125\n",
      "train loss:   1.140922\n",
      "train loss:   1.049264\n",
      "train loss:   1.194425\n",
      "train loss:   1.076605\n",
      "train loss:   1.013145\n",
      "train loss:   1.323705\n",
      "train loss:   1.171221\n",
      "train loss:   0.849868\n",
      "train loss:   1.065472\n",
      "train loss:   0.910492\n",
      "train loss:   0.828228\n",
      "train loss:   1.298725\n",
      "train loss:   1.280651\n",
      "train loss:   1.084387\n",
      "train loss:   0.800749\n",
      "train loss:   0.853509\n",
      "train loss:   0.969802\n",
      "train loss:   0.958748\n",
      "train loss:   0.753013\n",
      "train loss:   1.141575\n",
      "train loss:   0.925972\n",
      "train loss:   1.358772\n",
      "train loss:   0.908219\n",
      "########### epoch 26 ###########\n",
      "########### loop 4800 ###########\n",
      "test loss:   0.387463   test accuracy:   0.958333\n",
      "########### loop 4800 ###########\n",
      "train loss:   1.072768\n",
      "train loss:   0.841404\n",
      "train loss:   0.830378\n",
      "train loss:   0.961884\n",
      "train loss:   1.196411\n",
      "train loss:   0.942620\n",
      "train loss:   1.094837\n",
      "train loss:   1.081820\n",
      "train loss:   1.431939\n",
      "train loss:   1.193863\n",
      "train loss:   1.403538\n",
      "train loss:   1.146534\n",
      "train loss:   1.440926\n",
      "train loss:   1.097597\n",
      "train loss:   0.997636\n",
      "train loss:   0.957707\n",
      "train loss:   0.847528\n",
      "train loss:   0.953732\n",
      "train loss:   1.536558\n",
      "train loss:   1.228889\n",
      "train loss:   0.856275\n",
      "train loss:   0.745885\n",
      "train loss:   0.826871\n",
      "train loss:   1.176348\n",
      "train loss:   0.920888\n",
      "train loss:   0.793221\n",
      "train loss:   1.080562\n",
      "train loss:   1.165757\n",
      "train loss:   1.014623\n",
      "train loss:   1.123683\n",
      "train loss:   0.937141\n",
      "train loss:   1.029781\n",
      "train loss:   1.535713\n",
      "train loss:   0.964150\n",
      "train loss:   1.149700\n",
      "train loss:   1.056840\n",
      "train loss:   1.078948\n",
      "train loss:   1.462814\n",
      "train loss:   1.215202\n",
      "train loss:   0.832623\n",
      "train loss:   0.846623\n",
      "train loss:   0.775240\n",
      "train loss:   0.908327\n",
      "train loss:   0.912764\n",
      "train loss:   0.479411\n",
      "train loss:   0.923082\n",
      "train loss:   1.458087\n",
      "train loss:   0.671134\n",
      "train loss:   1.133589\n",
      "train loss:   0.847749\n",
      "########### epoch 26 ###########\n",
      "########### loop 4850 ###########\n",
      "test loss:   0.340289   test accuracy:   0.958333\n",
      "########### loop 4850 ###########\n",
      "train loss:   1.285224\n",
      "train loss:   1.253542\n",
      "train loss:   1.494391\n",
      "train loss:   1.088145\n",
      "train loss:   1.152738\n",
      "train loss:   1.354161\n",
      "train loss:   1.515272\n",
      "train loss:   0.839758\n",
      "train loss:   1.126965\n",
      "train loss:   1.046475\n",
      "train loss:   1.318687\n",
      "train loss:   1.054446\n",
      "train loss:   1.032771\n",
      "train loss:   1.191431\n",
      "train loss:   1.070771\n",
      "train loss:   1.234568\n",
      "train loss:   0.990402\n",
      "train loss:   1.167425\n",
      "train loss:   0.983532\n",
      "train loss:   1.406755\n",
      "train loss:   1.251069\n",
      "train loss:   0.988914\n",
      "train loss:   1.505666\n",
      "train loss:   1.003736\n",
      "train loss:   0.997556\n",
      "train loss:   1.164059\n",
      "train loss:   0.884851\n",
      "train loss:   1.328116\n",
      "train loss:   1.116808\n",
      "train loss:   0.977979\n",
      "train loss:   0.843322\n",
      "train loss:   0.915851\n",
      "train loss:   1.118031\n",
      "train loss:   1.050250\n",
      "train loss:   0.835323\n",
      "train loss:   0.751979\n",
      "train loss:   1.258311\n",
      "train loss:   1.399201\n",
      "train loss:   1.097239\n",
      "train loss:   1.196643\n",
      "train loss:   1.316377\n",
      "train loss:   0.782910\n",
      "train loss:   1.021361\n",
      "train loss:   1.497607\n",
      "train loss:   0.822388\n",
      "train loss:   1.482287\n",
      "train loss:   1.031102\n",
      "train loss:   0.934708\n",
      "train loss:   0.986173\n",
      "train loss:   0.896211\n",
      "########### epoch 27 ###########\n",
      "########### loop 4900 ###########\n",
      "test loss:   0.212270   test accuracy:   1.000000\n",
      "########### loop 4900 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.380018\n",
      "train loss:   1.114347\n",
      "train loss:   0.897975\n",
      "train loss:   0.960394\n",
      "train loss:   1.431781\n",
      "train loss:   1.295302\n",
      "train loss:   0.829550\n",
      "train loss:   1.400685\n",
      "train loss:   1.431256\n",
      "train loss:   1.228255\n",
      "train loss:   1.496534\n",
      "train loss:   1.287581\n",
      "train loss:   1.342091\n",
      "train loss:   1.034304\n",
      "train loss:   0.946767\n",
      "train loss:   1.096021\n",
      "train loss:   1.219749\n",
      "train loss:   0.992440\n",
      "train loss:   0.863159\n",
      "train loss:   0.992064\n",
      "train loss:   0.864270\n",
      "train loss:   1.657855\n",
      "train loss:   1.184812\n",
      "train loss:   0.815793\n",
      "train loss:   1.095057\n",
      "train loss:   0.985003\n",
      "train loss:   1.151894\n",
      "train loss:   1.151824\n",
      "train loss:   1.389624\n",
      "train loss:   1.122253\n",
      "train loss:   1.537025\n",
      "train loss:   1.074718\n",
      "train loss:   1.164627\n",
      "train loss:   0.910913\n",
      "train loss:   1.207764\n",
      "train loss:   1.130718\n",
      "train loss:   1.117781\n",
      "train loss:   1.261520\n",
      "train loss:   1.182034\n",
      "train loss:   1.256734\n",
      "train loss:   0.841387\n",
      "train loss:   1.132675\n",
      "train loss:   1.105161\n",
      "train loss:   0.937626\n",
      "train loss:   0.686312\n",
      "train loss:   1.259163\n",
      "train loss:   1.205813\n",
      "train loss:   1.092375\n",
      "train loss:   1.204650\n",
      "train loss:   1.040281\n",
      "########### epoch 27 ###########\n",
      "########### loop 4950 ###########\n",
      "test loss:   0.253933   test accuracy:   0.916667\n",
      "########### loop 4950 ###########\n",
      "train loss:   1.233063\n",
      "train loss:   1.239583\n",
      "train loss:   1.116174\n",
      "train loss:   0.932913\n",
      "train loss:   1.347719\n",
      "train loss:   1.058370\n",
      "train loss:   0.977970\n",
      "train loss:   1.235993\n",
      "train loss:   1.314008\n",
      "train loss:   0.664966\n",
      "train loss:   0.887507\n",
      "train loss:   1.122673\n",
      "train loss:   1.061007\n",
      "train loss:   0.724709\n",
      "train loss:   0.795233\n",
      "train loss:   0.882529\n",
      "train loss:   1.134483\n",
      "train loss:   1.256766\n",
      "train loss:   1.094172\n",
      "train loss:   0.679353\n",
      "train loss:   0.981591\n",
      "train loss:   0.900824\n",
      "train loss:   1.011288\n",
      "train loss:   1.126534\n",
      "train loss:   1.333565\n",
      "train loss:   0.891625\n",
      "train loss:   1.063559\n",
      "train loss:   1.047820\n",
      "train loss:   1.063797\n",
      "train loss:   1.138835\n",
      "train loss:   0.697814\n",
      "train loss:   0.938215\n",
      "train loss:   1.240955\n",
      "train loss:   1.293336\n",
      "train loss:   1.554377\n",
      "train loss:   0.908529\n",
      "train loss:   1.532603\n",
      "train loss:   0.668125\n",
      "train loss:   0.684821\n",
      "train loss:   1.291786\n",
      "train loss:   1.205782\n",
      "train loss:   1.428790\n",
      "train loss:   0.943085\n",
      "train loss:   1.096925\n",
      "train loss:   1.414789\n",
      "train loss:   1.134940\n",
      "train loss:   1.131506\n",
      "train loss:   1.110367\n",
      "train loss:   0.880150\n",
      "train loss:   1.109852\n",
      "########### epoch 27 ###########\n",
      "########### loop 5000 ###########\n",
      "test loss:   0.280308   test accuracy:   0.958333\n",
      "########### loop 5000 ###########\n",
      "train loss:   0.765018\n",
      "train loss:   0.943984\n",
      "train loss:   1.228423\n",
      "train loss:   1.010864\n",
      "train loss:   0.969910\n",
      "train loss:   1.088260\n",
      "train loss:   0.941382\n",
      "train loss:   0.904744\n",
      "train loss:   0.975703\n",
      "train loss:   1.231645\n",
      "train loss:   0.946247\n",
      "train loss:   1.191596\n",
      "train loss:   1.181712\n",
      "train loss:   1.105155\n",
      "train loss:   1.282930\n",
      "train loss:   1.086745\n",
      "train loss:   0.952594\n",
      "train loss:   1.188037\n",
      "train loss:   0.878777\n",
      "train loss:   1.209329\n",
      "train loss:   0.882011\n",
      "train loss:   0.670841\n",
      "train loss:   0.942720\n",
      "train loss:   0.879868\n",
      "train loss:   1.171062\n",
      "train loss:   1.356716\n",
      "train loss:   0.963792\n",
      "train loss:   1.373766\n",
      "train loss:   1.007511\n",
      "train loss:   0.856754\n",
      "train loss:   0.851333\n",
      "train loss:   1.320433\n",
      "train loss:   1.439604\n",
      "train loss:   1.193320\n",
      "train loss:   1.345757\n",
      "train loss:   0.936354\n",
      "train loss:   0.859313\n",
      "train loss:   1.180779\n",
      "train loss:   0.846874\n",
      "train loss:   0.904669\n",
      "train loss:   0.808173\n",
      "train loss:   0.960001\n",
      "train loss:   1.178042\n",
      "train loss:   1.061977\n",
      "train loss:   1.044894\n",
      "train loss:   0.814461\n",
      "train loss:   1.141931\n",
      "train loss:   1.004923\n",
      "train loss:   1.041410\n",
      "train loss:   1.052268\n",
      "########### epoch 27 ###########\n",
      "########### loop 5050 ###########\n",
      "test loss:   0.339382   test accuracy:   0.958333\n",
      "########### loop 5050 ###########\n",
      "train loss:   1.175406\n",
      "train loss:   1.063628\n",
      "train loss:   1.464432\n",
      "train loss:   1.287889\n",
      "train loss:   0.916534\n",
      "train loss:   0.925517\n",
      "train loss:   1.459973\n",
      "train loss:   1.111780\n",
      "train loss:   0.761102\n",
      "train loss:   0.743382\n",
      "train loss:   0.895695\n",
      "train loss:   1.198139\n",
      "train loss:   1.386338\n",
      "train loss:   1.089135\n",
      "train loss:   0.976376\n",
      "train loss:   0.885507\n",
      "train loss:   1.225218\n",
      "train loss:   1.433416\n",
      "train loss:   1.338184\n",
      "train loss:   1.259712\n",
      "train loss:   0.866475\n",
      "train loss:   0.778100\n",
      "train loss:   1.113114\n",
      "train loss:   1.031716\n",
      "train loss:   1.117192\n",
      "train loss:   1.064688\n",
      "train loss:   0.909011\n",
      "train loss:   1.488263\n",
      "train loss:   1.005224\n",
      "train loss:   1.307651\n",
      "train loss:   1.116792\n",
      "train loss:   1.106454\n",
      "train loss:   0.850258\n",
      "train loss:   1.232588\n",
      "train loss:   1.094736\n",
      "train loss:   0.854843\n",
      "train loss:   1.286782\n",
      "train loss:   1.315064\n",
      "train loss:   0.812264\n",
      "train loss:   1.202941\n",
      "train loss:   0.994080\n",
      "train loss:   1.245421\n",
      "train loss:   1.078545\n",
      "train loss:   1.119779\n",
      "train loss:   0.822680\n",
      "train loss:   0.949570\n",
      "train loss:   1.229678\n",
      "train loss:   1.000441\n",
      "train loss:   1.210573\n",
      "train loss:   1.275234\n",
      "########### epoch 28 ###########\n",
      "########### loop 5100 ###########\n",
      "test loss:   0.226490   test accuracy:   0.958333\n",
      "########### loop 5100 ###########\n",
      "train loss:   1.079318\n",
      "train loss:   0.946865\n",
      "train loss:   0.948929\n",
      "train loss:   0.843924\n",
      "train loss:   1.260540\n",
      "train loss:   0.804243\n",
      "train loss:   1.250786\n",
      "train loss:   0.943609\n",
      "train loss:   0.998971\n",
      "train loss:   1.175028\n",
      "train loss:   1.091536\n",
      "train loss:   1.089620\n",
      "train loss:   1.018724\n",
      "train loss:   0.998254\n",
      "train loss:   1.195229\n",
      "train loss:   1.033594\n",
      "train loss:   0.891298\n",
      "train loss:   0.885958\n",
      "train loss:   0.829349\n",
      "train loss:   1.107764\n",
      "train loss:   1.142955\n",
      "train loss:   0.858195\n",
      "train loss:   1.170632\n",
      "train loss:   1.230181\n",
      "train loss:   0.849253\n",
      "train loss:   1.454237\n",
      "train loss:   1.242823\n",
      "train loss:   0.722788\n",
      "train loss:   0.649558\n",
      "train loss:   1.422931\n",
      "train loss:   0.751107\n",
      "train loss:   0.836394\n",
      "train loss:   0.984211\n",
      "train loss:   0.827413\n",
      "train loss:   1.055343\n",
      "train loss:   1.226559\n",
      "train loss:   0.761804\n",
      "train loss:   1.234286\n",
      "train loss:   0.947155\n",
      "train loss:   1.456243\n",
      "train loss:   0.762967\n",
      "train loss:   0.673387\n",
      "train loss:   0.849576\n",
      "train loss:   1.096345\n",
      "train loss:   1.201729\n",
      "train loss:   0.699193\n",
      "train loss:   1.020452\n",
      "train loss:   1.014832\n",
      "train loss:   1.133587\n",
      "train loss:   1.124348\n",
      "########### epoch 28 ###########\n",
      "########### loop 5150 ###########\n",
      "test loss:   0.241034   test accuracy:   0.958333\n",
      "########### loop 5150 ###########\n",
      "train loss:   1.202784\n",
      "train loss:   0.957676\n",
      "train loss:   1.195707\n",
      "train loss:   0.825002\n",
      "train loss:   0.803003\n",
      "train loss:   1.154932\n",
      "train loss:   1.179887\n",
      "train loss:   0.807538\n",
      "train loss:   1.282939\n",
      "train loss:   0.954698\n",
      "train loss:   1.452486\n",
      "train loss:   1.152237\n",
      "train loss:   1.116249\n",
      "train loss:   0.642961\n",
      "train loss:   1.195891\n",
      "train loss:   1.128115\n",
      "train loss:   1.236849\n",
      "train loss:   0.869042\n",
      "train loss:   0.924019\n",
      "train loss:   0.878671\n",
      "train loss:   0.864498\n",
      "train loss:   1.073874\n",
      "train loss:   1.176970\n",
      "train loss:   1.044311\n",
      "train loss:   1.122046\n",
      "train loss:   0.790436\n",
      "train loss:   0.953968\n",
      "train loss:   1.354891\n",
      "train loss:   1.278152\n",
      "train loss:   1.103579\n",
      "train loss:   1.166205\n",
      "train loss:   1.232439\n",
      "train loss:   1.320540\n",
      "train loss:   0.777913\n",
      "train loss:   0.951200\n",
      "train loss:   0.957533\n",
      "train loss:   1.200542\n",
      "train loss:   0.628993\n",
      "train loss:   0.689012\n",
      "train loss:   0.706692\n",
      "train loss:   1.405545\n",
      "train loss:   1.008920\n",
      "train loss:   1.131297\n",
      "train loss:   1.076590\n",
      "train loss:   0.976514\n",
      "train loss:   1.107451\n",
      "train loss:   1.063651\n",
      "train loss:   1.009936\n",
      "train loss:   0.883390\n",
      "train loss:   1.195176\n",
      "########### epoch 28 ###########\n",
      "########### loop 5200 ###########\n",
      "test loss:   0.428919   test accuracy:   0.875000\n",
      "########### loop 5200 ###########\n",
      "train loss:   0.757688\n",
      "train loss:   0.859067\n",
      "train loss:   1.271737\n",
      "train loss:   1.247547\n",
      "train loss:   0.696527\n",
      "train loss:   0.982980\n",
      "train loss:   1.174779\n",
      "train loss:   0.896139\n",
      "train loss:   1.158251\n",
      "train loss:   1.061528\n",
      "train loss:   1.258649\n",
      "train loss:   0.698562\n",
      "train loss:   0.694593\n",
      "train loss:   1.046415\n",
      "train loss:   0.913677\n",
      "train loss:   1.067393\n",
      "train loss:   0.941368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.111066\n",
      "train loss:   1.292039\n",
      "train loss:   0.717570\n",
      "train loss:   1.167568\n",
      "train loss:   1.169629\n",
      "train loss:   1.059121\n",
      "train loss:   1.166087\n",
      "train loss:   0.774178\n",
      "train loss:   1.101164\n",
      "train loss:   1.211310\n",
      "train loss:   1.155519\n",
      "train loss:   1.159223\n",
      "train loss:   1.043685\n",
      "train loss:   1.078096\n",
      "train loss:   1.042249\n",
      "train loss:   0.429489\n",
      "train loss:   1.478359\n",
      "train loss:   0.978208\n",
      "train loss:   0.921386\n",
      "train loss:   1.266555\n",
      "train loss:   1.305231\n",
      "train loss:   1.196553\n",
      "train loss:   0.721195\n",
      "train loss:   0.824216\n",
      "train loss:   0.954199\n",
      "train loss:   0.753099\n",
      "train loss:   1.010012\n",
      "train loss:   0.882579\n",
      "train loss:   0.885341\n",
      "train loss:   1.140599\n",
      "train loss:   0.805798\n",
      "train loss:   1.000492\n",
      "train loss:   0.944801\n",
      "########### epoch 28 ###########\n",
      "########### loop 5250 ###########\n",
      "test loss:   0.339787   test accuracy:   0.958333\n",
      "########### loop 5250 ###########\n",
      "train loss:   1.171924\n",
      "train loss:   1.030447\n",
      "train loss:   0.944799\n",
      "train loss:   1.174055\n",
      "train loss:   1.214132\n",
      "train loss:   1.091710\n",
      "train loss:   1.255226\n",
      "train loss:   1.102776\n",
      "train loss:   1.127353\n",
      "train loss:   1.079744\n",
      "train loss:   1.474479\n",
      "train loss:   1.069539\n",
      "train loss:   1.388941\n",
      "train loss:   0.639308\n",
      "train loss:   0.971513\n",
      "train loss:   0.885584\n",
      "train loss:   0.943529\n",
      "train loss:   0.948291\n",
      "train loss:   1.121584\n",
      "train loss:   1.031022\n",
      "train loss:   1.070357\n",
      "train loss:   1.220220\n",
      "train loss:   0.665456\n",
      "train loss:   0.970278\n",
      "train loss:   1.155913\n",
      "train loss:   0.839357\n",
      "train loss:   0.942885\n",
      "train loss:   1.417659\n",
      "train loss:   1.192486\n",
      "train loss:   1.204144\n",
      "train loss:   0.769451\n",
      "train loss:   1.238487\n",
      "train loss:   0.930361\n",
      "train loss:   0.857228\n",
      "train loss:   0.529894\n",
      "train loss:   1.133181\n",
      "train loss:   0.935155\n",
      "train loss:   0.846882\n",
      "train loss:   1.069773\n",
      "train loss:   0.952243\n",
      "train loss:   0.995170\n",
      "train loss:   0.913679\n",
      "train loss:   0.756918\n",
      "train loss:   1.060582\n",
      "train loss:   0.842600\n",
      "train loss:   0.934051\n",
      "train loss:   1.017131\n",
      "train loss:   0.904854\n",
      "train loss:   1.427902\n",
      "train loss:   0.880168\n",
      "########### epoch 29 ###########\n",
      "########### loop 5300 ###########\n",
      "test loss:   0.555961   test accuracy:   0.833333\n",
      "########### loop 5300 ###########\n",
      "train loss:   0.755239\n",
      "train loss:   0.777843\n",
      "train loss:   0.947695\n",
      "train loss:   0.864412\n",
      "train loss:   1.082036\n",
      "train loss:   1.256563\n",
      "train loss:   1.276419\n",
      "train loss:   0.913150\n",
      "train loss:   1.125148\n",
      "train loss:   1.145537\n",
      "train loss:   0.884279\n",
      "train loss:   1.341341\n",
      "train loss:   0.856647\n",
      "train loss:   0.950495\n",
      "train loss:   1.345179\n",
      "train loss:   0.970863\n",
      "train loss:   1.489205\n",
      "train loss:   1.140614\n",
      "train loss:   1.132546\n",
      "train loss:   1.340518\n",
      "train loss:   1.055294\n",
      "train loss:   1.308577\n",
      "train loss:   1.403929\n",
      "train loss:   0.589748\n",
      "train loss:   1.350544\n",
      "train loss:   1.072039\n",
      "train loss:   0.737374\n",
      "train loss:   0.815922\n",
      "train loss:   0.892745\n",
      "train loss:   0.966438\n",
      "train loss:   0.878316\n",
      "train loss:   1.113919\n",
      "train loss:   1.139335\n",
      "train loss:   0.971574\n",
      "train loss:   1.228683\n",
      "train loss:   1.313702\n",
      "train loss:   1.339244\n",
      "train loss:   0.980043\n",
      "train loss:   1.049523\n",
      "train loss:   1.231827\n",
      "train loss:   0.853600\n",
      "train loss:   1.232311\n",
      "train loss:   1.127276\n",
      "train loss:   0.748075\n",
      "train loss:   1.160271\n",
      "train loss:   1.135930\n",
      "train loss:   0.982263\n",
      "train loss:   1.467027\n",
      "train loss:   1.236571\n",
      "train loss:   0.895967\n",
      "########### epoch 29 ###########\n",
      "########### loop 5350 ###########\n",
      "test loss:   0.281646   test accuracy:   0.958333\n",
      "########### loop 5350 ###########\n",
      "train loss:   1.032647\n",
      "train loss:   0.998655\n",
      "train loss:   0.760562\n",
      "train loss:   1.168608\n",
      "train loss:   0.858370\n",
      "train loss:   0.941206\n",
      "train loss:   1.207552\n",
      "train loss:   0.785449\n",
      "train loss:   0.788271\n",
      "train loss:   1.085839\n",
      "train loss:   1.354852\n",
      "train loss:   1.209921\n",
      "train loss:   0.922074\n",
      "train loss:   1.022551\n",
      "train loss:   1.242365\n",
      "train loss:   1.032087\n",
      "train loss:   0.935367\n",
      "train loss:   1.130940\n",
      "train loss:   0.539415\n",
      "train loss:   1.150108\n",
      "train loss:   1.013095\n",
      "train loss:   1.008982\n",
      "train loss:   1.155221\n",
      "train loss:   0.654567\n",
      "train loss:   1.149544\n",
      "train loss:   0.520794\n",
      "train loss:   1.329537\n",
      "train loss:   0.696077\n",
      "train loss:   0.855676\n",
      "train loss:   0.936124\n",
      "train loss:   0.465898\n",
      "train loss:   1.046804\n",
      "train loss:   1.266686\n",
      "train loss:   0.661676\n",
      "train loss:   1.072421\n",
      "train loss:   0.756498\n",
      "train loss:   1.163475\n",
      "train loss:   0.983347\n",
      "train loss:   1.069321\n",
      "train loss:   0.938275\n",
      "train loss:   1.010440\n",
      "train loss:   1.432676\n",
      "train loss:   0.838569\n",
      "train loss:   1.105276\n",
      "train loss:   0.905679\n",
      "train loss:   1.273516\n",
      "train loss:   1.137188\n",
      "train loss:   0.590871\n",
      "train loss:   0.858974\n",
      "train loss:   0.983519\n",
      "########### epoch 29 ###########\n",
      "########### loop 5400 ###########\n",
      "test loss:   0.589653   test accuracy:   0.875000\n",
      "########### loop 5400 ###########\n",
      "train loss:   1.025826\n",
      "train loss:   1.074437\n",
      "train loss:   1.324495\n",
      "train loss:   1.184443\n",
      "train loss:   1.013035\n",
      "train loss:   1.044155\n",
      "train loss:   1.072722\n",
      "train loss:   1.170042\n",
      "train loss:   1.016995\n",
      "train loss:   0.873985\n",
      "train loss:   1.437153\n",
      "train loss:   1.013315\n",
      "train loss:   1.091013\n",
      "train loss:   1.337947\n",
      "train loss:   1.055901\n",
      "train loss:   1.067315\n",
      "train loss:   0.933094\n",
      "train loss:   1.145337\n",
      "train loss:   0.613380\n",
      "train loss:   1.200597\n",
      "train loss:   0.987338\n",
      "train loss:   1.558849\n",
      "train loss:   1.289912\n",
      "train loss:   0.382083\n",
      "train loss:   0.640958\n",
      "train loss:   1.038599\n",
      "train loss:   1.269094\n",
      "train loss:   0.687828\n",
      "train loss:   1.233956\n",
      "train loss:   1.081482\n",
      "train loss:   0.885726\n",
      "train loss:   0.659363\n",
      "train loss:   1.127082\n",
      "train loss:   0.945544\n",
      "train loss:   1.019036\n",
      "train loss:   1.081564\n",
      "train loss:   0.846087\n",
      "train loss:   0.975391\n",
      "train loss:   0.905243\n",
      "train loss:   1.065695\n",
      "train loss:   1.234010\n",
      "train loss:   1.039985\n",
      "train loss:   0.801442\n",
      "train loss:   1.053252\n",
      "train loss:   1.031769\n",
      "train loss:   1.234326\n",
      "train loss:   1.165160\n",
      "train loss:   1.305571\n",
      "train loss:   1.237278\n",
      "train loss:   1.448926\n",
      "########### epoch 29 ###########\n",
      "########### loop 5450 ###########\n",
      "test loss:   0.365042   test accuracy:   0.958333\n",
      "########### loop 5450 ###########\n",
      "train loss:   0.766730\n",
      "train loss:   1.054981\n",
      "train loss:   0.938034\n",
      "train loss:   1.051351\n",
      "train loss:   0.819083\n",
      "train loss:   1.190406\n",
      "train loss:   0.973028\n",
      "train loss:   1.040842\n",
      "train loss:   1.347738\n",
      "train loss:   0.904991\n",
      "train loss:   1.000690\n",
      "train loss:   1.281628\n",
      "train loss:   0.873223\n",
      "train loss:   1.180274\n",
      "train loss:   1.072379\n",
      "train loss:   0.843654\n",
      "train loss:   1.350253\n",
      "train loss:   0.805869\n",
      "train loss:   1.098009\n",
      "train loss:   0.975718\n",
      "train loss:   1.166440\n",
      "train loss:   1.204285\n",
      "train loss:   1.227709\n",
      "train loss:   1.405048\n",
      "train loss:   1.169906\n",
      "train loss:   0.962752\n",
      "train loss:   0.845969\n",
      "train loss:   1.047000\n",
      "train loss:   1.262828\n",
      "train loss:   0.939238\n",
      "train loss:   1.438154\n",
      "train loss:   1.274984\n",
      "train loss:   1.473954\n",
      "train loss:   0.833110\n",
      "train loss:   0.960619\n",
      "train loss:   1.384407\n",
      "train loss:   1.298893\n",
      "train loss:   1.036532\n",
      "train loss:   1.174278\n",
      "train loss:   1.130022\n",
      "train loss:   1.056837\n",
      "train loss:   1.172779\n",
      "train loss:   0.920198\n",
      "train loss:   0.932117\n",
      "train loss:   1.033428\n",
      "train loss:   0.914513\n",
      "train loss:   0.961768\n",
      "train loss:   0.896356\n",
      "train loss:   0.833513\n",
      "train loss:   1.021431\n",
      "########### epoch 30 ###########\n",
      "########### loop 5500 ###########\n",
      "test loss:   0.523698   test accuracy:   0.916667\n",
      "########### loop 5500 ###########\n",
      "train loss:   0.759666\n",
      "train loss:   0.988139\n",
      "train loss:   0.938640\n",
      "train loss:   0.789993\n",
      "train loss:   0.858254\n",
      "train loss:   1.213007\n",
      "train loss:   0.911682\n",
      "train loss:   0.839610\n",
      "train loss:   0.861360\n",
      "train loss:   1.015417\n",
      "train loss:   0.955892\n",
      "train loss:   0.648006\n",
      "train loss:   1.414456\n",
      "train loss:   1.011457\n",
      "train loss:   0.854529\n",
      "train loss:   1.020543\n",
      "train loss:   1.048244\n",
      "train loss:   0.983641\n",
      "train loss:   1.141211\n",
      "train loss:   1.168012\n",
      "train loss:   0.842228\n",
      "train loss:   0.897673\n",
      "train loss:   1.149649\n",
      "train loss:   0.765530\n",
      "train loss:   1.026390\n",
      "train loss:   0.906119\n",
      "train loss:   0.699944\n",
      "train loss:   1.467834\n",
      "train loss:   0.765835\n",
      "train loss:   1.327832\n",
      "train loss:   0.989872\n",
      "train loss:   1.367998\n",
      "train loss:   1.315116\n",
      "train loss:   1.017748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.034217\n",
      "train loss:   1.129381\n",
      "train loss:   1.074260\n",
      "train loss:   1.048893\n",
      "train loss:   0.998388\n",
      "train loss:   1.022545\n",
      "train loss:   1.083534\n",
      "train loss:   0.807743\n",
      "train loss:   1.075551\n",
      "train loss:   0.917994\n",
      "train loss:   0.977940\n",
      "train loss:   1.402554\n",
      "train loss:   1.053493\n",
      "train loss:   0.775063\n",
      "train loss:   1.188459\n",
      "train loss:   0.997381\n",
      "########### epoch 30 ###########\n",
      "########### loop 5550 ###########\n",
      "test loss:   0.183156   test accuracy:   1.000000\n",
      "########### loop 5550 ###########\n",
      "train loss:   0.992054\n",
      "train loss:   1.221529\n",
      "train loss:   0.625883\n",
      "train loss:   1.329101\n",
      "train loss:   0.877939\n",
      "train loss:   0.707216\n",
      "train loss:   0.932311\n",
      "train loss:   0.715932\n",
      "train loss:   1.024316\n",
      "train loss:   1.422357\n",
      "train loss:   1.269491\n",
      "train loss:   0.802756\n",
      "train loss:   0.916826\n",
      "train loss:   1.024971\n",
      "train loss:   1.386727\n",
      "train loss:   0.902727\n",
      "train loss:   1.279357\n",
      "train loss:   1.126218\n",
      "train loss:   0.826845\n",
      "train loss:   1.293743\n",
      "train loss:   0.986398\n",
      "train loss:   1.255275\n",
      "train loss:   0.899416\n",
      "train loss:   0.952001\n",
      "train loss:   1.346426\n",
      "train loss:   0.751940\n",
      "train loss:   0.893449\n",
      "train loss:   0.893571\n",
      "train loss:   1.220548\n",
      "train loss:   1.049007\n",
      "train loss:   0.763675\n",
      "train loss:   1.028956\n",
      "train loss:   0.893512\n",
      "train loss:   1.219661\n",
      "train loss:   0.771927\n",
      "train loss:   0.713213\n",
      "train loss:   1.174355\n",
      "train loss:   1.070893\n",
      "train loss:   0.994763\n",
      "train loss:   1.089941\n",
      "train loss:   0.857837\n",
      "train loss:   0.981003\n",
      "train loss:   0.771394\n",
      "train loss:   1.446624\n",
      "train loss:   0.904046\n",
      "train loss:   0.509625\n",
      "train loss:   1.077609\n",
      "train loss:   1.006896\n",
      "train loss:   1.163176\n",
      "train loss:   1.031718\n",
      "########### epoch 30 ###########\n",
      "########### loop 5600 ###########\n",
      "test loss:   0.175002   test accuracy:   1.000000\n",
      "########### loop 5600 ###########\n",
      "train loss:   0.969914\n",
      "train loss:   1.433952\n",
      "train loss:   0.984501\n",
      "train loss:   1.007095\n",
      "train loss:   0.825443\n",
      "train loss:   1.170303\n",
      "train loss:   0.861780\n",
      "train loss:   1.037454\n",
      "train loss:   1.015235\n",
      "train loss:   1.235793\n",
      "train loss:   1.364110\n",
      "train loss:   1.158857\n",
      "train loss:   1.140593\n",
      "train loss:   0.966668\n",
      "train loss:   1.300313\n",
      "train loss:   1.060394\n",
      "train loss:   1.185291\n",
      "train loss:   1.162526\n",
      "train loss:   1.144323\n",
      "train loss:   1.106847\n",
      "train loss:   1.214640\n",
      "train loss:   1.109561\n",
      "train loss:   0.426474\n",
      "train loss:   1.117801\n",
      "train loss:   0.936327\n",
      "train loss:   1.394559\n",
      "train loss:   1.006177\n",
      "train loss:   1.574505\n",
      "train loss:   0.868966\n",
      "train loss:   1.346476\n",
      "train loss:   0.866815\n",
      "train loss:   1.227009\n",
      "train loss:   1.037502\n",
      "train loss:   0.698426\n",
      "train loss:   1.507934\n",
      "train loss:   0.886664\n",
      "train loss:   1.213827\n",
      "train loss:   0.996851\n",
      "train loss:   1.123971\n",
      "train loss:   1.238766\n",
      "train loss:   1.157871\n",
      "train loss:   0.888855\n",
      "train loss:   1.397265\n",
      "train loss:   0.976654\n",
      "train loss:   1.027216\n",
      "train loss:   1.020114\n",
      "train loss:   0.896637\n",
      "train loss:   0.876175\n",
      "train loss:   1.114742\n",
      "train loss:   1.079086\n",
      "########### epoch 31 ###########\n",
      "########### loop 5650 ###########\n",
      "test loss:   0.321248   test accuracy:   1.000000\n",
      "########### loop 5650 ###########\n",
      "train loss:   1.261232\n",
      "train loss:   1.190951\n",
      "train loss:   1.116717\n",
      "train loss:   0.727163\n",
      "train loss:   0.687442\n",
      "train loss:   0.995397\n",
      "train loss:   1.154098\n",
      "train loss:   0.963397\n",
      "train loss:   1.001319\n",
      "train loss:   1.085004\n",
      "train loss:   1.132242\n",
      "train loss:   1.100662\n",
      "train loss:   0.951876\n",
      "train loss:   1.357363\n",
      "train loss:   1.126098\n",
      "train loss:   1.420983\n",
      "train loss:   0.888016\n",
      "train loss:   1.096848\n",
      "train loss:   1.295274\n",
      "train loss:   1.306893\n",
      "train loss:   0.767542\n",
      "train loss:   0.915321\n",
      "train loss:   0.808120\n",
      "train loss:   0.891376\n",
      "train loss:   0.893002\n",
      "train loss:   0.830439\n",
      "train loss:   0.930829\n",
      "train loss:   1.042815\n",
      "train loss:   1.276133\n",
      "train loss:   1.087296\n",
      "train loss:   1.130970\n",
      "train loss:   0.798346\n",
      "train loss:   1.111297\n",
      "train loss:   1.044871\n",
      "train loss:   1.015586\n",
      "train loss:   1.073426\n",
      "train loss:   1.027398\n",
      "train loss:   0.839364\n",
      "train loss:   1.153956\n",
      "train loss:   0.891120\n",
      "train loss:   1.110404\n",
      "train loss:   0.950424\n",
      "train loss:   1.197506\n",
      "train loss:   0.769785\n",
      "train loss:   0.830709\n",
      "train loss:   0.948824\n",
      "train loss:   1.009942\n",
      "train loss:   1.025067\n",
      "train loss:   1.212839\n",
      "train loss:   0.870646\n",
      "########### epoch 31 ###########\n",
      "########### loop 5700 ###########\n",
      "test loss:   0.292635   test accuracy:   1.000000\n",
      "########### loop 5700 ###########\n",
      "train loss:   0.688290\n",
      "train loss:   0.858800\n",
      "train loss:   0.852126\n",
      "train loss:   0.998447\n",
      "train loss:   1.257067\n",
      "train loss:   0.743469\n",
      "train loss:   0.801837\n",
      "train loss:   1.016835\n",
      "train loss:   1.322151\n",
      "train loss:   1.199796\n",
      "train loss:   1.463516\n",
      "train loss:   1.161407\n",
      "train loss:   1.193878\n",
      "train loss:   0.961485\n",
      "train loss:   0.951375\n",
      "train loss:   1.132757\n",
      "train loss:   1.010455\n",
      "train loss:   1.247812\n",
      "train loss:   1.394622\n",
      "train loss:   1.145478\n",
      "train loss:   0.816369\n",
      "train loss:   1.054695\n",
      "train loss:   1.129748\n",
      "train loss:   0.983384\n",
      "train loss:   1.467886\n",
      "train loss:   0.787957\n",
      "train loss:   0.837755\n",
      "train loss:   0.853540\n",
      "train loss:   0.652779\n",
      "train loss:   0.854247\n",
      "train loss:   1.123695\n",
      "train loss:   1.017631\n",
      "train loss:   0.950376\n",
      "train loss:   1.374120\n",
      "train loss:   0.834253\n",
      "train loss:   0.966386\n",
      "train loss:   1.287894\n",
      "train loss:   0.824711\n",
      "train loss:   0.864022\n",
      "train loss:   1.043960\n",
      "train loss:   1.016413\n",
      "train loss:   0.824225\n",
      "train loss:   1.642688\n",
      "train loss:   0.823717\n",
      "train loss:   1.110771\n",
      "train loss:   1.024705\n",
      "train loss:   1.122303\n",
      "train loss:   1.178694\n",
      "train loss:   1.036111\n",
      "train loss:   1.088745\n",
      "########### epoch 31 ###########\n",
      "########### loop 5750 ###########\n",
      "test loss:   0.300528   test accuracy:   0.958333\n",
      "########### loop 5750 ###########\n",
      "train loss:   1.216511\n",
      "train loss:   0.950979\n",
      "train loss:   1.226172\n",
      "train loss:   1.101387\n",
      "train loss:   1.196242\n",
      "train loss:   1.026246\n",
      "train loss:   1.243456\n",
      "train loss:   0.665831\n",
      "train loss:   1.272113\n",
      "train loss:   0.819140\n",
      "train loss:   1.009866\n",
      "train loss:   0.962867\n",
      "train loss:   1.067419\n",
      "train loss:   1.130098\n",
      "train loss:   1.106602\n",
      "train loss:   1.232391\n",
      "train loss:   0.922282\n",
      "train loss:   1.306827\n",
      "train loss:   0.856873\n",
      "train loss:   1.064986\n",
      "train loss:   0.727336\n",
      "train loss:   1.019010\n",
      "train loss:   1.043833\n",
      "train loss:   1.250594\n",
      "train loss:   1.088268\n",
      "train loss:   0.918849\n",
      "train loss:   0.919611\n",
      "train loss:   1.430257\n",
      "train loss:   1.372917\n",
      "train loss:   1.496597\n",
      "train loss:   1.006578\n",
      "train loss:   0.827554\n",
      "train loss:   0.949917\n",
      "train loss:   1.256744\n",
      "train loss:   0.877319\n",
      "train loss:   0.893281\n",
      "train loss:   1.005093\n",
      "train loss:   0.946471\n",
      "train loss:   1.579626\n",
      "train loss:   1.118748\n",
      "train loss:   1.029490\n",
      "train loss:   1.187970\n",
      "train loss:   0.902885\n",
      "train loss:   1.213735\n",
      "train loss:   1.357267\n",
      "train loss:   1.103408\n",
      "train loss:   1.314320\n",
      "train loss:   1.290927\n",
      "train loss:   1.207448\n",
      "train loss:   0.909068\n",
      "########### epoch 31 ###########\n",
      "########### loop 5800 ###########\n",
      "test loss:   0.427685   test accuracy:   1.000000\n",
      "########### loop 5800 ###########\n",
      "train loss:   0.736377\n",
      "train loss:   1.203515\n",
      "train loss:   1.009347\n",
      "train loss:   1.079102\n",
      "train loss:   0.885836\n",
      "train loss:   0.929648\n",
      "train loss:   1.333324\n",
      "train loss:   0.935167\n",
      "train loss:   0.733727\n",
      "train loss:   0.956606\n",
      "train loss:   0.924482\n",
      "train loss:   0.863867\n",
      "train loss:   1.009923\n",
      "train loss:   1.354372\n",
      "train loss:   0.930772\n",
      "train loss:   1.258105\n",
      "train loss:   1.521485\n",
      "train loss:   1.096944\n",
      "train loss:   1.274953\n",
      "train loss:   1.104434\n",
      "train loss:   1.111161\n",
      "train loss:   1.248313\n",
      "train loss:   1.074323\n",
      "train loss:   0.623690\n",
      "train loss:   1.363151\n",
      "train loss:   1.153900\n",
      "train loss:   1.093398\n",
      "train loss:   1.255159\n",
      "train loss:   1.259750\n",
      "train loss:   0.848174\n",
      "train loss:   1.137874\n",
      "train loss:   1.094180\n",
      "train loss:   1.102599\n",
      "train loss:   0.809911\n",
      "train loss:   1.142559\n",
      "train loss:   1.326811\n",
      "train loss:   0.915992\n",
      "train loss:   0.805808\n",
      "train loss:   0.670148\n",
      "train loss:   1.561749\n",
      "train loss:   1.159762\n",
      "train loss:   1.162696\n",
      "train loss:   1.179380\n",
      "train loss:   1.561495\n",
      "train loss:   1.216817\n",
      "train loss:   1.102419\n",
      "train loss:   1.310040\n",
      "train loss:   0.766927\n",
      "train loss:   1.213204\n",
      "train loss:   1.011844\n",
      "########### epoch 32 ###########\n",
      "########### loop 5850 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.450346   test accuracy:   0.958333\n",
      "########### loop 5850 ###########\n",
      "train loss:   0.949068\n",
      "train loss:   0.853515\n",
      "train loss:   1.075015\n",
      "train loss:   1.266774\n",
      "train loss:   1.084663\n",
      "train loss:   1.006278\n",
      "train loss:   0.929766\n",
      "train loss:   1.187477\n",
      "train loss:   1.230596\n",
      "train loss:   1.316601\n",
      "train loss:   0.872054\n",
      "train loss:   0.727751\n",
      "train loss:   0.763974\n",
      "train loss:   0.757938\n",
      "train loss:   1.303533\n",
      "train loss:   1.113605\n",
      "train loss:   1.527463\n",
      "train loss:   1.158000\n",
      "train loss:   1.165823\n",
      "train loss:   1.165011\n",
      "train loss:   1.068720\n",
      "train loss:   1.167501\n",
      "train loss:   1.399571\n",
      "train loss:   0.790705\n",
      "train loss:   1.210272\n",
      "train loss:   1.307227\n",
      "train loss:   1.254867\n",
      "train loss:   0.683556\n",
      "train loss:   0.973089\n",
      "train loss:   1.010943\n",
      "train loss:   0.899361\n",
      "train loss:   1.426475\n",
      "train loss:   0.852104\n",
      "train loss:   1.046595\n",
      "train loss:   1.168037\n",
      "train loss:   1.038595\n",
      "train loss:   0.874323\n",
      "train loss:   0.823276\n",
      "train loss:   0.601267\n",
      "train loss:   1.006964\n",
      "train loss:   1.040692\n",
      "train loss:   1.349470\n",
      "train loss:   0.933657\n",
      "train loss:   1.140222\n",
      "train loss:   0.922394\n",
      "train loss:   1.179396\n",
      "train loss:   1.159003\n",
      "train loss:   1.164632\n",
      "train loss:   0.637781\n",
      "train loss:   1.034503\n",
      "########### epoch 32 ###########\n",
      "########### loop 5900 ###########\n",
      "test loss:   0.433884   test accuracy:   0.916667\n",
      "########### loop 5900 ###########\n",
      "train loss:   0.829172\n",
      "train loss:   0.803874\n",
      "train loss:   1.100197\n",
      "train loss:   1.129279\n",
      "train loss:   1.171836\n",
      "train loss:   1.011650\n",
      "train loss:   1.273111\n",
      "train loss:   0.912577\n",
      "train loss:   1.027803\n",
      "train loss:   0.953712\n",
      "train loss:   1.616986\n",
      "train loss:   0.957401\n",
      "train loss:   1.369002\n",
      "train loss:   1.036406\n",
      "train loss:   1.359113\n",
      "train loss:   1.594732\n",
      "train loss:   1.204575\n",
      "train loss:   0.657668\n",
      "train loss:   0.852678\n",
      "train loss:   1.090730\n",
      "train loss:   1.103977\n",
      "train loss:   1.186186\n",
      "train loss:   0.997603\n",
      "train loss:   1.074854\n",
      "train loss:   1.021275\n",
      "train loss:   0.888073\n",
      "train loss:   1.017500\n",
      "train loss:   0.948217\n",
      "train loss:   1.143205\n",
      "train loss:   0.857074\n",
      "train loss:   0.804932\n",
      "train loss:   0.722295\n",
      "train loss:   1.020753\n",
      "train loss:   1.026000\n",
      "train loss:   1.311496\n",
      "train loss:   1.072785\n",
      "train loss:   1.257949\n",
      "train loss:   1.071666\n",
      "train loss:   1.329207\n",
      "train loss:   1.242330\n",
      "train loss:   1.040586\n",
      "train loss:   0.972382\n",
      "train loss:   1.149633\n",
      "train loss:   1.016319\n",
      "train loss:   1.094425\n",
      "train loss:   1.291173\n",
      "train loss:   0.779170\n",
      "train loss:   1.446045\n",
      "train loss:   1.067822\n",
      "train loss:   1.284107\n",
      "########### epoch 32 ###########\n",
      "########### loop 5950 ###########\n",
      "test loss:   0.420989   test accuracy:   0.958333\n",
      "########### loop 5950 ###########\n",
      "train loss:   0.917930\n",
      "train loss:   1.182259\n",
      "train loss:   0.778363\n",
      "train loss:   0.888594\n",
      "train loss:   0.504522\n",
      "train loss:   1.064983\n",
      "train loss:   0.850907\n",
      "train loss:   0.916904\n",
      "train loss:   0.850312\n",
      "train loss:   1.247866\n",
      "train loss:   1.051375\n",
      "train loss:   1.370748\n",
      "train loss:   1.198710\n",
      "train loss:   0.840758\n",
      "train loss:   1.136277\n",
      "train loss:   1.191803\n",
      "train loss:   1.016314\n",
      "train loss:   0.892573\n",
      "train loss:   0.925333\n",
      "train loss:   0.933661\n",
      "train loss:   1.269784\n",
      "train loss:   0.980535\n",
      "train loss:   0.986971\n",
      "train loss:   1.303256\n",
      "train loss:   1.210528\n",
      "train loss:   0.780645\n",
      "train loss:   0.977331\n",
      "train loss:   1.178639\n",
      "train loss:   0.939651\n",
      "train loss:   1.272464\n",
      "train loss:   0.995367\n",
      "train loss:   0.825979\n",
      "train loss:   1.092905\n",
      "train loss:   1.202929\n",
      "train loss:   1.125989\n",
      "train loss:   1.185291\n",
      "train loss:   1.545986\n",
      "train loss:   0.824063\n",
      "train loss:   1.397895\n",
      "train loss:   1.101679\n",
      "train loss:   1.277647\n",
      "train loss:   1.232027\n",
      "train loss:   1.137275\n",
      "train loss:   1.114391\n",
      "train loss:   1.153850\n",
      "train loss:   0.803861\n",
      "train loss:   1.170167\n",
      "train loss:   0.826378\n",
      "train loss:   1.152250\n",
      "train loss:   1.202456\n",
      "########### epoch 32 ###########\n",
      "########### loop 6000 ###########\n",
      "test loss:   0.190990   test accuracy:   1.000000\n",
      "########### loop 6000 ###########\n",
      "train loss:   0.834210\n",
      "train loss:   0.930583\n",
      "train loss:   0.894605\n",
      "train loss:   0.948896\n",
      "train loss:   0.880094\n",
      "train loss:   0.898784\n",
      "train loss:   0.829673\n",
      "train loss:   0.879021\n",
      "train loss:   0.923943\n",
      "train loss:   1.130044\n",
      "train loss:   1.396893\n",
      "train loss:   0.779192\n",
      "train loss:   1.084302\n",
      "train loss:   1.017036\n",
      "train loss:   1.144009\n",
      "train loss:   0.747375\n",
      "train loss:   1.093681\n",
      "train loss:   1.257757\n",
      "train loss:   1.336482\n",
      "train loss:   1.157327\n",
      "train loss:   1.586190\n",
      "train loss:   1.618795\n",
      "train loss:   0.914152\n",
      "train loss:   1.245874\n",
      "train loss:   1.002705\n",
      "train loss:   0.776035\n",
      "train loss:   0.909755\n",
      "train loss:   1.080823\n",
      "train loss:   1.249636\n",
      "train loss:   1.108665\n",
      "train loss:   1.006838\n",
      "train loss:   1.326421\n",
      "train loss:   0.983887\n",
      "train loss:   1.155034\n",
      "train loss:   0.989888\n",
      "train loss:   1.045638\n",
      "train loss:   0.967273\n",
      "train loss:   1.169788\n",
      "train loss:   0.990157\n",
      "train loss:   1.335107\n",
      "train loss:   1.176033\n",
      "train loss:   1.181305\n",
      "train loss:   0.821649\n",
      "train loss:   1.151225\n",
      "train loss:   0.932697\n",
      "train loss:   1.066250\n",
      "train loss:   0.641964\n",
      "train loss:   0.866790\n",
      "train loss:   0.700257\n",
      "train loss:   0.779032\n",
      "########### epoch 33 ###########\n",
      "########### loop 6050 ###########\n",
      "test loss:   0.312702   test accuracy:   0.958333\n",
      "########### loop 6050 ###########\n",
      "train loss:   1.049912\n",
      "train loss:   1.051246\n",
      "train loss:   1.003435\n",
      "train loss:   0.926430\n",
      "train loss:   0.836113\n",
      "train loss:   1.087269\n",
      "train loss:   1.353777\n",
      "train loss:   1.397720\n",
      "train loss:   0.760278\n",
      "train loss:   0.847704\n",
      "train loss:   1.472837\n",
      "train loss:   1.340672\n",
      "train loss:   0.911919\n",
      "train loss:   1.188607\n",
      "train loss:   0.551625\n",
      "train loss:   1.028114\n",
      "train loss:   0.956209\n",
      "train loss:   1.135178\n",
      "train loss:   0.999655\n",
      "train loss:   0.957154\n",
      "train loss:   1.129715\n",
      "train loss:   1.014112\n",
      "train loss:   0.970117\n",
      "train loss:   1.048298\n",
      "train loss:   1.084512\n",
      "train loss:   1.330195\n",
      "train loss:   0.781102\n",
      "train loss:   0.878945\n",
      "train loss:   0.865938\n",
      "train loss:   0.780687\n",
      "train loss:   1.267149\n",
      "train loss:   1.053288\n",
      "train loss:   0.863932\n",
      "train loss:   0.725162\n",
      "train loss:   1.221435\n",
      "train loss:   0.929774\n",
      "train loss:   0.987789\n",
      "train loss:   1.175394\n",
      "train loss:   0.893392\n",
      "train loss:   1.005361\n",
      "train loss:   0.911516\n",
      "train loss:   1.161453\n",
      "train loss:   0.777749\n",
      "train loss:   1.566039\n",
      "train loss:   0.897790\n",
      "train loss:   1.007353\n",
      "train loss:   1.277499\n",
      "train loss:   0.840964\n",
      "train loss:   1.199322\n",
      "train loss:   1.078384\n",
      "########### epoch 33 ###########\n",
      "########### loop 6100 ###########\n",
      "test loss:   0.268923   test accuracy:   0.916667\n",
      "########### loop 6100 ###########\n",
      "train loss:   1.094214\n",
      "train loss:   0.865491\n",
      "train loss:   1.191414\n",
      "train loss:   1.205398\n",
      "train loss:   0.790450\n",
      "train loss:   0.836564\n",
      "train loss:   1.095224\n",
      "train loss:   0.775281\n",
      "train loss:   1.042346\n",
      "train loss:   1.153418\n",
      "train loss:   0.761485\n",
      "train loss:   1.125031\n",
      "train loss:   1.162840\n",
      "train loss:   1.159725\n",
      "train loss:   0.810471\n",
      "train loss:   1.123623\n",
      "train loss:   0.982300\n",
      "train loss:   0.733285\n",
      "train loss:   1.248020\n",
      "train loss:   1.044112\n",
      "train loss:   1.452300\n",
      "train loss:   1.036568\n",
      "train loss:   0.746305\n",
      "train loss:   1.013209\n",
      "train loss:   1.292440\n",
      "train loss:   0.900721\n",
      "train loss:   1.142612\n",
      "train loss:   1.409837\n",
      "train loss:   0.847658\n",
      "train loss:   0.873256\n",
      "train loss:   1.277655\n",
      "train loss:   1.143573\n",
      "train loss:   1.011033\n",
      "train loss:   1.071835\n",
      "train loss:   0.875580\n",
      "train loss:   1.386233\n",
      "train loss:   1.186088\n",
      "train loss:   1.382351\n",
      "train loss:   1.111363\n",
      "train loss:   0.814594\n",
      "train loss:   0.949961\n",
      "train loss:   0.736827\n",
      "train loss:   1.192548\n",
      "train loss:   0.960391\n",
      "train loss:   1.044127\n",
      "train loss:   0.849284\n",
      "train loss:   0.801581\n",
      "train loss:   1.141972\n",
      "train loss:   0.694204\n",
      "train loss:   1.363289\n",
      "########### epoch 33 ###########\n",
      "########### loop 6150 ###########\n",
      "test loss:   0.263450   test accuracy:   1.000000\n",
      "########### loop 6150 ###########\n",
      "train loss:   1.360182\n",
      "train loss:   0.764596\n",
      "train loss:   0.984235\n",
      "train loss:   0.814679\n",
      "train loss:   1.140113\n",
      "train loss:   0.960364\n",
      "train loss:   0.814569\n",
      "train loss:   0.958259\n",
      "train loss:   1.480985\n",
      "train loss:   1.001467\n",
      "train loss:   0.955884\n",
      "train loss:   0.926971\n",
      "train loss:   1.038299\n",
      "train loss:   0.982502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.121816\n",
      "train loss:   1.239184\n",
      "train loss:   0.864952\n",
      "train loss:   0.981920\n",
      "train loss:   0.835786\n",
      "train loss:   1.026441\n",
      "train loss:   1.086152\n",
      "train loss:   1.223094\n",
      "train loss:   0.977532\n",
      "train loss:   1.026706\n",
      "train loss:   1.104348\n",
      "train loss:   1.278064\n",
      "train loss:   1.065067\n",
      "train loss:   0.954694\n",
      "train loss:   0.781591\n",
      "train loss:   0.617061\n",
      "train loss:   1.083087\n",
      "train loss:   0.996905\n",
      "train loss:   1.127126\n",
      "train loss:   0.999741\n",
      "train loss:   0.887205\n",
      "train loss:   1.128651\n",
      "train loss:   0.831104\n",
      "train loss:   1.270418\n",
      "train loss:   1.090421\n",
      "train loss:   0.948822\n",
      "train loss:   1.292151\n",
      "train loss:   1.380898\n",
      "train loss:   1.166361\n",
      "train loss:   0.943763\n",
      "train loss:   1.093805\n",
      "train loss:   1.234187\n",
      "train loss:   0.712158\n",
      "train loss:   1.245798\n",
      "train loss:   0.940624\n",
      "train loss:   0.859209\n",
      "########### epoch 33 ###########\n",
      "########### loop 6200 ###########\n",
      "test loss:   0.469705   test accuracy:   0.833333\n",
      "########### loop 6200 ###########\n",
      "train loss:   0.989698\n",
      "train loss:   0.883766\n",
      "train loss:   1.008800\n",
      "train loss:   1.025275\n",
      "train loss:   0.961797\n",
      "train loss:   1.408994\n",
      "train loss:   0.978801\n",
      "train loss:   0.933225\n",
      "train loss:   1.307014\n",
      "train loss:   1.138931\n",
      "train loss:   0.943251\n",
      "train loss:   1.411933\n",
      "train loss:   1.271134\n",
      "train loss:   0.997358\n",
      "train loss:   1.016499\n",
      "train loss:   1.088588\n",
      "train loss:   1.175640\n",
      "train loss:   1.260812\n",
      "train loss:   0.950132\n",
      "train loss:   1.399574\n",
      "train loss:   0.853523\n",
      "train loss:   1.098679\n",
      "train loss:   0.893157\n",
      "train loss:   0.721518\n",
      "train loss:   0.866130\n",
      "train loss:   1.830150\n",
      "train loss:   1.233395\n",
      "train loss:   1.209596\n",
      "train loss:   0.883370\n",
      "train loss:   1.514386\n",
      "train loss:   0.715061\n",
      "train loss:   0.669568\n",
      "train loss:   1.086069\n",
      "train loss:   1.240970\n",
      "train loss:   0.953816\n",
      "train loss:   1.066613\n",
      "train loss:   1.000423\n",
      "train loss:   1.112528\n",
      "train loss:   0.973763\n",
      "train loss:   1.021755\n",
      "train loss:   1.172392\n",
      "train loss:   0.881014\n",
      "train loss:   0.990811\n",
      "train loss:   1.329426\n",
      "train loss:   1.059525\n",
      "train loss:   1.083051\n",
      "train loss:   0.755906\n",
      "train loss:   0.746406\n",
      "train loss:   1.148823\n",
      "train loss:   0.877978\n",
      "########### epoch 34 ###########\n",
      "########### loop 6250 ###########\n",
      "test loss:   0.471814   test accuracy:   0.875000\n",
      "########### loop 6250 ###########\n",
      "train loss:   0.860323\n",
      "train loss:   1.166840\n",
      "train loss:   0.920081\n",
      "train loss:   0.975002\n",
      "train loss:   1.148028\n",
      "train loss:   0.796189\n",
      "train loss:   1.156784\n",
      "train loss:   0.547377\n",
      "train loss:   1.038615\n",
      "train loss:   1.266846\n",
      "train loss:   1.164084\n",
      "train loss:   1.020286\n",
      "train loss:   0.936731\n",
      "train loss:   1.218994\n",
      "train loss:   0.982682\n",
      "train loss:   0.980652\n",
      "train loss:   0.977037\n",
      "train loss:   0.553626\n",
      "train loss:   1.188355\n",
      "train loss:   0.979263\n",
      "train loss:   1.342326\n",
      "train loss:   0.963438\n",
      "train loss:   0.928668\n",
      "train loss:   1.150365\n",
      "train loss:   1.366108\n",
      "train loss:   1.406794\n",
      "train loss:   1.039945\n",
      "train loss:   0.821797\n",
      "train loss:   0.928748\n",
      "train loss:   1.447570\n",
      "train loss:   1.120724\n",
      "train loss:   0.843559\n",
      "train loss:   1.082138\n",
      "train loss:   0.719781\n",
      "train loss:   0.825492\n",
      "train loss:   0.953529\n",
      "train loss:   1.091166\n",
      "train loss:   1.144161\n",
      "train loss:   1.134619\n",
      "train loss:   1.016917\n",
      "train loss:   1.293425\n",
      "train loss:   0.928390\n",
      "train loss:   1.152157\n",
      "train loss:   1.032297\n",
      "train loss:   0.729742\n",
      "train loss:   0.774030\n",
      "train loss:   0.986671\n",
      "train loss:   1.025322\n",
      "train loss:   1.114796\n",
      "train loss:   0.819618\n",
      "########### epoch 34 ###########\n",
      "########### loop 6300 ###########\n",
      "test loss:   0.368440   test accuracy:   0.958333\n",
      "########### loop 6300 ###########\n",
      "train loss:   1.353597\n",
      "train loss:   1.332882\n",
      "train loss:   0.965484\n",
      "train loss:   1.051465\n",
      "train loss:   1.115403\n",
      "train loss:   0.924905\n",
      "train loss:   1.171422\n",
      "train loss:   1.204904\n",
      "train loss:   1.187295\n",
      "train loss:   0.957572\n",
      "train loss:   0.967482\n",
      "train loss:   1.057884\n",
      "train loss:   1.014059\n",
      "train loss:   1.375254\n",
      "train loss:   0.884156\n",
      "train loss:   0.829704\n",
      "train loss:   1.481839\n",
      "train loss:   0.969825\n",
      "train loss:   1.179561\n",
      "train loss:   1.286084\n",
      "train loss:   1.081099\n",
      "train loss:   1.288108\n",
      "train loss:   1.072341\n",
      "train loss:   1.069837\n",
      "train loss:   0.943519\n",
      "train loss:   1.026637\n",
      "train loss:   1.171854\n",
      "train loss:   1.151332\n",
      "train loss:   1.353356\n",
      "train loss:   1.115130\n",
      "train loss:   1.200868\n",
      "train loss:   0.681346\n",
      "train loss:   1.191386\n",
      "train loss:   0.986854\n",
      "train loss:   0.879661\n",
      "train loss:   1.369278\n",
      "train loss:   1.078064\n",
      "train loss:   0.859958\n",
      "train loss:   1.023878\n",
      "train loss:   0.637475\n",
      "train loss:   1.002941\n",
      "train loss:   0.901475\n",
      "train loss:   0.919478\n",
      "train loss:   1.314902\n",
      "train loss:   1.295580\n",
      "train loss:   1.126921\n",
      "train loss:   1.096428\n",
      "train loss:   0.981007\n",
      "train loss:   1.040632\n",
      "train loss:   0.986013\n",
      "########### epoch 34 ###########\n",
      "########### loop 6350 ###########\n",
      "test loss:   0.281059   test accuracy:   0.958333\n",
      "########### loop 6350 ###########\n",
      "train loss:   0.778310\n",
      "train loss:   0.757295\n",
      "train loss:   0.781499\n",
      "train loss:   0.765371\n",
      "train loss:   0.959617\n",
      "train loss:   1.153121\n",
      "train loss:   1.110493\n",
      "train loss:   0.985026\n",
      "train loss:   0.824944\n",
      "train loss:   0.972911\n",
      "train loss:   0.757880\n",
      "train loss:   1.089393\n",
      "train loss:   1.066192\n",
      "train loss:   1.181723\n",
      "train loss:   1.339080\n",
      "train loss:   1.308378\n",
      "train loss:   1.227412\n",
      "train loss:   1.048890\n",
      "train loss:   0.978766\n",
      "train loss:   1.038688\n",
      "train loss:   1.061064\n",
      "train loss:   0.532306\n",
      "train loss:   0.917575\n",
      "train loss:   1.229983\n",
      "train loss:   0.797114\n",
      "train loss:   0.829793\n",
      "train loss:   1.504424\n",
      "train loss:   0.858091\n",
      "train loss:   0.986732\n",
      "train loss:   1.028886\n",
      "train loss:   1.342998\n",
      "train loss:   0.923348\n",
      "train loss:   1.044509\n",
      "train loss:   1.297318\n",
      "train loss:   1.568251\n",
      "train loss:   1.184654\n",
      "train loss:   0.987725\n",
      "train loss:   0.937135\n",
      "train loss:   0.859513\n",
      "train loss:   1.310350\n",
      "train loss:   0.957504\n",
      "train loss:   1.092712\n",
      "train loss:   0.917049\n",
      "train loss:   1.222082\n",
      "train loss:   1.200768\n",
      "train loss:   0.790530\n",
      "train loss:   1.083648\n",
      "train loss:   0.889336\n",
      "train loss:   0.907653\n",
      "train loss:   1.044282\n",
      "########### epoch 35 ###########\n",
      "########### loop 6400 ###########\n",
      "test loss:   0.521565   test accuracy:   0.833333\n",
      "########### loop 6400 ###########\n",
      "train loss:   1.069944\n",
      "train loss:   0.835498\n",
      "train loss:   1.084753\n",
      "train loss:   1.220877\n",
      "train loss:   0.569194\n",
      "train loss:   1.068638\n",
      "train loss:   1.185738\n",
      "train loss:   0.989844\n",
      "train loss:   0.810719\n",
      "train loss:   0.862713\n",
      "train loss:   0.866129\n",
      "train loss:   1.239583\n",
      "train loss:   1.001337\n",
      "train loss:   1.456500\n",
      "train loss:   1.383614\n",
      "train loss:   0.824646\n",
      "train loss:   0.894827\n",
      "train loss:   1.093701\n",
      "train loss:   0.740542\n",
      "train loss:   1.324428\n",
      "train loss:   1.103153\n",
      "train loss:   0.910038\n",
      "train loss:   1.143623\n",
      "train loss:   0.983834\n",
      "train loss:   1.114644\n",
      "train loss:   0.932136\n",
      "train loss:   1.139789\n",
      "train loss:   0.954938\n",
      "train loss:   1.409780\n",
      "train loss:   1.358958\n",
      "train loss:   0.607531\n",
      "train loss:   0.754124\n",
      "train loss:   1.040184\n",
      "train loss:   1.020037\n",
      "train loss:   1.024733\n",
      "train loss:   0.959301\n",
      "train loss:   0.906389\n",
      "train loss:   1.015480\n",
      "train loss:   1.305660\n",
      "train loss:   0.979833\n",
      "train loss:   1.401358\n",
      "train loss:   0.787354\n",
      "train loss:   0.742861\n",
      "train loss:   0.675555\n",
      "train loss:   0.879120\n",
      "train loss:   1.039817\n",
      "train loss:   1.193235\n",
      "train loss:   1.042766\n",
      "train loss:   1.049636\n",
      "train loss:   1.251445\n",
      "########### epoch 35 ###########\n",
      "########### loop 6450 ###########\n",
      "test loss:   0.147651   test accuracy:   1.000000\n",
      "########### loop 6450 ###########\n",
      "train loss:   1.293369\n",
      "train loss:   0.747797\n",
      "train loss:   1.194187\n",
      "train loss:   1.302643\n",
      "train loss:   1.011958\n",
      "train loss:   1.440789\n",
      "train loss:   1.096245\n",
      "train loss:   1.158736\n",
      "train loss:   1.046774\n",
      "train loss:   1.224560\n",
      "train loss:   0.653091\n",
      "train loss:   0.842068\n",
      "train loss:   1.285249\n",
      "train loss:   1.166402\n",
      "train loss:   0.790037\n",
      "train loss:   0.890435\n",
      "train loss:   1.212007\n",
      "train loss:   0.773450\n",
      "train loss:   0.961967\n",
      "train loss:   1.111664\n",
      "train loss:   1.015086\n",
      "train loss:   0.938014\n",
      "train loss:   0.962166\n",
      "train loss:   0.948593\n",
      "train loss:   1.011013\n",
      "train loss:   1.268086\n",
      "train loss:   1.091501\n",
      "train loss:   0.795176\n",
      "train loss:   0.805010\n",
      "train loss:   0.885404\n",
      "train loss:   0.893772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.969112\n",
      "train loss:   1.163419\n",
      "train loss:   0.938057\n",
      "train loss:   1.005207\n",
      "train loss:   1.353099\n",
      "train loss:   1.207762\n",
      "train loss:   1.123093\n",
      "train loss:   0.797873\n",
      "train loss:   0.844626\n",
      "train loss:   0.924709\n",
      "train loss:   0.923010\n",
      "train loss:   0.875199\n",
      "train loss:   1.241137\n",
      "train loss:   1.166379\n",
      "train loss:   0.871837\n",
      "train loss:   1.092156\n",
      "train loss:   1.081991\n",
      "train loss:   0.840472\n",
      "train loss:   0.792834\n",
      "########### epoch 35 ###########\n",
      "########### loop 6500 ###########\n",
      "test loss:   0.291849   test accuracy:   0.916667\n",
      "########### loop 6500 ###########\n",
      "train loss:   0.870779\n",
      "train loss:   1.216466\n",
      "train loss:   1.132357\n",
      "train loss:   1.176713\n",
      "train loss:   1.115934\n",
      "train loss:   1.054462\n",
      "train loss:   1.245872\n",
      "train loss:   0.834902\n",
      "train loss:   0.856513\n",
      "train loss:   0.933953\n",
      "train loss:   0.995669\n",
      "train loss:   0.884915\n",
      "train loss:   1.182856\n",
      "train loss:   0.775464\n",
      "train loss:   0.822726\n",
      "train loss:   1.104390\n",
      "train loss:   1.181754\n",
      "train loss:   1.366671\n",
      "train loss:   0.933878\n",
      "train loss:   0.994054\n",
      "train loss:   1.125884\n",
      "train loss:   0.752747\n",
      "train loss:   1.016902\n",
      "train loss:   1.348477\n",
      "train loss:   0.877831\n",
      "train loss:   1.083719\n",
      "train loss:   1.329451\n",
      "train loss:   1.208197\n",
      "train loss:   0.999402\n",
      "train loss:   1.227423\n",
      "train loss:   0.781358\n",
      "train loss:   1.161590\n",
      "train loss:   1.139845\n",
      "train loss:   0.675731\n",
      "train loss:   1.353711\n",
      "train loss:   1.086917\n",
      "train loss:   1.203397\n",
      "train loss:   0.961409\n",
      "train loss:   1.146833\n",
      "train loss:   1.165020\n",
      "train loss:   1.182767\n",
      "train loss:   0.786903\n",
      "train loss:   1.022580\n",
      "train loss:   1.091385\n",
      "train loss:   1.082207\n",
      "train loss:   0.949970\n",
      "train loss:   0.951329\n",
      "train loss:   0.889896\n",
      "train loss:   1.008721\n",
      "train loss:   0.759906\n",
      "########### epoch 35 ###########\n",
      "########### loop 6550 ###########\n",
      "test loss:   0.245059   test accuracy:   1.000000\n",
      "########### loop 6550 ###########\n",
      "train loss:   1.140057\n",
      "train loss:   1.169603\n",
      "train loss:   1.290187\n",
      "train loss:   1.011167\n",
      "train loss:   1.299456\n",
      "train loss:   1.198468\n",
      "train loss:   0.792926\n",
      "train loss:   1.197366\n",
      "train loss:   0.941684\n",
      "train loss:   0.996742\n",
      "train loss:   1.333109\n",
      "train loss:   1.351265\n",
      "train loss:   0.737709\n",
      "train loss:   1.088809\n",
      "train loss:   0.958703\n",
      "train loss:   1.242980\n",
      "train loss:   1.180045\n",
      "train loss:   1.228083\n",
      "train loss:   1.175683\n",
      "train loss:   1.274191\n",
      "train loss:   1.234178\n",
      "train loss:   0.611929\n",
      "train loss:   0.908537\n",
      "train loss:   0.997112\n",
      "train loss:   1.066121\n",
      "train loss:   1.004178\n",
      "train loss:   0.993611\n",
      "train loss:   0.966585\n",
      "train loss:   1.311120\n",
      "train loss:   1.087044\n",
      "train loss:   1.124768\n",
      "train loss:   1.160506\n",
      "train loss:   0.853189\n",
      "train loss:   1.165369\n",
      "train loss:   1.109788\n",
      "train loss:   1.099704\n",
      "train loss:   1.343132\n",
      "train loss:   1.324365\n",
      "train loss:   1.192906\n",
      "train loss:   0.819733\n",
      "train loss:   0.861148\n",
      "train loss:   1.264781\n",
      "train loss:   1.236523\n",
      "train loss:   1.199476\n",
      "train loss:   0.974068\n",
      "train loss:   1.052206\n",
      "train loss:   1.050970\n",
      "train loss:   1.184089\n",
      "train loss:   1.029679\n",
      "train loss:   1.094788\n",
      "########### epoch 36 ###########\n",
      "########### loop 6600 ###########\n",
      "test loss:   0.300050   test accuracy:   0.958333\n",
      "########### loop 6600 ###########\n",
      "train loss:   0.781505\n",
      "train loss:   1.015401\n",
      "train loss:   0.806180\n",
      "train loss:   1.207948\n",
      "train loss:   0.861261\n",
      "train loss:   0.756633\n",
      "train loss:   0.847062\n",
      "train loss:   1.209096\n",
      "train loss:   1.032082\n",
      "train loss:   1.595308\n",
      "train loss:   0.889982\n",
      "train loss:   0.889668\n",
      "train loss:   1.132284\n",
      "train loss:   1.210650\n",
      "train loss:   1.031877\n",
      "train loss:   1.114016\n",
      "train loss:   0.902060\n",
      "train loss:   1.325857\n",
      "train loss:   1.133434\n",
      "train loss:   1.327186\n",
      "train loss:   0.968601\n",
      "train loss:   1.328303\n",
      "train loss:   1.367008\n",
      "train loss:   0.843101\n",
      "train loss:   0.823979\n",
      "train loss:   1.431449\n",
      "train loss:   1.131604\n",
      "train loss:   1.161708\n",
      "train loss:   0.819325\n",
      "train loss:   0.790581\n",
      "train loss:   0.925683\n",
      "train loss:   1.340645\n",
      "train loss:   0.552819\n",
      "train loss:   0.707201\n",
      "train loss:   0.900235\n",
      "train loss:   0.740523\n",
      "train loss:   0.938086\n",
      "train loss:   1.245881\n",
      "train loss:   1.352640\n",
      "train loss:   0.794708\n",
      "train loss:   0.996942\n",
      "train loss:   0.810739\n",
      "train loss:   0.908283\n",
      "train loss:   1.079271\n",
      "train loss:   1.279034\n",
      "train loss:   1.144673\n",
      "train loss:   1.038880\n",
      "train loss:   0.917120\n",
      "train loss:   0.829814\n",
      "train loss:   0.857026\n",
      "########### epoch 36 ###########\n",
      "########### loop 6650 ###########\n",
      "test loss:   0.297759   test accuracy:   0.916667\n",
      "########### loop 6650 ###########\n",
      "train loss:   1.009170\n",
      "train loss:   1.020264\n",
      "train loss:   0.908938\n",
      "train loss:   1.000379\n",
      "train loss:   0.743220\n",
      "train loss:   0.828742\n",
      "train loss:   0.888506\n",
      "train loss:   0.997740\n",
      "train loss:   1.237537\n",
      "train loss:   0.967713\n",
      "train loss:   0.927267\n",
      "train loss:   1.244177\n",
      "train loss:   0.776691\n",
      "train loss:   1.149819\n",
      "train loss:   0.840740\n",
      "train loss:   0.945292\n",
      "train loss:   1.160726\n",
      "train loss:   0.969152\n",
      "train loss:   1.204788\n",
      "train loss:   1.003839\n",
      "train loss:   0.750564\n",
      "train loss:   0.595725\n",
      "train loss:   0.996588\n",
      "train loss:   1.182971\n",
      "train loss:   1.065460\n",
      "train loss:   1.154020\n",
      "train loss:   1.263300\n",
      "train loss:   0.878720\n",
      "train loss:   1.092262\n",
      "train loss:   1.253523\n",
      "train loss:   1.238834\n",
      "train loss:   1.506801\n",
      "train loss:   1.167457\n",
      "train loss:   0.982083\n",
      "train loss:   1.318517\n",
      "train loss:   1.327114\n",
      "train loss:   0.919978\n",
      "train loss:   1.144959\n",
      "train loss:   0.833046\n",
      "train loss:   0.988566\n",
      "train loss:   0.928797\n",
      "train loss:   1.231038\n",
      "train loss:   1.198712\n",
      "train loss:   0.778808\n",
      "train loss:   0.912679\n",
      "train loss:   1.246220\n",
      "train loss:   1.017464\n",
      "train loss:   0.661237\n",
      "train loss:   1.220313\n",
      "train loss:   1.108138\n",
      "########### epoch 36 ###########\n",
      "########### loop 6700 ###########\n",
      "test loss:   0.391166   test accuracy:   0.958333\n",
      "########### loop 6700 ###########\n",
      "train loss:   0.830416\n",
      "train loss:   1.299315\n",
      "train loss:   1.224255\n",
      "train loss:   1.024341\n",
      "train loss:   1.095020\n",
      "train loss:   0.837941\n",
      "train loss:   0.885226\n",
      "train loss:   1.108535\n",
      "train loss:   0.981133\n",
      "train loss:   1.242902\n",
      "train loss:   0.930614\n",
      "train loss:   1.108734\n",
      "train loss:   0.989824\n",
      "train loss:   1.480573\n",
      "train loss:   1.068643\n",
      "train loss:   1.023701\n",
      "train loss:   1.080178\n",
      "train loss:   0.877871\n",
      "train loss:   0.969713\n",
      "train loss:   0.820541\n",
      "train loss:   0.770269\n",
      "train loss:   0.719806\n",
      "train loss:   0.803206\n",
      "train loss:   1.236226\n",
      "train loss:   1.424590\n",
      "train loss:   1.347106\n",
      "train loss:   1.530890\n",
      "train loss:   1.138315\n",
      "train loss:   0.814067\n",
      "train loss:   0.871737\n",
      "train loss:   0.971222\n",
      "train loss:   1.291336\n",
      "train loss:   1.152121\n",
      "train loss:   0.992142\n",
      "train loss:   0.978776\n",
      "train loss:   1.011137\n",
      "train loss:   1.049878\n",
      "train loss:   1.006634\n",
      "train loss:   0.935823\n",
      "train loss:   1.044008\n",
      "train loss:   1.217705\n",
      "train loss:   1.156063\n",
      "train loss:   1.069912\n",
      "train loss:   0.705516\n",
      "train loss:   1.055853\n",
      "train loss:   0.879384\n",
      "train loss:   0.927227\n",
      "train loss:   0.991630\n",
      "train loss:   1.071363\n",
      "train loss:   0.561358\n",
      "########### epoch 36 ###########\n",
      "########### loop 6750 ###########\n",
      "test loss:   0.459124   test accuracy:   0.916667\n",
      "########### loop 6750 ###########\n",
      "train loss:   0.987437\n",
      "train loss:   1.228745\n",
      "train loss:   0.789752\n",
      "train loss:   0.780315\n",
      "train loss:   1.178941\n",
      "train loss:   0.962447\n",
      "train loss:   0.978166\n",
      "train loss:   0.856438\n",
      "train loss:   1.138921\n",
      "train loss:   0.928550\n",
      "train loss:   1.473443\n",
      "train loss:   0.894079\n",
      "train loss:   1.453862\n",
      "train loss:   1.130569\n",
      "train loss:   1.534302\n",
      "train loss:   0.830735\n",
      "train loss:   0.725461\n",
      "train loss:   0.984450\n",
      "train loss:   0.965852\n",
      "train loss:   1.035986\n",
      "train loss:   0.711366\n",
      "train loss:   1.360644\n",
      "train loss:   0.691240\n",
      "train loss:   1.166674\n",
      "train loss:   1.189494\n",
      "train loss:   0.863570\n",
      "train loss:   0.802325\n",
      "train loss:   1.076526\n",
      "train loss:   1.165985\n",
      "train loss:   1.091169\n",
      "train loss:   1.186303\n",
      "train loss:   1.040731\n",
      "train loss:   1.052767\n",
      "train loss:   1.367553\n",
      "train loss:   0.754498\n",
      "train loss:   1.371501\n",
      "train loss:   1.448670\n",
      "train loss:   0.998945\n",
      "train loss:   1.471405\n",
      "train loss:   1.219129\n",
      "train loss:   1.103722\n",
      "train loss:   1.247869\n",
      "train loss:   0.785366\n",
      "train loss:   0.664821\n",
      "train loss:   1.089752\n",
      "train loss:   1.279139\n",
      "train loss:   0.842593\n",
      "train loss:   0.878640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.803820\n",
      "train loss:   0.894981\n",
      "########### epoch 37 ###########\n",
      "########### loop 6800 ###########\n",
      "test loss:   0.305985   test accuracy:   0.916667\n",
      "########### loop 6800 ###########\n",
      "train loss:   0.891200\n",
      "train loss:   0.786167\n",
      "train loss:   0.909599\n",
      "train loss:   0.872821\n",
      "train loss:   0.999908\n",
      "train loss:   0.883169\n",
      "train loss:   0.924653\n",
      "train loss:   0.822039\n",
      "train loss:   1.343764\n",
      "train loss:   0.565942\n",
      "train loss:   1.156469\n",
      "train loss:   0.914305\n",
      "train loss:   1.262896\n",
      "train loss:   1.132582\n",
      "train loss:   1.238056\n",
      "train loss:   1.122545\n",
      "train loss:   0.717816\n",
      "train loss:   0.778198\n",
      "train loss:   0.703475\n",
      "train loss:   1.098856\n",
      "train loss:   1.224269\n",
      "train loss:   1.085576\n",
      "train loss:   1.274973\n",
      "train loss:   1.286631\n",
      "train loss:   0.913599\n",
      "train loss:   0.996956\n",
      "train loss:   0.582385\n",
      "train loss:   1.153600\n",
      "train loss:   0.873414\n",
      "train loss:   0.566625\n",
      "train loss:   1.282453\n",
      "train loss:   0.979171\n",
      "train loss:   1.077572\n",
      "train loss:   0.818615\n",
      "train loss:   1.202542\n",
      "train loss:   1.087001\n",
      "train loss:   0.933900\n",
      "train loss:   0.592356\n",
      "train loss:   1.048794\n",
      "train loss:   1.087538\n",
      "train loss:   1.085696\n",
      "train loss:   0.602729\n",
      "train loss:   1.201852\n",
      "train loss:   1.043381\n",
      "train loss:   1.473053\n",
      "train loss:   0.983189\n",
      "train loss:   0.622197\n",
      "train loss:   1.125055\n",
      "train loss:   1.051129\n",
      "train loss:   1.540807\n",
      "########### epoch 37 ###########\n",
      "########### loop 6850 ###########\n",
      "test loss:   0.221390   test accuracy:   0.958333\n",
      "########### loop 6850 ###########\n",
      "train loss:   0.681806\n",
      "train loss:   0.988839\n",
      "train loss:   0.987025\n",
      "train loss:   0.894908\n",
      "train loss:   1.147553\n",
      "train loss:   0.945554\n",
      "train loss:   0.732299\n",
      "train loss:   1.064494\n",
      "train loss:   1.008216\n",
      "train loss:   1.198710\n",
      "train loss:   0.489760\n",
      "train loss:   1.324390\n",
      "train loss:   1.106774\n",
      "train loss:   1.202256\n",
      "train loss:   1.093158\n",
      "train loss:   0.824959\n",
      "train loss:   0.869986\n",
      "train loss:   1.117559\n",
      "train loss:   0.850931\n",
      "train loss:   1.160105\n",
      "train loss:   0.928842\n",
      "train loss:   0.921691\n",
      "train loss:   1.213025\n",
      "train loss:   0.928492\n",
      "train loss:   1.145245\n",
      "train loss:   1.153686\n",
      "train loss:   1.263633\n",
      "train loss:   0.993107\n",
      "train loss:   1.113837\n",
      "train loss:   1.204672\n",
      "train loss:   0.844637\n",
      "train loss:   1.148660\n",
      "train loss:   0.883585\n",
      "train loss:   1.173289\n",
      "train loss:   1.067621\n",
      "train loss:   1.198225\n",
      "train loss:   1.257190\n",
      "train loss:   1.012136\n",
      "train loss:   1.040167\n",
      "train loss:   1.294596\n",
      "train loss:   1.310902\n",
      "train loss:   1.052785\n",
      "train loss:   0.899061\n",
      "train loss:   0.884492\n",
      "train loss:   1.127046\n",
      "train loss:   1.345960\n",
      "train loss:   0.868218\n",
      "train loss:   1.040927\n",
      "train loss:   0.560391\n",
      "train loss:   0.918496\n",
      "########### epoch 37 ###########\n",
      "########### loop 6900 ###########\n",
      "test loss:   0.515815   test accuracy:   0.916667\n",
      "########### loop 6900 ###########\n",
      "train loss:   1.513357\n",
      "train loss:   1.130851\n",
      "train loss:   1.139063\n",
      "train loss:   1.167503\n",
      "train loss:   0.766221\n",
      "train loss:   1.257481\n",
      "train loss:   0.716430\n",
      "train loss:   1.296280\n",
      "train loss:   0.931093\n",
      "train loss:   1.162723\n",
      "train loss:   0.531057\n",
      "train loss:   0.766907\n",
      "train loss:   0.992920\n",
      "train loss:   1.139226\n",
      "train loss:   0.922046\n",
      "train loss:   1.091433\n",
      "train loss:   0.834112\n",
      "train loss:   1.227198\n",
      "train loss:   1.140901\n",
      "train loss:   1.248815\n",
      "train loss:   1.070957\n",
      "train loss:   0.904924\n",
      "train loss:   0.973671\n",
      "train loss:   1.066253\n",
      "train loss:   1.237857\n",
      "train loss:   1.025372\n",
      "train loss:   1.140378\n",
      "train loss:   0.772204\n",
      "train loss:   1.113846\n",
      "train loss:   1.138874\n",
      "train loss:   1.260018\n",
      "train loss:   1.321803\n",
      "train loss:   0.981752\n",
      "train loss:   0.872110\n",
      "train loss:   1.147056\n",
      "train loss:   1.225827\n",
      "train loss:   0.876166\n",
      "train loss:   1.162346\n",
      "train loss:   0.954715\n",
      "train loss:   1.236650\n",
      "train loss:   1.001780\n",
      "train loss:   1.235652\n",
      "train loss:   0.828587\n",
      "train loss:   0.784804\n",
      "train loss:   0.974768\n",
      "train loss:   0.914596\n",
      "train loss:   0.650613\n",
      "train loss:   0.908178\n",
      "train loss:   1.223268\n",
      "train loss:   0.720532\n",
      "########### epoch 37 ###########\n",
      "########### loop 6950 ###########\n",
      "test loss:   0.317967   test accuracy:   0.958333\n",
      "########### loop 6950 ###########\n",
      "train loss:   1.318047\n",
      "train loss:   1.227011\n",
      "train loss:   0.847964\n",
      "train loss:   1.404728\n",
      "train loss:   0.809818\n",
      "train loss:   0.875428\n",
      "train loss:   0.776002\n",
      "train loss:   1.072666\n",
      "train loss:   1.011125\n",
      "train loss:   1.270210\n",
      "train loss:   0.606454\n",
      "train loss:   1.161916\n",
      "train loss:   0.776429\n",
      "train loss:   0.958052\n",
      "train loss:   1.045342\n",
      "train loss:   1.016147\n",
      "train loss:   0.858026\n",
      "train loss:   0.764675\n",
      "train loss:   1.014703\n",
      "train loss:   1.138621\n",
      "train loss:   0.699551\n",
      "train loss:   1.004625\n",
      "train loss:   1.180146\n",
      "train loss:   1.080336\n",
      "train loss:   1.004339\n",
      "train loss:   0.904065\n",
      "train loss:   1.630355\n",
      "train loss:   1.045460\n",
      "train loss:   1.490284\n",
      "train loss:   1.337390\n",
      "train loss:   0.717355\n",
      "train loss:   0.899213\n",
      "train loss:   1.159253\n",
      "train loss:   0.929550\n",
      "train loss:   1.094046\n",
      "train loss:   0.831193\n",
      "train loss:   0.788122\n",
      "train loss:   1.214324\n",
      "train loss:   1.531440\n",
      "train loss:   0.755189\n",
      "train loss:   0.970854\n",
      "train loss:   0.914984\n",
      "train loss:   0.934319\n",
      "train loss:   1.306576\n",
      "train loss:   1.008533\n",
      "train loss:   0.880200\n",
      "train loss:   0.815097\n",
      "train loss:   1.179070\n",
      "train loss:   1.044635\n",
      "train loss:   1.113896\n",
      "########### epoch 38 ###########\n",
      "########### loop 7000 ###########\n",
      "test loss:   0.294781   test accuracy:   0.958333\n",
      "########### loop 7000 ###########\n",
      "train loss:   0.731024\n",
      "train loss:   1.305499\n",
      "train loss:   0.985769\n",
      "train loss:   1.388683\n",
      "train loss:   1.120184\n",
      "train loss:   1.450249\n",
      "train loss:   0.843052\n",
      "train loss:   0.762844\n",
      "train loss:   0.782568\n",
      "train loss:   0.754474\n",
      "train loss:   1.098008\n",
      "train loss:   0.685928\n",
      "train loss:   1.329033\n",
      "train loss:   1.118794\n",
      "train loss:   0.606845\n",
      "train loss:   0.965052\n",
      "train loss:   0.816377\n",
      "train loss:   1.012266\n",
      "train loss:   1.281357\n",
      "train loss:   1.097102\n",
      "train loss:   1.002020\n",
      "train loss:   1.123866\n",
      "train loss:   1.189147\n",
      "train loss:   0.580639\n",
      "train loss:   0.977530\n",
      "train loss:   1.207384\n",
      "train loss:   1.082518\n",
      "train loss:   0.788589\n",
      "train loss:   1.165986\n",
      "train loss:   0.975112\n",
      "train loss:   1.006126\n",
      "train loss:   1.532212\n",
      "train loss:   0.890214\n",
      "train loss:   1.128223\n",
      "train loss:   0.513962\n",
      "train loss:   1.016214\n",
      "train loss:   1.390608\n",
      "train loss:   1.513142\n",
      "train loss:   0.839051\n",
      "train loss:   1.208927\n",
      "train loss:   0.886103\n",
      "train loss:   1.176739\n",
      "train loss:   1.102056\n",
      "train loss:   1.037363\n",
      "train loss:   0.813119\n",
      "train loss:   1.176869\n",
      "train loss:   1.057878\n",
      "train loss:   1.169238\n",
      "train loss:   1.111579\n",
      "train loss:   0.993240\n",
      "########### epoch 38 ###########\n",
      "########### loop 7050 ###########\n",
      "test loss:   0.164566   test accuracy:   1.000000\n",
      "########### loop 7050 ###########\n",
      "train loss:   0.971067\n",
      "train loss:   0.832892\n",
      "train loss:   1.364895\n",
      "train loss:   0.998867\n",
      "train loss:   1.263007\n",
      "train loss:   0.920452\n",
      "train loss:   0.901795\n",
      "train loss:   1.148894\n",
      "train loss:   1.280599\n",
      "train loss:   1.077554\n",
      "train loss:   0.844144\n",
      "train loss:   1.003350\n",
      "train loss:   1.218988\n",
      "train loss:   1.071891\n",
      "train loss:   1.290198\n",
      "train loss:   1.187464\n",
      "train loss:   0.996282\n",
      "train loss:   1.014722\n",
      "train loss:   0.783655\n",
      "train loss:   1.310864\n",
      "train loss:   1.023700\n",
      "train loss:   0.933207\n",
      "train loss:   0.879722\n",
      "train loss:   1.094222\n",
      "train loss:   1.234187\n",
      "train loss:   0.910873\n",
      "train loss:   1.411611\n",
      "train loss:   1.000776\n",
      "train loss:   0.973548\n",
      "train loss:   1.183813\n",
      "train loss:   0.922271\n",
      "train loss:   0.911560\n",
      "train loss:   0.788023\n",
      "train loss:   1.251708\n",
      "train loss:   0.892193\n",
      "train loss:   0.769360\n",
      "train loss:   0.913347\n",
      "train loss:   1.002892\n",
      "train loss:   1.011114\n",
      "train loss:   0.823863\n",
      "train loss:   1.043249\n",
      "train loss:   1.221230\n",
      "train loss:   0.895729\n",
      "train loss:   1.118606\n",
      "train loss:   0.834005\n",
      "train loss:   1.447945\n",
      "train loss:   0.974827\n",
      "train loss:   0.510776\n",
      "train loss:   0.956628\n",
      "train loss:   1.322467\n",
      "########### epoch 38 ###########\n",
      "########### loop 7100 ###########\n",
      "test loss:   0.294444   test accuracy:   0.916667\n",
      "########### loop 7100 ###########\n",
      "train loss:   1.238250\n",
      "train loss:   1.254272\n",
      "train loss:   1.040035\n",
      "train loss:   1.346905\n",
      "train loss:   0.938884\n",
      "train loss:   0.972218\n",
      "train loss:   1.226854\n",
      "train loss:   0.858927\n",
      "train loss:   1.328817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.030182\n",
      "train loss:   0.803092\n",
      "train loss:   0.744581\n",
      "train loss:   0.927462\n",
      "train loss:   1.111832\n",
      "train loss:   0.825087\n",
      "train loss:   1.225461\n",
      "train loss:   1.122425\n",
      "train loss:   1.441822\n",
      "train loss:   0.914562\n",
      "train loss:   0.904166\n",
      "train loss:   0.920628\n",
      "train loss:   1.065840\n",
      "train loss:   1.075699\n",
      "train loss:   1.565145\n",
      "train loss:   1.231800\n",
      "train loss:   1.236077\n",
      "train loss:   0.863126\n",
      "train loss:   0.803326\n",
      "train loss:   1.301973\n",
      "train loss:   0.943964\n",
      "train loss:   0.966072\n",
      "train loss:   1.254976\n",
      "train loss:   0.536203\n",
      "train loss:   1.285513\n",
      "train loss:   0.970496\n",
      "train loss:   1.417279\n",
      "train loss:   0.828753\n",
      "train loss:   0.795921\n",
      "train loss:   1.111745\n",
      "train loss:   1.264358\n",
      "train loss:   0.655205\n",
      "train loss:   1.080997\n",
      "train loss:   0.877095\n",
      "train loss:   0.922690\n",
      "train loss:   1.072188\n",
      "train loss:   1.263184\n",
      "train loss:   1.214167\n",
      "train loss:   0.889575\n",
      "train loss:   0.905462\n",
      "train loss:   0.804436\n",
      "########### epoch 39 ###########\n",
      "########### loop 7150 ###########\n",
      "test loss:   0.174560   test accuracy:   1.000000\n",
      "########### loop 7150 ###########\n",
      "train loss:   0.981465\n",
      "train loss:   0.923332\n",
      "train loss:   1.357372\n",
      "train loss:   1.331540\n",
      "train loss:   0.826761\n",
      "train loss:   1.027111\n",
      "train loss:   0.950298\n",
      "train loss:   1.166669\n",
      "train loss:   1.528599\n",
      "train loss:   0.725514\n",
      "train loss:   1.195369\n",
      "train loss:   1.335537\n",
      "train loss:   0.939269\n",
      "train loss:   0.991310\n",
      "train loss:   0.707079\n",
      "train loss:   1.123646\n",
      "train loss:   1.002968\n",
      "train loss:   1.215870\n",
      "train loss:   0.926252\n",
      "train loss:   1.185096\n",
      "train loss:   1.179977\n",
      "train loss:   1.207628\n",
      "train loss:   0.814386\n",
      "train loss:   0.998760\n",
      "train loss:   1.110132\n",
      "train loss:   1.215036\n",
      "train loss:   0.826745\n",
      "train loss:   1.211527\n",
      "train loss:   0.756751\n",
      "train loss:   1.066134\n",
      "train loss:   1.046431\n",
      "train loss:   0.937006\n",
      "train loss:   1.075907\n",
      "train loss:   0.816683\n",
      "train loss:   0.754798\n",
      "train loss:   0.784861\n",
      "train loss:   0.925996\n",
      "train loss:   0.868327\n",
      "train loss:   1.255493\n",
      "train loss:   1.311537\n",
      "train loss:   0.924014\n",
      "train loss:   1.111694\n",
      "train loss:   1.273384\n",
      "train loss:   1.014612\n",
      "train loss:   0.869604\n",
      "train loss:   0.823346\n",
      "train loss:   0.720701\n",
      "train loss:   0.939103\n",
      "train loss:   1.154043\n",
      "train loss:   0.968877\n",
      "########### epoch 39 ###########\n",
      "########### loop 7200 ###########\n",
      "test loss:   0.287038   test accuracy:   0.958333\n",
      "########### loop 7200 ###########\n",
      "train loss:   0.797716\n",
      "train loss:   1.409254\n",
      "train loss:   0.954024\n",
      "train loss:   1.017370\n",
      "train loss:   1.099852\n",
      "train loss:   1.226207\n",
      "train loss:   1.336610\n",
      "train loss:   0.790730\n",
      "train loss:   1.184591\n",
      "train loss:   1.088322\n",
      "train loss:   0.816860\n",
      "train loss:   0.957213\n",
      "train loss:   1.097290\n",
      "train loss:   1.355840\n",
      "train loss:   1.315976\n",
      "train loss:   0.745793\n",
      "train loss:   1.098262\n",
      "train loss:   1.346740\n",
      "train loss:   0.861837\n",
      "train loss:   1.195739\n",
      "train loss:   0.579537\n",
      "train loss:   1.060638\n",
      "train loss:   1.091269\n",
      "train loss:   0.994108\n",
      "train loss:   0.918495\n",
      "train loss:   0.950563\n",
      "train loss:   0.930052\n",
      "train loss:   0.898642\n",
      "train loss:   0.939886\n",
      "train loss:   0.748214\n",
      "train loss:   1.167516\n",
      "train loss:   1.312047\n",
      "train loss:   0.905620\n",
      "train loss:   1.296661\n",
      "train loss:   1.257306\n",
      "train loss:   0.669238\n",
      "train loss:   1.253344\n",
      "train loss:   1.500466\n",
      "train loss:   0.914410\n",
      "train loss:   0.696133\n",
      "train loss:   1.057943\n",
      "train loss:   0.973180\n",
      "train loss:   1.085429\n",
      "train loss:   0.921851\n",
      "train loss:   1.124498\n",
      "train loss:   0.817588\n",
      "train loss:   0.951139\n",
      "train loss:   1.160375\n",
      "train loss:   0.952314\n",
      "train loss:   1.104442\n",
      "########### epoch 39 ###########\n",
      "########### loop 7250 ###########\n",
      "test loss:   0.320548   test accuracy:   0.958333\n",
      "########### loop 7250 ###########\n",
      "train loss:   0.615818\n",
      "train loss:   1.215709\n",
      "train loss:   1.155335\n",
      "train loss:   0.766630\n",
      "train loss:   1.130847\n",
      "train loss:   1.068371\n",
      "train loss:   1.432730\n",
      "train loss:   0.761300\n",
      "train loss:   0.983324\n",
      "train loss:   1.368482\n",
      "train loss:   0.844808\n",
      "train loss:   0.975967\n",
      "train loss:   1.105378\n",
      "train loss:   0.671291\n",
      "train loss:   1.121167\n",
      "train loss:   1.425310\n",
      "train loss:   1.101808\n",
      "train loss:   1.033639\n",
      "train loss:   1.136204\n",
      "train loss:   0.961922\n",
      "train loss:   1.133960\n",
      "train loss:   1.201474\n",
      "train loss:   0.878932\n",
      "train loss:   0.963551\n",
      "train loss:   1.139777\n",
      "train loss:   0.842903\n",
      "train loss:   1.091270\n",
      "train loss:   1.593754\n",
      "train loss:   0.576139\n",
      "train loss:   0.835520\n",
      "train loss:   1.157740\n",
      "train loss:   0.844477\n",
      "train loss:   1.049617\n",
      "train loss:   0.974519\n",
      "train loss:   1.096417\n",
      "train loss:   1.181168\n",
      "train loss:   0.969213\n",
      "train loss:   0.978846\n",
      "train loss:   0.460581\n",
      "train loss:   0.854098\n",
      "train loss:   1.501918\n",
      "train loss:   1.304373\n",
      "train loss:   0.691055\n",
      "train loss:   0.742137\n",
      "train loss:   1.024754\n",
      "train loss:   1.046054\n",
      "train loss:   1.417633\n",
      "train loss:   1.015499\n",
      "train loss:   0.886962\n",
      "train loss:   0.876290\n",
      "########### epoch 39 ###########\n",
      "########### loop 7300 ###########\n",
      "test loss:   0.190668   test accuracy:   1.000000\n",
      "########### loop 7300 ###########\n",
      "train loss:   1.111597\n",
      "train loss:   1.236997\n",
      "train loss:   0.862374\n",
      "train loss:   0.890101\n",
      "train loss:   0.904631\n",
      "train loss:   1.072645\n",
      "train loss:   0.791409\n",
      "train loss:   1.296592\n",
      "train loss:   1.352846\n",
      "train loss:   1.309567\n",
      "train loss:   1.068055\n",
      "train loss:   0.709232\n",
      "train loss:   1.215143\n",
      "train loss:   0.749011\n",
      "train loss:   0.977196\n",
      "train loss:   0.784018\n",
      "train loss:   0.772048\n",
      "train loss:   1.248492\n",
      "train loss:   0.876124\n",
      "train loss:   1.193141\n",
      "train loss:   0.996302\n",
      "train loss:   1.072517\n",
      "train loss:   1.026186\n",
      "train loss:   0.745742\n",
      "train loss:   0.661504\n",
      "train loss:   1.274527\n",
      "train loss:   1.166545\n",
      "train loss:   1.556503\n",
      "train loss:   1.187406\n",
      "train loss:   0.519873\n",
      "train loss:   1.005253\n",
      "train loss:   1.240016\n",
      "train loss:   0.861519\n",
      "train loss:   1.120682\n",
      "train loss:   1.386329\n",
      "train loss:   1.107060\n",
      "train loss:   0.915536\n",
      "train loss:   0.852517\n",
      "train loss:   1.562558\n",
      "train loss:   1.164888\n",
      "train loss:   1.115556\n",
      "train loss:   1.130362\n",
      "train loss:   1.156257\n",
      "train loss:   0.997431\n",
      "train loss:   1.109558\n",
      "train loss:   1.115381\n",
      "train loss:   0.947139\n",
      "train loss:   1.062710\n",
      "train loss:   1.037087\n",
      "train loss:   1.048846\n",
      "########### epoch 40 ###########\n",
      "########### loop 7350 ###########\n",
      "test loss:   0.267115   test accuracy:   0.958333\n",
      "########### loop 7350 ###########\n",
      "train loss:   1.157521\n",
      "train loss:   0.719288\n",
      "train loss:   1.056364\n",
      "train loss:   1.292038\n",
      "train loss:   0.919603\n",
      "train loss:   1.094326\n",
      "train loss:   1.154093\n",
      "train loss:   1.148887\n",
      "train loss:   0.980517\n",
      "train loss:   1.253674\n",
      "train loss:   0.948769\n",
      "train loss:   1.019439\n",
      "train loss:   0.791404\n",
      "train loss:   1.349581\n",
      "train loss:   1.147757\n",
      "train loss:   1.002064\n",
      "train loss:   1.021503\n",
      "train loss:   0.802587\n",
      "train loss:   1.385451\n",
      "train loss:   0.971724\n",
      "train loss:   1.455397\n",
      "train loss:   1.017248\n",
      "train loss:   0.955947\n",
      "train loss:   1.195586\n",
      "train loss:   0.655533\n",
      "train loss:   1.097678\n",
      "train loss:   0.789163\n",
      "train loss:   1.262280\n",
      "train loss:   0.946626\n",
      "train loss:   0.900321\n",
      "train loss:   1.028669\n",
      "train loss:   1.021110\n",
      "train loss:   0.864789\n",
      "train loss:   0.661384\n",
      "train loss:   1.045027\n",
      "train loss:   1.115650\n",
      "train loss:   1.215429\n",
      "train loss:   1.036269\n",
      "train loss:   0.782524\n",
      "train loss:   0.962465\n",
      "train loss:   0.946206\n",
      "train loss:   1.370273\n",
      "train loss:   1.038277\n",
      "train loss:   1.115602\n",
      "train loss:   1.055680\n",
      "train loss:   1.118060\n",
      "train loss:   0.845805\n",
      "train loss:   1.077558\n",
      "train loss:   1.161718\n",
      "train loss:   1.066536\n",
      "########### epoch 40 ###########\n",
      "########### loop 7400 ###########\n",
      "test loss:   0.427368   test accuracy:   0.916667\n",
      "########### loop 7400 ###########\n",
      "train loss:   1.191557\n",
      "train loss:   0.924372\n",
      "train loss:   0.925683\n",
      "train loss:   0.952368\n",
      "train loss:   1.034473\n",
      "train loss:   0.836002\n",
      "train loss:   1.172569\n",
      "train loss:   1.110855\n",
      "train loss:   0.949675\n",
      "train loss:   0.866198\n",
      "train loss:   0.910401\n",
      "train loss:   0.961608\n",
      "train loss:   0.973506\n",
      "train loss:   1.104302\n",
      "train loss:   0.977117\n",
      "train loss:   1.116339\n",
      "train loss:   1.052272\n",
      "train loss:   0.703880\n",
      "train loss:   0.984280\n",
      "train loss:   0.983211\n",
      "train loss:   1.067670\n",
      "train loss:   0.817750\n",
      "train loss:   0.716656\n",
      "train loss:   0.646950\n",
      "train loss:   0.848666\n",
      "train loss:   1.080178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.816446\n",
      "train loss:   0.864582\n",
      "train loss:   1.270371\n",
      "train loss:   1.183472\n",
      "train loss:   0.884675\n",
      "train loss:   0.799101\n",
      "train loss:   0.918829\n",
      "train loss:   0.915590\n",
      "train loss:   0.868058\n",
      "train loss:   0.854295\n",
      "train loss:   0.857826\n",
      "train loss:   1.161729\n",
      "train loss:   0.883648\n",
      "train loss:   0.752891\n",
      "train loss:   1.229814\n",
      "train loss:   1.166921\n",
      "train loss:   1.147574\n",
      "train loss:   1.213951\n",
      "train loss:   0.760682\n",
      "train loss:   1.105825\n",
      "train loss:   1.066163\n",
      "train loss:   1.490581\n",
      "train loss:   1.315769\n",
      "train loss:   1.279326\n",
      "########### epoch 40 ###########\n",
      "########### loop 7450 ###########\n",
      "test loss:   0.179161   test accuracy:   1.000000\n",
      "########### loop 7450 ###########\n",
      "train loss:   0.302939\n",
      "train loss:   1.255494\n",
      "train loss:   1.302055\n",
      "train loss:   0.654959\n",
      "train loss:   0.835187\n",
      "train loss:   1.042368\n",
      "train loss:   0.858709\n",
      "train loss:   0.902428\n",
      "train loss:   0.788383\n",
      "train loss:   0.879574\n",
      "train loss:   1.032544\n",
      "train loss:   1.225783\n",
      "train loss:   0.659293\n",
      "train loss:   1.072948\n",
      "train loss:   0.980139\n",
      "train loss:   0.989672\n",
      "train loss:   1.185489\n",
      "train loss:   1.277157\n",
      "train loss:   0.970238\n",
      "train loss:   1.091886\n",
      "train loss:   1.127567\n",
      "train loss:   1.004345\n",
      "train loss:   0.831050\n",
      "train loss:   0.908521\n",
      "train loss:   1.358144\n",
      "train loss:   0.834220\n",
      "train loss:   0.926186\n",
      "train loss:   0.860064\n",
      "train loss:   0.928298\n",
      "train loss:   1.452478\n",
      "train loss:   0.955098\n",
      "train loss:   0.793759\n",
      "train loss:   1.361057\n",
      "train loss:   0.741214\n",
      "train loss:   1.083965\n",
      "train loss:   1.311098\n",
      "train loss:   0.859152\n",
      "train loss:   0.797742\n",
      "train loss:   0.881184\n",
      "train loss:   0.874744\n",
      "train loss:   0.867752\n",
      "train loss:   1.356532\n",
      "train loss:   0.938284\n",
      "train loss:   0.997455\n",
      "train loss:   1.344430\n",
      "train loss:   0.914599\n",
      "train loss:   0.944408\n",
      "train loss:   0.758593\n",
      "train loss:   0.996067\n",
      "train loss:   1.280970\n",
      "########### epoch 40 ###########\n",
      "########### loop 7500 ###########\n",
      "test loss:   0.301726   test accuracy:   0.958333\n",
      "########### loop 7500 ###########\n",
      "train loss:   1.232591\n",
      "train loss:   1.253001\n",
      "train loss:   0.819361\n",
      "train loss:   0.999981\n",
      "train loss:   0.734969\n",
      "train loss:   0.758370\n",
      "train loss:   1.385565\n",
      "train loss:   1.206009\n",
      "train loss:   0.918771\n",
      "train loss:   0.919909\n",
      "train loss:   0.909927\n",
      "train loss:   0.860530\n",
      "train loss:   0.974826\n",
      "train loss:   0.713419\n",
      "train loss:   1.172203\n",
      "train loss:   1.228546\n",
      "train loss:   1.036497\n",
      "train loss:   1.170999\n",
      "train loss:   0.632178\n",
      "train loss:   1.164831\n",
      "train loss:   1.226937\n",
      "train loss:   1.166732\n",
      "train loss:   0.783237\n",
      "train loss:   1.246466\n",
      "train loss:   0.972722\n",
      "train loss:   1.240153\n",
      "train loss:   0.703050\n",
      "train loss:   1.165479\n",
      "train loss:   1.285010\n",
      "train loss:   1.391627\n",
      "train loss:   0.524958\n",
      "train loss:   1.272930\n",
      "train loss:   1.176696\n",
      "train loss:   0.992625\n",
      "train loss:   0.694587\n",
      "train loss:   1.461366\n",
      "train loss:   1.095222\n",
      "train loss:   0.827614\n",
      "train loss:   1.152067\n",
      "train loss:   0.971235\n",
      "train loss:   1.088320\n",
      "train loss:   0.874267\n",
      "train loss:   0.783703\n",
      "train loss:   0.958255\n",
      "train loss:   1.156626\n",
      "train loss:   0.776119\n",
      "train loss:   0.938437\n",
      "train loss:   0.844197\n",
      "train loss:   1.187387\n",
      "train loss:   1.050969\n",
      "########### epoch 41 ###########\n",
      "########### loop 7550 ###########\n",
      "test loss:   0.316771   test accuracy:   0.958333\n",
      "########### loop 7550 ###########\n",
      "train loss:   1.141909\n",
      "train loss:   1.154036\n",
      "train loss:   1.361928\n",
      "train loss:   0.944916\n",
      "train loss:   1.224063\n",
      "train loss:   0.747557\n",
      "train loss:   1.053635\n",
      "train loss:   1.038654\n",
      "train loss:   0.934623\n",
      "train loss:   1.093347\n",
      "train loss:   1.378487\n",
      "train loss:   1.243314\n",
      "train loss:   1.031178\n",
      "train loss:   0.871533\n",
      "train loss:   1.027553\n",
      "train loss:   1.137545\n",
      "train loss:   1.182042\n",
      "train loss:   1.147367\n",
      "train loss:   0.995679\n",
      "train loss:   1.176730\n",
      "train loss:   1.323070\n",
      "train loss:   0.982666\n",
      "train loss:   0.710632\n",
      "train loss:   1.337843\n",
      "train loss:   1.023882\n",
      "train loss:   0.999633\n",
      "train loss:   0.842938\n",
      "train loss:   0.814699\n",
      "train loss:   0.756678\n",
      "train loss:   1.031645\n",
      "train loss:   0.914405\n",
      "train loss:   1.024769\n",
      "train loss:   1.300809\n",
      "train loss:   1.234916\n",
      "train loss:   0.653904\n",
      "train loss:   0.994311\n",
      "train loss:   1.199642\n",
      "train loss:   0.973697\n",
      "train loss:   0.867302\n",
      "train loss:   1.117904\n",
      "train loss:   0.846205\n",
      "train loss:   0.771859\n",
      "train loss:   1.173130\n",
      "train loss:   0.896827\n",
      "train loss:   1.054189\n",
      "train loss:   0.682185\n",
      "train loss:   1.041588\n",
      "train loss:   0.922041\n",
      "train loss:   1.084470\n",
      "train loss:   0.810220\n",
      "########### epoch 41 ###########\n",
      "########### loop 7600 ###########\n",
      "test loss:   0.417010   test accuracy:   0.916667\n",
      "########### loop 7600 ###########\n",
      "train loss:   0.804796\n",
      "train loss:   1.273213\n",
      "train loss:   0.836967\n",
      "train loss:   1.101512\n",
      "train loss:   0.851233\n",
      "train loss:   0.963835\n",
      "train loss:   1.223846\n",
      "train loss:   0.890746\n",
      "train loss:   0.849001\n",
      "train loss:   0.707577\n",
      "train loss:   0.771031\n",
      "train loss:   0.916527\n",
      "train loss:   0.605457\n",
      "train loss:   0.977987\n",
      "train loss:   0.685004\n",
      "train loss:   1.077898\n",
      "train loss:   1.061692\n",
      "train loss:   1.086753\n",
      "train loss:   0.720795\n",
      "train loss:   1.072276\n",
      "train loss:   1.097558\n",
      "train loss:   0.964330\n",
      "train loss:   0.944505\n",
      "train loss:   1.108480\n",
      "train loss:   0.957487\n",
      "train loss:   1.096921\n",
      "train loss:   1.015093\n",
      "train loss:   0.950254\n",
      "train loss:   1.202935\n",
      "train loss:   1.072257\n",
      "train loss:   1.040301\n",
      "train loss:   0.797083\n",
      "train loss:   0.975803\n",
      "train loss:   0.489223\n",
      "train loss:   0.977197\n",
      "train loss:   1.047429\n",
      "train loss:   1.006809\n",
      "train loss:   1.317691\n",
      "train loss:   0.682163\n",
      "train loss:   0.904622\n",
      "train loss:   1.081625\n",
      "train loss:   0.914355\n",
      "train loss:   1.081566\n",
      "train loss:   0.679381\n",
      "train loss:   0.958125\n",
      "train loss:   0.833515\n",
      "train loss:   1.300908\n",
      "train loss:   1.100987\n",
      "train loss:   1.294482\n",
      "train loss:   0.855338\n",
      "########### epoch 41 ###########\n",
      "########### loop 7650 ###########\n",
      "test loss:   0.408079   test accuracy:   0.875000\n",
      "########### loop 7650 ###########\n",
      "train loss:   0.912875\n",
      "train loss:   1.252184\n",
      "train loss:   0.957155\n",
      "train loss:   1.038204\n",
      "train loss:   1.194264\n",
      "train loss:   1.019367\n",
      "train loss:   1.047445\n",
      "train loss:   1.517066\n",
      "train loss:   0.747726\n",
      "train loss:   1.156697\n",
      "train loss:   0.533238\n",
      "train loss:   1.496719\n",
      "train loss:   1.215473\n",
      "train loss:   0.993572\n",
      "train loss:   0.963748\n",
      "train loss:   1.187010\n",
      "train loss:   1.101730\n",
      "train loss:   0.825669\n",
      "train loss:   1.024342\n",
      "train loss:   1.192929\n",
      "train loss:   1.323564\n",
      "train loss:   0.888483\n",
      "train loss:   0.996241\n",
      "train loss:   1.241984\n",
      "train loss:   0.996616\n",
      "train loss:   0.706956\n",
      "train loss:   0.982607\n",
      "train loss:   1.418556\n",
      "train loss:   0.772235\n",
      "train loss:   1.073669\n",
      "train loss:   1.268621\n",
      "train loss:   1.094242\n",
      "train loss:   1.008022\n",
      "train loss:   1.356600\n",
      "train loss:   1.362113\n",
      "train loss:   1.214861\n",
      "train loss:   0.694792\n",
      "train loss:   1.000486\n",
      "train loss:   1.136197\n",
      "train loss:   1.062464\n",
      "train loss:   0.923722\n",
      "train loss:   1.044069\n",
      "train loss:   1.114961\n",
      "train loss:   0.851051\n",
      "train loss:   0.900649\n",
      "train loss:   0.880982\n",
      "train loss:   0.774363\n",
      "train loss:   1.079539\n",
      "train loss:   0.957017\n",
      "train loss:   1.255129\n",
      "########### epoch 41 ###########\n",
      "########### loop 7700 ###########\n",
      "test loss:   0.513921   test accuracy:   0.875000\n",
      "########### loop 7700 ###########\n",
      "train loss:   0.874251\n",
      "train loss:   0.908739\n",
      "train loss:   1.070184\n",
      "train loss:   1.060842\n",
      "train loss:   1.287300\n",
      "train loss:   1.224253\n",
      "train loss:   1.004929\n",
      "train loss:   0.604308\n",
      "train loss:   0.999970\n",
      "train loss:   1.230129\n",
      "train loss:   0.936194\n",
      "train loss:   1.084389\n",
      "train loss:   1.264267\n",
      "train loss:   1.202272\n",
      "train loss:   0.768067\n",
      "train loss:   0.611467\n",
      "train loss:   0.764600\n",
      "train loss:   1.033142\n",
      "train loss:   1.086567\n",
      "train loss:   1.199866\n",
      "train loss:   1.117969\n",
      "train loss:   1.318415\n",
      "train loss:   0.676598\n",
      "train loss:   0.875800\n",
      "train loss:   0.997869\n",
      "train loss:   0.965624\n",
      "train loss:   0.957223\n",
      "train loss:   0.806132\n",
      "train loss:   1.127129\n",
      "train loss:   1.142870\n",
      "train loss:   1.011444\n",
      "train loss:   0.847467\n",
      "train loss:   0.990033\n",
      "train loss:   1.030895\n",
      "train loss:   1.314943\n",
      "train loss:   1.006701\n",
      "train loss:   1.287301\n",
      "train loss:   1.031661\n",
      "train loss:   1.109346\n",
      "train loss:   1.024510\n",
      "train loss:   1.243386\n",
      "train loss:   1.027027\n",
      "train loss:   1.072027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.954944\n",
      "train loss:   0.814939\n",
      "train loss:   1.042385\n",
      "train loss:   1.065000\n",
      "train loss:   1.218802\n",
      "train loss:   1.201864\n",
      "train loss:   0.917064\n",
      "########### epoch 42 ###########\n",
      "########### loop 7750 ###########\n",
      "test loss:   0.141952   test accuracy:   1.000000\n",
      "########### loop 7750 ###########\n",
      "train loss:   0.797530\n",
      "train loss:   0.953822\n",
      "train loss:   1.135725\n",
      "train loss:   1.079715\n",
      "train loss:   1.095990\n",
      "train loss:   0.875558\n",
      "train loss:   1.516773\n",
      "train loss:   0.908237\n",
      "train loss:   0.775753\n",
      "train loss:   1.038691\n",
      "train loss:   1.025435\n",
      "train loss:   0.829606\n",
      "train loss:   0.814164\n",
      "train loss:   1.141626\n",
      "train loss:   0.862044\n",
      "train loss:   0.866979\n",
      "train loss:   0.932398\n",
      "train loss:   0.961464\n",
      "train loss:   0.875849\n",
      "train loss:   1.076360\n",
      "train loss:   1.191758\n",
      "train loss:   0.786493\n",
      "train loss:   1.293988\n",
      "train loss:   1.230926\n",
      "train loss:   1.418882\n",
      "train loss:   0.991774\n",
      "train loss:   0.819928\n",
      "train loss:   1.098379\n",
      "train loss:   1.104760\n",
      "train loss:   1.289497\n",
      "train loss:   0.804497\n",
      "train loss:   0.899517\n",
      "train loss:   0.925960\n",
      "train loss:   1.173852\n",
      "train loss:   0.978604\n",
      "train loss:   1.102850\n",
      "train loss:   1.260409\n",
      "train loss:   0.971126\n",
      "train loss:   1.351759\n",
      "train loss:   1.015669\n",
      "train loss:   0.992437\n",
      "train loss:   0.775270\n",
      "train loss:   1.153974\n",
      "train loss:   0.852530\n",
      "train loss:   1.226483\n",
      "train loss:   1.181973\n",
      "train loss:   0.809569\n",
      "train loss:   0.793912\n",
      "train loss:   1.257714\n",
      "train loss:   1.213286\n",
      "########### epoch 42 ###########\n",
      "########### loop 7800 ###########\n",
      "test loss:   0.490678   test accuracy:   0.916667\n",
      "########### loop 7800 ###########\n",
      "train loss:   0.576657\n",
      "train loss:   0.884453\n",
      "train loss:   0.956538\n",
      "train loss:   1.263978\n",
      "train loss:   0.972869\n",
      "train loss:   0.658572\n",
      "train loss:   0.842480\n",
      "train loss:   1.302102\n",
      "train loss:   1.251623\n",
      "train loss:   1.130366\n",
      "train loss:   1.118987\n",
      "train loss:   0.771655\n",
      "train loss:   1.121411\n",
      "train loss:   0.792762\n",
      "train loss:   0.991226\n",
      "train loss:   1.069960\n",
      "train loss:   1.152915\n",
      "train loss:   1.100286\n",
      "train loss:   0.923656\n",
      "train loss:   1.150494\n",
      "train loss:   1.154543\n",
      "train loss:   0.927075\n",
      "train loss:   1.566855\n",
      "train loss:   1.286296\n",
      "train loss:   0.745741\n",
      "train loss:   1.109267\n",
      "train loss:   1.146422\n",
      "train loss:   0.994822\n",
      "train loss:   0.910458\n",
      "train loss:   1.142388\n",
      "train loss:   0.872401\n",
      "train loss:   0.980671\n",
      "train loss:   0.990103\n",
      "train loss:   1.044403\n",
      "train loss:   1.107404\n",
      "train loss:   0.861232\n",
      "train loss:   1.460631\n",
      "train loss:   1.326109\n",
      "train loss:   0.990432\n",
      "train loss:   0.797941\n",
      "train loss:   0.644550\n",
      "train loss:   1.484810\n",
      "train loss:   1.010298\n",
      "train loss:   0.807701\n",
      "train loss:   1.210007\n",
      "train loss:   1.038886\n",
      "train loss:   1.091227\n",
      "train loss:   1.109707\n",
      "train loss:   0.565169\n",
      "train loss:   0.608294\n",
      "########### epoch 42 ###########\n",
      "########### loop 7850 ###########\n",
      "test loss:   0.279476   test accuracy:   1.000000\n",
      "########### loop 7850 ###########\n",
      "train loss:   1.226850\n",
      "train loss:   1.178862\n",
      "train loss:   1.201401\n",
      "train loss:   1.333899\n",
      "train loss:   1.020058\n",
      "train loss:   1.302118\n",
      "train loss:   1.316667\n",
      "train loss:   0.980489\n",
      "train loss:   1.130967\n",
      "train loss:   1.191464\n",
      "train loss:   0.873176\n",
      "train loss:   0.915762\n",
      "train loss:   0.926096\n",
      "train loss:   0.872985\n",
      "train loss:   0.991422\n",
      "train loss:   0.812018\n",
      "train loss:   1.103256\n",
      "train loss:   1.230552\n",
      "train loss:   1.058037\n",
      "train loss:   1.029586\n",
      "train loss:   1.070112\n",
      "train loss:   0.841318\n",
      "train loss:   0.944385\n",
      "train loss:   0.671347\n",
      "train loss:   1.181366\n",
      "train loss:   0.608265\n",
      "train loss:   1.121033\n",
      "train loss:   0.972744\n",
      "train loss:   0.989200\n",
      "train loss:   0.997492\n",
      "train loss:   0.834242\n",
      "train loss:   0.964091\n",
      "train loss:   1.332047\n",
      "train loss:   1.258670\n",
      "train loss:   0.770677\n",
      "train loss:   1.071302\n",
      "train loss:   0.835862\n",
      "train loss:   0.764439\n",
      "train loss:   1.142804\n",
      "train loss:   1.393311\n",
      "train loss:   0.987125\n",
      "train loss:   0.824589\n",
      "train loss:   0.680470\n",
      "train loss:   0.871872\n",
      "train loss:   0.920319\n",
      "train loss:   0.980697\n",
      "train loss:   1.187494\n",
      "train loss:   0.798539\n",
      "train loss:   1.167332\n",
      "train loss:   0.744422\n",
      "########### epoch 43 ###########\n",
      "########### loop 7900 ###########\n",
      "test loss:   0.456343   test accuracy:   0.875000\n",
      "########### loop 7900 ###########\n",
      "train loss:   0.895972\n",
      "train loss:   1.093069\n",
      "train loss:   1.125637\n",
      "train loss:   0.703858\n",
      "train loss:   1.329780\n",
      "train loss:   1.510161\n",
      "train loss:   1.322535\n",
      "train loss:   1.002234\n",
      "train loss:   0.965377\n",
      "train loss:   0.999476\n",
      "train loss:   0.877802\n",
      "train loss:   1.031408\n",
      "train loss:   1.054594\n",
      "train loss:   0.969072\n",
      "train loss:   1.201941\n",
      "train loss:   1.320014\n",
      "train loss:   0.907103\n",
      "train loss:   0.829666\n",
      "train loss:   0.740663\n",
      "train loss:   1.104475\n",
      "train loss:   1.467877\n",
      "train loss:   1.217318\n",
      "train loss:   0.971148\n",
      "train loss:   1.311484\n",
      "train loss:   0.870270\n",
      "train loss:   0.870388\n",
      "train loss:   0.827640\n",
      "train loss:   0.687891\n",
      "train loss:   1.007874\n",
      "train loss:   1.203006\n",
      "train loss:   0.819296\n",
      "train loss:   0.786158\n",
      "train loss:   1.081824\n",
      "train loss:   1.153867\n",
      "train loss:   0.923937\n",
      "train loss:   0.899086\n",
      "train loss:   0.751524\n",
      "train loss:   1.085582\n",
      "train loss:   1.323478\n",
      "train loss:   1.028975\n",
      "train loss:   0.930386\n",
      "train loss:   1.043698\n",
      "train loss:   0.809426\n",
      "train loss:   0.764136\n",
      "train loss:   1.037325\n",
      "train loss:   1.061149\n",
      "train loss:   1.285047\n",
      "train loss:   1.150180\n",
      "train loss:   1.311159\n",
      "train loss:   0.920077\n",
      "########### epoch 43 ###########\n",
      "########### loop 7950 ###########\n",
      "test loss:   0.282609   test accuracy:   1.000000\n",
      "########### loop 7950 ###########\n",
      "train loss:   1.267179\n",
      "train loss:   1.156385\n",
      "train loss:   1.068313\n",
      "train loss:   1.271410\n",
      "train loss:   1.106977\n",
      "train loss:   1.104405\n",
      "train loss:   0.868814\n",
      "train loss:   1.151238\n",
      "train loss:   1.010678\n",
      "train loss:   1.007042\n",
      "train loss:   0.648921\n",
      "train loss:   0.822281\n",
      "train loss:   0.962382\n",
      "train loss:   0.832537\n",
      "train loss:   1.145837\n",
      "train loss:   1.177397\n",
      "train loss:   1.048385\n",
      "train loss:   1.110750\n",
      "train loss:   1.051584\n",
      "train loss:   0.674441\n",
      "train loss:   1.449925\n",
      "train loss:   1.076975\n",
      "train loss:   0.765762\n",
      "train loss:   0.909421\n",
      "train loss:   1.087983\n",
      "train loss:   1.137443\n",
      "train loss:   1.053151\n",
      "train loss:   1.380499\n",
      "train loss:   1.007021\n",
      "train loss:   1.351481\n",
      "train loss:   1.358700\n",
      "train loss:   1.170253\n",
      "train loss:   1.035574\n",
      "train loss:   1.001124\n",
      "train loss:   1.238453\n",
      "train loss:   1.224344\n",
      "train loss:   0.883380\n",
      "train loss:   0.821090\n",
      "train loss:   0.996548\n",
      "train loss:   1.020656\n",
      "train loss:   1.061023\n",
      "train loss:   1.165667\n",
      "train loss:   1.098402\n",
      "train loss:   0.987236\n",
      "train loss:   0.961548\n",
      "train loss:   1.140349\n",
      "train loss:   1.027035\n",
      "train loss:   1.024301\n",
      "train loss:   0.969268\n",
      "train loss:   1.253280\n",
      "########### epoch 43 ###########\n",
      "########### loop 8000 ###########\n",
      "test loss:   0.265973   test accuracy:   0.958333\n",
      "########### loop 8000 ###########\n",
      "train loss:   1.243256\n",
      "train loss:   1.253057\n",
      "train loss:   0.801854\n",
      "train loss:   1.367463\n",
      "train loss:   0.944657\n",
      "train loss:   1.098947\n",
      "train loss:   1.146852\n",
      "train loss:   1.468316\n",
      "train loss:   0.813615\n",
      "train loss:   0.658199\n",
      "train loss:   1.202413\n",
      "train loss:   1.120049\n",
      "train loss:   0.910241\n",
      "train loss:   0.951321\n",
      "train loss:   0.881379\n",
      "train loss:   0.805378\n",
      "train loss:   0.815583\n",
      "train loss:   0.918394\n",
      "train loss:   1.013628\n",
      "train loss:   0.674967\n",
      "train loss:   0.769318\n",
      "train loss:   1.094472\n",
      "train loss:   1.295039\n",
      "train loss:   0.986678\n",
      "train loss:   1.068980\n",
      "train loss:   0.907281\n",
      "train loss:   1.052817\n",
      "train loss:   1.347592\n",
      "train loss:   0.929565\n",
      "train loss:   1.085481\n",
      "train loss:   1.082657\n",
      "train loss:   1.017687\n",
      "train loss:   0.842283\n",
      "train loss:   1.092773\n",
      "train loss:   1.022468\n",
      "train loss:   0.957527\n",
      "train loss:   1.159248\n",
      "train loss:   1.330220\n",
      "train loss:   1.238962\n",
      "train loss:   1.017695\n",
      "train loss:   1.284438\n",
      "train loss:   0.607232\n",
      "train loss:   0.897008\n",
      "train loss:   1.077938\n",
      "train loss:   0.889496\n",
      "train loss:   0.891291\n",
      "train loss:   1.196954\n",
      "train loss:   0.815872\n",
      "train loss:   0.838970\n",
      "train loss:   0.805795\n",
      "########### epoch 43 ###########\n",
      "########### loop 8050 ###########\n",
      "test loss:   0.580308   test accuracy:   0.875000\n",
      "########### loop 8050 ###########\n",
      "train loss:   1.476344\n",
      "train loss:   1.025720\n",
      "train loss:   1.001077\n",
      "train loss:   1.030386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.795311\n",
      "train loss:   1.122418\n",
      "train loss:   0.886243\n",
      "train loss:   1.127493\n",
      "train loss:   0.683165\n",
      "train loss:   1.347264\n",
      "train loss:   0.774150\n",
      "train loss:   1.043977\n",
      "train loss:   1.551499\n",
      "train loss:   1.330065\n",
      "train loss:   1.152584\n",
      "train loss:   0.830329\n",
      "train loss:   0.956940\n",
      "train loss:   0.875924\n",
      "train loss:   1.309377\n",
      "train loss:   1.150221\n",
      "train loss:   1.028377\n",
      "train loss:   1.072302\n",
      "train loss:   1.132487\n",
      "train loss:   0.620458\n",
      "train loss:   1.351636\n",
      "train loss:   1.149460\n",
      "train loss:   0.715265\n",
      "train loss:   0.936629\n",
      "train loss:   0.946665\n",
      "train loss:   1.031294\n",
      "train loss:   1.008322\n",
      "train loss:   0.983269\n",
      "train loss:   1.254512\n",
      "train loss:   1.039734\n",
      "train loss:   1.250782\n",
      "train loss:   1.471442\n",
      "train loss:   0.789717\n",
      "train loss:   0.903907\n",
      "train loss:   1.167458\n",
      "train loss:   0.756240\n",
      "train loss:   1.439425\n",
      "train loss:   1.325256\n",
      "train loss:   0.900560\n",
      "train loss:   1.325459\n",
      "train loss:   0.782886\n",
      "train loss:   1.090607\n",
      "train loss:   0.834161\n",
      "train loss:   0.868589\n",
      "train loss:   1.213319\n",
      "train loss:   0.633212\n",
      "########### epoch 44 ###########\n",
      "########### loop 8100 ###########\n",
      "test loss:   0.302702   test accuracy:   0.916667\n",
      "########### loop 8100 ###########\n",
      "train loss:   1.182800\n",
      "train loss:   0.911623\n",
      "train loss:   0.857720\n",
      "train loss:   1.194983\n",
      "train loss:   1.174850\n",
      "train loss:   1.104162\n",
      "train loss:   1.130849\n",
      "train loss:   0.890874\n",
      "train loss:   1.044689\n",
      "train loss:   1.195739\n",
      "train loss:   0.890074\n",
      "train loss:   0.961602\n",
      "train loss:   1.038782\n",
      "train loss:   1.121019\n",
      "train loss:   1.326933\n",
      "train loss:   0.847319\n",
      "train loss:   1.147102\n",
      "train loss:   0.855088\n",
      "train loss:   1.020345\n",
      "train loss:   1.014998\n",
      "train loss:   0.830497\n",
      "train loss:   1.174336\n",
      "train loss:   1.284022\n",
      "train loss:   1.330166\n",
      "train loss:   0.990244\n",
      "train loss:   1.280618\n",
      "train loss:   0.954101\n",
      "train loss:   1.249631\n",
      "train loss:   1.198946\n",
      "train loss:   1.071330\n",
      "train loss:   1.405931\n",
      "train loss:   0.987495\n",
      "train loss:   0.849845\n",
      "train loss:   1.014250\n",
      "train loss:   1.155109\n",
      "train loss:   0.934683\n",
      "train loss:   0.878850\n",
      "train loss:   0.899841\n",
      "train loss:   0.945210\n",
      "train loss:   0.888255\n",
      "train loss:   0.967150\n",
      "train loss:   1.279665\n",
      "train loss:   0.567729\n",
      "train loss:   1.066375\n",
      "train loss:   0.924662\n",
      "train loss:   0.973653\n",
      "train loss:   0.829890\n",
      "train loss:   0.909836\n",
      "train loss:   1.301532\n",
      "train loss:   1.173857\n",
      "########### epoch 44 ###########\n",
      "########### loop 8150 ###########\n",
      "test loss:   0.263763   test accuracy:   0.958333\n",
      "########### loop 8150 ###########\n",
      "train loss:   0.947950\n",
      "train loss:   1.007948\n",
      "train loss:   1.109962\n",
      "train loss:   1.103167\n",
      "train loss:   0.994905\n",
      "train loss:   1.247573\n",
      "train loss:   1.146279\n",
      "train loss:   1.316733\n",
      "train loss:   1.158254\n",
      "train loss:   1.443057\n",
      "train loss:   1.053783\n",
      "train loss:   1.068234\n",
      "train loss:   0.941433\n",
      "train loss:   0.999674\n",
      "train loss:   1.096423\n",
      "train loss:   0.781410\n",
      "train loss:   1.047537\n",
      "train loss:   1.024027\n",
      "train loss:   0.991580\n",
      "train loss:   1.307603\n",
      "train loss:   1.110176\n",
      "train loss:   0.563885\n",
      "train loss:   0.787121\n",
      "train loss:   1.227304\n",
      "train loss:   0.837085\n",
      "train loss:   1.357213\n",
      "train loss:   1.034789\n",
      "train loss:   1.306833\n",
      "train loss:   1.030049\n",
      "train loss:   1.174543\n",
      "train loss:   1.147681\n",
      "train loss:   0.663496\n",
      "train loss:   1.127262\n",
      "train loss:   1.251300\n",
      "train loss:   1.218036\n",
      "train loss:   0.810798\n",
      "train loss:   0.915410\n",
      "train loss:   1.106216\n",
      "train loss:   1.260939\n",
      "train loss:   0.837438\n",
      "train loss:   0.989569\n",
      "train loss:   0.953014\n",
      "train loss:   0.878622\n",
      "train loss:   1.055170\n",
      "train loss:   0.912028\n",
      "train loss:   1.361787\n",
      "train loss:   1.048315\n",
      "train loss:   1.002510\n",
      "train loss:   1.260119\n",
      "train loss:   1.005645\n",
      "########### epoch 44 ###########\n",
      "########### loop 8200 ###########\n",
      "test loss:   0.373747   test accuracy:   0.958333\n",
      "########### loop 8200 ###########\n",
      "train loss:   0.778820\n",
      "train loss:   1.357276\n",
      "train loss:   0.853250\n",
      "train loss:   0.955484\n",
      "train loss:   1.382837\n",
      "train loss:   1.076506\n",
      "train loss:   0.828016\n",
      "train loss:   0.681189\n",
      "train loss:   1.281139\n",
      "train loss:   1.054802\n",
      "train loss:   0.998681\n",
      "train loss:   0.744754\n",
      "train loss:   0.822528\n",
      "train loss:   0.948217\n",
      "train loss:   1.064112\n",
      "train loss:   1.344364\n",
      "train loss:   1.175492\n",
      "train loss:   0.910190\n",
      "train loss:   1.100753\n",
      "train loss:   0.866462\n",
      "train loss:   1.078877\n",
      "train loss:   1.160914\n",
      "train loss:   1.149182\n",
      "train loss:   1.044350\n",
      "train loss:   0.743377\n",
      "train loss:   1.070049\n",
      "train loss:   1.147304\n",
      "train loss:   0.913134\n",
      "train loss:   0.823716\n",
      "train loss:   1.093390\n",
      "train loss:   0.985898\n",
      "train loss:   1.060801\n",
      "train loss:   0.960029\n",
      "train loss:   1.083979\n",
      "train loss:   1.182805\n",
      "train loss:   0.957347\n",
      "train loss:   1.261699\n",
      "train loss:   0.761823\n",
      "train loss:   1.038526\n",
      "train loss:   1.407026\n",
      "train loss:   0.672563\n",
      "train loss:   1.105189\n",
      "train loss:   1.064210\n",
      "train loss:   1.290313\n",
      "train loss:   1.110196\n",
      "train loss:   0.694577\n",
      "train loss:   0.844258\n",
      "train loss:   1.038978\n",
      "train loss:   1.235124\n",
      "train loss:   0.992624\n",
      "########### epoch 44 ###########\n",
      "########### loop 8250 ###########\n",
      "test loss:   0.248062   test accuracy:   0.958333\n",
      "########### loop 8250 ###########\n",
      "train loss:   1.024906\n",
      "train loss:   1.094644\n",
      "train loss:   1.049047\n",
      "train loss:   0.799609\n",
      "train loss:   0.907735\n",
      "train loss:   1.177339\n",
      "train loss:   1.025107\n",
      "train loss:   1.055176\n",
      "train loss:   1.026948\n",
      "train loss:   1.087331\n",
      "train loss:   1.239300\n",
      "train loss:   0.967920\n",
      "train loss:   0.946267\n",
      "train loss:   0.866906\n",
      "train loss:   1.172720\n",
      "train loss:   1.051299\n",
      "train loss:   1.125275\n",
      "train loss:   1.136513\n",
      "train loss:   0.899493\n",
      "train loss:   1.365605\n",
      "train loss:   1.074488\n",
      "train loss:   1.185621\n",
      "train loss:   1.033931\n",
      "train loss:   1.103462\n",
      "train loss:   1.360353\n",
      "train loss:   1.312721\n",
      "train loss:   1.078314\n",
      "train loss:   0.989506\n",
      "train loss:   1.498330\n",
      "train loss:   0.853948\n",
      "train loss:   0.989016\n",
      "train loss:   1.515472\n",
      "train loss:   1.390429\n",
      "train loss:   0.691820\n",
      "train loss:   1.085093\n",
      "train loss:   1.216413\n",
      "train loss:   0.927012\n",
      "train loss:   0.682435\n",
      "train loss:   1.350596\n",
      "train loss:   1.050979\n",
      "train loss:   0.947490\n",
      "train loss:   1.027413\n",
      "train loss:   1.278439\n",
      "train loss:   1.358625\n",
      "train loss:   1.004542\n",
      "train loss:   1.306996\n",
      "train loss:   0.814966\n",
      "train loss:   1.055631\n",
      "train loss:   0.837450\n",
      "train loss:   1.122893\n",
      "########### epoch 45 ###########\n",
      "########### loop 8300 ###########\n",
      "test loss:   0.396605   test accuracy:   0.916667\n",
      "########### loop 8300 ###########\n",
      "train loss:   1.102008\n",
      "train loss:   0.963916\n",
      "train loss:   1.041674\n",
      "train loss:   1.252761\n",
      "train loss:   0.958588\n",
      "train loss:   0.841644\n",
      "train loss:   0.634209\n",
      "train loss:   0.846932\n",
      "train loss:   0.934450\n",
      "train loss:   1.087153\n",
      "train loss:   0.923612\n",
      "train loss:   0.809281\n",
      "train loss:   1.181830\n",
      "train loss:   1.225010\n",
      "train loss:   1.033144\n",
      "train loss:   1.143266\n",
      "train loss:   1.221037\n",
      "train loss:   0.695699\n",
      "train loss:   0.943242\n",
      "train loss:   1.528571\n",
      "train loss:   1.245842\n",
      "train loss:   1.121529\n",
      "train loss:   0.821143\n",
      "train loss:   1.069036\n",
      "train loss:   1.241634\n",
      "train loss:   1.438403\n",
      "train loss:   0.868994\n",
      "train loss:   1.082474\n",
      "train loss:   1.366301\n",
      "train loss:   1.040456\n",
      "train loss:   1.114946\n",
      "train loss:   0.914941\n",
      "train loss:   1.360097\n",
      "train loss:   0.752315\n",
      "train loss:   0.975107\n",
      "train loss:   1.198030\n",
      "train loss:   1.015730\n",
      "train loss:   1.021217\n",
      "train loss:   0.925290\n",
      "train loss:   1.045952\n",
      "train loss:   1.501514\n",
      "train loss:   0.842463\n",
      "train loss:   1.353038\n",
      "train loss:   1.088658\n",
      "train loss:   0.904548\n",
      "train loss:   0.858385\n",
      "train loss:   0.824488\n",
      "train loss:   0.785687\n",
      "train loss:   0.844659\n",
      "train loss:   0.724686\n",
      "########### epoch 45 ###########\n",
      "########### loop 8350 ###########\n",
      "test loss:   0.368745   test accuracy:   0.958333\n",
      "########### loop 8350 ###########\n",
      "train loss:   0.626543\n",
      "train loss:   1.001235\n",
      "train loss:   0.933505\n",
      "train loss:   1.298273\n",
      "train loss:   1.303629\n",
      "train loss:   1.272642\n",
      "train loss:   0.734170\n",
      "train loss:   1.247949\n",
      "train loss:   1.004210\n",
      "train loss:   1.074227\n",
      "train loss:   0.913617\n",
      "train loss:   1.140143\n",
      "train loss:   0.892834\n",
      "train loss:   1.078853\n",
      "train loss:   1.126520\n",
      "train loss:   0.844210\n",
      "train loss:   1.009291\n",
      "train loss:   1.354714\n",
      "train loss:   1.161623\n",
      "train loss:   1.290847\n",
      "train loss:   0.709707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.018971\n",
      "train loss:   0.953574\n",
      "train loss:   1.083595\n",
      "train loss:   0.820286\n",
      "train loss:   1.138897\n",
      "train loss:   1.134984\n",
      "train loss:   0.958510\n",
      "train loss:   0.928453\n",
      "train loss:   1.198745\n",
      "train loss:   0.530566\n",
      "train loss:   0.924817\n",
      "train loss:   1.188883\n",
      "train loss:   1.310668\n",
      "train loss:   1.221386\n",
      "train loss:   1.255960\n",
      "train loss:   1.060542\n",
      "train loss:   1.119715\n",
      "train loss:   1.248374\n",
      "train loss:   0.980257\n",
      "train loss:   0.843596\n",
      "train loss:   0.830008\n",
      "train loss:   1.223992\n",
      "train loss:   1.111413\n",
      "train loss:   0.965946\n",
      "train loss:   0.662073\n",
      "train loss:   1.239480\n",
      "train loss:   1.263124\n",
      "train loss:   0.710530\n",
      "train loss:   1.200329\n",
      "########### epoch 45 ###########\n",
      "########### loop 8400 ###########\n",
      "test loss:   0.101885   test accuracy:   1.000000\n",
      "########### loop 8400 ###########\n",
      "train loss:   0.777001\n",
      "train loss:   1.455902\n",
      "train loss:   1.140546\n",
      "train loss:   1.022524\n",
      "train loss:   1.015222\n",
      "train loss:   0.953454\n",
      "train loss:   0.710338\n",
      "train loss:   1.015621\n",
      "train loss:   1.205351\n",
      "train loss:   0.794080\n",
      "train loss:   0.937705\n",
      "train loss:   0.881466\n",
      "train loss:   1.026072\n",
      "train loss:   1.325655\n",
      "train loss:   0.638300\n",
      "train loss:   1.066867\n",
      "train loss:   0.947391\n",
      "train loss:   1.189563\n",
      "train loss:   1.124489\n",
      "train loss:   0.892970\n",
      "train loss:   1.234893\n",
      "train loss:   1.251749\n",
      "train loss:   1.075482\n",
      "train loss:   0.816059\n",
      "train loss:   1.006940\n",
      "train loss:   1.099859\n",
      "train loss:   1.449360\n",
      "train loss:   1.100473\n",
      "train loss:   1.259267\n",
      "train loss:   1.273627\n",
      "train loss:   0.820522\n",
      "train loss:   1.339744\n",
      "train loss:   0.977225\n",
      "train loss:   0.758561\n",
      "train loss:   0.879713\n",
      "train loss:   0.978493\n",
      "train loss:   1.087163\n",
      "train loss:   1.284386\n",
      "train loss:   0.979227\n",
      "train loss:   1.108520\n",
      "train loss:   0.904547\n",
      "train loss:   0.753714\n",
      "train loss:   0.855209\n",
      "train loss:   1.014696\n",
      "train loss:   0.950007\n",
      "train loss:   0.635854\n",
      "train loss:   0.939031\n",
      "train loss:   0.862492\n",
      "train loss:   1.137329\n",
      "train loss:   1.523308\n",
      "########### epoch 45 ###########\n",
      "########### loop 8450 ###########\n",
      "test loss:   0.303116   test accuracy:   0.958333\n",
      "########### loop 8450 ###########\n",
      "train loss:   1.325264\n",
      "train loss:   0.864921\n",
      "train loss:   1.195056\n",
      "train loss:   1.020791\n",
      "train loss:   1.106295\n",
      "train loss:   0.714108\n",
      "train loss:   1.273920\n",
      "train loss:   1.104992\n",
      "train loss:   0.993421\n",
      "train loss:   1.161027\n",
      "train loss:   1.190455\n",
      "train loss:   0.908859\n",
      "train loss:   0.867474\n",
      "train loss:   1.092031\n",
      "train loss:   0.847894\n",
      "train loss:   1.116222\n",
      "train loss:   0.935654\n",
      "train loss:   1.170961\n",
      "train loss:   0.878786\n",
      "train loss:   0.778290\n",
      "train loss:   0.795077\n",
      "train loss:   1.144041\n",
      "train loss:   1.092450\n",
      "train loss:   1.213925\n",
      "train loss:   0.950354\n",
      "train loss:   0.694003\n",
      "train loss:   1.032935\n",
      "train loss:   0.932059\n",
      "train loss:   0.747750\n",
      "train loss:   1.052574\n",
      "train loss:   1.398757\n",
      "train loss:   1.177307\n",
      "train loss:   0.892177\n",
      "train loss:   0.890013\n",
      "train loss:   1.083529\n",
      "train loss:   1.016415\n",
      "train loss:   1.140178\n",
      "train loss:   0.903903\n",
      "train loss:   1.042531\n",
      "train loss:   1.159231\n",
      "train loss:   1.152943\n",
      "train loss:   0.833123\n",
      "train loss:   1.182288\n",
      "train loss:   1.052586\n",
      "train loss:   1.395492\n",
      "train loss:   0.875605\n",
      "train loss:   1.181907\n",
      "train loss:   0.937863\n",
      "train loss:   1.067593\n",
      "train loss:   0.762606\n",
      "########### epoch 46 ###########\n",
      "########### loop 8500 ###########\n",
      "test loss:   0.199390   test accuracy:   0.958333\n",
      "########### loop 8500 ###########\n",
      "train loss:   1.235626\n",
      "train loss:   0.900854\n",
      "train loss:   1.081886\n",
      "train loss:   0.562856\n",
      "train loss:   1.028054\n",
      "train loss:   1.198706\n",
      "train loss:   1.091944\n",
      "train loss:   1.357043\n",
      "train loss:   1.216748\n",
      "train loss:   0.894227\n",
      "train loss:   1.033415\n",
      "train loss:   1.057746\n",
      "train loss:   0.989650\n",
      "train loss:   0.925519\n",
      "train loss:   0.945612\n",
      "train loss:   1.294249\n",
      "train loss:   1.019950\n",
      "train loss:   1.034840\n",
      "train loss:   1.274644\n",
      "train loss:   1.144614\n",
      "train loss:   1.131867\n",
      "train loss:   1.143098\n",
      "train loss:   1.175969\n",
      "train loss:   0.788592\n",
      "train loss:   1.226009\n",
      "train loss:   0.947390\n",
      "train loss:   0.835460\n",
      "train loss:   0.697679\n",
      "train loss:   0.823250\n",
      "train loss:   1.044627\n",
      "train loss:   1.158485\n",
      "train loss:   0.902259\n",
      "train loss:   1.018067\n",
      "train loss:   0.922852\n",
      "train loss:   1.323117\n",
      "train loss:   1.027778\n",
      "train loss:   0.935774\n",
      "train loss:   1.010110\n",
      "train loss:   0.998220\n",
      "train loss:   1.292840\n",
      "train loss:   0.787315\n",
      "train loss:   0.804473\n",
      "train loss:   0.901267\n",
      "train loss:   1.183962\n",
      "train loss:   1.344181\n",
      "train loss:   1.025722\n",
      "train loss:   1.289424\n",
      "train loss:   1.095147\n",
      "train loss:   1.244389\n",
      "train loss:   0.414429\n",
      "########### epoch 46 ###########\n",
      "########### loop 8550 ###########\n",
      "test loss:   0.411497   test accuracy:   0.916667\n",
      "########### loop 8550 ###########\n",
      "train loss:   1.066154\n",
      "train loss:   0.931100\n",
      "train loss:   0.840697\n",
      "train loss:   1.107590\n",
      "train loss:   0.937661\n",
      "train loss:   1.154648\n",
      "train loss:   0.904841\n",
      "train loss:   1.470284\n",
      "train loss:   1.389083\n",
      "train loss:   0.769637\n",
      "train loss:   1.009625\n",
      "train loss:   1.040478\n",
      "train loss:   1.231770\n",
      "train loss:   1.090636\n",
      "train loss:   0.727674\n",
      "train loss:   1.106079\n",
      "train loss:   0.929916\n",
      "train loss:   0.803381\n",
      "train loss:   1.143053\n",
      "train loss:   1.470199\n",
      "train loss:   0.778581\n",
      "train loss:   0.793095\n",
      "train loss:   0.898875\n",
      "train loss:   1.023068\n",
      "train loss:   0.881343\n",
      "train loss:   1.016822\n",
      "train loss:   0.950435\n",
      "train loss:   1.223296\n",
      "train loss:   1.002995\n",
      "train loss:   0.992196\n",
      "train loss:   1.105939\n",
      "train loss:   1.019103\n",
      "train loss:   1.167901\n",
      "train loss:   0.722112\n",
      "train loss:   0.761925\n",
      "train loss:   0.877955\n",
      "train loss:   1.285957\n",
      "train loss:   1.003905\n",
      "train loss:   1.190166\n",
      "train loss:   1.159435\n",
      "train loss:   0.678879\n",
      "train loss:   0.940735\n",
      "train loss:   1.007702\n",
      "train loss:   0.787447\n",
      "train loss:   1.388907\n",
      "train loss:   0.885365\n",
      "train loss:   1.043397\n",
      "train loss:   1.127633\n",
      "train loss:   1.156534\n",
      "train loss:   1.163695\n",
      "########### epoch 46 ###########\n",
      "########### loop 8600 ###########\n",
      "test loss:   0.252681   test accuracy:   1.000000\n",
      "########### loop 8600 ###########\n",
      "train loss:   1.177407\n",
      "train loss:   1.058454\n",
      "train loss:   1.152149\n",
      "train loss:   0.963437\n",
      "train loss:   1.174130\n",
      "train loss:   0.923802\n",
      "train loss:   1.104039\n",
      "train loss:   0.887543\n",
      "train loss:   1.071720\n",
      "train loss:   1.065706\n",
      "train loss:   1.182801\n",
      "train loss:   1.013028\n",
      "train loss:   1.060348\n",
      "train loss:   1.006006\n",
      "train loss:   1.302235\n",
      "train loss:   1.230654\n",
      "train loss:   1.074491\n",
      "train loss:   0.986852\n",
      "train loss:   1.191255\n",
      "train loss:   1.087067\n",
      "train loss:   0.908629\n",
      "train loss:   1.119254\n",
      "train loss:   0.795841\n",
      "train loss:   1.309167\n",
      "train loss:   0.942552\n",
      "train loss:   1.089688\n",
      "train loss:   0.759088\n",
      "train loss:   0.987457\n",
      "train loss:   1.285769\n",
      "train loss:   1.410756\n",
      "train loss:   1.255261\n",
      "train loss:   0.900479\n",
      "train loss:   1.081724\n",
      "train loss:   0.700255\n",
      "train loss:   0.799385\n",
      "train loss:   1.316942\n",
      "train loss:   0.917384\n",
      "train loss:   0.789094\n",
      "train loss:   0.975906\n",
      "train loss:   1.304574\n",
      "train loss:   1.406937\n",
      "train loss:   0.913474\n",
      "train loss:   0.889516\n",
      "train loss:   1.090993\n",
      "train loss:   1.190354\n",
      "train loss:   1.077718\n",
      "train loss:   0.871060\n",
      "train loss:   1.001190\n",
      "train loss:   0.958615\n",
      "train loss:   1.053116\n",
      "########### epoch 47 ###########\n",
      "########### loop 8650 ###########\n",
      "test loss:   0.186741   test accuracy:   0.958333\n",
      "########### loop 8650 ###########\n",
      "train loss:   0.974782\n",
      "train loss:   1.422582\n",
      "train loss:   1.064877\n",
      "train loss:   0.887049\n",
      "train loss:   0.748330\n",
      "train loss:   0.825511\n",
      "train loss:   1.379983\n",
      "train loss:   1.109251\n",
      "train loss:   1.074515\n",
      "train loss:   1.126917\n",
      "train loss:   1.055218\n",
      "train loss:   1.081585\n",
      "train loss:   0.715035\n",
      "train loss:   1.246930\n",
      "train loss:   0.780916\n",
      "train loss:   0.782188\n",
      "train loss:   0.858787\n",
      "train loss:   1.063816\n",
      "train loss:   0.954356\n",
      "train loss:   1.144223\n",
      "train loss:   1.120942\n",
      "train loss:   0.878507\n",
      "train loss:   1.294569\n",
      "train loss:   0.959323\n",
      "train loss:   0.613840\n",
      "train loss:   1.056954\n",
      "train loss:   1.033344\n",
      "train loss:   1.174438\n",
      "train loss:   1.162436\n",
      "train loss:   1.029338\n",
      "train loss:   0.726533\n",
      "train loss:   0.835907\n",
      "train loss:   1.063229\n",
      "train loss:   1.122297\n",
      "train loss:   0.971320\n",
      "train loss:   1.119285\n",
      "train loss:   1.101331\n",
      "train loss:   1.087483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.092872\n",
      "train loss:   0.985230\n",
      "train loss:   1.387758\n",
      "train loss:   0.861105\n",
      "train loss:   0.821501\n",
      "train loss:   0.897654\n",
      "train loss:   1.389793\n",
      "train loss:   0.982085\n",
      "train loss:   0.973334\n",
      "train loss:   1.035010\n",
      "train loss:   1.019019\n",
      "train loss:   1.185737\n",
      "########### epoch 47 ###########\n",
      "########### loop 8700 ###########\n",
      "test loss:   0.241637   test accuracy:   0.958333\n",
      "########### loop 8700 ###########\n",
      "train loss:   0.929855\n",
      "train loss:   0.825967\n",
      "train loss:   0.654062\n",
      "train loss:   0.895624\n",
      "train loss:   0.887821\n",
      "train loss:   0.950617\n",
      "train loss:   1.284456\n",
      "train loss:   1.245243\n",
      "train loss:   1.036124\n",
      "train loss:   0.926791\n",
      "train loss:   0.886570\n",
      "train loss:   0.992465\n",
      "train loss:   0.959706\n",
      "train loss:   0.918804\n",
      "train loss:   0.976467\n",
      "train loss:   0.676665\n",
      "train loss:   0.817166\n",
      "train loss:   0.938862\n",
      "train loss:   0.566897\n",
      "train loss:   0.837176\n",
      "train loss:   1.147310\n",
      "train loss:   0.882377\n",
      "train loss:   1.319804\n",
      "train loss:   1.194109\n",
      "train loss:   0.602970\n",
      "train loss:   0.953805\n",
      "train loss:   1.062425\n",
      "train loss:   1.120528\n",
      "train loss:   0.930998\n",
      "train loss:   0.947894\n",
      "train loss:   1.215011\n",
      "train loss:   0.760626\n",
      "train loss:   1.186733\n",
      "train loss:   0.976721\n",
      "train loss:   0.994872\n",
      "train loss:   1.061275\n",
      "train loss:   0.866286\n",
      "train loss:   1.141387\n",
      "train loss:   0.669741\n",
      "train loss:   0.725492\n",
      "train loss:   1.304996\n",
      "train loss:   1.187693\n",
      "train loss:   1.131270\n",
      "train loss:   0.973720\n",
      "train loss:   0.747129\n",
      "train loss:   0.875488\n",
      "train loss:   1.116188\n",
      "train loss:   1.125350\n",
      "train loss:   0.873226\n",
      "train loss:   1.028773\n",
      "########### epoch 47 ###########\n",
      "########### loop 8750 ###########\n",
      "test loss:   0.338126   test accuracy:   0.958333\n",
      "########### loop 8750 ###########\n",
      "train loss:   1.105674\n",
      "train loss:   1.058482\n",
      "train loss:   0.811156\n",
      "train loss:   0.823346\n",
      "train loss:   1.021752\n",
      "train loss:   1.201808\n",
      "train loss:   1.120322\n",
      "train loss:   1.062450\n",
      "train loss:   1.027301\n",
      "train loss:   0.923522\n",
      "train loss:   0.961974\n",
      "train loss:   0.698121\n",
      "train loss:   0.833702\n",
      "train loss:   0.927898\n",
      "train loss:   0.931133\n",
      "train loss:   0.921327\n",
      "train loss:   1.119741\n",
      "train loss:   0.836728\n",
      "train loss:   1.330601\n",
      "train loss:   0.875046\n",
      "train loss:   0.934603\n",
      "train loss:   0.812007\n",
      "train loss:   0.954213\n",
      "train loss:   0.970513\n",
      "train loss:   1.163226\n",
      "train loss:   1.184881\n",
      "train loss:   0.899507\n",
      "train loss:   0.853363\n",
      "train loss:   1.315641\n",
      "train loss:   1.229383\n",
      "train loss:   0.718933\n",
      "train loss:   1.034733\n",
      "train loss:   1.127116\n",
      "train loss:   1.286851\n",
      "train loss:   1.127805\n",
      "train loss:   0.901334\n",
      "train loss:   1.088659\n",
      "train loss:   0.737582\n",
      "train loss:   1.007312\n",
      "train loss:   0.840445\n",
      "train loss:   1.157805\n",
      "train loss:   1.099118\n",
      "train loss:   1.001198\n",
      "train loss:   1.142432\n",
      "train loss:   0.620248\n",
      "train loss:   0.771162\n",
      "train loss:   0.884462\n",
      "train loss:   0.923432\n",
      "train loss:   0.958731\n",
      "train loss:   0.992174\n",
      "########### epoch 47 ###########\n",
      "########### loop 8800 ###########\n",
      "test loss:   0.340626   test accuracy:   0.916667\n",
      "########### loop 8800 ###########\n",
      "train loss:   0.577021\n",
      "train loss:   0.989973\n",
      "train loss:   0.833387\n",
      "train loss:   0.861679\n",
      "train loss:   1.348454\n",
      "train loss:   0.780574\n",
      "train loss:   1.017023\n",
      "train loss:   0.787691\n",
      "train loss:   1.298760\n",
      "train loss:   0.801760\n",
      "train loss:   0.958823\n",
      "train loss:   0.948975\n",
      "train loss:   1.167288\n",
      "train loss:   0.955561\n",
      "train loss:   1.298938\n",
      "train loss:   1.080876\n",
      "train loss:   0.966348\n",
      "train loss:   0.848754\n",
      "train loss:   0.785476\n",
      "train loss:   1.260864\n",
      "train loss:   0.967045\n",
      "train loss:   1.072103\n",
      "train loss:   1.018541\n",
      "train loss:   1.024560\n",
      "train loss:   0.941519\n",
      "train loss:   1.298324\n",
      "train loss:   0.640405\n",
      "train loss:   1.001885\n",
      "train loss:   0.904716\n",
      "train loss:   0.702135\n",
      "train loss:   0.673594\n",
      "train loss:   1.142952\n",
      "train loss:   0.959658\n",
      "train loss:   0.866980\n",
      "train loss:   1.029532\n",
      "train loss:   1.030563\n",
      "train loss:   1.160687\n",
      "train loss:   1.342897\n",
      "train loss:   0.909751\n",
      "train loss:   0.800257\n",
      "train loss:   0.989118\n",
      "train loss:   0.790847\n",
      "train loss:   1.108813\n",
      "train loss:   0.941175\n",
      "train loss:   1.288927\n",
      "train loss:   1.360570\n",
      "train loss:   1.087343\n",
      "train loss:   1.036696\n",
      "train loss:   0.919412\n",
      "train loss:   1.018165\n",
      "########### epoch 48 ###########\n",
      "########### loop 8850 ###########\n",
      "test loss:   0.297957   test accuracy:   0.916667\n",
      "########### loop 8850 ###########\n",
      "train loss:   1.244021\n",
      "train loss:   0.907746\n",
      "train loss:   1.136340\n",
      "train loss:   0.977618\n",
      "train loss:   0.781425\n",
      "train loss:   0.785507\n",
      "train loss:   1.259031\n",
      "train loss:   1.191669\n",
      "train loss:   0.892064\n",
      "train loss:   1.050131\n",
      "train loss:   1.286082\n",
      "train loss:   1.001124\n",
      "train loss:   0.814682\n",
      "train loss:   0.849734\n",
      "train loss:   1.308652\n",
      "train loss:   1.238163\n",
      "train loss:   0.945341\n",
      "train loss:   0.926279\n",
      "train loss:   1.135253\n",
      "train loss:   1.162182\n",
      "train loss:   1.092422\n",
      "train loss:   1.108406\n",
      "train loss:   1.168977\n",
      "train loss:   0.783612\n",
      "train loss:   1.195710\n",
      "train loss:   1.069690\n",
      "train loss:   1.108321\n",
      "train loss:   0.832991\n",
      "train loss:   0.875452\n",
      "train loss:   0.931297\n",
      "train loss:   0.731136\n",
      "train loss:   0.930770\n",
      "train loss:   1.174415\n",
      "train loss:   1.006031\n",
      "train loss:   0.867349\n",
      "train loss:   0.924961\n",
      "train loss:   1.000262\n",
      "train loss:   0.695772\n",
      "train loss:   0.619389\n",
      "train loss:   1.008299\n",
      "train loss:   1.070989\n",
      "train loss:   0.877348\n",
      "train loss:   0.738751\n",
      "train loss:   1.053693\n",
      "train loss:   1.001521\n",
      "train loss:   0.857413\n",
      "train loss:   1.163480\n",
      "train loss:   1.163208\n",
      "train loss:   1.457513\n",
      "train loss:   1.101273\n",
      "########### epoch 48 ###########\n",
      "########### loop 8900 ###########\n",
      "test loss:   0.249323   test accuracy:   0.958333\n",
      "########### loop 8900 ###########\n",
      "train loss:   0.938095\n",
      "train loss:   1.248030\n",
      "train loss:   1.053946\n",
      "train loss:   1.109528\n",
      "train loss:   0.913683\n",
      "train loss:   0.900603\n",
      "train loss:   0.970574\n",
      "train loss:   1.277985\n",
      "train loss:   1.016981\n",
      "train loss:   0.903781\n",
      "train loss:   1.082400\n",
      "train loss:   0.899645\n",
      "train loss:   1.186488\n",
      "train loss:   0.888908\n",
      "train loss:   0.881053\n",
      "train loss:   1.303524\n",
      "train loss:   0.814543\n",
      "train loss:   1.091742\n",
      "train loss:   0.950141\n",
      "train loss:   1.270061\n",
      "train loss:   0.894814\n",
      "train loss:   0.652025\n",
      "train loss:   0.860834\n",
      "train loss:   1.140967\n",
      "train loss:   0.808467\n",
      "train loss:   1.205546\n",
      "train loss:   1.096208\n",
      "train loss:   1.169690\n",
      "train loss:   0.917480\n",
      "train loss:   0.957506\n",
      "train loss:   0.786864\n",
      "train loss:   0.852610\n",
      "train loss:   0.959556\n",
      "train loss:   0.832902\n",
      "train loss:   1.051930\n",
      "train loss:   0.919011\n",
      "train loss:   1.159097\n",
      "train loss:   0.992312\n",
      "train loss:   0.895060\n",
      "train loss:   1.232647\n",
      "train loss:   1.041150\n",
      "train loss:   0.975473\n",
      "train loss:   1.376778\n",
      "train loss:   0.792093\n",
      "train loss:   0.970090\n",
      "train loss:   0.969459\n",
      "train loss:   0.728662\n",
      "train loss:   0.993389\n",
      "train loss:   1.236901\n",
      "train loss:   1.149327\n",
      "########### epoch 48 ###########\n",
      "########### loop 8950 ###########\n",
      "test loss:   0.158419   test accuracy:   1.000000\n",
      "########### loop 8950 ###########\n",
      "train loss:   0.932942\n",
      "train loss:   1.308038\n",
      "train loss:   1.134002\n",
      "train loss:   0.900010\n",
      "train loss:   0.768943\n",
      "train loss:   1.053875\n",
      "train loss:   1.363775\n",
      "train loss:   1.109866\n",
      "train loss:   1.110721\n",
      "train loss:   1.351394\n",
      "train loss:   0.957460\n",
      "train loss:   0.521728\n",
      "train loss:   0.960052\n",
      "train loss:   1.211517\n",
      "train loss:   0.936058\n",
      "train loss:   0.781762\n",
      "train loss:   0.831473\n",
      "train loss:   1.173922\n",
      "train loss:   1.137870\n",
      "train loss:   0.842075\n",
      "train loss:   0.876534\n",
      "train loss:   1.104528\n",
      "train loss:   1.060166\n",
      "train loss:   1.081884\n",
      "train loss:   0.860241\n",
      "train loss:   0.888232\n",
      "train loss:   1.247663\n",
      "train loss:   1.162193\n",
      "train loss:   0.916487\n",
      "train loss:   1.084697\n",
      "train loss:   0.838546\n",
      "train loss:   0.795628\n",
      "train loss:   1.443875\n",
      "train loss:   1.021392\n",
      "train loss:   1.333711\n",
      "train loss:   1.208320\n",
      "train loss:   0.964689\n",
      "train loss:   1.008186\n",
      "train loss:   1.436732\n",
      "train loss:   0.739495\n",
      "train loss:   1.112276\n",
      "train loss:   1.017606\n",
      "train loss:   1.311737\n",
      "train loss:   1.288877\n",
      "train loss:   0.870358\n",
      "train loss:   1.086639\n",
      "train loss:   0.899067\n",
      "train loss:   0.723129\n",
      "train loss:   1.065250\n",
      "train loss:   1.079224\n",
      "########### epoch 48 ###########\n",
      "########### loop 9000 ###########\n",
      "test loss:   0.287960   test accuracy:   1.000000\n",
      "########### loop 9000 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.936240\n",
      "train loss:   0.712840\n",
      "train loss:   0.968826\n",
      "train loss:   1.036717\n",
      "train loss:   0.799844\n",
      "train loss:   1.075672\n",
      "train loss:   1.060148\n",
      "train loss:   1.124645\n",
      "train loss:   1.505946\n",
      "train loss:   0.780721\n",
      "train loss:   0.828561\n",
      "train loss:   0.815139\n",
      "train loss:   1.191867\n",
      "train loss:   0.781885\n",
      "train loss:   1.133041\n",
      "train loss:   1.246209\n",
      "train loss:   1.174730\n",
      "train loss:   0.658607\n",
      "train loss:   0.846510\n",
      "train loss:   0.604169\n",
      "train loss:   0.704162\n",
      "train loss:   0.610559\n",
      "train loss:   1.037939\n",
      "train loss:   0.956209\n",
      "train loss:   0.770427\n",
      "train loss:   1.073688\n",
      "train loss:   1.404320\n",
      "train loss:   0.878629\n",
      "train loss:   1.238519\n",
      "train loss:   0.838180\n",
      "train loss:   0.886453\n",
      "train loss:   0.830706\n",
      "train loss:   1.006443\n",
      "train loss:   1.203421\n",
      "train loss:   0.803327\n",
      "train loss:   1.133796\n",
      "train loss:   1.045712\n",
      "train loss:   1.209194\n",
      "train loss:   0.721053\n",
      "train loss:   0.908826\n",
      "train loss:   0.844032\n",
      "train loss:   0.708987\n",
      "train loss:   0.897769\n",
      "train loss:   1.074337\n",
      "train loss:   0.703749\n",
      "train loss:   1.017571\n",
      "train loss:   1.208412\n",
      "train loss:   0.734160\n",
      "train loss:   0.814822\n",
      "train loss:   0.727586\n",
      "########### epoch 49 ###########\n",
      "########### loop 9050 ###########\n",
      "test loss:   0.207632   test accuracy:   1.000000\n",
      "########### loop 9050 ###########\n",
      "train loss:   0.737405\n",
      "train loss:   0.766039\n",
      "train loss:   1.135538\n",
      "train loss:   1.139277\n",
      "train loss:   0.935858\n",
      "train loss:   1.138578\n",
      "train loss:   0.926470\n",
      "train loss:   1.055713\n",
      "train loss:   0.964144\n",
      "train loss:   0.768180\n",
      "train loss:   1.019104\n",
      "train loss:   0.726210\n",
      "train loss:   1.051232\n",
      "train loss:   1.382383\n",
      "train loss:   0.970863\n",
      "train loss:   1.065430\n",
      "train loss:   0.569252\n",
      "train loss:   0.970420\n",
      "train loss:   1.108125\n",
      "train loss:   1.267254\n",
      "train loss:   0.772572\n",
      "train loss:   1.212588\n",
      "train loss:   1.189358\n",
      "train loss:   0.778136\n",
      "train loss:   1.195033\n",
      "train loss:   0.856769\n",
      "train loss:   1.075010\n",
      "train loss:   0.907447\n",
      "train loss:   1.078502\n",
      "train loss:   0.922457\n",
      "train loss:   0.805771\n",
      "train loss:   0.956588\n",
      "train loss:   0.999417\n",
      "train loss:   0.776114\n",
      "train loss:   0.978630\n",
      "train loss:   0.706291\n",
      "train loss:   1.045274\n",
      "train loss:   0.979587\n",
      "train loss:   0.995578\n",
      "train loss:   1.124993\n",
      "train loss:   0.876656\n",
      "train loss:   1.142323\n",
      "train loss:   0.994074\n",
      "train loss:   1.082224\n",
      "train loss:   0.728193\n",
      "train loss:   1.380217\n",
      "train loss:   1.214795\n",
      "train loss:   1.131371\n",
      "train loss:   1.180456\n",
      "train loss:   1.103323\n",
      "########### epoch 49 ###########\n",
      "########### loop 9100 ###########\n",
      "test loss:   0.203129   test accuracy:   1.000000\n",
      "########### loop 9100 ###########\n",
      "train loss:   0.930302\n",
      "train loss:   1.036041\n",
      "train loss:   1.154387\n",
      "train loss:   0.887068\n",
      "train loss:   0.822511\n",
      "train loss:   0.936123\n",
      "train loss:   0.931042\n",
      "train loss:   1.021733\n",
      "train loss:   0.955489\n",
      "train loss:   1.083087\n",
      "train loss:   1.190952\n",
      "train loss:   0.654566\n",
      "train loss:   0.839284\n",
      "train loss:   1.109030\n",
      "train loss:   1.238708\n",
      "train loss:   0.835922\n",
      "train loss:   1.085247\n",
      "train loss:   1.012533\n",
      "train loss:   1.029671\n",
      "train loss:   1.058640\n",
      "train loss:   1.130548\n",
      "train loss:   1.193765\n",
      "train loss:   0.799700\n",
      "train loss:   1.478482\n",
      "train loss:   1.035945\n",
      "train loss:   0.935396\n",
      "train loss:   1.196927\n",
      "train loss:   1.040058\n",
      "train loss:   0.747014\n",
      "train loss:   0.861484\n",
      "train loss:   0.992194\n",
      "train loss:   0.921356\n",
      "train loss:   0.859011\n",
      "train loss:   0.720185\n",
      "train loss:   0.914204\n",
      "train loss:   1.038611\n",
      "train loss:   0.957253\n",
      "train loss:   1.222579\n",
      "train loss:   1.054615\n",
      "train loss:   1.090367\n",
      "train loss:   0.822753\n",
      "train loss:   0.825352\n",
      "train loss:   0.910942\n",
      "train loss:   0.961901\n",
      "train loss:   1.100387\n",
      "train loss:   1.113253\n",
      "train loss:   1.077274\n",
      "train loss:   0.985669\n",
      "train loss:   0.894402\n",
      "train loss:   0.741400\n",
      "########### epoch 49 ###########\n",
      "########### loop 9150 ###########\n",
      "test loss:   0.323835   test accuracy:   1.000000\n",
      "########### loop 9150 ###########\n",
      "train loss:   1.090082\n",
      "train loss:   0.798490\n",
      "train loss:   0.992171\n",
      "train loss:   1.048943\n",
      "train loss:   1.620673\n",
      "train loss:   0.734889\n",
      "train loss:   1.229909\n",
      "train loss:   1.209709\n",
      "train loss:   0.955199\n",
      "train loss:   0.769512\n",
      "train loss:   0.871711\n",
      "train loss:   0.635532\n",
      "train loss:   1.014795\n",
      "train loss:   0.932431\n",
      "train loss:   0.426933\n",
      "train loss:   1.042479\n",
      "train loss:   0.933702\n",
      "train loss:   1.091422\n",
      "train loss:   0.837797\n",
      "train loss:   0.969619\n",
      "train loss:   1.035238\n",
      "train loss:   0.731949\n",
      "train loss:   1.186150\n",
      "train loss:   1.280832\n",
      "train loss:   1.318930\n",
      "train loss:   0.938486\n",
      "train loss:   1.020787\n",
      "train loss:   0.839464\n",
      "train loss:   1.086779\n",
      "train loss:   0.725612\n",
      "train loss:   0.974163\n",
      "train loss:   0.934846\n",
      "train loss:   0.969414\n",
      "train loss:   1.403136\n",
      "train loss:   1.305869\n",
      "train loss:   1.305095\n",
      "train loss:   0.862749\n",
      "train loss:   0.970402\n",
      "train loss:   1.014388\n",
      "train loss:   0.831596\n",
      "train loss:   1.170387\n",
      "train loss:   1.047192\n",
      "train loss:   1.298242\n",
      "train loss:   1.061660\n",
      "train loss:   1.125835\n",
      "train loss:   1.100058\n",
      "train loss:   1.213959\n",
      "train loss:   0.722589\n",
      "train loss:   1.074116\n",
      "train loss:   1.091586\n",
      "########### epoch 49 ###########\n",
      "########### loop 9200 ###########\n",
      "test loss:   0.358621   test accuracy:   0.916667\n",
      "########### loop 9200 ###########\n",
      "train loss:   1.300384\n",
      "train loss:   0.809853\n",
      "train loss:   1.013963\n",
      "train loss:   0.918255\n",
      "train loss:   0.761966\n",
      "train loss:   1.098801\n",
      "train loss:   1.274039\n",
      "train loss:   0.881941\n",
      "train loss:   1.308245\n",
      "train loss:   1.275841\n",
      "train loss:   1.333711\n",
      "train loss:   1.281260\n",
      "train loss:   0.999102\n",
      "train loss:   0.667736\n",
      "train loss:   1.108802\n",
      "train loss:   1.059264\n",
      "train loss:   1.364126\n",
      "train loss:   0.831463\n",
      "train loss:   0.853009\n",
      "train loss:   1.179307\n",
      "train loss:   1.543520\n",
      "train loss:   1.146831\n",
      "train loss:   1.280574\n",
      "train loss:   1.146879\n",
      "train loss:   1.250958\n",
      "train loss:   1.548270\n",
      "train loss:   1.257651\n",
      "train loss:   0.880079\n",
      "train loss:   0.840413\n",
      "train loss:   1.211446\n",
      "train loss:   0.954148\n",
      "train loss:   1.032924\n",
      "train loss:   1.140571\n",
      "train loss:   0.957605\n",
      "train loss:   1.028816\n",
      "train loss:   1.192419\n",
      "train loss:   1.416562\n",
      "train loss:   0.655805\n",
      "train loss:   1.279584\n",
      "train loss:   1.153432\n",
      "train loss:   0.810778\n",
      "train loss:   0.921613\n",
      "train loss:   0.833047\n",
      "train loss:   1.321238\n",
      "train loss:   0.972162\n",
      "train loss:   1.328559\n",
      "train loss:   1.060715\n",
      "train loss:   1.099720\n",
      "train loss:   1.091498\n",
      "train loss:   1.265268\n",
      "########### epoch 50 ###########\n",
      "########### loop 9250 ###########\n",
      "test loss:   0.188953   test accuracy:   0.958333\n",
      "########### loop 9250 ###########\n",
      "train loss:   1.048749\n",
      "train loss:   0.883471\n",
      "train loss:   1.011738\n",
      "train loss:   0.959607\n",
      "train loss:   1.052971\n",
      "train loss:   1.063420\n",
      "train loss:   0.915451\n",
      "train loss:   1.103432\n",
      "train loss:   0.838309\n",
      "train loss:   1.201564\n",
      "train loss:   1.384654\n",
      "train loss:   0.900843\n",
      "train loss:   1.210361\n",
      "train loss:   1.104612\n",
      "train loss:   1.006214\n",
      "train loss:   1.049313\n",
      "train loss:   1.133848\n",
      "train loss:   0.919965\n",
      "train loss:   1.506560\n",
      "train loss:   0.851718\n",
      "train loss:   0.841148\n",
      "train loss:   0.832744\n",
      "train loss:   0.954667\n",
      "train loss:   1.235634\n",
      "train loss:   0.838565\n",
      "train loss:   1.036400\n",
      "train loss:   1.161251\n",
      "train loss:   0.692287\n",
      "train loss:   1.171093\n",
      "train loss:   0.948246\n",
      "train loss:   0.856847\n",
      "train loss:   1.210112\n",
      "train loss:   0.756746\n",
      "train loss:   1.137328\n",
      "train loss:   1.398221\n",
      "train loss:   0.915147\n",
      "train loss:   1.228678\n",
      "train loss:   1.373218\n",
      "train loss:   1.194838\n",
      "train loss:   1.049496\n",
      "train loss:   0.913326\n",
      "train loss:   1.015053\n",
      "train loss:   1.347500\n",
      "train loss:   0.940444\n",
      "train loss:   0.964400\n",
      "train loss:   0.656040\n",
      "train loss:   0.903291\n",
      "train loss:   1.192404\n",
      "train loss:   1.107054\n",
      "train loss:   1.137566\n",
      "########### epoch 50 ###########\n",
      "########### loop 9300 ###########\n",
      "test loss:   0.239775   test accuracy:   1.000000\n",
      "########### loop 9300 ###########\n",
      "train loss:   1.231255\n",
      "train loss:   1.067627\n",
      "train loss:   0.699477\n",
      "train loss:   0.848952\n",
      "train loss:   0.975720\n",
      "train loss:   0.768842\n",
      "train loss:   0.677284\n",
      "train loss:   0.809192\n",
      "train loss:   1.144087\n",
      "train loss:   1.001913\n",
      "train loss:   0.659637\n",
      "train loss:   0.999117\n",
      "train loss:   0.794589\n",
      "train loss:   1.106802\n",
      "train loss:   1.363842\n",
      "train loss:   1.312444\n",
      "train loss:   0.936397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.107955\n",
      "train loss:   0.656040\n",
      "train loss:   0.922132\n",
      "train loss:   1.322501\n",
      "train loss:   0.838179\n",
      "train loss:   0.961898\n",
      "train loss:   0.845010\n",
      "train loss:   1.153559\n",
      "train loss:   0.894995\n",
      "train loss:   1.302978\n",
      "train loss:   0.930309\n",
      "train loss:   0.821662\n",
      "train loss:   1.210432\n",
      "train loss:   0.753741\n",
      "train loss:   1.015372\n",
      "train loss:   0.843600\n",
      "train loss:   1.272972\n",
      "train loss:   0.981971\n",
      "train loss:   1.066962\n",
      "train loss:   1.041654\n",
      "train loss:   1.167102\n",
      "train loss:   1.120513\n",
      "train loss:   0.742429\n",
      "train loss:   0.782976\n",
      "train loss:   1.144763\n",
      "train loss:   0.695385\n",
      "train loss:   1.217590\n",
      "train loss:   0.828674\n",
      "train loss:   0.988882\n",
      "train loss:   0.897283\n",
      "train loss:   1.504021\n",
      "train loss:   1.141384\n",
      "train loss:   1.092015\n",
      "########### epoch 50 ###########\n",
      "########### loop 9350 ###########\n",
      "test loss:   0.516201   test accuracy:   0.833333\n",
      "########### loop 9350 ###########\n",
      "train loss:   1.183978\n",
      "train loss:   1.081134\n",
      "train loss:   1.157809\n",
      "train loss:   0.934133\n",
      "train loss:   1.069824\n",
      "train loss:   0.879455\n",
      "train loss:   0.883196\n",
      "train loss:   0.948469\n",
      "train loss:   1.286417\n",
      "train loss:   0.644055\n",
      "train loss:   1.007096\n",
      "train loss:   0.953408\n",
      "train loss:   1.132717\n",
      "train loss:   1.135354\n",
      "train loss:   1.126662\n",
      "train loss:   0.834623\n",
      "train loss:   0.826501\n",
      "train loss:   0.722370\n",
      "train loss:   0.981561\n",
      "train loss:   1.071317\n",
      "train loss:   1.234632\n",
      "train loss:   1.193816\n",
      "train loss:   1.097843\n",
      "train loss:   1.099086\n",
      "train loss:   1.152699\n",
      "train loss:   0.631838\n",
      "train loss:   1.349661\n",
      "train loss:   1.158570\n",
      "train loss:   0.963583\n",
      "train loss:   1.242788\n",
      "train loss:   0.872142\n",
      "train loss:   0.757946\n",
      "train loss:   1.184943\n",
      "train loss:   1.059439\n",
      "train loss:   0.744710\n",
      "train loss:   0.852444\n",
      "train loss:   1.349210\n",
      "train loss:   1.466250\n",
      "train loss:   1.146024\n",
      "train loss:   1.069717\n",
      "train loss:   0.691288\n",
      "train loss:   1.115288\n",
      "train loss:   1.306993\n",
      "train loss:   1.112116\n",
      "train loss:   0.948978\n",
      "train loss:   1.168931\n",
      "train loss:   1.276646\n",
      "train loss:   1.132757\n",
      "train loss:   0.783978\n",
      "train loss:   1.064533\n",
      "########### epoch 51 ###########\n",
      "########### loop 9400 ###########\n",
      "test loss:   0.366237   test accuracy:   0.916667\n",
      "########### loop 9400 ###########\n",
      "train loss:   1.127109\n",
      "train loss:   0.738062\n",
      "train loss:   1.032711\n",
      "train loss:   1.167800\n",
      "train loss:   1.072007\n",
      "train loss:   1.030584\n",
      "train loss:   0.757937\n",
      "train loss:   0.861535\n",
      "train loss:   1.345854\n",
      "train loss:   1.233174\n",
      "train loss:   1.074690\n",
      "train loss:   0.940809\n",
      "train loss:   0.781512\n",
      "train loss:   0.849838\n",
      "train loss:   0.788970\n",
      "train loss:   1.253130\n",
      "train loss:   1.425274\n",
      "train loss:   0.973708\n",
      "train loss:   0.791276\n",
      "train loss:   1.007379\n",
      "train loss:   1.359568\n",
      "train loss:   1.211979\n",
      "train loss:   0.748076\n",
      "train loss:   1.245312\n",
      "train loss:   0.801658\n",
      "train loss:   0.918349\n",
      "train loss:   0.944831\n",
      "train loss:   0.950986\n",
      "train loss:   0.895523\n",
      "train loss:   0.693698\n",
      "train loss:   1.230984\n",
      "train loss:   0.988165\n",
      "train loss:   1.018736\n",
      "train loss:   0.897816\n",
      "train loss:   1.265378\n",
      "train loss:   0.855537\n",
      "train loss:   0.992542\n",
      "train loss:   0.944331\n",
      "train loss:   1.384633\n",
      "train loss:   0.795282\n",
      "train loss:   1.029273\n",
      "train loss:   1.125003\n",
      "train loss:   1.174465\n",
      "train loss:   0.964298\n",
      "train loss:   1.326523\n",
      "train loss:   1.338427\n",
      "train loss:   1.294229\n",
      "train loss:   1.028916\n",
      "train loss:   1.090700\n",
      "train loss:   0.917447\n",
      "########### epoch 51 ###########\n",
      "########### loop 9450 ###########\n",
      "test loss:   0.146161   test accuracy:   1.000000\n",
      "########### loop 9450 ###########\n",
      "train loss:   0.878872\n",
      "train loss:   1.183696\n",
      "train loss:   1.417280\n",
      "train loss:   0.825420\n",
      "train loss:   1.143665\n",
      "train loss:   1.048464\n",
      "train loss:   0.782814\n",
      "train loss:   1.264502\n",
      "train loss:   0.735729\n",
      "train loss:   1.211614\n",
      "train loss:   1.393547\n",
      "train loss:   1.235354\n",
      "train loss:   0.876386\n",
      "train loss:   0.690543\n",
      "train loss:   1.061783\n",
      "train loss:   1.218649\n",
      "train loss:   1.338000\n",
      "train loss:   0.928675\n",
      "train loss:   0.992544\n",
      "train loss:   1.248084\n",
      "train loss:   1.266974\n",
      "train loss:   0.852709\n",
      "train loss:   0.796768\n",
      "train loss:   0.932111\n",
      "train loss:   1.053622\n",
      "train loss:   0.651358\n",
      "train loss:   1.110792\n",
      "train loss:   0.936153\n",
      "train loss:   1.127155\n",
      "train loss:   0.661786\n",
      "train loss:   1.078549\n",
      "train loss:   0.990775\n",
      "train loss:   1.261332\n",
      "train loss:   1.227876\n",
      "train loss:   1.406546\n",
      "train loss:   0.836126\n",
      "train loss:   0.835969\n",
      "train loss:   1.184124\n",
      "train loss:   1.168876\n",
      "train loss:   1.050148\n",
      "train loss:   0.974863\n",
      "train loss:   0.959611\n",
      "train loss:   1.289065\n",
      "train loss:   0.802775\n",
      "train loss:   1.241679\n",
      "train loss:   1.160607\n",
      "train loss:   1.137463\n",
      "train loss:   0.699952\n",
      "train loss:   0.875142\n",
      "train loss:   1.298873\n",
      "########### epoch 51 ###########\n",
      "########### loop 9500 ###########\n",
      "test loss:   0.267673   test accuracy:   0.916667\n",
      "########### loop 9500 ###########\n",
      "train loss:   0.690537\n",
      "train loss:   1.525282\n",
      "train loss:   1.253592\n",
      "train loss:   1.121264\n",
      "train loss:   0.735054\n",
      "train loss:   1.039028\n",
      "train loss:   0.960990\n",
      "train loss:   1.356910\n",
      "train loss:   1.008882\n",
      "train loss:   0.917569\n",
      "train loss:   0.907181\n",
      "train loss:   1.157817\n",
      "train loss:   0.716562\n",
      "train loss:   1.338271\n",
      "train loss:   0.870621\n",
      "train loss:   0.942612\n",
      "train loss:   0.937261\n",
      "train loss:   0.689678\n",
      "train loss:   1.236475\n",
      "train loss:   0.995695\n",
      "train loss:   1.100763\n",
      "train loss:   0.821192\n",
      "train loss:   1.162330\n",
      "train loss:   0.947711\n",
      "train loss:   1.049695\n",
      "train loss:   1.103902\n",
      "train loss:   1.233256\n",
      "train loss:   1.143322\n",
      "train loss:   0.985620\n",
      "train loss:   1.239129\n",
      "train loss:   0.841758\n",
      "train loss:   0.694769\n",
      "train loss:   0.774933\n",
      "train loss:   1.226621\n",
      "train loss:   1.000039\n",
      "train loss:   1.506661\n",
      "train loss:   0.907492\n",
      "train loss:   1.115679\n",
      "train loss:   0.971916\n",
      "train loss:   1.054753\n",
      "train loss:   0.892733\n",
      "train loss:   0.807860\n",
      "train loss:   1.113280\n",
      "train loss:   0.746409\n",
      "train loss:   0.872621\n",
      "train loss:   0.831328\n",
      "train loss:   1.091335\n",
      "train loss:   1.145331\n",
      "train loss:   1.221136\n",
      "train loss:   0.995136\n",
      "########### epoch 51 ###########\n",
      "########### loop 9550 ###########\n",
      "test loss:   0.218007   test accuracy:   0.958333\n",
      "########### loop 9550 ###########\n",
      "train loss:   0.947773\n",
      "train loss:   0.678580\n",
      "train loss:   1.187200\n",
      "train loss:   1.097783\n",
      "train loss:   1.313056\n",
      "train loss:   0.914594\n",
      "train loss:   1.226259\n",
      "train loss:   0.825413\n",
      "train loss:   0.526012\n",
      "train loss:   1.454009\n",
      "train loss:   1.229632\n",
      "train loss:   0.606182\n",
      "train loss:   0.909986\n",
      "train loss:   0.957845\n",
      "train loss:   1.153557\n",
      "train loss:   1.160745\n",
      "train loss:   0.927777\n",
      "train loss:   1.108528\n",
      "train loss:   1.216018\n",
      "train loss:   0.682694\n",
      "train loss:   0.678367\n",
      "train loss:   0.648092\n",
      "train loss:   1.267384\n",
      "train loss:   1.018377\n",
      "train loss:   1.106223\n",
      "train loss:   1.128704\n",
      "train loss:   1.237143\n",
      "train loss:   0.860450\n",
      "train loss:   1.162299\n",
      "train loss:   1.216484\n",
      "train loss:   0.932364\n",
      "train loss:   0.946032\n",
      "train loss:   0.830971\n",
      "train loss:   0.880721\n",
      "train loss:   1.137461\n",
      "train loss:   1.247837\n",
      "train loss:   0.749498\n",
      "train loss:   0.771397\n",
      "train loss:   0.792297\n",
      "train loss:   0.743311\n",
      "train loss:   1.030589\n",
      "train loss:   1.159238\n",
      "train loss:   1.012528\n",
      "train loss:   1.158474\n",
      "train loss:   1.284534\n",
      "train loss:   1.281006\n",
      "train loss:   0.725052\n",
      "train loss:   0.906926\n",
      "train loss:   1.462101\n",
      "train loss:   0.519700\n",
      "########### epoch 52 ###########\n",
      "########### loop 9600 ###########\n",
      "test loss:   0.344177   test accuracy:   0.916667\n",
      "########### loop 9600 ###########\n",
      "train loss:   1.345435\n",
      "train loss:   0.958600\n",
      "train loss:   1.031971\n",
      "train loss:   0.893601\n",
      "train loss:   1.025843\n",
      "train loss:   0.937456\n",
      "train loss:   1.223271\n",
      "train loss:   0.702854\n",
      "train loss:   1.066769\n",
      "train loss:   0.923581\n",
      "train loss:   1.282072\n",
      "train loss:   1.383911\n",
      "train loss:   0.910407\n",
      "train loss:   0.822819\n",
      "train loss:   0.987648\n",
      "train loss:   0.658979\n",
      "train loss:   0.905711\n",
      "train loss:   0.876677\n",
      "train loss:   0.915345\n",
      "train loss:   1.421131\n",
      "train loss:   0.849813\n",
      "train loss:   0.823389\n",
      "train loss:   0.967289\n",
      "train loss:   1.330558\n",
      "train loss:   0.865597\n",
      "train loss:   1.193214\n",
      "train loss:   0.983418\n",
      "train loss:   1.282913\n",
      "train loss:   0.858709\n",
      "train loss:   1.174791\n",
      "train loss:   0.970851\n",
      "train loss:   1.320073\n",
      "train loss:   0.760163\n",
      "train loss:   1.160866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.170311\n",
      "train loss:   1.181403\n",
      "train loss:   1.072454\n",
      "train loss:   0.923629\n",
      "train loss:   1.060782\n",
      "train loss:   0.803407\n",
      "train loss:   0.990054\n",
      "train loss:   1.078159\n",
      "train loss:   0.795507\n",
      "train loss:   1.123329\n",
      "train loss:   1.349103\n",
      "train loss:   0.963792\n",
      "train loss:   1.365422\n",
      "train loss:   1.274032\n",
      "train loss:   1.228506\n",
      "train loss:   0.739205\n",
      "########### epoch 52 ###########\n",
      "########### loop 9650 ###########\n",
      "test loss:   0.323098   test accuracy:   0.875000\n",
      "########### loop 9650 ###########\n",
      "train loss:   1.201291\n",
      "train loss:   1.154184\n",
      "train loss:   0.947166\n",
      "train loss:   1.011143\n",
      "train loss:   0.796807\n",
      "train loss:   0.751747\n",
      "train loss:   1.055394\n",
      "train loss:   1.068418\n",
      "train loss:   1.258377\n",
      "train loss:   0.667463\n",
      "train loss:   1.043098\n",
      "train loss:   0.729758\n",
      "train loss:   0.544792\n",
      "train loss:   1.022900\n",
      "train loss:   1.265682\n",
      "train loss:   0.793394\n",
      "train loss:   0.772601\n",
      "train loss:   1.291598\n",
      "train loss:   1.135238\n",
      "train loss:   0.518571\n",
      "train loss:   0.765142\n",
      "train loss:   0.819817\n",
      "train loss:   1.263348\n",
      "train loss:   1.042815\n",
      "train loss:   0.854202\n",
      "train loss:   1.104882\n",
      "train loss:   0.867799\n",
      "train loss:   0.989811\n",
      "train loss:   1.305279\n",
      "train loss:   1.016888\n",
      "train loss:   1.072926\n",
      "train loss:   0.774392\n",
      "train loss:   0.910186\n",
      "train loss:   1.104168\n",
      "train loss:   0.918610\n",
      "train loss:   1.121757\n",
      "train loss:   0.824177\n",
      "train loss:   0.883096\n",
      "train loss:   0.799899\n",
      "train loss:   0.846481\n",
      "train loss:   0.857942\n",
      "train loss:   1.062010\n",
      "train loss:   0.783515\n",
      "train loss:   0.915984\n",
      "train loss:   0.924405\n",
      "train loss:   1.241435\n",
      "train loss:   1.058190\n",
      "train loss:   1.262589\n",
      "train loss:   1.520319\n",
      "train loss:   1.067624\n",
      "########### epoch 52 ###########\n",
      "########### loop 9700 ###########\n",
      "test loss:   0.626100   test accuracy:   0.833333\n",
      "########### loop 9700 ###########\n",
      "train loss:   0.918639\n",
      "train loss:   1.198778\n",
      "train loss:   0.883503\n",
      "train loss:   0.884533\n",
      "train loss:   1.076298\n",
      "train loss:   1.050896\n",
      "train loss:   1.345553\n",
      "train loss:   1.036030\n",
      "train loss:   0.964171\n",
      "train loss:   1.029812\n",
      "train loss:   1.119372\n",
      "train loss:   0.732548\n",
      "train loss:   1.002315\n",
      "train loss:   0.493301\n",
      "train loss:   1.190214\n",
      "train loss:   1.158057\n",
      "train loss:   1.032861\n",
      "train loss:   1.003569\n",
      "train loss:   1.002441\n",
      "train loss:   1.009873\n",
      "train loss:   0.998874\n",
      "train loss:   1.278902\n",
      "train loss:   0.787762\n",
      "train loss:   0.995686\n",
      "train loss:   1.388531\n",
      "train loss:   1.107876\n",
      "train loss:   0.849439\n",
      "train loss:   0.645901\n",
      "train loss:   0.997506\n",
      "train loss:   0.935596\n",
      "train loss:   0.973795\n",
      "train loss:   0.425312\n",
      "train loss:   0.826399\n",
      "train loss:   0.862617\n",
      "train loss:   1.278910\n",
      "train loss:   0.788386\n",
      "train loss:   1.190361\n",
      "train loss:   0.943409\n",
      "train loss:   0.802755\n",
      "train loss:   1.107000\n",
      "train loss:   1.011101\n",
      "train loss:   1.303555\n",
      "train loss:   1.201620\n",
      "train loss:   0.751046\n",
      "train loss:   1.032320\n",
      "train loss:   1.448713\n",
      "train loss:   1.382485\n",
      "train loss:   0.800821\n",
      "train loss:   1.233164\n",
      "train loss:   0.865216\n",
      "########### epoch 52 ###########\n",
      "########### loop 9750 ###########\n",
      "test loss:   0.321931   test accuracy:   0.958333\n",
      "########### loop 9750 ###########\n",
      "train loss:   0.893399\n",
      "train loss:   0.601160\n",
      "train loss:   0.903596\n",
      "train loss:   1.542477\n",
      "train loss:   0.879782\n",
      "train loss:   1.049275\n",
      "train loss:   0.855582\n",
      "train loss:   0.962609\n",
      "train loss:   1.339998\n",
      "train loss:   0.775266\n",
      "train loss:   0.933818\n",
      "train loss:   0.908905\n",
      "train loss:   1.044299\n",
      "train loss:   0.890509\n",
      "train loss:   1.046271\n",
      "train loss:   0.874653\n",
      "train loss:   0.786216\n",
      "train loss:   0.905397\n",
      "train loss:   1.538368\n",
      "train loss:   1.103723\n",
      "train loss:   1.191016\n",
      "train loss:   0.881763\n",
      "train loss:   1.144021\n",
      "train loss:   0.893455\n",
      "train loss:   0.764728\n",
      "train loss:   1.050098\n",
      "train loss:   1.535212\n",
      "train loss:   1.258201\n",
      "train loss:   0.875988\n",
      "train loss:   1.075952\n",
      "train loss:   1.110623\n",
      "train loss:   1.079696\n",
      "train loss:   1.294751\n",
      "train loss:   1.015135\n",
      "train loss:   1.382453\n",
      "train loss:   1.246914\n",
      "train loss:   0.952714\n",
      "train loss:   1.107241\n",
      "train loss:   0.813078\n",
      "train loss:   0.826274\n",
      "train loss:   1.069172\n",
      "train loss:   0.874771\n",
      "train loss:   0.721456\n",
      "train loss:   1.053138\n",
      "train loss:   1.238560\n",
      "train loss:   0.939891\n",
      "train loss:   1.189383\n",
      "train loss:   1.138213\n",
      "train loss:   0.671198\n",
      "train loss:   1.054768\n",
      "########### epoch 53 ###########\n",
      "########### loop 9800 ###########\n",
      "test loss:   0.323537   test accuracy:   1.000000\n",
      "########### loop 9800 ###########\n",
      "train loss:   0.663294\n",
      "train loss:   0.831489\n",
      "train loss:   1.262144\n",
      "train loss:   0.974493\n",
      "train loss:   0.258716\n",
      "train loss:   0.822489\n",
      "train loss:   1.016647\n",
      "train loss:   1.334366\n",
      "train loss:   1.218882\n",
      "train loss:   0.719897\n",
      "train loss:   0.999775\n",
      "train loss:   0.960455\n",
      "train loss:   0.857355\n",
      "train loss:   1.343908\n",
      "train loss:   0.991861\n",
      "train loss:   1.012884\n",
      "train loss:   1.184977\n",
      "train loss:   0.720554\n",
      "train loss:   1.206586\n",
      "train loss:   1.263070\n",
      "train loss:   1.172125\n",
      "train loss:   0.934837\n",
      "train loss:   0.948369\n",
      "train loss:   0.860506\n",
      "train loss:   0.732107\n",
      "train loss:   1.151551\n",
      "train loss:   0.819282\n",
      "train loss:   1.083046\n",
      "train loss:   0.888001\n",
      "train loss:   0.966638\n",
      "train loss:   0.569784\n",
      "train loss:   0.731551\n",
      "train loss:   1.213512\n",
      "train loss:   0.776071\n",
      "train loss:   0.716920\n",
      "train loss:   1.049581\n",
      "train loss:   1.077601\n",
      "train loss:   0.714821\n",
      "train loss:   1.007615\n",
      "train loss:   1.042204\n",
      "train loss:   0.753707\n",
      "train loss:   1.055537\n",
      "train loss:   0.977894\n",
      "train loss:   0.973360\n",
      "train loss:   0.802037\n",
      "train loss:   0.845260\n",
      "train loss:   0.896696\n",
      "train loss:   1.126963\n",
      "train loss:   0.940964\n",
      "train loss:   0.711836\n",
      "########### epoch 53 ###########\n",
      "########### loop 9850 ###########\n",
      "test loss:   0.602941   test accuracy:   0.916667\n",
      "########### loop 9850 ###########\n",
      "train loss:   1.220292\n",
      "train loss:   0.837622\n",
      "train loss:   1.037840\n",
      "train loss:   1.260538\n",
      "train loss:   1.216308\n",
      "train loss:   0.706579\n",
      "train loss:   0.813335\n",
      "train loss:   1.041258\n",
      "train loss:   0.973065\n",
      "train loss:   0.964922\n",
      "train loss:   1.188042\n",
      "train loss:   0.784664\n",
      "train loss:   0.991089\n",
      "train loss:   1.004852\n",
      "train loss:   0.961774\n",
      "train loss:   0.906740\n",
      "train loss:   1.172539\n",
      "train loss:   1.213066\n",
      "train loss:   0.839275\n",
      "train loss:   0.770396\n",
      "train loss:   0.935535\n",
      "train loss:   1.106191\n",
      "train loss:   1.060512\n",
      "train loss:   1.150338\n",
      "train loss:   0.934260\n",
      "train loss:   1.049696\n",
      "train loss:   0.946104\n",
      "train loss:   0.942953\n",
      "train loss:   1.078166\n",
      "train loss:   1.123877\n",
      "train loss:   0.798255\n",
      "train loss:   1.023478\n",
      "train loss:   0.953131\n",
      "train loss:   0.755403\n",
      "train loss:   1.309142\n",
      "train loss:   0.925545\n",
      "train loss:   0.859370\n",
      "train loss:   1.068940\n",
      "train loss:   0.921602\n",
      "train loss:   0.861211\n",
      "train loss:   1.370428\n",
      "train loss:   0.803149\n",
      "train loss:   0.941249\n",
      "train loss:   1.217969\n",
      "train loss:   0.835241\n",
      "train loss:   0.997821\n",
      "train loss:   0.939848\n",
      "train loss:   1.102199\n",
      "train loss:   1.224580\n",
      "train loss:   1.094724\n",
      "########### epoch 53 ###########\n",
      "########### loop 9900 ###########\n",
      "test loss:   0.260376   test accuracy:   0.958333\n",
      "########### loop 9900 ###########\n",
      "train loss:   0.708840\n",
      "train loss:   1.068174\n",
      "train loss:   0.873501\n",
      "train loss:   1.156200\n",
      "train loss:   0.788499\n",
      "train loss:   1.313311\n",
      "train loss:   0.974349\n",
      "train loss:   0.810684\n",
      "train loss:   1.014524\n",
      "train loss:   1.378308\n",
      "train loss:   1.051604\n",
      "train loss:   1.030729\n",
      "train loss:   1.023910\n",
      "train loss:   0.948171\n",
      "train loss:   1.186922\n",
      "train loss:   0.965894\n",
      "train loss:   1.242013\n",
      "train loss:   0.948945\n",
      "train loss:   1.220185\n",
      "train loss:   0.922225\n",
      "train loss:   0.856273\n",
      "train loss:   0.996200\n",
      "train loss:   0.968641\n",
      "train loss:   1.061079\n",
      "train loss:   1.058290\n",
      "train loss:   0.981147\n",
      "train loss:   1.214437\n",
      "train loss:   1.042854\n",
      "train loss:   0.881380\n",
      "train loss:   1.155537\n",
      "train loss:   1.100422\n",
      "train loss:   0.883555\n",
      "train loss:   0.924078\n",
      "train loss:   0.918618\n",
      "train loss:   1.021223\n",
      "train loss:   1.215670\n",
      "train loss:   0.959417\n",
      "train loss:   0.914305\n",
      "train loss:   0.878252\n",
      "train loss:   1.134277\n",
      "train loss:   0.917471\n",
      "train loss:   0.922677\n",
      "train loss:   0.927412\n",
      "train loss:   1.157475\n",
      "train loss:   1.025202\n",
      "train loss:   0.926645\n",
      "train loss:   0.837913\n",
      "train loss:   0.995632\n",
      "train loss:   0.655126\n",
      "train loss:   0.881285\n",
      "########### epoch 53 ###########\n",
      "########### loop 9950 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.494720   test accuracy:   0.875000\n",
      "########### loop 9950 ###########\n",
      "train loss:   1.334790\n",
      "train loss:   0.840167\n",
      "train loss:   1.231776\n",
      "train loss:   1.152812\n",
      "train loss:   1.359885\n",
      "train loss:   1.173958\n",
      "train loss:   1.150061\n",
      "train loss:   0.933233\n",
      "train loss:   0.824577\n",
      "train loss:   0.921996\n",
      "train loss:   1.193352\n",
      "train loss:   1.301476\n",
      "train loss:   0.895316\n",
      "train loss:   1.042674\n",
      "train loss:   0.855099\n",
      "train loss:   0.967338\n",
      "train loss:   1.098143\n",
      "train loss:   0.759472\n",
      "train loss:   1.087467\n",
      "train loss:   0.955496\n",
      "train loss:   0.729408\n",
      "train loss:   0.899534\n",
      "train loss:   0.901495\n",
      "train loss:   1.352103\n",
      "train loss:   0.789466\n",
      "train loss:   1.257634\n",
      "train loss:   1.237370\n",
      "train loss:   0.965792\n",
      "train loss:   0.754980\n",
      "train loss:   1.150683\n",
      "train loss:   0.805967\n",
      "train loss:   0.961253\n",
      "train loss:   0.865512\n",
      "train loss:   0.922374\n",
      "train loss:   0.822783\n",
      "train loss:   1.209180\n",
      "train loss:   1.037001\n",
      "train loss:   0.803394\n",
      "train loss:   1.365967\n",
      "train loss:   0.797894\n",
      "train loss:   1.021712\n",
      "train loss:   1.253919\n",
      "train loss:   0.979715\n",
      "train loss:   1.111184\n",
      "train loss:   0.792396\n",
      "train loss:   0.872569\n",
      "train loss:   1.059714\n",
      "train loss:   1.336115\n",
      "train loss:   1.087038\n",
      "train loss:   0.979885\n",
      "########### epoch 54 ###########\n",
      "########### loop 10000 ###########\n",
      "test loss:   0.170520   test accuracy:   1.000000\n",
      "########### loop 10000 ###########\n",
      "train loss:   0.868414\n",
      "train loss:   1.222607\n",
      "train loss:   0.827617\n",
      "train loss:   0.847827\n",
      "train loss:   1.147393\n",
      "train loss:   1.250110\n",
      "train loss:   1.397576\n",
      "train loss:   0.935440\n",
      "train loss:   1.216289\n",
      "train loss:   0.916016\n",
      "train loss:   0.900959\n",
      "train loss:   0.849598\n",
      "train loss:   0.949765\n",
      "train loss:   1.027066\n",
      "train loss:   1.012853\n",
      "train loss:   0.992172\n",
      "train loss:   0.873121\n",
      "train loss:   1.191396\n",
      "train loss:   0.872368\n",
      "train loss:   0.889159\n",
      "train loss:   0.980208\n",
      "train loss:   1.304564\n",
      "train loss:   1.460857\n",
      "train loss:   0.910281\n",
      "train loss:   1.045401\n",
      "train loss:   1.129773\n",
      "train loss:   1.142895\n",
      "train loss:   0.899759\n",
      "train loss:   1.004076\n",
      "train loss:   0.914196\n",
      "train loss:   0.736199\n",
      "train loss:   0.837393\n",
      "train loss:   0.978312\n",
      "train loss:   0.787000\n",
      "train loss:   0.711656\n",
      "train loss:   0.951875\n",
      "train loss:   1.086125\n",
      "train loss:   1.096638\n",
      "train loss:   1.236435\n",
      "train loss:   0.698542\n",
      "train loss:   0.645085\n",
      "train loss:   1.021879\n",
      "train loss:   1.292939\n",
      "train loss:   1.186864\n",
      "train loss:   0.925084\n",
      "train loss:   1.260990\n",
      "train loss:   0.786680\n",
      "train loss:   1.022416\n",
      "train loss:   1.299276\n",
      "train loss:   0.924670\n",
      "########### epoch 54 ###########\n",
      "########### loop 10050 ###########\n",
      "test loss:   0.196500   test accuracy:   1.000000\n",
      "########### loop 10050 ###########\n",
      "train loss:   1.022426\n",
      "train loss:   0.837357\n",
      "train loss:   0.987957\n",
      "train loss:   1.297184\n",
      "train loss:   1.101115\n",
      "train loss:   0.942613\n",
      "train loss:   0.879409\n",
      "train loss:   1.191496\n",
      "train loss:   0.888065\n",
      "train loss:   1.428949\n",
      "train loss:   0.942925\n",
      "train loss:   0.697582\n",
      "train loss:   0.872476\n",
      "train loss:   1.060403\n",
      "train loss:   0.742379\n",
      "train loss:   0.992433\n",
      "train loss:   0.921276\n",
      "train loss:   1.020037\n",
      "train loss:   0.806578\n",
      "train loss:   1.472057\n",
      "train loss:   1.116007\n",
      "train loss:   1.092966\n",
      "train loss:   0.925083\n",
      "train loss:   0.875186\n",
      "train loss:   1.297680\n",
      "train loss:   1.059527\n",
      "train loss:   1.441471\n",
      "train loss:   0.554290\n",
      "train loss:   1.108711\n",
      "train loss:   1.024888\n",
      "train loss:   0.728275\n",
      "train loss:   1.021177\n",
      "train loss:   1.396604\n",
      "train loss:   1.494239\n",
      "train loss:   0.879573\n",
      "train loss:   0.924499\n",
      "train loss:   0.734714\n",
      "train loss:   0.890039\n",
      "train loss:   1.213827\n",
      "train loss:   1.023015\n",
      "train loss:   1.499834\n",
      "train loss:   1.167941\n",
      "train loss:   0.898256\n",
      "train loss:   0.996352\n",
      "train loss:   1.163077\n",
      "train loss:   0.923876\n",
      "train loss:   0.948326\n",
      "train loss:   0.894778\n",
      "train loss:   0.981552\n",
      "train loss:   1.061675\n",
      "########### epoch 54 ###########\n",
      "########### loop 10100 ###########\n",
      "test loss:   0.399240   test accuracy:   0.916667\n",
      "########### loop 10100 ###########\n",
      "train loss:   0.917472\n",
      "train loss:   1.246261\n",
      "train loss:   1.133238\n",
      "train loss:   1.250796\n",
      "train loss:   0.644024\n",
      "train loss:   1.101538\n",
      "train loss:   0.966790\n",
      "train loss:   1.098047\n",
      "train loss:   0.908240\n",
      "train loss:   1.241555\n",
      "train loss:   0.922013\n",
      "train loss:   0.609799\n",
      "train loss:   0.808099\n",
      "train loss:   1.071650\n",
      "train loss:   1.026333\n",
      "train loss:   0.597282\n",
      "train loss:   1.091413\n",
      "train loss:   1.212537\n",
      "train loss:   1.478003\n",
      "train loss:   0.872517\n",
      "train loss:   0.586985\n",
      "train loss:   1.469110\n",
      "train loss:   1.137749\n",
      "train loss:   1.202224\n",
      "train loss:   0.771988\n",
      "train loss:   1.030980\n",
      "train loss:   0.887851\n",
      "train loss:   0.873769\n",
      "train loss:   1.123083\n",
      "train loss:   0.787392\n",
      "train loss:   0.670854\n",
      "train loss:   1.070741\n",
      "train loss:   0.903777\n",
      "train loss:   0.970070\n",
      "train loss:   1.128189\n",
      "train loss:   0.823748\n",
      "train loss:   0.982608\n",
      "train loss:   0.931039\n",
      "train loss:   0.792314\n",
      "train loss:   1.127644\n",
      "train loss:   1.200612\n",
      "train loss:   1.016726\n",
      "train loss:   0.581770\n",
      "train loss:   1.238854\n",
      "train loss:   0.886763\n",
      "train loss:   1.160762\n",
      "train loss:   0.727670\n",
      "train loss:   0.781236\n",
      "train loss:   1.013316\n",
      "train loss:   1.050642\n",
      "########### epoch 54 ###########\n",
      "########### loop 10150 ###########\n",
      "test loss:   0.233197   test accuracy:   0.958333\n",
      "########### loop 10150 ###########\n",
      "train loss:   0.700480\n",
      "train loss:   0.832229\n",
      "train loss:   0.878953\n",
      "train loss:   1.031926\n",
      "train loss:   1.474203\n",
      "train loss:   0.869473\n",
      "train loss:   0.900930\n",
      "train loss:   0.925663\n",
      "train loss:   1.110466\n",
      "train loss:   0.935008\n",
      "train loss:   1.002467\n",
      "train loss:   0.921059\n",
      "train loss:   1.240264\n",
      "train loss:   1.217342\n",
      "train loss:   1.070966\n",
      "train loss:   0.940785\n",
      "train loss:   1.353025\n",
      "train loss:   1.356525\n",
      "train loss:   0.726950\n",
      "train loss:   0.993922\n",
      "train loss:   0.629986\n",
      "train loss:   1.365683\n",
      "train loss:   1.078237\n",
      "train loss:   0.492757\n",
      "train loss:   1.086881\n",
      "train loss:   1.331950\n",
      "train loss:   0.929089\n",
      "train loss:   1.084193\n",
      "train loss:   1.285528\n",
      "train loss:   0.916535\n",
      "train loss:   1.358629\n",
      "train loss:   0.748943\n",
      "train loss:   1.075956\n",
      "train loss:   0.695250\n",
      "train loss:   1.026463\n",
      "train loss:   1.334275\n",
      "train loss:   1.239632\n",
      "train loss:   0.891146\n",
      "train loss:   0.810817\n",
      "train loss:   0.788249\n",
      "train loss:   1.050258\n",
      "train loss:   1.382703\n",
      "train loss:   0.764477\n",
      "train loss:   1.163647\n",
      "train loss:   0.709266\n",
      "train loss:   0.859724\n",
      "train loss:   0.893154\n",
      "train loss:   0.795885\n",
      "train loss:   1.106103\n",
      "train loss:   1.198505\n",
      "########### epoch 55 ###########\n",
      "########### loop 10200 ###########\n",
      "test loss:   0.350798   test accuracy:   0.958333\n",
      "########### loop 10200 ###########\n",
      "train loss:   1.421358\n",
      "train loss:   0.870077\n",
      "train loss:   1.098920\n",
      "train loss:   0.996721\n",
      "train loss:   1.151158\n",
      "train loss:   1.060879\n",
      "train loss:   0.840774\n",
      "train loss:   1.184750\n",
      "train loss:   1.048184\n",
      "train loss:   0.991662\n",
      "train loss:   1.316833\n",
      "train loss:   0.793586\n",
      "train loss:   0.980387\n",
      "train loss:   1.290928\n",
      "train loss:   0.823505\n",
      "train loss:   1.156595\n",
      "train loss:   0.559422\n",
      "train loss:   1.062005\n",
      "train loss:   1.039350\n",
      "train loss:   1.227360\n",
      "train loss:   0.857467\n",
      "train loss:   0.758856\n",
      "train loss:   1.296765\n",
      "train loss:   0.972063\n",
      "train loss:   0.884348\n",
      "train loss:   0.762748\n",
      "train loss:   0.864952\n",
      "train loss:   0.891928\n",
      "train loss:   1.077487\n",
      "train loss:   1.161572\n",
      "train loss:   1.143987\n",
      "train loss:   1.157123\n",
      "train loss:   1.317341\n",
      "train loss:   0.873213\n",
      "train loss:   0.769931\n",
      "train loss:   0.963752\n",
      "train loss:   1.030491\n",
      "train loss:   0.902737\n",
      "train loss:   0.667261\n",
      "train loss:   0.833138\n",
      "train loss:   0.899411\n",
      "train loss:   0.930795\n",
      "train loss:   1.244777\n",
      "train loss:   0.978504\n",
      "train loss:   1.253276\n",
      "train loss:   1.083014\n",
      "train loss:   1.055730\n",
      "train loss:   0.802478\n",
      "train loss:   0.684934\n",
      "train loss:   1.050243\n",
      "########### epoch 55 ###########\n",
      "########### loop 10250 ###########\n",
      "test loss:   0.683322   test accuracy:   0.833333\n",
      "########### loop 10250 ###########\n",
      "train loss:   0.709204\n",
      "train loss:   1.062693\n",
      "train loss:   1.066002\n",
      "train loss:   0.771357\n",
      "train loss:   1.128560\n",
      "train loss:   1.437586\n",
      "train loss:   1.178738\n",
      "train loss:   1.027058\n",
      "train loss:   0.816118\n",
      "train loss:   0.846718\n",
      "train loss:   1.131926\n",
      "train loss:   1.134507\n",
      "train loss:   0.952501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.128347\n",
      "train loss:   0.927212\n",
      "train loss:   1.132570\n",
      "train loss:   0.751088\n",
      "train loss:   0.888588\n",
      "train loss:   1.243097\n",
      "train loss:   1.036919\n",
      "train loss:   0.972761\n",
      "train loss:   0.988680\n",
      "train loss:   1.372952\n",
      "train loss:   0.806403\n",
      "train loss:   1.161321\n",
      "train loss:   1.077036\n",
      "train loss:   1.345477\n",
      "train loss:   1.030723\n",
      "train loss:   1.181427\n",
      "train loss:   0.879211\n",
      "train loss:   0.892637\n",
      "train loss:   1.062499\n",
      "train loss:   1.278184\n",
      "train loss:   1.013695\n",
      "train loss:   1.110832\n",
      "train loss:   1.094291\n",
      "train loss:   0.980182\n",
      "train loss:   1.560170\n",
      "train loss:   0.961823\n",
      "train loss:   1.127398\n",
      "train loss:   0.949669\n",
      "train loss:   1.105294\n",
      "train loss:   1.176562\n",
      "train loss:   0.822827\n",
      "train loss:   1.214393\n",
      "train loss:   0.778128\n",
      "train loss:   1.146947\n",
      "train loss:   0.967000\n",
      "train loss:   1.347338\n",
      "train loss:   1.553356\n",
      "########### epoch 55 ###########\n",
      "########### loop 10300 ###########\n",
      "test loss:   0.340967   test accuracy:   0.958333\n",
      "########### loop 10300 ###########\n",
      "train loss:   1.019325\n",
      "train loss:   1.245628\n",
      "train loss:   0.931969\n",
      "train loss:   0.927433\n",
      "train loss:   1.403259\n",
      "train loss:   0.990824\n",
      "train loss:   1.018508\n",
      "train loss:   0.686807\n",
      "train loss:   0.720484\n",
      "train loss:   1.210768\n",
      "train loss:   1.184543\n",
      "train loss:   0.948681\n",
      "train loss:   0.869102\n",
      "train loss:   1.190464\n",
      "train loss:   0.834576\n",
      "train loss:   1.099391\n",
      "train loss:   0.927734\n",
      "train loss:   1.055339\n",
      "train loss:   0.963832\n",
      "train loss:   0.837141\n",
      "train loss:   1.089265\n",
      "train loss:   1.202377\n",
      "train loss:   0.965560\n",
      "train loss:   1.112878\n",
      "train loss:   1.067899\n",
      "train loss:   1.320294\n",
      "train loss:   0.762648\n",
      "train loss:   1.384510\n",
      "train loss:   1.278450\n",
      "train loss:   1.100912\n",
      "train loss:   0.983018\n",
      "train loss:   1.238039\n",
      "train loss:   0.954417\n",
      "train loss:   0.607368\n",
      "train loss:   1.142984\n",
      "train loss:   1.008432\n",
      "train loss:   0.960109\n",
      "train loss:   0.938603\n",
      "train loss:   0.780895\n",
      "train loss:   0.954284\n",
      "train loss:   1.100282\n",
      "train loss:   1.207418\n",
      "train loss:   1.076855\n",
      "train loss:   1.068836\n",
      "train loss:   1.403159\n",
      "train loss:   0.993690\n",
      "train loss:   1.134778\n",
      "train loss:   0.856929\n",
      "train loss:   0.931150\n",
      "train loss:   0.999028\n",
      "########### epoch 56 ###########\n",
      "########### loop 10350 ###########\n",
      "test loss:   0.240686   test accuracy:   1.000000\n",
      "########### loop 10350 ###########\n",
      "train loss:   0.762021\n",
      "train loss:   0.729145\n",
      "train loss:   0.740047\n",
      "train loss:   1.320904\n",
      "train loss:   0.739825\n",
      "train loss:   0.743393\n",
      "train loss:   1.064646\n",
      "train loss:   1.117603\n",
      "train loss:   1.010053\n",
      "train loss:   1.282788\n",
      "train loss:   1.182822\n",
      "train loss:   1.100445\n",
      "train loss:   0.994242\n",
      "train loss:   1.067889\n",
      "train loss:   1.051306\n",
      "train loss:   1.078977\n",
      "train loss:   0.961441\n",
      "train loss:   0.988724\n",
      "train loss:   1.057868\n",
      "train loss:   1.382554\n",
      "train loss:   0.960948\n",
      "train loss:   1.131224\n",
      "train loss:   0.914087\n",
      "train loss:   0.853739\n",
      "train loss:   0.664011\n",
      "train loss:   1.052627\n",
      "train loss:   1.131706\n",
      "train loss:   0.739884\n",
      "train loss:   1.420230\n",
      "train loss:   1.219170\n",
      "train loss:   1.123231\n",
      "train loss:   0.978165\n",
      "train loss:   0.940250\n",
      "train loss:   1.292107\n",
      "train loss:   1.023130\n",
      "train loss:   0.465649\n",
      "train loss:   1.180901\n",
      "train loss:   1.072162\n",
      "train loss:   0.745090\n",
      "train loss:   1.046571\n",
      "train loss:   0.681276\n",
      "train loss:   0.902605\n",
      "train loss:   0.911590\n",
      "train loss:   1.139284\n",
      "train loss:   1.555136\n",
      "train loss:   0.867047\n",
      "train loss:   1.010906\n",
      "train loss:   1.017490\n",
      "train loss:   0.709964\n",
      "train loss:   0.687093\n",
      "########### epoch 56 ###########\n",
      "########### loop 10400 ###########\n",
      "test loss:   0.333837   test accuracy:   0.916667\n",
      "########### loop 10400 ###########\n",
      "train loss:   1.094886\n",
      "train loss:   0.878129\n",
      "train loss:   0.964529\n",
      "train loss:   0.801620\n",
      "train loss:   1.411619\n",
      "train loss:   1.118859\n",
      "train loss:   1.030127\n",
      "train loss:   1.285703\n",
      "train loss:   0.986541\n",
      "train loss:   0.804643\n",
      "train loss:   0.865064\n",
      "train loss:   0.783906\n",
      "train loss:   0.901572\n",
      "train loss:   0.951641\n",
      "train loss:   1.064586\n",
      "train loss:   1.166382\n",
      "train loss:   1.019255\n",
      "train loss:   1.002970\n",
      "train loss:   0.737246\n",
      "train loss:   1.203172\n",
      "train loss:   1.074734\n",
      "train loss:   1.100912\n",
      "train loss:   0.936710\n",
      "train loss:   0.947873\n",
      "train loss:   0.904852\n",
      "train loss:   0.955320\n",
      "train loss:   0.857796\n",
      "train loss:   1.336228\n",
      "train loss:   1.140350\n",
      "train loss:   1.124152\n",
      "train loss:   0.983652\n",
      "train loss:   0.716355\n",
      "train loss:   1.259109\n",
      "train loss:   1.025408\n",
      "train loss:   1.182960\n",
      "train loss:   1.221283\n",
      "train loss:   1.132586\n",
      "train loss:   1.074444\n",
      "train loss:   1.003568\n",
      "train loss:   1.113988\n",
      "train loss:   1.012898\n",
      "train loss:   1.075752\n",
      "train loss:   0.777207\n",
      "train loss:   1.139669\n",
      "train loss:   0.990351\n",
      "train loss:   0.677315\n",
      "train loss:   0.810154\n",
      "train loss:   1.007744\n",
      "train loss:   1.118138\n",
      "train loss:   0.868760\n",
      "########### epoch 56 ###########\n",
      "########### loop 10450 ###########\n",
      "test loss:   0.433808   test accuracy:   0.791667\n",
      "########### loop 10450 ###########\n",
      "train loss:   0.949227\n",
      "train loss:   1.076565\n",
      "train loss:   1.142656\n",
      "train loss:   0.866398\n",
      "train loss:   0.956218\n",
      "train loss:   1.146158\n",
      "train loss:   1.389314\n",
      "train loss:   0.716407\n",
      "train loss:   0.958891\n",
      "train loss:   1.063285\n",
      "train loss:   1.121739\n",
      "train loss:   1.160106\n",
      "train loss:   0.838560\n",
      "train loss:   1.197188\n",
      "train loss:   1.155418\n",
      "train loss:   0.937051\n",
      "train loss:   0.926327\n",
      "train loss:   1.100359\n",
      "train loss:   0.974043\n",
      "train loss:   1.086759\n",
      "train loss:   1.016895\n",
      "train loss:   0.919541\n",
      "train loss:   1.248841\n",
      "train loss:   1.082918\n",
      "train loss:   1.093087\n",
      "train loss:   1.423917\n",
      "train loss:   0.878974\n",
      "train loss:   1.001224\n",
      "train loss:   1.066958\n",
      "train loss:   0.796262\n",
      "train loss:   1.130373\n",
      "train loss:   0.858627\n",
      "train loss:   0.971612\n",
      "train loss:   0.835416\n",
      "train loss:   0.734381\n",
      "train loss:   1.260389\n",
      "train loss:   0.976333\n",
      "train loss:   0.707455\n",
      "train loss:   0.958240\n",
      "train loss:   1.048267\n",
      "train loss:   0.927678\n",
      "train loss:   1.194531\n",
      "train loss:   1.073130\n",
      "train loss:   1.167843\n",
      "train loss:   1.213629\n",
      "train loss:   1.367113\n",
      "train loss:   1.197520\n",
      "train loss:   1.337603\n",
      "train loss:   1.081020\n",
      "train loss:   1.135341\n",
      "########### epoch 56 ###########\n",
      "########### loop 10500 ###########\n",
      "test loss:   0.157851   test accuracy:   1.000000\n",
      "########### loop 10500 ###########\n",
      "train loss:   0.989721\n",
      "train loss:   0.714531\n",
      "train loss:   0.778668\n",
      "train loss:   1.067028\n",
      "train loss:   1.290609\n",
      "train loss:   1.090386\n",
      "train loss:   1.205022\n",
      "train loss:   1.247688\n",
      "train loss:   1.273988\n",
      "train loss:   1.155530\n",
      "train loss:   1.028502\n",
      "train loss:   0.912983\n",
      "train loss:   0.878106\n",
      "train loss:   1.255002\n",
      "train loss:   0.881252\n",
      "train loss:   1.115799\n",
      "train loss:   1.208190\n",
      "train loss:   0.677527\n",
      "train loss:   0.952402\n",
      "train loss:   0.987844\n",
      "train loss:   0.799335\n",
      "train loss:   1.063962\n",
      "train loss:   0.785121\n",
      "train loss:   0.967270\n",
      "train loss:   0.823578\n",
      "train loss:   0.876382\n",
      "train loss:   1.035385\n",
      "train loss:   0.858274\n",
      "train loss:   1.280660\n",
      "train loss:   1.211685\n",
      "train loss:   0.895132\n",
      "train loss:   1.263402\n",
      "train loss:   1.197547\n",
      "train loss:   0.917504\n",
      "train loss:   0.878575\n",
      "train loss:   0.872831\n",
      "train loss:   0.905918\n",
      "train loss:   0.924148\n",
      "train loss:   0.984699\n",
      "train loss:   0.946285\n",
      "train loss:   1.096285\n",
      "train loss:   0.753037\n",
      "train loss:   1.181140\n",
      "train loss:   1.073251\n",
      "train loss:   1.065898\n",
      "train loss:   0.979564\n",
      "train loss:   0.906225\n",
      "train loss:   1.078976\n",
      "train loss:   1.375990\n",
      "train loss:   1.087466\n",
      "########### epoch 57 ###########\n",
      "########### loop 10550 ###########\n",
      "test loss:   0.373989   test accuracy:   0.958333\n",
      "########### loop 10550 ###########\n",
      "train loss:   0.684545\n",
      "train loss:   1.023287\n",
      "train loss:   1.042159\n",
      "train loss:   0.881574\n",
      "train loss:   1.302231\n",
      "train loss:   1.138396\n",
      "train loss:   0.810288\n",
      "train loss:   0.974831\n",
      "train loss:   1.112411\n",
      "train loss:   0.916441\n",
      "train loss:   0.988276\n",
      "train loss:   1.001383\n",
      "train loss:   1.044356\n",
      "train loss:   1.062774\n",
      "train loss:   1.048738\n",
      "train loss:   0.976792\n",
      "train loss:   1.126607\n",
      "train loss:   0.750103\n",
      "train loss:   0.764813\n",
      "train loss:   0.953443\n",
      "train loss:   0.880185\n",
      "train loss:   1.214534\n",
      "train loss:   0.947450\n",
      "train loss:   1.065141\n",
      "train loss:   1.105312\n",
      "train loss:   1.081471\n",
      "train loss:   1.335484\n",
      "train loss:   1.022373\n",
      "train loss:   0.822014\n",
      "train loss:   0.823639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.108224\n",
      "train loss:   0.878774\n",
      "train loss:   1.399778\n",
      "train loss:   1.205428\n",
      "train loss:   0.922568\n",
      "train loss:   1.394557\n",
      "train loss:   1.184285\n",
      "train loss:   1.223515\n",
      "train loss:   0.797994\n",
      "train loss:   0.817980\n",
      "train loss:   1.236406\n",
      "train loss:   0.862530\n",
      "train loss:   1.055082\n",
      "train loss:   1.167295\n",
      "train loss:   0.877814\n",
      "train loss:   1.079569\n",
      "train loss:   0.772383\n",
      "train loss:   0.913149\n",
      "train loss:   0.901062\n",
      "train loss:   1.040156\n",
      "########### epoch 57 ###########\n",
      "########### loop 10600 ###########\n",
      "test loss:   0.203016   test accuracy:   1.000000\n",
      "########### loop 10600 ###########\n",
      "train loss:   1.264800\n",
      "train loss:   1.208526\n",
      "train loss:   0.857233\n",
      "train loss:   1.325834\n",
      "train loss:   0.946628\n",
      "train loss:   0.956679\n",
      "train loss:   1.202485\n",
      "train loss:   0.769541\n",
      "train loss:   1.023455\n",
      "train loss:   1.099278\n",
      "train loss:   0.663078\n",
      "train loss:   0.919697\n",
      "train loss:   1.143948\n",
      "train loss:   1.061115\n",
      "train loss:   1.024461\n",
      "train loss:   1.115030\n",
      "train loss:   0.890930\n",
      "train loss:   1.141347\n",
      "train loss:   1.424323\n",
      "train loss:   1.281360\n",
      "train loss:   0.767423\n",
      "train loss:   0.936411\n",
      "train loss:   0.748725\n",
      "train loss:   1.046166\n",
      "train loss:   1.200641\n",
      "train loss:   0.882052\n",
      "train loss:   0.783230\n",
      "train loss:   1.062171\n",
      "train loss:   0.928472\n",
      "train loss:   1.251804\n",
      "train loss:   0.942073\n",
      "train loss:   1.231641\n",
      "train loss:   1.089483\n",
      "train loss:   0.906537\n",
      "train loss:   0.977003\n",
      "train loss:   0.944388\n",
      "train loss:   1.039016\n",
      "train loss:   0.962857\n",
      "train loss:   0.731923\n",
      "train loss:   0.944565\n",
      "train loss:   1.290353\n",
      "train loss:   0.716091\n",
      "train loss:   0.884189\n",
      "train loss:   0.963057\n",
      "train loss:   1.201310\n",
      "train loss:   0.815962\n",
      "train loss:   0.965540\n",
      "train loss:   1.213545\n",
      "train loss:   0.800549\n",
      "train loss:   1.167462\n",
      "########### epoch 57 ###########\n",
      "########### loop 10650 ###########\n",
      "test loss:   0.183390   test accuracy:   0.958333\n",
      "########### loop 10650 ###########\n",
      "train loss:   0.572132\n",
      "train loss:   1.246540\n",
      "train loss:   1.008964\n",
      "train loss:   1.388473\n",
      "train loss:   1.007367\n",
      "train loss:   0.968582\n",
      "train loss:   0.776440\n",
      "train loss:   0.918998\n",
      "train loss:   0.937931\n",
      "train loss:   1.335069\n",
      "train loss:   1.211160\n",
      "train loss:   0.830929\n",
      "train loss:   0.668443\n",
      "train loss:   0.903520\n",
      "train loss:   0.791455\n",
      "train loss:   0.833688\n",
      "train loss:   0.685729\n",
      "train loss:   1.257774\n",
      "train loss:   0.918241\n",
      "train loss:   1.197114\n",
      "train loss:   1.119699\n",
      "train loss:   1.292825\n",
      "train loss:   1.072489\n",
      "train loss:   1.284485\n",
      "train loss:   0.818200\n",
      "train loss:   1.201010\n",
      "train loss:   0.924759\n",
      "train loss:   0.934059\n",
      "train loss:   0.943483\n",
      "train loss:   1.055396\n",
      "train loss:   0.815927\n",
      "train loss:   0.972235\n",
      "train loss:   1.158668\n",
      "train loss:   1.044418\n",
      "train loss:   1.270566\n",
      "train loss:   0.813124\n",
      "train loss:   1.020979\n",
      "train loss:   0.981665\n",
      "train loss:   1.017687\n",
      "train loss:   1.114528\n",
      "train loss:   0.846648\n",
      "train loss:   1.254964\n",
      "train loss:   0.804264\n",
      "train loss:   1.215852\n",
      "train loss:   0.862084\n",
      "train loss:   0.693652\n",
      "train loss:   1.204677\n",
      "train loss:   1.369778\n",
      "train loss:   1.155969\n",
      "train loss:   0.904664\n",
      "########### epoch 57 ###########\n",
      "########### loop 10700 ###########\n",
      "test loss:   0.353942   test accuracy:   0.958333\n",
      "########### loop 10700 ###########\n",
      "train loss:   1.068326\n",
      "train loss:   0.845876\n",
      "train loss:   0.672407\n",
      "train loss:   0.875996\n",
      "train loss:   0.984405\n",
      "train loss:   1.110585\n",
      "train loss:   1.388254\n",
      "train loss:   0.864473\n",
      "train loss:   0.926933\n",
      "train loss:   1.319578\n",
      "train loss:   1.361801\n",
      "train loss:   1.145680\n",
      "train loss:   0.678301\n",
      "train loss:   0.977367\n",
      "train loss:   1.151719\n",
      "train loss:   0.818488\n",
      "train loss:   0.870819\n",
      "train loss:   1.054754\n",
      "train loss:   1.344898\n",
      "train loss:   1.166986\n",
      "train loss:   1.205261\n",
      "train loss:   0.887785\n",
      "train loss:   0.915035\n",
      "train loss:   1.029273\n",
      "train loss:   0.791960\n",
      "train loss:   1.554975\n",
      "train loss:   0.968250\n",
      "train loss:   1.091346\n",
      "train loss:   1.320785\n",
      "train loss:   1.083465\n",
      "train loss:   1.001927\n",
      "train loss:   1.045125\n",
      "train loss:   1.227693\n",
      "train loss:   0.801632\n",
      "train loss:   1.019925\n",
      "train loss:   1.106139\n",
      "train loss:   0.787237\n",
      "train loss:   0.942718\n",
      "train loss:   1.223156\n",
      "train loss:   1.053407\n",
      "train loss:   1.031485\n",
      "train loss:   0.857435\n",
      "train loss:   0.772970\n",
      "train loss:   1.662096\n",
      "train loss:   0.989799\n",
      "train loss:   0.616270\n",
      "train loss:   1.049419\n",
      "train loss:   0.751075\n",
      "train loss:   1.228278\n",
      "train loss:   0.925381\n",
      "########### epoch 58 ###########\n",
      "########### loop 10750 ###########\n",
      "test loss:   0.188026   test accuracy:   1.000000\n",
      "########### loop 10750 ###########\n",
      "train loss:   1.016793\n",
      "train loss:   1.469053\n",
      "train loss:   1.053682\n",
      "train loss:   0.980985\n",
      "train loss:   0.977355\n",
      "train loss:   0.834108\n",
      "train loss:   1.002372\n",
      "train loss:   0.767644\n",
      "train loss:   0.957141\n",
      "train loss:   1.302750\n",
      "train loss:   1.147646\n",
      "train loss:   0.907666\n",
      "train loss:   0.821412\n",
      "train loss:   1.227197\n",
      "train loss:   0.530157\n",
      "train loss:   0.763234\n",
      "train loss:   1.107605\n",
      "train loss:   1.054863\n",
      "train loss:   0.868912\n",
      "train loss:   0.554344\n",
      "train loss:   0.711745\n",
      "train loss:   1.055670\n",
      "train loss:   1.223065\n",
      "train loss:   1.098309\n",
      "train loss:   1.088449\n",
      "train loss:   0.741667\n",
      "train loss:   1.100041\n",
      "train loss:   0.802734\n",
      "train loss:   0.887272\n",
      "train loss:   0.888121\n",
      "train loss:   1.171427\n",
      "train loss:   0.819325\n",
      "train loss:   0.917227\n",
      "train loss:   0.773653\n",
      "train loss:   0.841396\n",
      "train loss:   0.978030\n",
      "train loss:   0.805249\n",
      "train loss:   0.982186\n",
      "train loss:   0.592980\n",
      "train loss:   1.081128\n",
      "train loss:   1.161240\n",
      "train loss:   1.373698\n",
      "train loss:   0.700075\n",
      "train loss:   1.110726\n",
      "train loss:   0.718931\n",
      "train loss:   0.892818\n",
      "train loss:   1.034325\n",
      "train loss:   0.834949\n",
      "train loss:   0.671033\n",
      "train loss:   0.920983\n",
      "########### epoch 58 ###########\n",
      "########### loop 10800 ###########\n",
      "test loss:   0.409336   test accuracy:   0.916667\n",
      "########### loop 10800 ###########\n",
      "train loss:   0.913040\n",
      "train loss:   1.296869\n",
      "train loss:   0.751571\n",
      "train loss:   0.688796\n",
      "train loss:   0.874439\n",
      "train loss:   0.985772\n",
      "train loss:   1.141599\n",
      "train loss:   0.954643\n",
      "train loss:   1.022820\n",
      "train loss:   1.288068\n",
      "train loss:   1.422511\n",
      "train loss:   1.076074\n",
      "train loss:   0.992905\n",
      "train loss:   0.823315\n",
      "train loss:   1.262982\n",
      "train loss:   1.319224\n",
      "train loss:   0.726314\n",
      "train loss:   0.927423\n",
      "train loss:   0.856280\n",
      "train loss:   0.868067\n",
      "train loss:   0.766796\n",
      "train loss:   1.350620\n",
      "train loss:   0.674555\n",
      "train loss:   1.216341\n",
      "train loss:   1.074295\n",
      "train loss:   0.994715\n",
      "train loss:   0.964884\n",
      "train loss:   1.084305\n",
      "train loss:   0.849035\n",
      "train loss:   0.798373\n",
      "train loss:   1.553099\n",
      "train loss:   1.085721\n",
      "train loss:   1.057162\n",
      "train loss:   0.917206\n",
      "train loss:   1.199313\n",
      "train loss:   0.810891\n",
      "train loss:   0.759359\n",
      "train loss:   1.126008\n",
      "train loss:   0.874105\n",
      "train loss:   0.740403\n",
      "train loss:   1.060323\n",
      "train loss:   1.197732\n",
      "train loss:   0.764191\n",
      "train loss:   0.715437\n",
      "train loss:   0.991657\n",
      "train loss:   0.380877\n",
      "train loss:   0.944264\n",
      "train loss:   1.109457\n",
      "train loss:   1.055966\n",
      "train loss:   1.024367\n",
      "########### epoch 58 ###########\n",
      "########### loop 10850 ###########\n",
      "test loss:   0.286495   test accuracy:   0.916667\n",
      "########### loop 10850 ###########\n",
      "train loss:   1.050141\n",
      "train loss:   1.373194\n",
      "train loss:   0.942683\n",
      "train loss:   0.991555\n",
      "train loss:   1.418300\n",
      "train loss:   1.156600\n",
      "train loss:   1.017560\n",
      "train loss:   1.088341\n",
      "train loss:   1.251878\n",
      "train loss:   0.713721\n",
      "train loss:   0.695263\n",
      "train loss:   1.043612\n",
      "train loss:   1.311273\n",
      "train loss:   0.924187\n",
      "train loss:   0.935334\n",
      "train loss:   1.066752\n",
      "train loss:   0.595302\n",
      "train loss:   0.937122\n",
      "train loss:   0.886771\n",
      "train loss:   1.069944\n",
      "train loss:   0.983667\n",
      "train loss:   0.992857\n",
      "train loss:   0.976448\n",
      "train loss:   0.847834\n",
      "train loss:   0.911867\n",
      "train loss:   1.080317\n",
      "train loss:   0.903157\n",
      "train loss:   0.841235\n",
      "train loss:   0.901997\n",
      "train loss:   1.053036\n",
      "train loss:   0.898306\n",
      "train loss:   1.043627\n",
      "train loss:   0.936641\n",
      "train loss:   1.201334\n",
      "train loss:   1.076647\n",
      "train loss:   1.331516\n",
      "train loss:   0.736140\n",
      "train loss:   0.985380\n",
      "train loss:   0.764023\n",
      "train loss:   0.784436\n",
      "train loss:   0.790891\n",
      "train loss:   1.212918\n",
      "train loss:   0.837697\n",
      "train loss:   0.989125\n",
      "train loss:   1.149529\n",
      "train loss:   1.273352\n",
      "train loss:   0.974358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.942694\n",
      "train loss:   1.128874\n",
      "train loss:   1.316306\n",
      "########### epoch 58 ###########\n",
      "########### loop 10900 ###########\n",
      "test loss:   0.546844   test accuracy:   0.875000\n",
      "########### loop 10900 ###########\n",
      "train loss:   1.102846\n",
      "train loss:   0.833476\n",
      "train loss:   0.954781\n",
      "train loss:   1.275881\n",
      "train loss:   1.111616\n",
      "train loss:   1.238807\n",
      "train loss:   0.719228\n",
      "train loss:   1.055695\n",
      "train loss:   1.110770\n",
      "train loss:   1.209442\n",
      "train loss:   1.023664\n",
      "train loss:   1.045740\n",
      "train loss:   0.857723\n",
      "train loss:   1.412686\n",
      "train loss:   1.149137\n",
      "train loss:   0.942132\n",
      "train loss:   1.245456\n",
      "train loss:   0.896865\n",
      "train loss:   1.205415\n",
      "train loss:   1.183359\n",
      "train loss:   1.041804\n",
      "train loss:   1.097777\n",
      "train loss:   0.519475\n",
      "train loss:   1.119733\n",
      "train loss:   0.919272\n",
      "train loss:   0.878523\n",
      "train loss:   1.078001\n",
      "train loss:   0.644791\n",
      "train loss:   0.746214\n",
      "train loss:   0.870101\n",
      "train loss:   0.762147\n",
      "train loss:   0.944450\n",
      "train loss:   1.017117\n",
      "train loss:   1.390059\n",
      "train loss:   0.905948\n",
      "train loss:   0.818866\n",
      "train loss:   1.020088\n",
      "train loss:   1.169253\n",
      "train loss:   1.203466\n",
      "train loss:   1.188190\n",
      "train loss:   1.211548\n",
      "train loss:   0.885100\n",
      "train loss:   0.891977\n",
      "train loss:   0.783916\n",
      "train loss:   1.110377\n",
      "train loss:   1.064592\n",
      "train loss:   1.287151\n",
      "train loss:   0.974542\n",
      "train loss:   0.791097\n",
      "train loss:   0.916255\n",
      "########### epoch 59 ###########\n",
      "########### loop 10950 ###########\n",
      "test loss:   0.267976   test accuracy:   0.958333\n",
      "########### loop 10950 ###########\n",
      "train loss:   0.691673\n",
      "train loss:   1.435406\n",
      "train loss:   1.152995\n",
      "train loss:   1.078678\n",
      "train loss:   1.082288\n",
      "train loss:   1.173227\n",
      "train loss:   0.596520\n",
      "train loss:   1.403337\n",
      "train loss:   0.774142\n",
      "train loss:   0.949763\n",
      "train loss:   0.508335\n",
      "train loss:   1.067286\n",
      "train loss:   0.911084\n",
      "train loss:   0.906651\n",
      "train loss:   1.382899\n",
      "train loss:   0.954497\n",
      "train loss:   1.440544\n",
      "train loss:   0.900599\n",
      "train loss:   0.413778\n",
      "train loss:   0.629969\n",
      "train loss:   1.117890\n",
      "train loss:   1.203971\n",
      "train loss:   0.975954\n",
      "train loss:   1.022861\n",
      "train loss:   0.962148\n",
      "train loss:   0.921031\n",
      "train loss:   0.977040\n",
      "train loss:   0.692893\n",
      "train loss:   1.103812\n",
      "train loss:   0.906803\n",
      "train loss:   1.034516\n",
      "train loss:   1.455270\n",
      "train loss:   0.859412\n",
      "train loss:   1.097659\n",
      "train loss:   1.160248\n",
      "train loss:   1.019137\n",
      "train loss:   1.190231\n",
      "train loss:   0.829834\n",
      "train loss:   0.970764\n",
      "train loss:   1.126034\n",
      "train loss:   0.692607\n",
      "train loss:   1.213771\n",
      "train loss:   1.075935\n",
      "train loss:   1.216512\n",
      "train loss:   0.784021\n",
      "train loss:   1.164438\n",
      "train loss:   1.083978\n",
      "train loss:   1.143518\n",
      "train loss:   0.545793\n",
      "train loss:   1.405597\n",
      "########### epoch 59 ###########\n",
      "########### loop 11000 ###########\n",
      "test loss:   0.342604   test accuracy:   0.958333\n",
      "########### loop 11000 ###########\n",
      "train loss:   0.921971\n",
      "train loss:   0.890443\n",
      "train loss:   1.157047\n",
      "train loss:   0.855887\n",
      "train loss:   1.048515\n",
      "train loss:   0.696300\n",
      "train loss:   0.821032\n",
      "train loss:   1.363424\n",
      "train loss:   0.859407\n",
      "train loss:   0.567283\n",
      "train loss:   0.983196\n",
      "train loss:   0.833177\n",
      "train loss:   0.926006\n",
      "train loss:   1.148945\n",
      "train loss:   0.930743\n",
      "train loss:   0.880329\n",
      "train loss:   1.016438\n",
      "train loss:   1.279467\n",
      "train loss:   1.261885\n",
      "train loss:   0.917378\n",
      "train loss:   0.710222\n",
      "train loss:   1.294001\n",
      "train loss:   1.128372\n",
      "train loss:   1.026547\n",
      "train loss:   1.428355\n",
      "train loss:   0.956702\n",
      "train loss:   1.095636\n",
      "train loss:   0.958376\n",
      "train loss:   0.969440\n",
      "train loss:   1.249238\n",
      "train loss:   0.851493\n",
      "train loss:   1.178741\n",
      "train loss:   0.712219\n",
      "train loss:   1.128249\n",
      "train loss:   0.923621\n",
      "train loss:   0.934981\n",
      "train loss:   1.149208\n",
      "train loss:   0.962273\n",
      "train loss:   0.886697\n",
      "train loss:   1.090343\n",
      "train loss:   0.874708\n",
      "train loss:   1.084077\n",
      "train loss:   0.796918\n",
      "train loss:   1.052442\n",
      "train loss:   1.211124\n",
      "train loss:   1.031720\n",
      "train loss:   1.395502\n",
      "train loss:   0.818849\n",
      "train loss:   0.960522\n",
      "train loss:   1.113406\n",
      "########### epoch 59 ###########\n",
      "########### loop 11050 ###########\n",
      "test loss:   0.361604   test accuracy:   0.916667\n",
      "########### loop 11050 ###########\n",
      "train loss:   1.110227\n",
      "train loss:   0.944407\n",
      "train loss:   1.044289\n",
      "train loss:   1.322008\n",
      "train loss:   1.232527\n",
      "train loss:   0.962206\n",
      "train loss:   1.115854\n",
      "train loss:   0.594563\n",
      "train loss:   0.852334\n",
      "train loss:   0.667991\n",
      "train loss:   1.168997\n",
      "train loss:   0.766277\n",
      "train loss:   0.956993\n",
      "train loss:   1.067249\n",
      "train loss:   1.351135\n",
      "train loss:   0.524594\n",
      "train loss:   1.300151\n",
      "train loss:   1.013777\n",
      "train loss:   0.782137\n",
      "train loss:   0.887879\n",
      "train loss:   1.108294\n",
      "train loss:   0.762388\n",
      "train loss:   0.864835\n",
      "train loss:   0.705399\n",
      "train loss:   1.141049\n",
      "train loss:   1.132630\n",
      "train loss:   1.195739\n",
      "train loss:   1.418264\n",
      "train loss:   0.907313\n",
      "train loss:   0.854944\n",
      "train loss:   0.847531\n",
      "train loss:   0.982489\n",
      "train loss:   1.302341\n",
      "train loss:   1.052032\n",
      "train loss:   0.994172\n",
      "train loss:   0.963332\n",
      "train loss:   0.877148\n",
      "train loss:   1.159938\n",
      "train loss:   0.611840\n",
      "train loss:   0.752165\n",
      "train loss:   1.105354\n",
      "train loss:   1.102781\n",
      "train loss:   1.210239\n",
      "train loss:   0.964539\n",
      "train loss:   0.733610\n",
      "train loss:   1.029211\n",
      "train loss:   1.225834\n",
      "train loss:   0.993565\n",
      "train loss:   0.649117\n",
      "train loss:   0.902827\n",
      "########### epoch 60 ###########\n",
      "########### loop 11100 ###########\n",
      "test loss:   0.301995   test accuracy:   0.958333\n",
      "########### loop 11100 ###########\n",
      "train loss:   0.915879\n",
      "train loss:   0.755541\n",
      "train loss:   1.018830\n",
      "train loss:   1.062194\n",
      "train loss:   1.144324\n",
      "train loss:   0.888342\n",
      "train loss:   1.088889\n",
      "train loss:   1.367014\n",
      "train loss:   0.951206\n",
      "train loss:   1.345086\n",
      "train loss:   1.057425\n",
      "train loss:   1.100075\n",
      "train loss:   1.080895\n",
      "train loss:   0.908363\n",
      "train loss:   0.948852\n",
      "train loss:   1.019246\n",
      "train loss:   0.608895\n",
      "train loss:   1.072147\n",
      "train loss:   0.939767\n",
      "train loss:   1.144385\n",
      "train loss:   1.004969\n",
      "train loss:   1.061197\n",
      "train loss:   1.344459\n",
      "train loss:   1.117001\n",
      "train loss:   1.044290\n",
      "train loss:   1.043896\n",
      "train loss:   0.730418\n",
      "train loss:   1.013906\n",
      "train loss:   1.018915\n",
      "train loss:   1.082915\n",
      "train loss:   0.798450\n",
      "train loss:   1.092393\n",
      "train loss:   1.347013\n",
      "train loss:   0.677395\n",
      "train loss:   0.835504\n",
      "train loss:   0.916306\n",
      "train loss:   0.572513\n",
      "train loss:   1.354441\n",
      "train loss:   1.057229\n",
      "train loss:   1.101632\n",
      "train loss:   1.247532\n",
      "train loss:   0.816355\n",
      "train loss:   0.851206\n",
      "train loss:   1.011379\n",
      "train loss:   1.345673\n",
      "train loss:   1.100662\n",
      "train loss:   1.064883\n",
      "train loss:   0.899292\n",
      "train loss:   0.983426\n",
      "train loss:   0.962301\n",
      "########### epoch 60 ###########\n",
      "########### loop 11150 ###########\n",
      "test loss:   0.269675   test accuracy:   0.958333\n",
      "########### loop 11150 ###########\n",
      "train loss:   1.272070\n",
      "train loss:   1.171929\n",
      "train loss:   0.943929\n",
      "train loss:   0.668759\n",
      "train loss:   0.714856\n",
      "train loss:   1.033114\n",
      "train loss:   1.335122\n",
      "train loss:   1.283933\n",
      "train loss:   0.979754\n",
      "train loss:   1.186431\n",
      "train loss:   0.968918\n",
      "train loss:   1.114779\n",
      "train loss:   1.115730\n",
      "train loss:   0.679307\n",
      "train loss:   1.285615\n",
      "train loss:   0.770030\n",
      "train loss:   1.417178\n",
      "train loss:   0.646040\n",
      "train loss:   0.595597\n",
      "train loss:   0.956465\n",
      "train loss:   0.619854\n",
      "train loss:   0.824410\n",
      "train loss:   1.116384\n",
      "train loss:   0.925907\n",
      "train loss:   0.865529\n",
      "train loss:   0.860559\n",
      "train loss:   1.212482\n",
      "train loss:   1.005743\n",
      "train loss:   1.123687\n",
      "train loss:   0.741577\n",
      "train loss:   1.245204\n",
      "train loss:   0.676395\n",
      "train loss:   0.922468\n",
      "train loss:   1.068988\n",
      "train loss:   0.851211\n",
      "train loss:   1.063991\n",
      "train loss:   0.832388\n",
      "train loss:   1.192061\n",
      "train loss:   0.790316\n",
      "train loss:   0.995291\n",
      "train loss:   0.844357\n",
      "train loss:   1.052197\n",
      "train loss:   0.706376\n",
      "train loss:   1.089807\n",
      "train loss:   0.721877\n",
      "train loss:   0.825595\n",
      "train loss:   0.966483\n",
      "train loss:   0.750647\n",
      "train loss:   0.834671\n",
      "train loss:   0.846432\n",
      "########### epoch 60 ###########\n",
      "########### loop 11200 ###########\n",
      "test loss:   0.228583   test accuracy:   0.958333\n",
      "########### loop 11200 ###########\n",
      "train loss:   0.768504\n",
      "train loss:   1.099872\n",
      "train loss:   0.831449\n",
      "train loss:   0.827422\n",
      "train loss:   0.866012\n",
      "train loss:   1.006007\n",
      "train loss:   1.445034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.252815\n",
      "train loss:   0.965914\n",
      "train loss:   1.181557\n",
      "train loss:   1.155376\n",
      "train loss:   0.907048\n",
      "train loss:   0.953433\n",
      "train loss:   1.388510\n",
      "train loss:   1.206728\n",
      "train loss:   1.116181\n",
      "train loss:   0.570657\n",
      "train loss:   0.849888\n",
      "train loss:   1.054970\n",
      "train loss:   1.270175\n",
      "train loss:   1.015205\n",
      "train loss:   1.209685\n",
      "train loss:   1.149094\n",
      "train loss:   1.153537\n",
      "train loss:   0.924876\n",
      "train loss:   1.060345\n",
      "train loss:   1.140953\n",
      "train loss:   1.409161\n",
      "train loss:   1.113546\n",
      "train loss:   0.795257\n",
      "train loss:   1.147183\n",
      "train loss:   1.035528\n",
      "train loss:   0.821241\n",
      "train loss:   0.941236\n",
      "train loss:   0.708787\n",
      "train loss:   1.019920\n",
      "train loss:   0.921255\n",
      "train loss:   1.243473\n",
      "train loss:   1.023895\n",
      "train loss:   0.911024\n",
      "train loss:   1.163882\n",
      "train loss:   0.844859\n",
      "train loss:   1.175994\n",
      "train loss:   0.765138\n",
      "train loss:   1.222834\n",
      "train loss:   1.070416\n",
      "train loss:   1.006339\n",
      "train loss:   0.660792\n",
      "train loss:   0.916648\n",
      "train loss:   1.063941\n",
      "########### epoch 60 ###########\n",
      "########### loop 11250 ###########\n",
      "test loss:   0.317772   test accuracy:   0.958333\n",
      "########### loop 11250 ###########\n",
      "train loss:   0.849409\n",
      "train loss:   0.890105\n",
      "train loss:   1.043598\n",
      "train loss:   0.755731\n",
      "train loss:   0.795120\n",
      "train loss:   1.093818\n",
      "train loss:   1.211082\n",
      "train loss:   0.884665\n",
      "train loss:   1.014412\n",
      "train loss:   0.942267\n",
      "train loss:   1.331571\n",
      "train loss:   1.054507\n",
      "train loss:   1.003010\n",
      "train loss:   1.052674\n",
      "train loss:   0.940283\n",
      "train loss:   1.057984\n",
      "train loss:   1.097187\n",
      "train loss:   0.856418\n",
      "train loss:   0.868793\n",
      "train loss:   0.966165\n",
      "train loss:   0.858420\n",
      "train loss:   0.743550\n",
      "train loss:   1.243417\n",
      "train loss:   0.836378\n",
      "train loss:   1.148404\n",
      "train loss:   1.124680\n",
      "train loss:   1.117078\n",
      "train loss:   0.856881\n",
      "train loss:   1.223538\n",
      "train loss:   0.798659\n",
      "train loss:   0.930836\n",
      "train loss:   1.004761\n",
      "train loss:   0.599871\n",
      "train loss:   0.991665\n",
      "train loss:   1.223410\n",
      "train loss:   0.791612\n",
      "train loss:   0.997807\n",
      "train loss:   1.511630\n",
      "train loss:   1.130265\n",
      "train loss:   0.789623\n",
      "train loss:   0.987230\n",
      "train loss:   0.850606\n",
      "train loss:   0.836026\n",
      "train loss:   1.303147\n",
      "train loss:   1.231138\n",
      "train loss:   0.845862\n",
      "train loss:   0.955316\n",
      "train loss:   0.885316\n",
      "train loss:   1.019892\n",
      "train loss:   0.855586\n",
      "########### epoch 61 ###########\n",
      "########### loop 11300 ###########\n",
      "test loss:   0.190196   test accuracy:   0.958333\n",
      "########### loop 11300 ###########\n",
      "train loss:   1.104087\n",
      "train loss:   1.293998\n",
      "train loss:   1.075896\n",
      "train loss:   1.136547\n",
      "train loss:   1.199502\n",
      "train loss:   1.133308\n",
      "train loss:   1.201270\n",
      "train loss:   0.812725\n",
      "train loss:   1.234894\n",
      "train loss:   0.914701\n",
      "train loss:   1.248474\n",
      "train loss:   0.833475\n",
      "train loss:   1.306947\n",
      "train loss:   1.120655\n",
      "train loss:   0.703329\n",
      "train loss:   0.760575\n",
      "train loss:   1.282818\n",
      "train loss:   1.207081\n",
      "train loss:   1.115885\n",
      "train loss:   0.999469\n",
      "train loss:   0.738192\n",
      "train loss:   0.860843\n",
      "train loss:   0.736693\n",
      "train loss:   1.126290\n",
      "train loss:   1.290743\n",
      "train loss:   1.166612\n",
      "train loss:   0.817132\n",
      "train loss:   1.090084\n",
      "train loss:   1.087274\n",
      "train loss:   1.016798\n",
      "train loss:   1.195932\n",
      "train loss:   1.017011\n",
      "train loss:   0.731905\n",
      "train loss:   0.675335\n",
      "train loss:   0.957468\n",
      "train loss:   1.173175\n",
      "train loss:   1.336968\n",
      "train loss:   1.180429\n",
      "train loss:   1.325279\n",
      "train loss:   1.246092\n",
      "train loss:   1.035074\n",
      "train loss:   1.035525\n",
      "train loss:   1.092813\n",
      "train loss:   1.133250\n",
      "train loss:   0.914451\n",
      "train loss:   0.805861\n",
      "train loss:   0.980632\n",
      "train loss:   0.964416\n",
      "train loss:   0.744759\n",
      "train loss:   0.895854\n",
      "########### epoch 61 ###########\n",
      "########### loop 11350 ###########\n",
      "test loss:   0.352444   test accuracy:   0.916667\n",
      "########### loop 11350 ###########\n",
      "train loss:   0.982653\n",
      "train loss:   1.092600\n",
      "train loss:   1.401959\n",
      "train loss:   0.916927\n",
      "train loss:   0.955181\n",
      "train loss:   1.176208\n",
      "train loss:   0.565812\n",
      "train loss:   1.012614\n",
      "train loss:   0.806719\n",
      "train loss:   0.958248\n",
      "train loss:   1.165244\n",
      "train loss:   0.867918\n",
      "train loss:   0.811358\n",
      "train loss:   1.056037\n",
      "train loss:   0.698743\n",
      "train loss:   1.200371\n",
      "train loss:   0.976654\n",
      "train loss:   0.825016\n",
      "train loss:   0.966945\n",
      "train loss:   0.865432\n",
      "train loss:   0.935375\n",
      "train loss:   1.215737\n",
      "train loss:   1.032253\n",
      "train loss:   1.135099\n",
      "train loss:   0.884471\n",
      "train loss:   0.875407\n",
      "train loss:   1.049502\n",
      "train loss:   0.897470\n",
      "train loss:   0.874824\n",
      "train loss:   1.231189\n",
      "train loss:   0.838456\n",
      "train loss:   0.955119\n",
      "train loss:   0.850892\n",
      "train loss:   1.159307\n",
      "train loss:   0.763629\n",
      "train loss:   0.769356\n",
      "train loss:   0.985804\n",
      "train loss:   1.065703\n",
      "train loss:   0.991334\n",
      "train loss:   0.968717\n",
      "train loss:   1.344382\n",
      "train loss:   1.073656\n",
      "train loss:   0.719896\n",
      "train loss:   1.581145\n",
      "train loss:   0.894818\n",
      "train loss:   0.556646\n",
      "train loss:   0.912095\n",
      "train loss:   0.933788\n",
      "train loss:   0.875569\n",
      "train loss:   0.908977\n",
      "########### epoch 61 ###########\n",
      "########### loop 11400 ###########\n",
      "test loss:   0.171414   test accuracy:   1.000000\n",
      "########### loop 11400 ###########\n",
      "train loss:   0.666885\n",
      "train loss:   1.079999\n",
      "train loss:   1.069940\n",
      "train loss:   1.283765\n",
      "train loss:   1.332490\n",
      "train loss:   0.929088\n",
      "train loss:   0.990985\n",
      "train loss:   1.070548\n",
      "train loss:   0.997324\n",
      "train loss:   0.663423\n",
      "train loss:   0.809024\n",
      "train loss:   0.850927\n",
      "train loss:   1.020643\n",
      "train loss:   0.814046\n",
      "train loss:   0.923360\n",
      "train loss:   1.091801\n",
      "train loss:   0.940951\n",
      "train loss:   0.813398\n",
      "train loss:   1.008061\n",
      "train loss:   1.065388\n",
      "train loss:   0.783917\n",
      "train loss:   0.543686\n",
      "train loss:   0.886625\n",
      "train loss:   0.630975\n",
      "train loss:   1.134714\n",
      "train loss:   0.744534\n",
      "train loss:   1.080523\n",
      "train loss:   0.876010\n",
      "train loss:   0.716144\n",
      "train loss:   1.021989\n",
      "train loss:   0.829508\n",
      "train loss:   1.158847\n",
      "train loss:   1.047853\n",
      "train loss:   0.937935\n",
      "train loss:   0.932374\n",
      "train loss:   0.724272\n",
      "train loss:   1.054223\n",
      "train loss:   0.786634\n",
      "train loss:   1.152228\n",
      "train loss:   1.067951\n",
      "train loss:   1.257165\n",
      "train loss:   0.948407\n",
      "train loss:   0.741493\n",
      "train loss:   1.178964\n",
      "train loss:   1.146929\n",
      "train loss:   0.890207\n",
      "train loss:   0.793517\n",
      "train loss:   1.179797\n",
      "train loss:   1.161520\n",
      "train loss:   0.950512\n",
      "########### epoch 61 ###########\n",
      "########### loop 11450 ###########\n",
      "test loss:   0.295994   test accuracy:   0.958333\n",
      "########### loop 11450 ###########\n",
      "train loss:   1.008773\n",
      "train loss:   0.903953\n",
      "train loss:   1.030307\n",
      "train loss:   0.885366\n",
      "train loss:   0.766846\n",
      "train loss:   1.123214\n",
      "train loss:   0.715757\n",
      "train loss:   1.034039\n",
      "train loss:   0.550145\n",
      "train loss:   1.329662\n",
      "train loss:   0.593644\n",
      "train loss:   1.058716\n",
      "train loss:   1.196872\n",
      "train loss:   1.578962\n",
      "train loss:   0.674469\n",
      "train loss:   0.733282\n",
      "train loss:   0.800490\n",
      "train loss:   1.173758\n",
      "train loss:   0.785248\n",
      "train loss:   0.884607\n",
      "train loss:   0.712696\n",
      "train loss:   0.944697\n",
      "train loss:   0.869087\n",
      "train loss:   1.177413\n",
      "train loss:   1.129462\n",
      "train loss:   1.119415\n",
      "train loss:   0.828369\n",
      "train loss:   0.732341\n",
      "train loss:   1.113791\n",
      "train loss:   1.033632\n",
      "train loss:   0.880384\n",
      "train loss:   0.935717\n",
      "train loss:   0.998812\n",
      "train loss:   0.962569\n",
      "train loss:   1.390842\n",
      "train loss:   1.166924\n",
      "train loss:   0.894599\n",
      "train loss:   0.971704\n",
      "train loss:   1.263120\n",
      "train loss:   1.101169\n",
      "train loss:   1.302059\n",
      "train loss:   0.937566\n",
      "train loss:   1.041822\n",
      "train loss:   1.068055\n",
      "train loss:   1.099829\n",
      "train loss:   0.777695\n",
      "train loss:   0.930480\n",
      "train loss:   0.908719\n",
      "train loss:   1.222642\n",
      "train loss:   1.212860\n",
      "########### epoch 62 ###########\n",
      "########### loop 11500 ###########\n",
      "test loss:   0.331747   test accuracy:   0.875000\n",
      "########### loop 11500 ###########\n",
      "train loss:   1.473637\n",
      "train loss:   1.158839\n",
      "train loss:   0.937234\n",
      "train loss:   0.997219\n",
      "train loss:   0.901554\n",
      "train loss:   1.106992\n",
      "train loss:   1.227072\n",
      "train loss:   0.964714\n",
      "train loss:   1.168715\n",
      "train loss:   1.116907\n",
      "train loss:   1.129488\n",
      "train loss:   1.113048\n",
      "train loss:   0.972326\n",
      "train loss:   1.307294\n",
      "train loss:   1.208757\n",
      "train loss:   0.823149\n",
      "train loss:   1.212580\n",
      "train loss:   1.076973\n",
      "train loss:   0.776240\n",
      "train loss:   1.086045\n",
      "train loss:   0.901071\n",
      "train loss:   1.139405\n",
      "train loss:   1.109184\n",
      "train loss:   0.808183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.936484\n",
      "train loss:   1.048478\n",
      "train loss:   0.884792\n",
      "train loss:   0.825101\n",
      "train loss:   1.135879\n",
      "train loss:   1.056155\n",
      "train loss:   1.101021\n",
      "train loss:   1.322119\n",
      "train loss:   1.111069\n",
      "train loss:   1.029013\n",
      "train loss:   0.708405\n",
      "train loss:   1.355258\n",
      "train loss:   1.028665\n",
      "train loss:   0.972742\n",
      "train loss:   1.054753\n",
      "train loss:   0.952256\n",
      "train loss:   1.356941\n",
      "train loss:   0.764672\n",
      "train loss:   1.021017\n",
      "train loss:   0.879396\n",
      "train loss:   1.009981\n",
      "train loss:   1.106345\n",
      "train loss:   1.040716\n",
      "train loss:   0.868136\n",
      "train loss:   0.755652\n",
      "train loss:   1.113755\n",
      "########### epoch 62 ###########\n",
      "########### loop 11550 ###########\n",
      "test loss:   0.357034   test accuracy:   0.875000\n",
      "########### loop 11550 ###########\n",
      "train loss:   0.909710\n",
      "train loss:   1.076811\n",
      "train loss:   1.041519\n",
      "train loss:   0.895516\n",
      "train loss:   1.176689\n",
      "train loss:   1.199878\n",
      "train loss:   1.020489\n",
      "train loss:   1.030430\n",
      "train loss:   1.004120\n",
      "train loss:   0.987947\n",
      "train loss:   0.669744\n",
      "train loss:   1.034766\n",
      "train loss:   0.816294\n",
      "train loss:   1.141420\n",
      "train loss:   1.000360\n",
      "train loss:   1.053266\n",
      "train loss:   1.012147\n",
      "train loss:   1.080034\n",
      "train loss:   1.094457\n",
      "train loss:   1.138958\n",
      "train loss:   1.169328\n",
      "train loss:   1.044333\n",
      "train loss:   0.935323\n",
      "train loss:   0.794860\n",
      "train loss:   0.779305\n",
      "train loss:   1.154825\n",
      "train loss:   0.642889\n",
      "train loss:   1.208155\n",
      "train loss:   0.807570\n",
      "train loss:   1.028362\n",
      "train loss:   1.174307\n",
      "train loss:   1.017552\n",
      "train loss:   0.622057\n",
      "train loss:   1.039729\n",
      "train loss:   0.979965\n",
      "train loss:   0.879558\n",
      "train loss:   1.001721\n",
      "train loss:   1.029678\n",
      "train loss:   0.904984\n",
      "train loss:   1.063245\n",
      "train loss:   1.151736\n",
      "train loss:   1.332815\n",
      "train loss:   0.611454\n",
      "train loss:   0.718202\n",
      "train loss:   1.245039\n",
      "train loss:   0.880706\n",
      "train loss:   0.951991\n",
      "train loss:   0.989434\n",
      "train loss:   0.915516\n",
      "train loss:   1.246022\n",
      "########### epoch 62 ###########\n",
      "########### loop 11600 ###########\n",
      "test loss:   0.284152   test accuracy:   1.000000\n",
      "########### loop 11600 ###########\n",
      "train loss:   1.133031\n",
      "train loss:   0.905819\n",
      "train loss:   0.936810\n",
      "train loss:   0.961111\n",
      "train loss:   0.510948\n",
      "train loss:   1.140445\n",
      "train loss:   1.187392\n",
      "train loss:   0.561278\n",
      "train loss:   0.981658\n",
      "train loss:   0.969889\n",
      "train loss:   0.767666\n",
      "train loss:   1.086236\n",
      "train loss:   1.179251\n",
      "train loss:   1.334216\n",
      "train loss:   1.265715\n",
      "train loss:   0.984616\n",
      "train loss:   1.023164\n",
      "train loss:   1.081091\n",
      "train loss:   0.962651\n",
      "train loss:   0.983041\n",
      "train loss:   0.959361\n",
      "train loss:   0.867109\n",
      "train loss:   0.831130\n",
      "train loss:   0.925825\n",
      "train loss:   0.833424\n",
      "train loss:   1.086636\n",
      "train loss:   1.081730\n",
      "train loss:   0.941532\n",
      "train loss:   0.818357\n",
      "train loss:   1.141745\n",
      "train loss:   0.906713\n",
      "train loss:   0.930324\n",
      "train loss:   0.775156\n",
      "train loss:   1.113996\n",
      "train loss:   0.502015\n",
      "train loss:   0.925434\n",
      "train loss:   0.933661\n",
      "train loss:   1.030732\n",
      "train loss:   1.059110\n",
      "train loss:   0.871232\n",
      "train loss:   0.675340\n",
      "train loss:   1.234897\n",
      "train loss:   1.299927\n",
      "train loss:   0.812963\n",
      "train loss:   0.719079\n",
      "train loss:   1.083205\n",
      "train loss:   0.715677\n",
      "train loss:   0.878653\n",
      "train loss:   1.049077\n",
      "train loss:   1.202596\n",
      "########### epoch 62 ###########\n",
      "########### loop 11650 ###########\n",
      "test loss:   0.353167   test accuracy:   0.875000\n",
      "########### loop 11650 ###########\n",
      "train loss:   0.962638\n",
      "train loss:   0.947319\n",
      "train loss:   0.943628\n",
      "train loss:   1.087489\n",
      "train loss:   0.795466\n",
      "train loss:   1.105605\n",
      "train loss:   0.934536\n",
      "train loss:   0.849100\n",
      "train loss:   0.463323\n",
      "train loss:   0.579126\n",
      "train loss:   1.256083\n",
      "train loss:   1.077352\n",
      "train loss:   1.151005\n",
      "train loss:   1.031318\n",
      "train loss:   1.286998\n",
      "train loss:   0.881581\n",
      "train loss:   1.000738\n",
      "train loss:   0.901552\n",
      "train loss:   1.111525\n",
      "train loss:   0.983894\n",
      "train loss:   0.741952\n",
      "train loss:   1.006567\n",
      "train loss:   0.818235\n",
      "train loss:   0.906091\n",
      "train loss:   0.795958\n",
      "train loss:   1.073439\n",
      "train loss:   1.207922\n",
      "train loss:   1.154714\n",
      "train loss:   1.007706\n",
      "train loss:   0.982288\n",
      "train loss:   0.841368\n",
      "train loss:   1.016621\n",
      "train loss:   0.850079\n",
      "train loss:   1.410359\n",
      "train loss:   0.918112\n",
      "train loss:   0.907158\n",
      "train loss:   1.026200\n",
      "train loss:   1.135278\n",
      "train loss:   0.717988\n",
      "train loss:   0.988280\n",
      "train loss:   0.511544\n",
      "train loss:   0.659206\n",
      "train loss:   1.223329\n",
      "train loss:   1.130827\n",
      "train loss:   0.944901\n",
      "train loss:   0.736339\n",
      "train loss:   1.003721\n",
      "train loss:   1.048896\n",
      "train loss:   1.246956\n",
      "train loss:   1.212039\n",
      "########### epoch 63 ###########\n",
      "########### loop 11700 ###########\n",
      "test loss:   0.111055   test accuracy:   1.000000\n",
      "########### loop 11700 ###########\n",
      "train loss:   1.088476\n",
      "train loss:   1.155362\n",
      "train loss:   1.015879\n",
      "train loss:   0.885767\n",
      "train loss:   0.953279\n",
      "train loss:   0.702045\n",
      "train loss:   0.905294\n",
      "train loss:   0.909430\n",
      "train loss:   0.712461\n",
      "train loss:   0.860919\n",
      "train loss:   0.943637\n",
      "train loss:   0.969664\n",
      "train loss:   1.132817\n",
      "train loss:   1.056619\n",
      "train loss:   1.042541\n",
      "train loss:   1.066025\n",
      "train loss:   0.908438\n",
      "train loss:   0.861081\n",
      "train loss:   1.108102\n",
      "train loss:   1.236727\n",
      "train loss:   0.926446\n",
      "train loss:   0.977843\n",
      "train loss:   0.998176\n",
      "train loss:   0.946049\n",
      "train loss:   0.998900\n",
      "train loss:   1.035001\n",
      "train loss:   1.428232\n",
      "train loss:   0.786489\n",
      "train loss:   1.235617\n",
      "train loss:   1.082573\n",
      "train loss:   1.039613\n",
      "train loss:   0.926534\n",
      "train loss:   0.643287\n",
      "train loss:   0.888738\n",
      "train loss:   1.219668\n",
      "train loss:   0.674700\n",
      "train loss:   0.767939\n",
      "train loss:   0.963996\n",
      "train loss:   1.100650\n",
      "train loss:   0.911136\n",
      "train loss:   1.013844\n",
      "train loss:   0.671876\n",
      "train loss:   0.768535\n",
      "train loss:   1.036314\n",
      "train loss:   0.947074\n",
      "train loss:   0.894212\n",
      "train loss:   1.059758\n",
      "train loss:   0.569279\n",
      "train loss:   0.807031\n",
      "train loss:   1.317223\n",
      "########### epoch 63 ###########\n",
      "########### loop 11750 ###########\n",
      "test loss:   0.284098   test accuracy:   0.916667\n",
      "########### loop 11750 ###########\n",
      "train loss:   0.904094\n",
      "train loss:   0.948508\n",
      "train loss:   1.242508\n",
      "train loss:   1.345021\n",
      "train loss:   0.524266\n",
      "train loss:   1.251710\n",
      "train loss:   0.980404\n",
      "train loss:   1.102699\n",
      "train loss:   1.093469\n",
      "train loss:   0.705933\n",
      "train loss:   0.776948\n",
      "train loss:   0.915400\n",
      "train loss:   0.796609\n",
      "train loss:   1.164765\n",
      "train loss:   1.047165\n",
      "train loss:   0.859973\n",
      "train loss:   1.016163\n",
      "train loss:   1.199551\n",
      "train loss:   0.800283\n",
      "train loss:   1.122576\n",
      "train loss:   1.071697\n",
      "train loss:   0.885856\n",
      "train loss:   1.035523\n",
      "train loss:   1.126057\n",
      "train loss:   0.944862\n",
      "train loss:   0.875027\n",
      "train loss:   1.085171\n",
      "train loss:   1.538160\n",
      "train loss:   1.331095\n",
      "train loss:   0.886201\n",
      "train loss:   0.663855\n",
      "train loss:   0.946273\n",
      "train loss:   0.984696\n",
      "train loss:   1.035164\n",
      "train loss:   0.980678\n",
      "train loss:   1.060707\n",
      "train loss:   1.102368\n",
      "train loss:   1.056369\n",
      "train loss:   0.907755\n",
      "train loss:   1.162134\n",
      "train loss:   1.027515\n",
      "train loss:   0.768344\n",
      "train loss:   0.856611\n",
      "train loss:   1.276079\n",
      "train loss:   1.007596\n",
      "train loss:   0.791021\n",
      "train loss:   1.070385\n",
      "train loss:   0.961022\n",
      "train loss:   0.800289\n",
      "train loss:   1.348350\n",
      "########### epoch 63 ###########\n",
      "########### loop 11800 ###########\n",
      "test loss:   0.259769   test accuracy:   0.958333\n",
      "########### loop 11800 ###########\n",
      "train loss:   1.191867\n",
      "train loss:   0.742201\n",
      "train loss:   1.140668\n",
      "train loss:   0.980925\n",
      "train loss:   0.885787\n",
      "train loss:   1.258961\n",
      "train loss:   1.000084\n",
      "train loss:   0.802236\n",
      "train loss:   0.966294\n",
      "train loss:   0.947123\n",
      "train loss:   1.173595\n",
      "train loss:   0.813879\n",
      "train loss:   1.022276\n",
      "train loss:   1.113017\n",
      "train loss:   1.132093\n",
      "train loss:   1.056743\n",
      "train loss:   1.189925\n",
      "train loss:   1.107174\n",
      "train loss:   0.729913\n",
      "train loss:   1.178786\n",
      "train loss:   1.256073\n",
      "train loss:   1.059774\n",
      "train loss:   1.195555\n",
      "train loss:   1.166415\n",
      "train loss:   0.892166\n",
      "train loss:   1.064910\n",
      "train loss:   0.969657\n",
      "train loss:   1.011881\n",
      "train loss:   0.964995\n",
      "train loss:   1.083822\n",
      "train loss:   0.993378\n",
      "train loss:   0.983222\n",
      "train loss:   0.821426\n",
      "train loss:   1.241268\n",
      "train loss:   1.230841\n",
      "train loss:   1.292071\n",
      "train loss:   0.772885\n",
      "train loss:   0.965613\n",
      "train loss:   0.572797\n",
      "train loss:   1.066893\n",
      "train loss:   0.959545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.769047\n",
      "train loss:   1.033089\n",
      "train loss:   0.706317\n",
      "train loss:   0.701781\n",
      "train loss:   1.156050\n",
      "train loss:   0.753828\n",
      "train loss:   1.171635\n",
      "train loss:   1.371891\n",
      "train loss:   1.004526\n",
      "########### epoch 64 ###########\n",
      "########### loop 11850 ###########\n",
      "test loss:   0.226553   test accuracy:   0.958333\n",
      "########### loop 11850 ###########\n",
      "train loss:   0.895378\n",
      "train loss:   1.066584\n",
      "train loss:   0.875239\n",
      "train loss:   1.044959\n",
      "train loss:   1.093320\n",
      "train loss:   0.756411\n",
      "train loss:   1.079822\n",
      "train loss:   1.308863\n",
      "train loss:   0.928925\n",
      "train loss:   0.916228\n",
      "train loss:   1.023157\n",
      "train loss:   1.111846\n",
      "train loss:   0.627624\n",
      "train loss:   0.837729\n",
      "train loss:   0.857543\n",
      "train loss:   1.212837\n",
      "train loss:   0.791318\n",
      "train loss:   0.801262\n",
      "train loss:   1.077620\n",
      "train loss:   0.708587\n",
      "train loss:   1.357963\n",
      "train loss:   1.327815\n",
      "train loss:   1.255002\n",
      "train loss:   0.552591\n",
      "train loss:   0.991584\n",
      "train loss:   1.023958\n",
      "train loss:   0.597474\n",
      "train loss:   1.277280\n",
      "train loss:   0.773825\n",
      "train loss:   1.089600\n",
      "train loss:   1.119703\n",
      "train loss:   0.934087\n",
      "train loss:   1.126914\n",
      "train loss:   1.253159\n",
      "train loss:   1.283975\n",
      "train loss:   0.979070\n",
      "train loss:   0.808123\n",
      "train loss:   1.106534\n",
      "train loss:   0.944386\n",
      "train loss:   0.729496\n",
      "train loss:   1.035158\n",
      "train loss:   1.199339\n",
      "train loss:   0.939157\n",
      "train loss:   1.159109\n",
      "train loss:   0.901750\n",
      "train loss:   0.825302\n",
      "train loss:   1.130469\n",
      "train loss:   1.131087\n",
      "train loss:   1.075966\n",
      "train loss:   0.783440\n",
      "########### epoch 64 ###########\n",
      "########### loop 11900 ###########\n",
      "test loss:   0.379111   test accuracy:   0.916667\n",
      "########### loop 11900 ###########\n",
      "train loss:   0.824836\n",
      "train loss:   0.970269\n",
      "train loss:   0.981471\n",
      "train loss:   1.160893\n",
      "train loss:   1.072371\n",
      "train loss:   0.963666\n",
      "train loss:   1.164191\n",
      "train loss:   0.950251\n",
      "train loss:   1.321696\n",
      "train loss:   0.619078\n",
      "train loss:   1.090940\n",
      "train loss:   1.029516\n",
      "train loss:   1.044454\n",
      "train loss:   1.051426\n",
      "train loss:   0.910975\n",
      "train loss:   0.405086\n",
      "train loss:   0.810298\n",
      "train loss:   0.596603\n",
      "train loss:   0.783651\n",
      "train loss:   1.126933\n",
      "train loss:   0.884942\n",
      "train loss:   0.804401\n",
      "train loss:   0.909233\n",
      "train loss:   0.766314\n",
      "train loss:   0.939993\n",
      "train loss:   0.701726\n",
      "train loss:   0.890722\n",
      "train loss:   0.788823\n",
      "train loss:   0.913347\n",
      "train loss:   1.132484\n",
      "train loss:   0.494124\n",
      "train loss:   1.087475\n",
      "train loss:   0.741363\n",
      "train loss:   0.984054\n",
      "train loss:   1.123080\n",
      "train loss:   0.856998\n",
      "train loss:   1.164150\n",
      "train loss:   0.937092\n",
      "train loss:   0.806878\n",
      "train loss:   1.032893\n",
      "train loss:   1.018438\n",
      "train loss:   0.878519\n",
      "train loss:   0.697998\n",
      "train loss:   1.371167\n",
      "train loss:   1.083377\n",
      "train loss:   1.252814\n",
      "train loss:   0.970959\n",
      "train loss:   1.043961\n",
      "train loss:   1.104592\n",
      "train loss:   1.024471\n",
      "########### epoch 64 ###########\n",
      "########### loop 11950 ###########\n",
      "test loss:   0.304085   test accuracy:   0.958333\n",
      "########### loop 11950 ###########\n",
      "train loss:   0.871485\n",
      "train loss:   1.057594\n",
      "train loss:   0.936612\n",
      "train loss:   0.791014\n",
      "train loss:   0.958986\n",
      "train loss:   1.108368\n",
      "train loss:   0.945430\n",
      "train loss:   0.797207\n",
      "train loss:   1.081753\n",
      "train loss:   1.117351\n",
      "train loss:   0.904985\n",
      "train loss:   0.935474\n",
      "train loss:   0.481233\n",
      "train loss:   0.744056\n",
      "train loss:   0.593244\n",
      "train loss:   1.142824\n",
      "train loss:   0.945244\n",
      "train loss:   1.186622\n",
      "train loss:   0.992656\n",
      "train loss:   0.961727\n",
      "train loss:   0.968327\n",
      "train loss:   1.107819\n",
      "train loss:   0.864733\n",
      "train loss:   0.900794\n",
      "train loss:   1.279426\n",
      "train loss:   0.954418\n",
      "train loss:   0.780455\n",
      "train loss:   1.041554\n",
      "train loss:   1.230337\n",
      "train loss:   1.087162\n",
      "train loss:   0.884003\n",
      "train loss:   0.943706\n",
      "train loss:   1.139021\n",
      "train loss:   1.361062\n",
      "train loss:   1.292611\n",
      "train loss:   0.914506\n",
      "train loss:   0.890797\n",
      "train loss:   0.841718\n",
      "train loss:   0.761210\n",
      "train loss:   0.791287\n",
      "train loss:   0.962415\n",
      "train loss:   1.069170\n",
      "train loss:   0.547384\n",
      "train loss:   1.056316\n",
      "train loss:   1.057850\n",
      "train loss:   0.994153\n",
      "train loss:   1.196714\n",
      "train loss:   0.912421\n",
      "train loss:   0.834019\n",
      "train loss:   1.096921\n",
      "########### epoch 64 ###########\n",
      "########### loop 12000 ###########\n",
      "test loss:   0.205022   test accuracy:   0.958333\n",
      "########### loop 12000 ###########\n",
      "train loss:   0.996303\n",
      "train loss:   1.174272\n",
      "train loss:   1.373254\n",
      "train loss:   0.853168\n",
      "train loss:   0.775503\n",
      "train loss:   0.989613\n",
      "train loss:   0.884673\n",
      "train loss:   1.063507\n",
      "train loss:   0.717836\n",
      "train loss:   0.873547\n",
      "train loss:   0.846879\n",
      "train loss:   0.889353\n",
      "train loss:   0.851649\n",
      "train loss:   1.078679\n",
      "train loss:   1.093990\n",
      "train loss:   1.043520\n",
      "train loss:   0.823966\n",
      "train loss:   0.808525\n",
      "train loss:   1.439228\n",
      "train loss:   1.230000\n",
      "train loss:   1.056842\n",
      "train loss:   0.922788\n",
      "train loss:   1.021537\n",
      "train loss:   0.847756\n",
      "train loss:   0.998470\n",
      "train loss:   0.854155\n",
      "train loss:   1.017742\n",
      "train loss:   1.004962\n",
      "train loss:   0.881542\n",
      "train loss:   0.656711\n",
      "train loss:   0.961426\n",
      "train loss:   1.130144\n",
      "train loss:   1.349421\n",
      "train loss:   0.618090\n",
      "train loss:   1.345884\n",
      "train loss:   1.192192\n",
      "train loss:   1.141312\n",
      "train loss:   1.040773\n",
      "train loss:   0.852216\n",
      "train loss:   1.008314\n",
      "train loss:   1.036381\n",
      "train loss:   1.093749\n",
      "train loss:   0.902621\n",
      "train loss:   0.970561\n",
      "train loss:   1.149641\n",
      "train loss:   0.666458\n",
      "train loss:   1.162177\n",
      "train loss:   0.605553\n",
      "train loss:   1.138579\n",
      "train loss:   1.099885\n",
      "########### epoch 65 ###########\n",
      "########### loop 12050 ###########\n",
      "test loss:   0.480481   test accuracy:   0.791667\n",
      "########### loop 12050 ###########\n",
      "train loss:   1.077932\n",
      "train loss:   1.032186\n",
      "train loss:   1.072593\n",
      "train loss:   0.846723\n",
      "train loss:   1.462817\n",
      "train loss:   1.119693\n",
      "train loss:   1.246994\n",
      "train loss:   1.030736\n",
      "train loss:   0.814733\n",
      "train loss:   0.660604\n",
      "train loss:   1.003855\n",
      "train loss:   1.149575\n",
      "train loss:   1.139336\n",
      "train loss:   0.947610\n",
      "train loss:   0.939662\n",
      "train loss:   1.025826\n",
      "train loss:   0.758325\n",
      "train loss:   0.810029\n",
      "train loss:   0.623575\n",
      "train loss:   1.089170\n",
      "train loss:   1.137178\n",
      "train loss:   1.127146\n",
      "train loss:   0.660450\n",
      "train loss:   1.247602\n",
      "train loss:   0.997422\n",
      "train loss:   0.726156\n",
      "train loss:   1.188937\n",
      "train loss:   1.431757\n",
      "train loss:   1.154891\n",
      "train loss:   1.349558\n",
      "train loss:   1.061021\n",
      "train loss:   0.656376\n",
      "train loss:   1.178965\n",
      "train loss:   1.333523\n",
      "train loss:   0.648721\n",
      "train loss:   1.155862\n",
      "train loss:   1.235366\n",
      "train loss:   1.449756\n",
      "train loss:   0.994861\n",
      "train loss:   1.137704\n",
      "train loss:   1.149254\n",
      "train loss:   0.801788\n",
      "train loss:   1.048187\n",
      "train loss:   0.889374\n",
      "train loss:   0.395971\n",
      "train loss:   1.092982\n",
      "train loss:   0.817441\n",
      "train loss:   0.732630\n",
      "train loss:   0.895378\n",
      "train loss:   1.014650\n",
      "########### epoch 65 ###########\n",
      "########### loop 12100 ###########\n",
      "test loss:   0.294133   test accuracy:   0.916667\n",
      "########### loop 12100 ###########\n",
      "train loss:   1.041506\n",
      "train loss:   1.180387\n",
      "train loss:   0.582486\n",
      "train loss:   1.059240\n",
      "train loss:   1.284210\n",
      "train loss:   1.183486\n",
      "train loss:   1.205823\n",
      "train loss:   0.800732\n",
      "train loss:   0.573283\n",
      "train loss:   1.249912\n",
      "train loss:   0.829333\n",
      "train loss:   0.957117\n",
      "train loss:   1.251519\n",
      "train loss:   1.147463\n",
      "train loss:   1.140672\n",
      "train loss:   0.842376\n",
      "train loss:   0.896755\n",
      "train loss:   1.342738\n",
      "train loss:   1.225388\n",
      "train loss:   0.575885\n",
      "train loss:   0.878556\n",
      "train loss:   0.838500\n",
      "train loss:   1.375040\n",
      "train loss:   0.789245\n",
      "train loss:   1.234313\n",
      "train loss:   1.121373\n",
      "train loss:   0.921331\n",
      "train loss:   1.190525\n",
      "train loss:   0.827395\n",
      "train loss:   0.569893\n",
      "train loss:   0.928127\n",
      "train loss:   1.141868\n",
      "train loss:   1.060142\n",
      "train loss:   0.819547\n",
      "train loss:   1.137142\n",
      "train loss:   0.951375\n",
      "train loss:   1.036886\n",
      "train loss:   1.296590\n",
      "train loss:   0.801077\n",
      "train loss:   1.031031\n",
      "train loss:   1.154243\n",
      "train loss:   0.769607\n",
      "train loss:   1.011274\n",
      "train loss:   0.715352\n",
      "train loss:   0.958439\n",
      "train loss:   0.938544\n",
      "train loss:   0.777223\n",
      "train loss:   0.825025\n",
      "train loss:   1.041918\n",
      "train loss:   1.205600\n",
      "########### epoch 65 ###########\n",
      "########### loop 12150 ###########\n",
      "test loss:   0.222029   test accuracy:   0.958333\n",
      "########### loop 12150 ###########\n",
      "train loss:   1.439070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.831438\n",
      "train loss:   1.142684\n",
      "train loss:   0.734763\n",
      "train loss:   0.923749\n",
      "train loss:   0.771729\n",
      "train loss:   0.821855\n",
      "train loss:   0.804425\n",
      "train loss:   0.948333\n",
      "train loss:   1.190785\n",
      "train loss:   0.820178\n",
      "train loss:   0.767127\n",
      "train loss:   1.082969\n",
      "train loss:   0.806442\n",
      "train loss:   0.677971\n",
      "train loss:   1.289104\n",
      "train loss:   1.043384\n",
      "train loss:   0.838326\n",
      "train loss:   1.127448\n",
      "train loss:   1.081537\n",
      "train loss:   1.169325\n",
      "train loss:   0.811399\n",
      "train loss:   0.990492\n",
      "train loss:   0.887080\n",
      "train loss:   1.177899\n",
      "train loss:   1.054538\n",
      "train loss:   1.195310\n",
      "train loss:   0.885834\n",
      "train loss:   0.626417\n",
      "train loss:   0.833553\n",
      "train loss:   0.535692\n",
      "train loss:   0.883428\n",
      "train loss:   1.125957\n",
      "train loss:   1.295601\n",
      "train loss:   0.888217\n",
      "train loss:   0.918304\n",
      "train loss:   1.228926\n",
      "train loss:   0.771601\n",
      "train loss:   1.065378\n",
      "train loss:   1.131222\n",
      "train loss:   0.895853\n",
      "train loss:   0.983524\n",
      "train loss:   0.968827\n",
      "train loss:   1.043046\n",
      "train loss:   1.204434\n",
      "train loss:   0.507012\n",
      "train loss:   0.903454\n",
      "train loss:   1.027672\n",
      "train loss:   1.006972\n",
      "train loss:   0.783423\n",
      "########### epoch 65 ###########\n",
      "########### loop 12200 ###########\n",
      "test loss:   0.177544   test accuracy:   1.000000\n",
      "########### loop 12200 ###########\n",
      "train loss:   0.935800\n",
      "train loss:   0.851341\n",
      "train loss:   1.147946\n",
      "train loss:   0.756878\n",
      "train loss:   1.322044\n",
      "train loss:   0.820785\n",
      "train loss:   1.009127\n",
      "train loss:   1.023811\n",
      "train loss:   0.982261\n",
      "train loss:   1.139746\n",
      "train loss:   0.897952\n",
      "train loss:   1.284405\n",
      "train loss:   0.852817\n",
      "train loss:   1.168025\n",
      "train loss:   1.085368\n",
      "train loss:   0.815856\n",
      "train loss:   0.875282\n",
      "train loss:   1.030898\n",
      "train loss:   1.179418\n",
      "train loss:   1.185128\n",
      "train loss:   1.197492\n",
      "train loss:   1.004941\n",
      "train loss:   0.964336\n",
      "train loss:   0.861367\n",
      "train loss:   1.273238\n",
      "train loss:   0.961847\n",
      "train loss:   0.932220\n",
      "train loss:   1.100006\n",
      "train loss:   1.327722\n",
      "train loss:   0.787775\n",
      "train loss:   1.164407\n",
      "train loss:   1.096700\n",
      "train loss:   1.153275\n",
      "train loss:   0.993268\n",
      "train loss:   1.227098\n",
      "train loss:   0.588028\n",
      "train loss:   0.853660\n",
      "train loss:   1.016647\n",
      "train loss:   0.953366\n",
      "train loss:   1.052696\n",
      "train loss:   0.575529\n",
      "train loss:   1.324172\n",
      "train loss:   0.940189\n",
      "train loss:   1.074794\n",
      "train loss:   0.945540\n",
      "train loss:   0.634061\n",
      "train loss:   1.085162\n",
      "train loss:   0.987420\n",
      "train loss:   0.993557\n",
      "train loss:   0.788850\n",
      "########### epoch 66 ###########\n",
      "########### loop 12250 ###########\n",
      "test loss:   0.289287   test accuracy:   0.875000\n",
      "########### loop 12250 ###########\n",
      "train loss:   1.194875\n",
      "train loss:   0.913139\n",
      "train loss:   1.221669\n",
      "train loss:   0.909232\n",
      "train loss:   1.357168\n",
      "train loss:   1.181538\n",
      "train loss:   0.863048\n",
      "train loss:   0.806031\n",
      "train loss:   0.870166\n",
      "train loss:   0.724041\n",
      "train loss:   1.331635\n",
      "train loss:   0.973481\n",
      "train loss:   1.076069\n",
      "train loss:   1.156921\n",
      "train loss:   1.026414\n",
      "train loss:   0.786563\n",
      "train loss:   1.043503\n",
      "train loss:   0.600332\n",
      "train loss:   1.056272\n",
      "train loss:   1.039719\n",
      "train loss:   1.317993\n",
      "train loss:   0.902085\n",
      "train loss:   1.125278\n",
      "train loss:   0.655709\n",
      "train loss:   1.118375\n",
      "train loss:   0.936289\n",
      "train loss:   1.295568\n",
      "train loss:   1.432181\n",
      "train loss:   1.070973\n",
      "train loss:   1.098940\n",
      "train loss:   0.959394\n",
      "train loss:   0.881193\n",
      "train loss:   0.888668\n",
      "train loss:   1.010937\n",
      "train loss:   1.313786\n",
      "train loss:   1.175816\n",
      "train loss:   1.226434\n",
      "train loss:   1.217445\n",
      "train loss:   0.975965\n",
      "train loss:   1.187107\n",
      "train loss:   0.744609\n",
      "train loss:   0.998548\n",
      "train loss:   1.231966\n",
      "train loss:   0.993395\n",
      "train loss:   1.169551\n",
      "train loss:   1.135173\n",
      "train loss:   0.960072\n",
      "train loss:   1.329116\n",
      "train loss:   0.845408\n",
      "train loss:   0.925502\n",
      "########### epoch 66 ###########\n",
      "########### loop 12300 ###########\n",
      "test loss:   0.391167   test accuracy:   0.875000\n",
      "########### loop 12300 ###########\n",
      "train loss:   0.869369\n",
      "train loss:   1.124994\n",
      "train loss:   1.269727\n",
      "train loss:   0.913490\n",
      "train loss:   0.637626\n",
      "train loss:   0.847973\n",
      "train loss:   1.166731\n",
      "train loss:   1.014917\n",
      "train loss:   0.815917\n",
      "train loss:   0.865315\n",
      "train loss:   0.975595\n",
      "train loss:   0.772861\n",
      "train loss:   1.252930\n",
      "train loss:   0.907744\n",
      "train loss:   0.874432\n",
      "train loss:   1.335784\n",
      "train loss:   1.137656\n",
      "train loss:   1.300467\n",
      "train loss:   1.456436\n",
      "train loss:   1.063483\n",
      "train loss:   0.779488\n",
      "train loss:   1.234335\n",
      "train loss:   0.966162\n",
      "train loss:   0.847956\n",
      "train loss:   1.338594\n",
      "train loss:   0.978412\n",
      "train loss:   0.556704\n",
      "train loss:   1.206601\n",
      "train loss:   1.365832\n",
      "train loss:   0.869355\n",
      "train loss:   1.093298\n",
      "train loss:   0.871895\n",
      "train loss:   1.004300\n",
      "train loss:   0.903729\n",
      "train loss:   0.799962\n",
      "train loss:   0.708135\n",
      "train loss:   1.028404\n",
      "train loss:   0.978044\n",
      "train loss:   1.097218\n",
      "train loss:   1.075045\n",
      "train loss:   1.008624\n",
      "train loss:   0.828567\n",
      "train loss:   0.959681\n",
      "train loss:   1.272613\n",
      "train loss:   1.116918\n",
      "train loss:   1.062416\n",
      "train loss:   1.045859\n",
      "train loss:   0.933047\n",
      "train loss:   0.925930\n",
      "train loss:   0.764904\n",
      "########### epoch 66 ###########\n",
      "########### loop 12350 ###########\n",
      "test loss:   0.268255   test accuracy:   0.958333\n",
      "########### loop 12350 ###########\n",
      "train loss:   1.016404\n",
      "train loss:   1.037247\n",
      "train loss:   1.310300\n",
      "train loss:   1.123530\n",
      "train loss:   0.829784\n",
      "train loss:   1.054011\n",
      "train loss:   1.070760\n",
      "train loss:   0.917536\n",
      "train loss:   0.678377\n",
      "train loss:   1.146557\n",
      "train loss:   1.012315\n",
      "train loss:   1.297992\n",
      "train loss:   0.880038\n",
      "train loss:   1.120063\n",
      "train loss:   1.297481\n",
      "train loss:   0.821606\n",
      "train loss:   1.080182\n",
      "train loss:   0.962425\n",
      "train loss:   1.084477\n",
      "train loss:   1.064791\n",
      "train loss:   1.345487\n",
      "train loss:   1.101670\n",
      "train loss:   0.791356\n",
      "train loss:   1.069689\n",
      "train loss:   0.640052\n",
      "train loss:   1.106758\n",
      "train loss:   0.818800\n",
      "train loss:   1.125400\n",
      "train loss:   1.184133\n",
      "train loss:   0.889053\n",
      "train loss:   1.098940\n",
      "train loss:   1.090097\n",
      "train loss:   1.035494\n",
      "train loss:   1.143563\n",
      "train loss:   0.821325\n",
      "train loss:   1.075913\n",
      "train loss:   1.248315\n",
      "train loss:   0.821667\n",
      "train loss:   1.064384\n",
      "train loss:   0.850594\n",
      "train loss:   1.143305\n",
      "train loss:   1.052397\n",
      "train loss:   1.001518\n",
      "train loss:   1.452315\n",
      "train loss:   0.865314\n",
      "train loss:   1.173946\n",
      "train loss:   0.696059\n",
      "train loss:   1.276199\n",
      "train loss:   0.865423\n",
      "train loss:   0.825262\n",
      "########### epoch 66 ###########\n",
      "########### loop 12400 ###########\n",
      "test loss:   0.317824   test accuracy:   0.958333\n",
      "########### loop 12400 ###########\n",
      "train loss:   1.169226\n",
      "train loss:   1.136719\n",
      "train loss:   1.154537\n",
      "train loss:   0.795512\n",
      "train loss:   1.193260\n",
      "train loss:   1.132065\n",
      "train loss:   0.905998\n",
      "train loss:   0.966182\n",
      "train loss:   0.932503\n",
      "train loss:   1.147144\n",
      "train loss:   0.997092\n",
      "train loss:   0.761367\n",
      "train loss:   1.019176\n",
      "train loss:   0.842165\n",
      "train loss:   0.962004\n",
      "train loss:   0.738871\n",
      "train loss:   0.621118\n",
      "train loss:   1.056269\n",
      "train loss:   1.391350\n",
      "train loss:   0.837085\n",
      "train loss:   0.693586\n",
      "train loss:   0.686195\n",
      "train loss:   1.179902\n",
      "train loss:   1.036118\n",
      "train loss:   0.868375\n",
      "train loss:   0.969094\n",
      "train loss:   0.928216\n",
      "train loss:   1.141834\n",
      "train loss:   1.080832\n",
      "train loss:   1.168143\n",
      "train loss:   1.051979\n",
      "train loss:   0.678798\n",
      "train loss:   0.831520\n",
      "train loss:   0.843519\n",
      "train loss:   1.098816\n",
      "train loss:   0.942575\n",
      "train loss:   1.225220\n",
      "train loss:   1.109517\n",
      "train loss:   0.808292\n",
      "train loss:   1.054669\n",
      "train loss:   1.049170\n",
      "train loss:   1.237018\n",
      "train loss:   0.932597\n",
      "train loss:   0.951508\n",
      "train loss:   1.032455\n",
      "train loss:   1.111932\n",
      "train loss:   1.128653\n",
      "train loss:   0.766863\n",
      "train loss:   0.934518\n",
      "train loss:   1.439350\n",
      "########### epoch 67 ###########\n",
      "########### loop 12450 ###########\n",
      "test loss:   0.300930   test accuracy:   0.916667\n",
      "########### loop 12450 ###########\n",
      "train loss:   1.058759\n",
      "train loss:   0.907039\n",
      "train loss:   1.036688\n",
      "train loss:   0.994457\n",
      "train loss:   0.927157\n",
      "train loss:   1.102672\n",
      "train loss:   1.285423\n",
      "train loss:   0.935358\n",
      "train loss:   1.298410\n",
      "train loss:   1.031695\n",
      "train loss:   0.910769\n",
      "train loss:   1.022756\n",
      "train loss:   1.179894\n",
      "train loss:   1.267009\n",
      "train loss:   1.574100\n",
      "train loss:   0.955887\n",
      "train loss:   1.133597\n",
      "train loss:   0.919770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.786139\n",
      "train loss:   0.704165\n",
      "train loss:   1.147701\n",
      "train loss:   1.172298\n",
      "train loss:   0.739751\n",
      "train loss:   1.071398\n",
      "train loss:   1.130306\n",
      "train loss:   0.971015\n",
      "train loss:   1.039390\n",
      "train loss:   0.668325\n",
      "train loss:   0.696867\n",
      "train loss:   1.332252\n",
      "train loss:   1.262059\n",
      "train loss:   1.049126\n",
      "train loss:   0.930151\n",
      "train loss:   1.163152\n",
      "train loss:   0.877937\n",
      "train loss:   0.954827\n",
      "train loss:   0.791836\n",
      "train loss:   1.000917\n",
      "train loss:   1.254083\n",
      "train loss:   1.190582\n",
      "train loss:   0.925312\n",
      "train loss:   0.957396\n",
      "train loss:   0.919529\n",
      "train loss:   0.835891\n",
      "train loss:   0.765779\n",
      "train loss:   1.331697\n",
      "train loss:   0.875024\n",
      "train loss:   0.574520\n",
      "train loss:   1.181699\n",
      "train loss:   1.032018\n",
      "########### epoch 67 ###########\n",
      "########### loop 12500 ###########\n",
      "test loss:   0.185647   test accuracy:   0.958333\n",
      "########### loop 12500 ###########\n",
      "train loss:   1.323699\n",
      "train loss:   0.813716\n",
      "train loss:   1.157081\n",
      "train loss:   0.574961\n",
      "train loss:   0.906696\n",
      "train loss:   0.972903\n",
      "train loss:   1.145411\n",
      "train loss:   0.994750\n",
      "train loss:   1.014163\n",
      "train loss:   0.856389\n",
      "train loss:   1.328362\n",
      "train loss:   0.715258\n",
      "train loss:   1.128329\n",
      "train loss:   0.946127\n",
      "train loss:   0.559571\n",
      "train loss:   0.820003\n",
      "train loss:   1.165280\n",
      "train loss:   1.072375\n",
      "train loss:   0.909261\n",
      "train loss:   0.795596\n",
      "train loss:   0.848865\n",
      "train loss:   1.073305\n",
      "train loss:   0.769103\n",
      "train loss:   0.704825\n",
      "train loss:   1.269103\n",
      "train loss:   1.081851\n",
      "train loss:   1.204635\n",
      "train loss:   0.839985\n",
      "train loss:   1.253349\n",
      "train loss:   0.607424\n",
      "train loss:   0.701410\n",
      "train loss:   1.005936\n",
      "train loss:   0.983743\n",
      "train loss:   1.133455\n",
      "train loss:   0.835568\n",
      "train loss:   0.866271\n",
      "train loss:   1.140416\n",
      "train loss:   1.044154\n",
      "train loss:   0.955660\n",
      "train loss:   1.208432\n",
      "train loss:   0.804585\n",
      "train loss:   0.841767\n",
      "train loss:   0.622281\n",
      "train loss:   0.954249\n",
      "train loss:   0.977454\n",
      "train loss:   1.133034\n",
      "train loss:   1.011255\n",
      "train loss:   1.135175\n",
      "train loss:   1.309361\n",
      "train loss:   1.073157\n",
      "########### epoch 67 ###########\n",
      "########### loop 12550 ###########\n",
      "test loss:   0.194687   test accuracy:   0.958333\n",
      "########### loop 12550 ###########\n",
      "train loss:   1.133268\n",
      "train loss:   1.171823\n",
      "train loss:   0.922786\n",
      "train loss:   1.052478\n",
      "train loss:   0.879333\n",
      "train loss:   1.156864\n",
      "train loss:   0.895037\n",
      "train loss:   1.215440\n",
      "train loss:   0.835672\n",
      "train loss:   1.238809\n",
      "train loss:   0.941221\n",
      "train loss:   0.988282\n",
      "train loss:   0.523865\n",
      "train loss:   1.091161\n",
      "train loss:   1.218531\n",
      "train loss:   1.237235\n",
      "train loss:   0.955620\n",
      "train loss:   1.267509\n",
      "train loss:   0.676297\n",
      "train loss:   1.332030\n",
      "train loss:   0.927490\n",
      "train loss:   0.841769\n",
      "train loss:   0.827890\n",
      "train loss:   1.013568\n",
      "train loss:   1.052302\n",
      "train loss:   0.926079\n",
      "train loss:   0.821974\n",
      "train loss:   0.695329\n",
      "train loss:   0.990581\n",
      "train loss:   0.713818\n",
      "train loss:   0.947487\n",
      "train loss:   0.876304\n",
      "train loss:   1.220927\n",
      "train loss:   0.927746\n",
      "train loss:   1.204291\n",
      "train loss:   0.761928\n",
      "train loss:   1.010120\n",
      "train loss:   1.072744\n",
      "train loss:   1.068380\n",
      "train loss:   0.752864\n",
      "train loss:   1.368486\n",
      "train loss:   0.773414\n",
      "train loss:   0.876957\n",
      "train loss:   0.922876\n",
      "train loss:   1.021628\n",
      "train loss:   1.242138\n",
      "train loss:   0.698640\n",
      "train loss:   0.911497\n",
      "train loss:   1.077743\n",
      "train loss:   1.147851\n",
      "########### epoch 68 ###########\n",
      "########### loop 12600 ###########\n",
      "test loss:   0.318215   test accuracy:   0.916667\n",
      "########### loop 12600 ###########\n",
      "train loss:   0.980468\n",
      "train loss:   1.237134\n",
      "train loss:   1.034195\n",
      "train loss:   0.960106\n",
      "train loss:   1.306022\n",
      "train loss:   0.955795\n",
      "train loss:   1.266884\n",
      "train loss:   0.867076\n",
      "train loss:   0.737481\n",
      "train loss:   1.076802\n",
      "train loss:   0.748083\n",
      "train loss:   0.899931\n",
      "train loss:   0.511717\n",
      "train loss:   1.122833\n",
      "train loss:   0.731634\n",
      "train loss:   0.947959\n",
      "train loss:   1.241936\n",
      "train loss:   1.012877\n",
      "train loss:   1.057163\n",
      "train loss:   1.144351\n",
      "train loss:   1.040172\n",
      "train loss:   1.231403\n",
      "train loss:   0.803328\n",
      "train loss:   1.169709\n",
      "train loss:   1.016849\n",
      "train loss:   0.956457\n",
      "train loss:   1.428748\n",
      "train loss:   1.085794\n",
      "train loss:   0.895970\n",
      "train loss:   1.061216\n",
      "train loss:   1.014888\n",
      "train loss:   1.218572\n",
      "train loss:   1.377642\n",
      "train loss:   1.155704\n",
      "train loss:   1.233446\n",
      "train loss:   1.128858\n",
      "train loss:   1.061147\n",
      "train loss:   0.906145\n",
      "train loss:   1.027769\n",
      "train loss:   0.428707\n",
      "train loss:   0.730608\n",
      "train loss:   0.881251\n",
      "train loss:   0.878131\n",
      "train loss:   1.312262\n",
      "train loss:   0.875283\n",
      "train loss:   1.180758\n",
      "train loss:   0.965044\n",
      "train loss:   0.615961\n",
      "train loss:   0.955600\n",
      "train loss:   0.591811\n",
      "########### epoch 68 ###########\n",
      "########### loop 12650 ###########\n",
      "test loss:   0.264380   test accuracy:   0.916667\n",
      "########### loop 12650 ###########\n",
      "train loss:   0.976166\n",
      "train loss:   0.782849\n",
      "train loss:   0.966823\n",
      "train loss:   0.924859\n",
      "train loss:   0.991381\n",
      "train loss:   0.900737\n",
      "train loss:   0.942533\n",
      "train loss:   1.215561\n",
      "train loss:   1.292303\n",
      "train loss:   1.018927\n",
      "train loss:   0.801041\n",
      "train loss:   0.986848\n",
      "train loss:   0.908209\n",
      "train loss:   1.097688\n",
      "train loss:   0.751959\n",
      "train loss:   1.093840\n",
      "train loss:   0.962874\n",
      "train loss:   0.757645\n",
      "train loss:   0.967107\n",
      "train loss:   0.873966\n",
      "train loss:   1.266999\n",
      "train loss:   0.837037\n",
      "train loss:   1.050783\n",
      "train loss:   0.900380\n",
      "train loss:   1.125463\n",
      "train loss:   1.277831\n",
      "train loss:   1.216181\n",
      "train loss:   0.864868\n",
      "train loss:   1.191731\n",
      "train loss:   1.206001\n",
      "train loss:   0.707337\n",
      "train loss:   0.695080\n",
      "train loss:   1.128695\n",
      "train loss:   0.786500\n",
      "train loss:   1.195762\n",
      "train loss:   0.753714\n",
      "train loss:   0.910430\n",
      "train loss:   1.233530\n",
      "train loss:   0.875420\n",
      "train loss:   0.671499\n",
      "train loss:   1.066439\n",
      "train loss:   1.193231\n",
      "train loss:   0.994328\n",
      "train loss:   0.889210\n",
      "train loss:   1.143200\n",
      "train loss:   1.123535\n",
      "train loss:   1.578576\n",
      "train loss:   0.737296\n",
      "train loss:   1.145948\n",
      "train loss:   1.062921\n",
      "########### epoch 68 ###########\n",
      "########### loop 12700 ###########\n",
      "test loss:   0.146625   test accuracy:   1.000000\n",
      "########### loop 12700 ###########\n",
      "train loss:   1.051041\n",
      "train loss:   0.687650\n",
      "train loss:   1.217619\n",
      "train loss:   1.039800\n",
      "train loss:   1.115917\n",
      "train loss:   1.134330\n",
      "train loss:   0.760738\n",
      "train loss:   0.687918\n",
      "train loss:   1.049989\n",
      "train loss:   1.040263\n",
      "train loss:   0.596008\n",
      "train loss:   1.097830\n",
      "train loss:   1.083710\n",
      "train loss:   1.020721\n",
      "train loss:   1.116044\n",
      "train loss:   1.030428\n",
      "train loss:   1.101952\n",
      "train loss:   0.954172\n",
      "train loss:   0.910179\n",
      "train loss:   1.297764\n",
      "train loss:   1.253642\n",
      "train loss:   0.880740\n",
      "train loss:   1.284817\n",
      "train loss:   0.344030\n",
      "train loss:   1.169622\n",
      "train loss:   0.745999\n",
      "train loss:   0.766367\n",
      "train loss:   0.950798\n",
      "train loss:   0.763549\n",
      "train loss:   1.060930\n",
      "train loss:   0.779549\n",
      "train loss:   1.195017\n",
      "train loss:   1.038352\n",
      "train loss:   1.285744\n",
      "train loss:   1.238152\n",
      "train loss:   1.034933\n",
      "train loss:   1.226063\n",
      "train loss:   1.080810\n",
      "train loss:   0.850153\n",
      "train loss:   0.788700\n",
      "train loss:   0.986960\n",
      "train loss:   0.992887\n",
      "train loss:   0.705267\n",
      "train loss:   0.900915\n",
      "train loss:   0.967956\n",
      "train loss:   1.456075\n",
      "train loss:   0.850012\n",
      "train loss:   0.798776\n",
      "train loss:   0.859245\n",
      "train loss:   0.948316\n",
      "########### epoch 68 ###########\n",
      "########### loop 12750 ###########\n",
      "test loss:   0.278177   test accuracy:   0.916667\n",
      "########### loop 12750 ###########\n",
      "train loss:   0.843726\n",
      "train loss:   1.243290\n",
      "train loss:   1.147869\n",
      "train loss:   0.941325\n",
      "train loss:   0.972053\n",
      "train loss:   0.649103\n",
      "train loss:   1.077976\n",
      "train loss:   1.150295\n",
      "train loss:   1.185176\n",
      "train loss:   1.134654\n",
      "train loss:   1.077599\n",
      "train loss:   0.922454\n",
      "train loss:   0.757114\n",
      "train loss:   1.188222\n",
      "train loss:   0.905800\n",
      "train loss:   1.150894\n",
      "train loss:   0.940017\n",
      "train loss:   0.763820\n",
      "train loss:   0.841720\n",
      "train loss:   0.841576\n",
      "train loss:   1.221767\n",
      "train loss:   0.964355\n",
      "train loss:   0.878216\n",
      "train loss:   1.053157\n",
      "train loss:   0.786222\n",
      "train loss:   0.703034\n",
      "train loss:   0.911659\n",
      "train loss:   1.138078\n",
      "train loss:   0.696301\n",
      "train loss:   0.939326\n",
      "train loss:   0.952124\n",
      "train loss:   0.970937\n",
      "train loss:   0.734696\n",
      "train loss:   0.788106\n",
      "train loss:   0.879203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.108158\n",
      "train loss:   0.962435\n",
      "train loss:   0.936150\n",
      "train loss:   0.903437\n",
      "train loss:   1.135527\n",
      "train loss:   1.053476\n",
      "train loss:   0.785820\n",
      "train loss:   0.948493\n",
      "train loss:   0.740712\n",
      "train loss:   0.747495\n",
      "train loss:   1.093251\n",
      "train loss:   0.912970\n",
      "train loss:   0.739979\n",
      "train loss:   1.109030\n",
      "train loss:   1.124349\n",
      "########### epoch 69 ###########\n",
      "########### loop 12800 ###########\n",
      "test loss:   0.299983   test accuracy:   0.958333\n",
      "########### loop 12800 ###########\n",
      "train loss:   0.899915\n",
      "train loss:   0.911874\n",
      "train loss:   0.794657\n",
      "train loss:   1.117650\n",
      "train loss:   0.962742\n",
      "train loss:   0.509936\n",
      "train loss:   0.629730\n",
      "train loss:   0.941473\n",
      "train loss:   0.882083\n",
      "train loss:   1.083899\n",
      "train loss:   0.895103\n",
      "train loss:   0.786973\n",
      "train loss:   0.670526\n",
      "train loss:   1.086773\n",
      "train loss:   0.757963\n",
      "train loss:   1.380358\n",
      "train loss:   0.973257\n",
      "train loss:   0.870933\n",
      "train loss:   0.757127\n",
      "train loss:   1.032240\n",
      "train loss:   1.145454\n",
      "train loss:   1.235954\n",
      "train loss:   0.724692\n",
      "train loss:   0.989154\n",
      "train loss:   0.530789\n",
      "train loss:   1.024765\n",
      "train loss:   0.617796\n",
      "train loss:   0.766080\n",
      "train loss:   1.040024\n",
      "train loss:   0.840069\n",
      "train loss:   0.764227\n",
      "train loss:   1.072750\n",
      "train loss:   0.684551\n",
      "train loss:   0.852297\n",
      "train loss:   0.878526\n",
      "train loss:   1.058343\n",
      "train loss:   0.896623\n",
      "train loss:   0.935149\n",
      "train loss:   0.417473\n",
      "train loss:   0.441787\n",
      "train loss:   0.724070\n",
      "train loss:   0.638073\n",
      "train loss:   1.151163\n",
      "train loss:   0.815874\n",
      "train loss:   0.820974\n",
      "train loss:   0.929358\n",
      "train loss:   1.085730\n",
      "train loss:   0.885735\n",
      "train loss:   1.095621\n",
      "train loss:   0.752262\n",
      "########### epoch 69 ###########\n",
      "########### loop 12850 ###########\n",
      "test loss:   0.154105   test accuracy:   1.000000\n",
      "########### loop 12850 ###########\n",
      "train loss:   0.653958\n",
      "train loss:   1.076129\n",
      "train loss:   1.136845\n",
      "train loss:   1.193174\n",
      "train loss:   1.072513\n",
      "train loss:   0.684628\n",
      "train loss:   1.026713\n",
      "train loss:   1.044075\n",
      "train loss:   0.649630\n",
      "train loss:   1.106157\n",
      "train loss:   1.001019\n",
      "train loss:   0.525664\n",
      "train loss:   1.201637\n",
      "train loss:   1.138725\n",
      "train loss:   1.047860\n",
      "train loss:   0.852559\n",
      "train loss:   0.775958\n",
      "train loss:   1.227271\n",
      "train loss:   1.235259\n",
      "train loss:   0.837726\n",
      "train loss:   0.933161\n",
      "train loss:   0.955456\n",
      "train loss:   0.982933\n",
      "train loss:   0.987374\n",
      "train loss:   1.158114\n",
      "train loss:   1.153171\n",
      "train loss:   1.279996\n",
      "train loss:   0.677257\n",
      "train loss:   1.232026\n",
      "train loss:   0.952260\n",
      "train loss:   0.807731\n",
      "train loss:   1.008358\n",
      "train loss:   1.037158\n",
      "train loss:   1.054008\n",
      "train loss:   1.115132\n",
      "train loss:   0.877463\n",
      "train loss:   1.016839\n",
      "train loss:   1.176830\n",
      "train loss:   1.017586\n",
      "train loss:   0.888535\n",
      "train loss:   0.977358\n",
      "train loss:   0.834219\n",
      "train loss:   0.916881\n",
      "train loss:   0.974538\n",
      "train loss:   0.749404\n",
      "train loss:   0.822677\n",
      "train loss:   1.233867\n",
      "train loss:   0.747464\n",
      "train loss:   1.233042\n",
      "train loss:   1.036852\n",
      "########### epoch 69 ###########\n",
      "########### loop 12900 ###########\n",
      "test loss:   0.197635   test accuracy:   0.916667\n",
      "########### loop 12900 ###########\n",
      "train loss:   1.114183\n",
      "train loss:   0.761817\n",
      "train loss:   1.237911\n",
      "train loss:   0.856873\n",
      "train loss:   0.969764\n",
      "train loss:   0.747213\n",
      "train loss:   0.830978\n",
      "train loss:   1.185615\n",
      "train loss:   1.093886\n",
      "train loss:   1.222309\n",
      "train loss:   1.035389\n",
      "train loss:   0.953000\n",
      "train loss:   0.575056\n",
      "train loss:   0.796995\n",
      "train loss:   1.068191\n",
      "train loss:   0.988653\n",
      "train loss:   0.892166\n",
      "train loss:   1.054960\n",
      "train loss:   1.247879\n",
      "train loss:   1.022442\n",
      "train loss:   0.979840\n",
      "train loss:   0.849682\n",
      "train loss:   1.400435\n",
      "train loss:   1.337057\n",
      "train loss:   0.960044\n",
      "train loss:   0.978862\n",
      "train loss:   0.661826\n",
      "train loss:   0.865603\n",
      "train loss:   1.196101\n",
      "train loss:   1.161645\n",
      "train loss:   0.947641\n",
      "train loss:   1.102098\n",
      "train loss:   1.217104\n",
      "train loss:   0.920085\n",
      "train loss:   0.954210\n",
      "train loss:   0.790879\n",
      "train loss:   1.108112\n",
      "train loss:   0.920165\n",
      "train loss:   0.971987\n",
      "train loss:   0.847011\n",
      "train loss:   1.156300\n",
      "train loss:   1.212128\n",
      "train loss:   1.154051\n",
      "train loss:   1.037158\n",
      "train loss:   0.912552\n",
      "train loss:   0.995649\n",
      "train loss:   1.478768\n",
      "train loss:   0.893714\n",
      "train loss:   1.077751\n",
      "train loss:   0.897771\n",
      "########### epoch 69 ###########\n",
      "########### loop 12950 ###########\n",
      "test loss:   0.310565   test accuracy:   0.916667\n",
      "########### loop 12950 ###########\n",
      "train loss:   0.844113\n",
      "train loss:   1.170570\n",
      "train loss:   0.548754\n",
      "train loss:   0.827756\n",
      "train loss:   1.155180\n",
      "train loss:   0.950628\n",
      "train loss:   0.904035\n",
      "train loss:   1.034116\n",
      "train loss:   0.989611\n",
      "train loss:   1.082302\n",
      "train loss:   0.507034\n",
      "train loss:   1.040978\n",
      "train loss:   1.319208\n",
      "train loss:   1.123492\n",
      "train loss:   1.077285\n",
      "train loss:   0.910019\n",
      "train loss:   0.883224\n",
      "train loss:   0.768998\n",
      "train loss:   1.067075\n",
      "train loss:   1.080865\n",
      "train loss:   0.914600\n",
      "train loss:   1.299373\n",
      "train loss:   0.811315\n",
      "train loss:   1.202702\n",
      "train loss:   0.852833\n",
      "train loss:   0.801230\n",
      "train loss:   1.069367\n",
      "train loss:   1.342159\n",
      "train loss:   1.008536\n",
      "train loss:   0.879989\n",
      "train loss:   0.898148\n",
      "train loss:   0.883693\n",
      "train loss:   0.928791\n",
      "train loss:   0.739963\n",
      "train loss:   0.975030\n",
      "train loss:   1.223181\n",
      "train loss:   1.047018\n",
      "train loss:   0.996600\n",
      "train loss:   0.983287\n",
      "train loss:   1.186003\n",
      "train loss:   0.999691\n",
      "train loss:   1.083561\n",
      "train loss:   1.138030\n",
      "train loss:   1.058917\n",
      "train loss:   0.678794\n",
      "train loss:   0.779581\n",
      "train loss:   0.913720\n",
      "train loss:   0.729222\n",
      "train loss:   0.996872\n",
      "train loss:   1.081561\n",
      "########### epoch 70 ###########\n",
      "########### loop 13000 ###########\n",
      "test loss:   0.249495   test accuracy:   0.958333\n",
      "########### loop 13000 ###########\n",
      "train loss:   1.196406\n",
      "train loss:   0.749444\n",
      "train loss:   1.103893\n",
      "train loss:   0.457403\n",
      "train loss:   1.171109\n",
      "train loss:   0.716178\n",
      "train loss:   0.722031\n",
      "train loss:   1.171576\n",
      "train loss:   1.079842\n",
      "train loss:   0.809376\n",
      "train loss:   1.299753\n",
      "train loss:   0.647367\n",
      "train loss:   0.999752\n",
      "train loss:   0.859639\n",
      "train loss:   0.947865\n",
      "train loss:   0.895460\n",
      "train loss:   1.039762\n",
      "train loss:   0.887934\n",
      "train loss:   0.864635\n",
      "train loss:   1.417805\n",
      "train loss:   1.154428\n",
      "train loss:   1.264385\n",
      "train loss:   0.749529\n",
      "train loss:   1.282338\n",
      "train loss:   1.121158\n",
      "train loss:   1.111134\n",
      "train loss:   1.184779\n",
      "train loss:   0.929062\n",
      "train loss:   0.958234\n",
      "train loss:   0.812954\n",
      "train loss:   1.012847\n",
      "train loss:   1.102165\n",
      "train loss:   0.845856\n",
      "train loss:   0.876681\n",
      "train loss:   1.041235\n",
      "train loss:   1.104109\n",
      "train loss:   1.147335\n",
      "train loss:   1.250493\n",
      "train loss:   0.781566\n",
      "train loss:   1.047703\n",
      "train loss:   0.939724\n",
      "train loss:   0.741004\n",
      "train loss:   1.269372\n",
      "train loss:   1.105544\n",
      "train loss:   1.382014\n",
      "train loss:   1.038749\n",
      "train loss:   1.067668\n",
      "train loss:   0.793611\n",
      "train loss:   1.019409\n",
      "train loss:   1.759686\n",
      "########### epoch 70 ###########\n",
      "########### loop 13050 ###########\n",
      "test loss:   0.449160   test accuracy:   0.916667\n",
      "########### loop 13050 ###########\n",
      "train loss:   1.062360\n",
      "train loss:   0.761892\n",
      "train loss:   0.752584\n",
      "train loss:   0.951410\n",
      "train loss:   1.211273\n",
      "train loss:   0.995841\n",
      "train loss:   1.070514\n",
      "train loss:   1.263202\n",
      "train loss:   0.775676\n",
      "train loss:   1.249265\n",
      "train loss:   0.870208\n",
      "train loss:   0.805701\n",
      "train loss:   1.006661\n",
      "train loss:   1.120780\n",
      "train loss:   1.081738\n",
      "train loss:   1.361812\n",
      "train loss:   1.034453\n",
      "train loss:   1.399414\n",
      "train loss:   1.106904\n",
      "train loss:   0.874182\n",
      "train loss:   1.169670\n",
      "train loss:   1.158339\n",
      "train loss:   1.134261\n",
      "train loss:   1.105215\n",
      "train loss:   0.914396\n",
      "train loss:   0.929293\n",
      "train loss:   0.940413\n",
      "train loss:   1.169620\n",
      "train loss:   0.787661\n",
      "train loss:   1.143239\n",
      "train loss:   1.059258\n",
      "train loss:   0.855530\n",
      "train loss:   0.779063\n",
      "train loss:   0.978209\n",
      "train loss:   1.054881\n",
      "train loss:   1.081687\n",
      "train loss:   0.810572\n",
      "train loss:   0.710034\n",
      "train loss:   1.089706\n",
      "train loss:   0.995169\n",
      "train loss:   0.649264\n",
      "train loss:   1.321005\n",
      "train loss:   0.963076\n",
      "train loss:   1.347621\n",
      "train loss:   1.297997\n",
      "train loss:   0.639731\n",
      "train loss:   1.226047\n",
      "train loss:   0.930919\n",
      "train loss:   1.276452\n",
      "train loss:   1.491859\n",
      "########### epoch 70 ###########\n",
      "########### loop 13100 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.275484   test accuracy:   0.916667\n",
      "########### loop 13100 ###########\n",
      "train loss:   1.324043\n",
      "train loss:   1.036913\n",
      "train loss:   1.133505\n",
      "train loss:   1.092453\n",
      "train loss:   1.128130\n",
      "train loss:   1.122579\n",
      "train loss:   0.952947\n",
      "train loss:   0.881665\n",
      "train loss:   0.870329\n",
      "train loss:   1.023536\n",
      "train loss:   1.103917\n",
      "train loss:   0.900218\n",
      "train loss:   0.858098\n",
      "train loss:   1.239113\n",
      "train loss:   0.893760\n",
      "train loss:   0.875525\n",
      "train loss:   1.035386\n",
      "train loss:   1.022990\n",
      "train loss:   1.053523\n",
      "train loss:   1.216982\n",
      "train loss:   1.173580\n",
      "train loss:   1.159494\n",
      "train loss:   1.079508\n",
      "train loss:   0.874628\n",
      "train loss:   1.280973\n",
      "train loss:   1.098187\n",
      "train loss:   1.316924\n",
      "train loss:   1.261551\n",
      "train loss:   0.456360\n",
      "train loss:   1.074006\n",
      "train loss:   0.846893\n",
      "train loss:   0.697988\n",
      "train loss:   1.191151\n",
      "train loss:   0.602144\n",
      "train loss:   0.893073\n",
      "train loss:   1.052490\n",
      "train loss:   0.845713\n",
      "train loss:   0.985116\n",
      "train loss:   1.268045\n",
      "train loss:   0.781174\n",
      "train loss:   0.572948\n",
      "train loss:   1.237346\n",
      "train loss:   1.319471\n",
      "train loss:   0.915541\n",
      "train loss:   0.845233\n",
      "train loss:   1.135612\n",
      "train loss:   1.018160\n",
      "train loss:   1.055575\n",
      "train loss:   0.820483\n",
      "train loss:   1.255709\n",
      "########### epoch 70 ###########\n",
      "########### loop 13150 ###########\n",
      "test loss:   0.446361   test accuracy:   0.916667\n",
      "########### loop 13150 ###########\n",
      "train loss:   0.889284\n",
      "train loss:   0.973373\n",
      "train loss:   1.016396\n",
      "train loss:   0.970970\n",
      "train loss:   1.106475\n",
      "train loss:   0.615994\n",
      "train loss:   0.781904\n",
      "train loss:   0.856283\n",
      "train loss:   1.516262\n",
      "train loss:   0.672560\n",
      "train loss:   1.027649\n",
      "train loss:   0.911205\n",
      "train loss:   0.629766\n",
      "train loss:   1.286467\n",
      "train loss:   1.236129\n",
      "train loss:   0.835206\n",
      "train loss:   1.038475\n",
      "train loss:   0.860773\n",
      "train loss:   1.056173\n",
      "train loss:   0.786147\n",
      "train loss:   1.022903\n",
      "train loss:   1.136468\n",
      "train loss:   0.979725\n",
      "train loss:   0.716667\n",
      "train loss:   1.139846\n",
      "train loss:   1.150931\n",
      "train loss:   0.644468\n",
      "train loss:   1.080671\n",
      "train loss:   0.908414\n",
      "train loss:   1.122330\n",
      "train loss:   1.055174\n",
      "train loss:   0.981924\n",
      "train loss:   0.810796\n",
      "train loss:   1.259803\n",
      "train loss:   0.801439\n",
      "train loss:   0.875222\n",
      "train loss:   0.790519\n",
      "train loss:   1.052859\n",
      "train loss:   1.357387\n",
      "train loss:   1.225388\n",
      "train loss:   0.721879\n",
      "train loss:   0.760980\n",
      "train loss:   1.003571\n",
      "train loss:   0.988414\n",
      "train loss:   1.012243\n",
      "train loss:   1.074930\n",
      "train loss:   0.893986\n",
      "train loss:   1.056523\n",
      "train loss:   1.144986\n",
      "train loss:   1.203543\n",
      "########### epoch 71 ###########\n",
      "########### loop 13200 ###########\n",
      "test loss:   0.274606   test accuracy:   0.916667\n",
      "########### loop 13200 ###########\n",
      "train loss:   1.008376\n",
      "train loss:   0.632802\n",
      "train loss:   0.828759\n",
      "train loss:   0.979006\n",
      "train loss:   1.094449\n",
      "train loss:   0.950053\n",
      "train loss:   0.777968\n",
      "train loss:   0.793383\n",
      "train loss:   0.745371\n",
      "train loss:   1.108264\n",
      "train loss:   0.864332\n",
      "train loss:   0.766140\n",
      "train loss:   1.013625\n",
      "train loss:   0.721777\n",
      "train loss:   0.839547\n",
      "train loss:   0.861627\n",
      "train loss:   0.870208\n",
      "train loss:   1.177464\n",
      "train loss:   0.870531\n",
      "train loss:   1.392859\n",
      "train loss:   1.126907\n",
      "train loss:   1.181087\n",
      "train loss:   0.661325\n",
      "train loss:   1.107058\n",
      "train loss:   0.639235\n",
      "train loss:   1.109113\n",
      "train loss:   1.088088\n",
      "train loss:   0.893939\n",
      "train loss:   1.078800\n",
      "train loss:   1.081436\n",
      "train loss:   0.962736\n",
      "train loss:   1.183948\n",
      "train loss:   1.029314\n",
      "train loss:   1.058619\n",
      "train loss:   0.608226\n",
      "train loss:   0.796217\n",
      "train loss:   0.876948\n",
      "train loss:   1.049093\n",
      "train loss:   1.020022\n",
      "train loss:   1.034175\n",
      "train loss:   0.775640\n",
      "train loss:   1.183157\n",
      "train loss:   1.156129\n",
      "train loss:   0.997936\n",
      "train loss:   1.134266\n",
      "train loss:   1.144098\n",
      "train loss:   0.644123\n",
      "train loss:   1.101507\n",
      "train loss:   0.881011\n",
      "train loss:   0.931726\n",
      "########### epoch 71 ###########\n",
      "########### loop 13250 ###########\n",
      "test loss:   0.311056   test accuracy:   0.958333\n",
      "########### loop 13250 ###########\n",
      "train loss:   0.828612\n",
      "train loss:   1.040809\n",
      "train loss:   1.044809\n",
      "train loss:   0.922302\n",
      "train loss:   0.959263\n",
      "train loss:   0.865761\n",
      "train loss:   1.090275\n",
      "train loss:   1.167344\n",
      "train loss:   1.373686\n",
      "train loss:   0.978433\n",
      "train loss:   0.844217\n",
      "train loss:   1.144513\n",
      "train loss:   0.737146\n",
      "train loss:   0.554296\n",
      "train loss:   1.185533\n",
      "train loss:   1.198223\n",
      "train loss:   0.884540\n",
      "train loss:   0.773184\n",
      "train loss:   0.801441\n",
      "train loss:   1.063205\n",
      "train loss:   0.774883\n",
      "train loss:   0.924079\n",
      "train loss:   1.053107\n",
      "train loss:   0.968421\n",
      "train loss:   1.006464\n",
      "train loss:   1.064416\n",
      "train loss:   0.926170\n",
      "train loss:   1.292518\n",
      "train loss:   0.899308\n",
      "train loss:   1.195091\n",
      "train loss:   0.917917\n",
      "train loss:   0.995889\n",
      "train loss:   0.921173\n",
      "train loss:   0.926894\n",
      "train loss:   1.080083\n",
      "train loss:   0.961063\n",
      "train loss:   0.853834\n",
      "train loss:   0.976339\n",
      "train loss:   0.928973\n",
      "train loss:   1.051655\n",
      "train loss:   1.067984\n",
      "train loss:   1.322476\n",
      "train loss:   1.135242\n",
      "train loss:   0.756107\n",
      "train loss:   1.173796\n",
      "train loss:   1.015492\n",
      "train loss:   1.156075\n",
      "train loss:   1.295064\n",
      "train loss:   0.734080\n",
      "train loss:   1.000353\n",
      "########### epoch 71 ###########\n",
      "########### loop 13300 ###########\n",
      "test loss:   0.315865   test accuracy:   0.958333\n",
      "########### loop 13300 ###########\n",
      "train loss:   1.252541\n",
      "train loss:   0.795252\n",
      "train loss:   0.553876\n",
      "train loss:   0.788374\n",
      "train loss:   1.132570\n",
      "train loss:   0.866618\n",
      "train loss:   1.000509\n",
      "train loss:   0.950879\n",
      "train loss:   1.064676\n",
      "train loss:   1.008881\n",
      "train loss:   1.250707\n",
      "train loss:   1.237928\n",
      "train loss:   1.108233\n",
      "train loss:   0.984825\n",
      "train loss:   0.993531\n",
      "train loss:   1.053170\n",
      "train loss:   0.966120\n",
      "train loss:   0.738728\n",
      "train loss:   0.829391\n",
      "train loss:   1.164421\n",
      "train loss:   1.167753\n",
      "train loss:   0.871820\n",
      "train loss:   1.094593\n",
      "train loss:   0.524361\n",
      "train loss:   0.716757\n",
      "train loss:   1.151414\n",
      "train loss:   0.950267\n",
      "train loss:   1.372254\n",
      "train loss:   0.938741\n",
      "train loss:   1.097043\n",
      "train loss:   0.895733\n",
      "train loss:   1.002090\n",
      "train loss:   0.894774\n",
      "train loss:   0.844062\n",
      "train loss:   1.022617\n",
      "train loss:   1.105902\n",
      "train loss:   1.209898\n",
      "train loss:   0.861380\n",
      "train loss:   1.220638\n",
      "train loss:   0.697674\n",
      "train loss:   1.024225\n",
      "train loss:   1.066648\n",
      "train loss:   1.077480\n",
      "train loss:   0.930489\n",
      "train loss:   0.672346\n",
      "train loss:   1.118564\n",
      "train loss:   0.505797\n",
      "train loss:   1.129800\n",
      "train loss:   0.935690\n",
      "train loss:   1.048303\n",
      "########### epoch 72 ###########\n",
      "########### loop 13350 ###########\n",
      "test loss:   0.186495   test accuracy:   0.958333\n",
      "########### loop 13350 ###########\n",
      "train loss:   0.944218\n",
      "train loss:   1.089397\n",
      "train loss:   0.742889\n",
      "train loss:   1.098604\n",
      "train loss:   1.098824\n",
      "train loss:   1.070341\n",
      "train loss:   0.977362\n",
      "train loss:   1.243011\n",
      "train loss:   0.797366\n",
      "train loss:   1.036423\n",
      "train loss:   0.719189\n",
      "train loss:   0.765735\n",
      "train loss:   0.770172\n",
      "train loss:   0.833235\n",
      "train loss:   1.013891\n",
      "train loss:   1.191791\n",
      "train loss:   0.911424\n",
      "train loss:   1.100432\n",
      "train loss:   1.136930\n",
      "train loss:   0.860878\n",
      "train loss:   1.069493\n",
      "train loss:   0.636153\n",
      "train loss:   0.903286\n",
      "train loss:   0.912916\n",
      "train loss:   1.207447\n",
      "train loss:   0.925664\n",
      "train loss:   0.709941\n",
      "train loss:   0.770807\n",
      "train loss:   1.012596\n",
      "train loss:   1.159628\n",
      "train loss:   0.770375\n",
      "train loss:   1.061819\n",
      "train loss:   0.867516\n",
      "train loss:   1.048597\n",
      "train loss:   1.279329\n",
      "train loss:   0.824699\n",
      "train loss:   0.677282\n",
      "train loss:   0.727901\n",
      "train loss:   1.191392\n",
      "train loss:   1.080391\n",
      "train loss:   0.944734\n",
      "train loss:   0.655359\n",
      "train loss:   0.830156\n",
      "train loss:   0.982335\n",
      "train loss:   1.063523\n",
      "train loss:   1.099249\n",
      "train loss:   0.889508\n",
      "train loss:   1.108879\n",
      "train loss:   0.934068\n",
      "train loss:   0.923680\n",
      "########### epoch 72 ###########\n",
      "########### loop 13400 ###########\n",
      "test loss:   0.325996   test accuracy:   0.916667\n",
      "########### loop 13400 ###########\n",
      "train loss:   0.982387\n",
      "train loss:   0.944015\n",
      "train loss:   1.304634\n",
      "train loss:   0.878247\n",
      "train loss:   1.006041\n",
      "train loss:   1.098673\n",
      "train loss:   1.240554\n",
      "train loss:   1.339766\n",
      "train loss:   0.741683\n",
      "train loss:   0.781305\n",
      "train loss:   0.872654\n",
      "train loss:   1.086341\n",
      "train loss:   1.067705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.813823\n",
      "train loss:   1.412283\n",
      "train loss:   1.128963\n",
      "train loss:   1.408805\n",
      "train loss:   1.066351\n",
      "train loss:   0.988463\n",
      "train loss:   0.856878\n",
      "train loss:   0.753738\n",
      "train loss:   0.737257\n",
      "train loss:   1.028576\n",
      "train loss:   0.918821\n",
      "train loss:   0.959237\n",
      "train loss:   1.036752\n",
      "train loss:   1.080163\n",
      "train loss:   0.718794\n",
      "train loss:   0.605164\n",
      "train loss:   1.055241\n",
      "train loss:   0.922872\n",
      "train loss:   1.131540\n",
      "train loss:   0.973261\n",
      "train loss:   0.840844\n",
      "train loss:   1.002509\n",
      "train loss:   1.040432\n",
      "train loss:   1.055909\n",
      "train loss:   1.133847\n",
      "train loss:   0.746223\n",
      "train loss:   1.082981\n",
      "train loss:   1.048821\n",
      "train loss:   0.724896\n",
      "train loss:   1.163530\n",
      "train loss:   0.867838\n",
      "train loss:   0.720984\n",
      "train loss:   0.657835\n",
      "train loss:   0.793290\n",
      "train loss:   0.809628\n",
      "train loss:   1.098452\n",
      "train loss:   0.840714\n",
      "########### epoch 72 ###########\n",
      "########### loop 13450 ###########\n",
      "test loss:   0.274394   test accuracy:   0.916667\n",
      "########### loop 13450 ###########\n",
      "train loss:   0.761674\n",
      "train loss:   0.837202\n",
      "train loss:   1.087371\n",
      "train loss:   1.019496\n",
      "train loss:   1.592055\n",
      "train loss:   1.256108\n",
      "train loss:   1.286944\n",
      "train loss:   1.023325\n",
      "train loss:   1.053666\n",
      "train loss:   0.933391\n",
      "train loss:   1.168096\n",
      "train loss:   0.847166\n",
      "train loss:   0.966629\n",
      "train loss:   0.850320\n",
      "train loss:   0.953708\n",
      "train loss:   0.913571\n",
      "train loss:   0.821554\n",
      "train loss:   1.171782\n",
      "train loss:   1.139659\n",
      "train loss:   1.188474\n",
      "train loss:   1.243641\n",
      "train loss:   1.020532\n",
      "train loss:   0.922773\n",
      "train loss:   0.757532\n",
      "train loss:   1.130974\n",
      "train loss:   0.964508\n",
      "train loss:   0.936609\n",
      "train loss:   1.225660\n",
      "train loss:   1.264988\n",
      "train loss:   1.192823\n",
      "train loss:   1.198779\n",
      "train loss:   0.879278\n",
      "train loss:   1.041530\n",
      "train loss:   1.268445\n",
      "train loss:   1.237573\n",
      "train loss:   1.492133\n",
      "train loss:   1.366118\n",
      "train loss:   1.191173\n",
      "train loss:   1.187220\n",
      "train loss:   0.967741\n",
      "train loss:   0.996360\n",
      "train loss:   0.938493\n",
      "train loss:   0.716776\n",
      "train loss:   0.859232\n",
      "train loss:   0.881254\n",
      "train loss:   0.942220\n",
      "train loss:   1.322295\n",
      "train loss:   1.005660\n",
      "train loss:   0.910100\n",
      "train loss:   1.072070\n",
      "########### epoch 72 ###########\n",
      "########### loop 13500 ###########\n",
      "test loss:   0.309157   test accuracy:   0.875000\n",
      "########### loop 13500 ###########\n",
      "train loss:   0.996166\n",
      "train loss:   0.856275\n",
      "train loss:   0.938952\n",
      "train loss:   0.974966\n",
      "train loss:   1.113877\n",
      "train loss:   0.938267\n",
      "train loss:   0.690125\n",
      "train loss:   0.887386\n",
      "train loss:   0.989281\n",
      "train loss:   1.091041\n",
      "train loss:   1.150106\n",
      "train loss:   1.353042\n",
      "train loss:   1.187436\n",
      "train loss:   1.104580\n",
      "train loss:   0.894548\n",
      "train loss:   0.739357\n",
      "train loss:   1.087582\n",
      "train loss:   0.992986\n",
      "train loss:   0.887548\n",
      "train loss:   0.875557\n",
      "train loss:   1.068051\n",
      "train loss:   1.321423\n",
      "train loss:   1.168680\n",
      "train loss:   1.282908\n",
      "train loss:   0.742115\n",
      "train loss:   0.738468\n",
      "train loss:   0.843826\n",
      "train loss:   1.096954\n",
      "train loss:   1.062007\n",
      "train loss:   1.306813\n",
      "train loss:   0.955469\n",
      "train loss:   1.348631\n",
      "train loss:   0.594854\n",
      "train loss:   1.266815\n",
      "train loss:   0.735132\n",
      "train loss:   0.782817\n",
      "train loss:   1.086695\n",
      "train loss:   1.075071\n",
      "train loss:   1.268237\n",
      "train loss:   0.838967\n",
      "train loss:   0.606381\n",
      "train loss:   1.390460\n",
      "train loss:   1.190666\n",
      "train loss:   1.311411\n",
      "train loss:   1.161829\n",
      "train loss:   0.922207\n",
      "train loss:   1.286444\n",
      "train loss:   0.599505\n",
      "train loss:   1.300256\n",
      "train loss:   0.922318\n",
      "########### epoch 73 ###########\n",
      "########### loop 13550 ###########\n",
      "test loss:   0.593724   test accuracy:   0.750000\n",
      "########### loop 13550 ###########\n",
      "train loss:   1.285103\n",
      "train loss:   1.132128\n",
      "train loss:   0.970176\n",
      "train loss:   0.958572\n",
      "train loss:   1.325247\n",
      "train loss:   1.010285\n",
      "train loss:   1.066550\n",
      "train loss:   0.979897\n",
      "train loss:   0.864592\n",
      "train loss:   1.063928\n",
      "train loss:   0.970516\n",
      "train loss:   0.869274\n",
      "train loss:   0.921403\n",
      "train loss:   0.716771\n",
      "train loss:   0.920487\n",
      "train loss:   0.780167\n",
      "train loss:   1.184775\n",
      "train loss:   1.292277\n",
      "train loss:   1.018471\n",
      "train loss:   0.894804\n",
      "train loss:   1.358616\n",
      "train loss:   1.117909\n",
      "train loss:   0.901381\n",
      "train loss:   1.160201\n",
      "train loss:   0.866155\n",
      "train loss:   0.885443\n",
      "train loss:   1.307823\n",
      "train loss:   0.970801\n",
      "train loss:   1.138052\n",
      "train loss:   1.252555\n",
      "train loss:   1.152039\n",
      "train loss:   1.245147\n",
      "train loss:   1.181270\n",
      "train loss:   1.002544\n",
      "train loss:   1.126341\n",
      "train loss:   1.305310\n",
      "train loss:   1.032612\n",
      "train loss:   0.765284\n",
      "train loss:   1.130701\n",
      "train loss:   1.003521\n",
      "train loss:   0.948997\n",
      "train loss:   1.164455\n",
      "train loss:   1.135203\n",
      "train loss:   1.143990\n",
      "train loss:   0.779071\n",
      "train loss:   0.877492\n",
      "train loss:   1.315911\n",
      "train loss:   1.188199\n",
      "train loss:   0.966174\n",
      "train loss:   0.754675\n",
      "########### epoch 73 ###########\n",
      "########### loop 13600 ###########\n",
      "test loss:   0.207903   test accuracy:   0.958333\n",
      "########### loop 13600 ###########\n",
      "train loss:   1.309424\n",
      "train loss:   0.766849\n",
      "train loss:   1.170777\n",
      "train loss:   1.074122\n",
      "train loss:   1.066014\n",
      "train loss:   0.852263\n",
      "train loss:   1.105829\n",
      "train loss:   1.096201\n",
      "train loss:   0.885513\n",
      "train loss:   0.750191\n",
      "train loss:   0.832520\n",
      "train loss:   1.010350\n",
      "train loss:   0.757449\n",
      "train loss:   0.940120\n",
      "train loss:   0.941186\n",
      "train loss:   0.969648\n",
      "train loss:   0.921625\n",
      "train loss:   1.146760\n",
      "train loss:   1.106924\n",
      "train loss:   1.099411\n",
      "train loss:   0.733415\n",
      "train loss:   0.982439\n",
      "train loss:   1.048625\n",
      "train loss:   1.052074\n",
      "train loss:   1.165348\n",
      "train loss:   0.861582\n",
      "train loss:   1.096958\n",
      "train loss:   0.964638\n",
      "train loss:   0.986812\n",
      "train loss:   0.921827\n",
      "train loss:   1.051162\n",
      "train loss:   0.902377\n",
      "train loss:   1.054728\n",
      "train loss:   0.946219\n",
      "train loss:   0.909905\n",
      "train loss:   0.478105\n",
      "train loss:   0.998990\n",
      "train loss:   0.935292\n",
      "train loss:   1.076118\n",
      "train loss:   0.708842\n",
      "train loss:   0.619465\n",
      "train loss:   0.973631\n",
      "train loss:   0.918154\n",
      "train loss:   1.187034\n",
      "train loss:   0.728844\n",
      "train loss:   0.740666\n",
      "train loss:   1.207169\n",
      "train loss:   1.005019\n",
      "train loss:   0.770235\n",
      "train loss:   1.461730\n",
      "########### epoch 73 ###########\n",
      "########### loop 13650 ###########\n",
      "test loss:   0.363345   test accuracy:   0.916667\n",
      "########### loop 13650 ###########\n",
      "train loss:   0.952583\n",
      "train loss:   0.889051\n",
      "train loss:   0.824256\n",
      "train loss:   0.819973\n",
      "train loss:   1.031888\n",
      "train loss:   1.098068\n",
      "train loss:   0.885657\n",
      "train loss:   0.697176\n",
      "train loss:   1.009750\n",
      "train loss:   0.813153\n",
      "train loss:   1.292740\n",
      "train loss:   0.912457\n",
      "train loss:   0.470465\n",
      "train loss:   1.060373\n",
      "train loss:   0.937434\n",
      "train loss:   0.965291\n",
      "train loss:   1.448691\n",
      "train loss:   1.123601\n",
      "train loss:   1.022362\n",
      "train loss:   0.920260\n",
      "train loss:   1.041017\n",
      "train loss:   1.270374\n",
      "train loss:   0.900837\n",
      "train loss:   1.445724\n",
      "train loss:   0.680393\n",
      "train loss:   1.090800\n",
      "train loss:   1.496531\n",
      "train loss:   0.828386\n",
      "train loss:   1.240428\n",
      "train loss:   0.939178\n",
      "train loss:   1.004674\n",
      "train loss:   0.985034\n",
      "train loss:   1.005005\n",
      "train loss:   0.700897\n",
      "train loss:   0.926677\n",
      "train loss:   1.146428\n",
      "train loss:   0.985043\n",
      "train loss:   0.895973\n",
      "train loss:   0.868417\n",
      "train loss:   0.990130\n",
      "train loss:   1.040501\n",
      "train loss:   1.004424\n",
      "train loss:   1.088011\n",
      "train loss:   0.971964\n",
      "train loss:   0.749331\n",
      "train loss:   1.005579\n",
      "train loss:   0.687125\n",
      "train loss:   0.659023\n",
      "train loss:   0.591811\n",
      "train loss:   0.875170\n",
      "########### epoch 73 ###########\n",
      "########### loop 13700 ###########\n",
      "test loss:   0.297975   test accuracy:   0.916667\n",
      "########### loop 13700 ###########\n",
      "train loss:   0.907326\n",
      "train loss:   0.827469\n",
      "train loss:   0.865645\n",
      "train loss:   1.370943\n",
      "train loss:   1.033841\n",
      "train loss:   0.889041\n",
      "train loss:   0.970572\n",
      "train loss:   1.075139\n",
      "train loss:   0.662370\n",
      "train loss:   0.827146\n",
      "train loss:   1.096722\n",
      "train loss:   0.745257\n",
      "train loss:   0.697983\n",
      "train loss:   1.100953\n",
      "train loss:   0.843184\n",
      "train loss:   1.320877\n",
      "train loss:   1.133432\n",
      "train loss:   1.005007\n",
      "train loss:   0.762056\n",
      "train loss:   0.862376\n",
      "train loss:   1.013468\n",
      "train loss:   0.945444\n",
      "train loss:   0.794951\n",
      "train loss:   0.973634\n",
      "train loss:   1.144081\n",
      "train loss:   1.018635\n",
      "train loss:   0.944352\n",
      "train loss:   0.881923\n",
      "train loss:   0.928413\n",
      "train loss:   0.851645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.755526\n",
      "train loss:   0.790677\n",
      "train loss:   0.648450\n",
      "train loss:   0.971743\n",
      "train loss:   1.051992\n",
      "train loss:   0.706430\n",
      "train loss:   0.569140\n",
      "train loss:   1.311327\n",
      "train loss:   0.990504\n",
      "train loss:   1.177925\n",
      "train loss:   0.917106\n",
      "train loss:   1.208332\n",
      "train loss:   1.034114\n",
      "train loss:   1.290999\n",
      "train loss:   1.506481\n",
      "train loss:   0.850787\n",
      "train loss:   1.073053\n",
      "train loss:   0.894172\n",
      "train loss:   1.123805\n",
      "train loss:   1.172693\n",
      "########### epoch 74 ###########\n",
      "########### loop 13750 ###########\n",
      "test loss:   0.331212   test accuracy:   0.916667\n",
      "########### loop 13750 ###########\n",
      "train loss:   1.283912\n",
      "train loss:   0.842818\n",
      "train loss:   0.959379\n",
      "train loss:   1.303487\n",
      "train loss:   0.755705\n",
      "train loss:   1.018780\n",
      "train loss:   1.024726\n",
      "train loss:   1.047814\n",
      "train loss:   1.214232\n",
      "train loss:   0.868956\n",
      "train loss:   0.861627\n",
      "train loss:   1.245003\n",
      "train loss:   1.270404\n",
      "train loss:   1.082180\n",
      "train loss:   1.633912\n",
      "train loss:   1.312499\n",
      "train loss:   1.159402\n",
      "train loss:   1.100977\n",
      "train loss:   0.668126\n",
      "train loss:   0.769804\n",
      "train loss:   1.007033\n",
      "train loss:   1.058105\n",
      "train loss:   1.031728\n",
      "train loss:   1.350833\n",
      "train loss:   0.950692\n",
      "train loss:   0.811568\n",
      "train loss:   0.720177\n",
      "train loss:   1.139351\n",
      "train loss:   0.816917\n",
      "train loss:   0.720064\n",
      "train loss:   0.855797\n",
      "train loss:   1.023769\n",
      "train loss:   0.878448\n",
      "train loss:   1.188618\n",
      "train loss:   1.217925\n",
      "train loss:   0.956017\n",
      "train loss:   1.081358\n",
      "train loss:   1.036322\n",
      "train loss:   0.743335\n",
      "train loss:   0.962946\n",
      "train loss:   1.143090\n",
      "train loss:   0.946146\n",
      "train loss:   0.798993\n",
      "train loss:   0.910248\n",
      "train loss:   0.938495\n",
      "train loss:   0.951554\n",
      "train loss:   0.552301\n",
      "train loss:   1.135695\n",
      "train loss:   0.864908\n",
      "train loss:   1.267713\n",
      "########### epoch 74 ###########\n",
      "########### loop 13800 ###########\n",
      "test loss:   0.243660   test accuracy:   1.000000\n",
      "########### loop 13800 ###########\n",
      "train loss:   0.934554\n",
      "train loss:   0.828709\n",
      "train loss:   1.104510\n",
      "train loss:   0.830438\n",
      "train loss:   1.032277\n",
      "train loss:   0.835919\n",
      "train loss:   1.122198\n",
      "train loss:   0.882433\n",
      "train loss:   1.237202\n",
      "train loss:   1.353469\n",
      "train loss:   0.819402\n",
      "train loss:   1.019328\n",
      "train loss:   0.926176\n",
      "train loss:   0.970942\n",
      "train loss:   1.103210\n",
      "train loss:   1.022123\n",
      "train loss:   0.900354\n",
      "train loss:   0.988668\n",
      "train loss:   0.986101\n",
      "train loss:   1.014201\n",
      "train loss:   0.918441\n",
      "train loss:   0.966188\n",
      "train loss:   1.046570\n",
      "train loss:   0.934720\n",
      "train loss:   0.826301\n",
      "train loss:   1.143741\n",
      "train loss:   0.757594\n",
      "train loss:   1.365455\n",
      "train loss:   0.935530\n",
      "train loss:   1.122613\n",
      "train loss:   1.007611\n",
      "train loss:   0.718014\n",
      "train loss:   1.161149\n",
      "train loss:   0.774866\n",
      "train loss:   1.244919\n",
      "train loss:   0.921437\n",
      "train loss:   1.166395\n",
      "train loss:   0.840647\n",
      "train loss:   0.877460\n",
      "train loss:   1.022812\n",
      "train loss:   0.845852\n",
      "train loss:   0.910784\n",
      "train loss:   1.037172\n",
      "train loss:   0.964030\n",
      "train loss:   0.841815\n",
      "train loss:   1.240976\n",
      "train loss:   1.024598\n",
      "train loss:   1.170079\n",
      "train loss:   0.993143\n",
      "train loss:   1.047532\n",
      "########### epoch 74 ###########\n",
      "########### loop 13850 ###########\n",
      "test loss:   0.226149   test accuracy:   0.958333\n",
      "########### loop 13850 ###########\n",
      "train loss:   0.994143\n",
      "train loss:   0.839113\n",
      "train loss:   0.954033\n",
      "train loss:   1.073892\n",
      "train loss:   1.116170\n",
      "train loss:   0.832243\n",
      "train loss:   1.405809\n",
      "train loss:   1.066433\n",
      "train loss:   0.737305\n",
      "train loss:   0.883295\n",
      "train loss:   0.995583\n",
      "train loss:   0.840944\n",
      "train loss:   1.109382\n",
      "train loss:   1.378047\n",
      "train loss:   0.865261\n",
      "train loss:   0.970700\n",
      "train loss:   1.022881\n",
      "train loss:   1.129959\n",
      "train loss:   0.778115\n",
      "train loss:   0.552409\n",
      "train loss:   0.912598\n",
      "train loss:   0.845907\n",
      "train loss:   0.961621\n",
      "train loss:   0.757043\n",
      "train loss:   0.904465\n",
      "train loss:   1.102230\n",
      "train loss:   0.855799\n",
      "train loss:   0.727926\n",
      "train loss:   0.859336\n",
      "train loss:   1.271288\n",
      "train loss:   1.006099\n",
      "train loss:   1.182433\n",
      "train loss:   0.980179\n",
      "train loss:   0.974185\n",
      "train loss:   1.456576\n",
      "train loss:   0.920810\n",
      "train loss:   1.023319\n",
      "train loss:   1.337197\n",
      "train loss:   0.724505\n",
      "train loss:   0.976885\n",
      "train loss:   1.092811\n",
      "train loss:   0.937799\n",
      "train loss:   0.891156\n",
      "train loss:   0.865430\n",
      "train loss:   1.420591\n",
      "train loss:   0.910001\n",
      "train loss:   0.706629\n",
      "train loss:   1.127302\n",
      "train loss:   1.258090\n",
      "train loss:   1.011970\n",
      "########### epoch 74 ###########\n",
      "########### loop 13900 ###########\n",
      "test loss:   0.354318   test accuracy:   0.916667\n",
      "########### loop 13900 ###########\n",
      "train loss:   1.492175\n",
      "train loss:   1.098811\n",
      "train loss:   1.131888\n",
      "train loss:   1.125567\n",
      "train loss:   1.088675\n",
      "train loss:   1.110171\n",
      "train loss:   0.666064\n",
      "train loss:   0.891021\n",
      "train loss:   1.110637\n",
      "train loss:   1.045053\n",
      "train loss:   1.169924\n",
      "train loss:   1.054662\n",
      "train loss:   1.069023\n",
      "train loss:   0.882622\n",
      "train loss:   1.113715\n",
      "train loss:   1.077168\n",
      "train loss:   0.989313\n",
      "train loss:   1.208974\n",
      "train loss:   0.928266\n",
      "train loss:   1.123426\n",
      "train loss:   1.102774\n",
      "train loss:   0.882819\n",
      "train loss:   0.998075\n",
      "train loss:   1.007693\n",
      "train loss:   1.104842\n",
      "train loss:   1.008878\n",
      "train loss:   1.203079\n",
      "train loss:   0.763047\n",
      "train loss:   1.167319\n",
      "train loss:   1.238769\n",
      "train loss:   1.048552\n",
      "train loss:   0.547787\n",
      "train loss:   0.932609\n",
      "train loss:   0.997037\n",
      "train loss:   1.267926\n",
      "train loss:   0.912330\n",
      "train loss:   0.999105\n",
      "train loss:   1.018920\n",
      "train loss:   0.847726\n",
      "train loss:   0.789591\n",
      "train loss:   0.951508\n",
      "train loss:   0.649268\n",
      "train loss:   1.018713\n",
      "train loss:   0.895657\n",
      "train loss:   0.967162\n",
      "train loss:   0.849460\n",
      "train loss:   0.888082\n",
      "train loss:   0.846492\n",
      "train loss:   0.970269\n",
      "train loss:   0.855784\n",
      "########### epoch 75 ###########\n",
      "########### loop 13950 ###########\n",
      "test loss:   0.405287   test accuracy:   0.916667\n",
      "########### loop 13950 ###########\n",
      "train loss:   0.633828\n",
      "train loss:   1.066772\n",
      "train loss:   0.635083\n",
      "train loss:   0.810240\n",
      "train loss:   0.928437\n",
      "train loss:   0.969738\n",
      "train loss:   1.117499\n",
      "train loss:   1.293295\n",
      "train loss:   0.862889\n",
      "train loss:   1.405058\n",
      "train loss:   1.112818\n",
      "train loss:   0.993208\n",
      "train loss:   0.857513\n",
      "train loss:   0.856219\n",
      "train loss:   1.033837\n",
      "train loss:   1.229932\n",
      "train loss:   1.142339\n",
      "train loss:   0.873200\n",
      "train loss:   1.029483\n",
      "train loss:   0.843264\n",
      "train loss:   1.167509\n",
      "train loss:   0.542901\n",
      "train loss:   1.015159\n",
      "train loss:   0.667728\n",
      "train loss:   1.312133\n",
      "train loss:   1.031967\n",
      "train loss:   0.811347\n",
      "train loss:   1.063364\n",
      "train loss:   0.936970\n",
      "train loss:   0.917864\n",
      "train loss:   1.075169\n",
      "train loss:   1.165121\n",
      "train loss:   0.605223\n",
      "train loss:   0.940073\n",
      "train loss:   1.058291\n",
      "train loss:   1.297909\n",
      "train loss:   0.742298\n",
      "train loss:   0.741758\n",
      "train loss:   0.860804\n",
      "train loss:   1.013789\n",
      "train loss:   0.842480\n",
      "train loss:   1.006852\n",
      "train loss:   1.113978\n",
      "train loss:   0.760011\n",
      "train loss:   0.793108\n",
      "train loss:   0.823519\n",
      "train loss:   0.995695\n",
      "train loss:   0.897988\n",
      "train loss:   0.848020\n",
      "train loss:   0.740378\n",
      "########### epoch 75 ###########\n",
      "########### loop 14000 ###########\n",
      "test loss:   0.311717   test accuracy:   0.958333\n",
      "########### loop 14000 ###########\n",
      "train loss:   1.076172\n",
      "train loss:   1.091846\n",
      "train loss:   1.018785\n",
      "train loss:   0.713713\n",
      "train loss:   1.136637\n",
      "train loss:   1.004202\n",
      "train loss:   1.187909\n",
      "train loss:   1.387167\n",
      "train loss:   0.980375\n",
      "train loss:   1.003739\n",
      "train loss:   0.870913\n",
      "train loss:   1.105453\n",
      "train loss:   0.970682\n",
      "train loss:   0.838777\n",
      "train loss:   0.906257\n",
      "train loss:   1.122074\n",
      "train loss:   1.069848\n",
      "train loss:   1.280186\n",
      "train loss:   1.006287\n",
      "train loss:   1.132141\n",
      "train loss:   0.789391\n",
      "train loss:   0.927321\n",
      "train loss:   1.165603\n",
      "train loss:   1.144328\n",
      "train loss:   0.851495\n",
      "train loss:   0.755497\n",
      "train loss:   0.935542\n",
      "train loss:   0.970229\n",
      "train loss:   0.879032\n",
      "train loss:   1.024883\n",
      "train loss:   1.030061\n",
      "train loss:   1.029676\n",
      "train loss:   0.750287\n",
      "train loss:   0.874421\n",
      "train loss:   1.159132\n",
      "train loss:   0.893865\n",
      "train loss:   0.997751\n",
      "train loss:   1.115505\n",
      "train loss:   0.918001\n",
      "train loss:   0.892034\n",
      "train loss:   1.038791\n",
      "train loss:   0.795883\n",
      "train loss:   0.972452\n",
      "train loss:   1.209165\n",
      "train loss:   0.844872\n",
      "train loss:   1.092473\n",
      "train loss:   0.824070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.912533\n",
      "train loss:   1.014833\n",
      "train loss:   0.949964\n",
      "########### epoch 75 ###########\n",
      "########### loop 14050 ###########\n",
      "test loss:   0.416545   test accuracy:   0.875000\n",
      "########### loop 14050 ###########\n",
      "train loss:   1.211657\n",
      "train loss:   1.171271\n",
      "train loss:   0.770944\n",
      "train loss:   1.213489\n",
      "train loss:   0.837766\n",
      "train loss:   0.745119\n",
      "train loss:   0.647714\n",
      "train loss:   1.044317\n",
      "train loss:   1.433404\n",
      "train loss:   1.036713\n",
      "train loss:   1.001060\n",
      "train loss:   0.752576\n",
      "train loss:   1.249144\n",
      "train loss:   0.866648\n",
      "train loss:   1.014774\n",
      "train loss:   0.970983\n",
      "train loss:   0.776415\n",
      "train loss:   0.834751\n",
      "train loss:   0.805533\n",
      "train loss:   0.733199\n",
      "train loss:   0.824333\n",
      "train loss:   1.000940\n",
      "train loss:   0.962455\n",
      "train loss:   1.095941\n",
      "train loss:   0.962674\n",
      "train loss:   0.767144\n",
      "train loss:   0.754612\n",
      "train loss:   0.749269\n",
      "train loss:   0.848805\n",
      "train loss:   0.692926\n",
      "train loss:   0.936544\n",
      "train loss:   0.770974\n",
      "train loss:   1.236554\n",
      "train loss:   0.882118\n",
      "train loss:   0.970511\n",
      "train loss:   0.832240\n",
      "train loss:   1.226156\n",
      "train loss:   1.119310\n",
      "train loss:   0.768460\n",
      "train loss:   1.428494\n",
      "train loss:   0.989207\n",
      "train loss:   0.836180\n",
      "train loss:   1.068520\n",
      "train loss:   1.265235\n",
      "train loss:   0.781064\n",
      "train loss:   0.920705\n",
      "train loss:   1.134200\n",
      "train loss:   0.892938\n",
      "train loss:   0.906179\n",
      "train loss:   0.874438\n",
      "########### epoch 76 ###########\n",
      "########### loop 14100 ###########\n",
      "test loss:   0.298563   test accuracy:   0.875000\n",
      "########### loop 14100 ###########\n",
      "train loss:   1.032158\n",
      "train loss:   1.160668\n",
      "train loss:   1.163563\n",
      "train loss:   1.128315\n",
      "train loss:   1.154334\n",
      "train loss:   0.638747\n",
      "train loss:   0.804062\n",
      "train loss:   1.356370\n",
      "train loss:   0.947152\n",
      "train loss:   1.126430\n",
      "train loss:   1.034442\n",
      "train loss:   0.827150\n",
      "train loss:   1.244372\n",
      "train loss:   0.846298\n",
      "train loss:   0.719358\n",
      "train loss:   1.009576\n",
      "train loss:   1.384076\n",
      "train loss:   1.003206\n",
      "train loss:   1.040025\n",
      "train loss:   1.024282\n",
      "train loss:   0.975393\n",
      "train loss:   0.737408\n",
      "train loss:   1.194713\n",
      "train loss:   1.028162\n",
      "train loss:   0.915150\n",
      "train loss:   0.981948\n",
      "train loss:   1.260164\n",
      "train loss:   1.058319\n",
      "train loss:   0.957958\n",
      "train loss:   1.123373\n",
      "train loss:   0.677855\n",
      "train loss:   0.482964\n",
      "train loss:   1.320306\n",
      "train loss:   0.953411\n",
      "train loss:   1.053750\n",
      "train loss:   1.399984\n",
      "train loss:   1.027388\n",
      "train loss:   1.050823\n",
      "train loss:   0.961005\n",
      "train loss:   1.014214\n",
      "train loss:   0.837336\n",
      "train loss:   0.798605\n",
      "train loss:   0.953180\n",
      "train loss:   0.901695\n",
      "train loss:   0.922249\n",
      "train loss:   0.768071\n",
      "train loss:   0.930672\n",
      "train loss:   0.488993\n",
      "train loss:   1.132151\n",
      "train loss:   1.070200\n",
      "########### epoch 76 ###########\n",
      "########### loop 14150 ###########\n",
      "test loss:   0.196250   test accuracy:   0.958333\n",
      "########### loop 14150 ###########\n",
      "train loss:   1.190395\n",
      "train loss:   0.816245\n",
      "train loss:   1.095754\n",
      "train loss:   1.122649\n",
      "train loss:   0.949307\n",
      "train loss:   1.377892\n",
      "train loss:   1.196686\n",
      "train loss:   0.898271\n",
      "train loss:   0.934973\n",
      "train loss:   0.936829\n",
      "train loss:   0.802859\n",
      "train loss:   1.301932\n",
      "train loss:   0.374004\n",
      "train loss:   1.042921\n",
      "train loss:   0.831894\n",
      "train loss:   0.854671\n",
      "train loss:   1.162699\n",
      "train loss:   1.024977\n",
      "train loss:   0.855486\n",
      "train loss:   0.922008\n",
      "train loss:   0.928418\n",
      "train loss:   0.918741\n",
      "train loss:   0.620880\n",
      "train loss:   0.862861\n",
      "train loss:   1.507344\n",
      "train loss:   0.988325\n",
      "train loss:   1.082142\n",
      "train loss:   0.778963\n",
      "train loss:   1.085371\n",
      "train loss:   1.394159\n",
      "train loss:   0.938329\n",
      "train loss:   1.016022\n",
      "train loss:   0.847256\n",
      "train loss:   0.996385\n",
      "train loss:   1.057195\n",
      "train loss:   1.256819\n",
      "train loss:   1.010382\n",
      "train loss:   0.947648\n",
      "train loss:   0.574106\n",
      "train loss:   0.810681\n",
      "train loss:   1.034110\n",
      "train loss:   0.775703\n",
      "train loss:   1.048386\n",
      "train loss:   1.245203\n",
      "train loss:   1.332788\n",
      "train loss:   1.113191\n",
      "train loss:   0.493394\n",
      "train loss:   0.896769\n",
      "train loss:   0.885750\n",
      "train loss:   0.798711\n",
      "########### epoch 76 ###########\n",
      "########### loop 14200 ###########\n",
      "test loss:   0.218567   test accuracy:   0.958333\n",
      "########### loop 14200 ###########\n",
      "train loss:   0.547116\n",
      "train loss:   1.092820\n",
      "train loss:   0.995834\n",
      "train loss:   0.946422\n",
      "train loss:   1.227428\n",
      "train loss:   1.246838\n",
      "train loss:   0.851591\n",
      "train loss:   0.793816\n",
      "train loss:   0.628451\n",
      "train loss:   0.737334\n",
      "train loss:   0.900124\n",
      "train loss:   1.202418\n",
      "train loss:   1.442717\n",
      "train loss:   1.108897\n",
      "train loss:   0.868837\n",
      "train loss:   0.902623\n",
      "train loss:   1.132957\n",
      "train loss:   1.047872\n",
      "train loss:   0.884682\n",
      "train loss:   0.847640\n",
      "train loss:   1.449774\n",
      "train loss:   1.133219\n",
      "train loss:   1.053002\n",
      "train loss:   0.963869\n",
      "train loss:   1.050117\n",
      "train loss:   0.953026\n",
      "train loss:   1.241186\n",
      "train loss:   0.973831\n",
      "train loss:   1.211533\n",
      "train loss:   1.271207\n",
      "train loss:   0.901872\n",
      "train loss:   1.266509\n",
      "train loss:   1.341499\n",
      "train loss:   1.122444\n",
      "train loss:   0.813199\n",
      "train loss:   0.723368\n",
      "train loss:   0.844046\n",
      "train loss:   0.654179\n",
      "train loss:   0.804436\n",
      "train loss:   0.759237\n",
      "train loss:   0.995204\n",
      "train loss:   0.701857\n",
      "train loss:   1.024923\n",
      "train loss:   0.783891\n",
      "train loss:   0.719447\n",
      "train loss:   1.201546\n",
      "train loss:   1.089775\n",
      "train loss:   1.144939\n",
      "train loss:   0.893493\n",
      "train loss:   0.645434\n",
      "########### epoch 76 ###########\n",
      "########### loop 14250 ###########\n",
      "test loss:   0.326101   test accuracy:   0.958333\n",
      "########### loop 14250 ###########\n",
      "train loss:   1.163438\n",
      "train loss:   0.968061\n",
      "train loss:   1.050664\n",
      "train loss:   0.920029\n",
      "train loss:   1.009734\n",
      "train loss:   1.063973\n",
      "train loss:   0.942245\n",
      "train loss:   0.856135\n",
      "train loss:   0.748062\n",
      "train loss:   1.142853\n",
      "train loss:   1.066067\n",
      "train loss:   0.837535\n",
      "train loss:   0.687768\n",
      "train loss:   1.000077\n",
      "train loss:   0.881489\n",
      "train loss:   1.084373\n",
      "train loss:   0.836567\n",
      "train loss:   0.890487\n",
      "train loss:   1.140132\n",
      "train loss:   1.325830\n",
      "train loss:   1.303953\n",
      "train loss:   1.142803\n",
      "train loss:   0.614622\n",
      "train loss:   1.052549\n",
      "train loss:   1.263628\n",
      "train loss:   1.148360\n",
      "train loss:   0.873624\n",
      "train loss:   0.775516\n",
      "train loss:   0.982344\n",
      "train loss:   0.927551\n",
      "train loss:   0.911809\n",
      "train loss:   1.110232\n",
      "train loss:   1.130165\n",
      "train loss:   1.346537\n",
      "train loss:   0.935023\n",
      "train loss:   0.936915\n",
      "train loss:   1.208470\n",
      "train loss:   0.848425\n",
      "train loss:   1.025657\n",
      "train loss:   1.176898\n",
      "train loss:   0.650923\n",
      "train loss:   1.282615\n",
      "train loss:   0.699399\n",
      "train loss:   0.768133\n",
      "train loss:   0.825036\n",
      "train loss:   1.140680\n",
      "train loss:   1.021018\n",
      "train loss:   0.924054\n",
      "train loss:   0.701814\n",
      "train loss:   1.277644\n",
      "########### epoch 77 ###########\n",
      "########### loop 14300 ###########\n",
      "test loss:   0.219008   test accuracy:   0.958333\n",
      "########### loop 14300 ###########\n",
      "train loss:   0.756992\n",
      "train loss:   1.132937\n",
      "train loss:   0.841975\n",
      "train loss:   0.698647\n",
      "train loss:   0.533871\n",
      "train loss:   0.683687\n",
      "train loss:   0.737665\n",
      "train loss:   0.923987\n",
      "train loss:   1.034857\n",
      "train loss:   0.617654\n",
      "train loss:   0.966881\n",
      "train loss:   0.921191\n",
      "train loss:   1.181840\n",
      "train loss:   0.821707\n",
      "train loss:   0.678838\n",
      "train loss:   0.876168\n",
      "train loss:   1.026120\n",
      "train loss:   0.970823\n",
      "train loss:   1.077793\n",
      "train loss:   0.782449\n",
      "train loss:   0.762012\n",
      "train loss:   0.898428\n",
      "train loss:   1.091078\n",
      "train loss:   1.086678\n",
      "train loss:   1.084124\n",
      "train loss:   1.167296\n",
      "train loss:   1.103974\n",
      "train loss:   0.949834\n",
      "train loss:   0.930147\n",
      "train loss:   0.720913\n",
      "train loss:   0.969980\n",
      "train loss:   0.818249\n",
      "train loss:   0.976456\n",
      "train loss:   0.976326\n",
      "train loss:   0.803527\n",
      "train loss:   1.041918\n",
      "train loss:   1.149655\n",
      "train loss:   0.919234\n",
      "train loss:   1.180822\n",
      "train loss:   0.968410\n",
      "train loss:   0.510625\n",
      "train loss:   1.040366\n",
      "train loss:   1.118012\n",
      "train loss:   0.832991\n",
      "train loss:   1.131261\n",
      "train loss:   1.003343\n",
      "train loss:   1.038809\n",
      "train loss:   1.237079\n",
      "train loss:   0.822067\n",
      "train loss:   0.927222\n",
      "########### epoch 77 ###########\n",
      "########### loop 14350 ###########\n",
      "test loss:   0.466242   test accuracy:   0.916667\n",
      "########### loop 14350 ###########\n",
      "train loss:   1.058375\n",
      "train loss:   0.934804\n",
      "train loss:   1.198413\n",
      "train loss:   0.916726\n",
      "train loss:   1.201865\n",
      "train loss:   1.099130\n",
      "train loss:   1.214935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.242515\n",
      "train loss:   0.986155\n",
      "train loss:   0.917897\n",
      "train loss:   1.417547\n",
      "train loss:   1.053021\n",
      "train loss:   1.381758\n",
      "train loss:   0.767571\n",
      "train loss:   1.015839\n",
      "train loss:   0.931793\n",
      "train loss:   0.915821\n",
      "train loss:   1.535669\n",
      "train loss:   1.109818\n",
      "train loss:   0.975918\n",
      "train loss:   0.827992\n",
      "train loss:   0.583890\n",
      "train loss:   1.099054\n",
      "train loss:   1.326254\n",
      "train loss:   1.349082\n",
      "train loss:   0.856525\n",
      "train loss:   1.023311\n",
      "train loss:   0.975474\n",
      "train loss:   1.039807\n",
      "train loss:   1.278486\n",
      "train loss:   1.293824\n",
      "train loss:   0.907752\n",
      "train loss:   0.950897\n",
      "train loss:   1.214806\n",
      "train loss:   1.005680\n",
      "train loss:   1.042623\n",
      "train loss:   0.808955\n",
      "train loss:   1.137389\n",
      "train loss:   0.954697\n",
      "train loss:   0.805769\n",
      "train loss:   0.887552\n",
      "train loss:   1.290250\n",
      "train loss:   0.858389\n",
      "train loss:   1.323306\n",
      "train loss:   0.932223\n",
      "train loss:   0.960975\n",
      "train loss:   0.892356\n",
      "train loss:   0.793586\n",
      "train loss:   1.128617\n",
      "train loss:   0.999839\n",
      "########### epoch 77 ###########\n",
      "########### loop 14400 ###########\n",
      "test loss:   0.256764   test accuracy:   0.958333\n",
      "########### loop 14400 ###########\n",
      "train loss:   1.116724\n",
      "train loss:   1.023848\n",
      "train loss:   1.146100\n",
      "train loss:   0.968428\n",
      "train loss:   0.997064\n",
      "train loss:   0.899396\n",
      "train loss:   0.859671\n",
      "train loss:   0.623655\n",
      "train loss:   1.062425\n",
      "train loss:   1.161534\n",
      "train loss:   1.142694\n",
      "train loss:   0.931873\n",
      "train loss:   0.957158\n",
      "train loss:   1.183345\n",
      "train loss:   0.862855\n",
      "train loss:   1.197181\n",
      "train loss:   0.896093\n",
      "train loss:   1.033477\n",
      "train loss:   1.275013\n",
      "train loss:   0.730080\n",
      "train loss:   0.920881\n",
      "train loss:   0.778376\n",
      "train loss:   0.906310\n",
      "train loss:   1.174737\n",
      "train loss:   0.939477\n",
      "train loss:   0.602913\n",
      "train loss:   0.766859\n",
      "train loss:   0.647021\n",
      "train loss:   0.450871\n",
      "train loss:   1.449139\n",
      "train loss:   1.123091\n",
      "train loss:   1.002163\n",
      "train loss:   0.821020\n",
      "train loss:   0.895200\n",
      "train loss:   0.922144\n",
      "train loss:   1.108677\n",
      "train loss:   0.774315\n",
      "train loss:   0.799707\n",
      "train loss:   1.382762\n",
      "train loss:   1.143598\n",
      "train loss:   0.884506\n",
      "train loss:   0.605950\n",
      "train loss:   0.704920\n",
      "train loss:   1.031167\n",
      "train loss:   0.893250\n",
      "train loss:   0.997419\n",
      "train loss:   0.912181\n",
      "train loss:   0.848597\n",
      "train loss:   0.954643\n",
      "train loss:   1.364320\n",
      "########### epoch 77 ###########\n",
      "########### loop 14450 ###########\n",
      "test loss:   0.201681   test accuracy:   0.958333\n",
      "########### loop 14450 ###########\n",
      "train loss:   0.909512\n",
      "train loss:   0.925689\n",
      "train loss:   1.032184\n",
      "train loss:   1.124057\n",
      "train loss:   0.518345\n",
      "train loss:   1.227992\n",
      "train loss:   0.931662\n",
      "train loss:   0.784747\n",
      "train loss:   0.595773\n",
      "train loss:   1.189711\n",
      "train loss:   0.664575\n",
      "train loss:   0.984223\n",
      "train loss:   0.982335\n",
      "train loss:   0.958100\n",
      "train loss:   1.087963\n",
      "train loss:   0.908399\n",
      "train loss:   1.014437\n",
      "train loss:   1.055426\n",
      "train loss:   0.950876\n",
      "train loss:   0.936771\n",
      "train loss:   0.789364\n",
      "train loss:   0.611934\n",
      "train loss:   1.420204\n",
      "train loss:   0.874482\n",
      "train loss:   0.972704\n",
      "train loss:   0.581631\n",
      "train loss:   0.656286\n",
      "train loss:   1.183123\n",
      "train loss:   0.937119\n",
      "train loss:   0.587155\n",
      "train loss:   1.017994\n",
      "train loss:   0.646442\n",
      "train loss:   1.073001\n",
      "train loss:   1.077328\n",
      "train loss:   0.898690\n",
      "train loss:   0.748862\n",
      "train loss:   0.939672\n",
      "train loss:   0.797659\n",
      "train loss:   1.295789\n",
      "train loss:   0.910681\n",
      "train loss:   1.114732\n",
      "train loss:   0.853056\n",
      "train loss:   1.015836\n",
      "train loss:   1.075389\n",
      "train loss:   0.890494\n",
      "train loss:   0.847358\n",
      "train loss:   0.893050\n",
      "train loss:   0.592239\n",
      "train loss:   1.316659\n",
      "train loss:   1.131174\n",
      "########### epoch 78 ###########\n",
      "########### loop 14500 ###########\n",
      "test loss:   0.236913   test accuracy:   0.958333\n",
      "########### loop 14500 ###########\n",
      "train loss:   1.047903\n",
      "train loss:   0.870886\n",
      "train loss:   0.989904\n",
      "train loss:   1.145670\n",
      "train loss:   0.939499\n",
      "train loss:   0.994553\n",
      "train loss:   0.794016\n",
      "train loss:   1.253323\n",
      "train loss:   0.781123\n",
      "train loss:   1.331346\n",
      "train loss:   0.649336\n",
      "train loss:   0.990369\n",
      "train loss:   1.059295\n",
      "train loss:   1.347281\n",
      "train loss:   0.933936\n",
      "train loss:   1.013983\n",
      "train loss:   1.169554\n",
      "train loss:   0.776641\n",
      "train loss:   1.362144\n",
      "train loss:   0.912823\n",
      "train loss:   0.796285\n",
      "train loss:   0.565053\n",
      "train loss:   1.120793\n",
      "train loss:   1.421667\n",
      "train loss:   1.191799\n",
      "train loss:   0.873053\n",
      "train loss:   1.480088\n",
      "train loss:   0.906999\n",
      "train loss:   0.985956\n",
      "train loss:   0.972634\n",
      "train loss:   0.612884\n",
      "train loss:   0.643398\n",
      "train loss:   0.803151\n",
      "train loss:   0.916995\n",
      "train loss:   0.963055\n",
      "train loss:   0.985841\n",
      "train loss:   0.720084\n",
      "train loss:   1.063121\n",
      "train loss:   1.187583\n",
      "train loss:   0.998773\n",
      "train loss:   1.218199\n",
      "train loss:   0.770147\n",
      "train loss:   0.707028\n",
      "train loss:   0.778723\n",
      "train loss:   0.971096\n",
      "train loss:   0.779556\n",
      "train loss:   1.244067\n",
      "train loss:   0.953640\n",
      "train loss:   0.880750\n",
      "train loss:   1.173842\n",
      "########### epoch 78 ###########\n",
      "########### loop 14550 ###########\n",
      "test loss:   0.217335   test accuracy:   0.958333\n",
      "########### loop 14550 ###########\n",
      "train loss:   0.993843\n",
      "train loss:   1.026151\n",
      "train loss:   0.832780\n",
      "train loss:   0.964520\n",
      "train loss:   0.868971\n",
      "train loss:   0.936552\n",
      "train loss:   1.090987\n",
      "train loss:   1.224490\n",
      "train loss:   1.005645\n",
      "train loss:   1.187132\n",
      "train loss:   1.088137\n",
      "train loss:   1.109267\n",
      "train loss:   1.409078\n",
      "train loss:   0.960727\n",
      "train loss:   0.600911\n",
      "train loss:   0.667867\n",
      "train loss:   1.074162\n",
      "train loss:   1.098194\n",
      "train loss:   0.986298\n",
      "train loss:   0.778246\n",
      "train loss:   1.097674\n",
      "train loss:   0.735192\n",
      "train loss:   1.106673\n",
      "train loss:   1.073206\n",
      "train loss:   1.243641\n",
      "train loss:   1.034626\n",
      "train loss:   1.009497\n",
      "train loss:   0.842406\n",
      "train loss:   0.923072\n",
      "train loss:   1.077803\n",
      "train loss:   0.761516\n",
      "train loss:   0.798315\n",
      "train loss:   0.778481\n",
      "train loss:   1.087643\n",
      "train loss:   1.082242\n",
      "train loss:   1.153817\n",
      "train loss:   0.950790\n",
      "train loss:   0.861815\n",
      "train loss:   0.842917\n",
      "train loss:   0.983773\n",
      "train loss:   1.119475\n",
      "train loss:   1.028894\n",
      "train loss:   1.112979\n",
      "train loss:   0.986983\n",
      "train loss:   1.274785\n",
      "train loss:   0.987607\n",
      "train loss:   0.992948\n",
      "train loss:   0.964181\n",
      "train loss:   0.809456\n",
      "train loss:   0.981390\n",
      "########### epoch 78 ###########\n",
      "########### loop 14600 ###########\n",
      "test loss:   0.158794   test accuracy:   1.000000\n",
      "########### loop 14600 ###########\n",
      "train loss:   1.171278\n",
      "train loss:   0.959939\n",
      "train loss:   0.814974\n",
      "train loss:   1.174790\n",
      "train loss:   1.170347\n",
      "train loss:   0.619115\n",
      "train loss:   1.099176\n",
      "train loss:   0.875329\n",
      "train loss:   1.021740\n",
      "train loss:   1.203575\n",
      "train loss:   0.973943\n",
      "train loss:   1.145094\n",
      "train loss:   1.004189\n",
      "train loss:   0.945495\n",
      "train loss:   0.964829\n",
      "train loss:   1.074524\n",
      "train loss:   0.948255\n",
      "train loss:   0.878995\n",
      "train loss:   0.942133\n",
      "train loss:   0.937163\n",
      "train loss:   0.724252\n",
      "train loss:   1.152509\n",
      "train loss:   0.967541\n",
      "train loss:   0.937993\n",
      "train loss:   0.929066\n",
      "train loss:   1.173602\n",
      "train loss:   1.063581\n",
      "train loss:   1.132529\n",
      "train loss:   1.022216\n",
      "train loss:   1.060205\n",
      "train loss:   0.983904\n",
      "train loss:   0.867738\n",
      "train loss:   0.576773\n",
      "train loss:   0.985808\n",
      "train loss:   0.846177\n",
      "train loss:   0.969413\n",
      "train loss:   0.993550\n",
      "train loss:   1.000072\n",
      "train loss:   1.031008\n",
      "train loss:   0.539591\n",
      "train loss:   1.235636\n",
      "train loss:   0.887044\n",
      "train loss:   0.840293\n",
      "train loss:   1.398895\n",
      "train loss:   1.159898\n",
      "train loss:   1.211851\n",
      "train loss:   1.373312\n",
      "train loss:   0.852091\n",
      "train loss:   0.983139\n",
      "train loss:   0.976448\n",
      "########### epoch 78 ###########\n",
      "########### loop 14650 ###########\n",
      "test loss:   0.254721   test accuracy:   0.916667\n",
      "########### loop 14650 ###########\n",
      "train loss:   0.735531\n",
      "train loss:   1.085830\n",
      "train loss:   1.110472\n",
      "train loss:   1.073384\n",
      "train loss:   0.757733\n",
      "train loss:   0.998386\n",
      "train loss:   0.679761\n",
      "train loss:   1.036440\n",
      "train loss:   1.012667\n",
      "train loss:   0.991860\n",
      "train loss:   1.340996\n",
      "train loss:   0.998995\n",
      "train loss:   1.111562\n",
      "train loss:   0.984666\n",
      "train loss:   1.035355\n",
      "train loss:   0.930482\n",
      "train loss:   0.873881\n",
      "train loss:   1.401461\n",
      "train loss:   1.115575\n",
      "train loss:   1.214570\n",
      "train loss:   1.238485\n",
      "train loss:   0.774898\n",
      "train loss:   0.963245\n",
      "train loss:   1.064939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.857576\n",
      "train loss:   0.984889\n",
      "train loss:   0.815668\n",
      "train loss:   0.894575\n",
      "train loss:   1.517377\n",
      "train loss:   1.391195\n",
      "train loss:   1.105997\n",
      "train loss:   0.855303\n",
      "train loss:   0.927453\n",
      "train loss:   1.072151\n",
      "train loss:   1.419473\n",
      "train loss:   1.181163\n",
      "train loss:   1.331655\n",
      "train loss:   0.685805\n",
      "train loss:   1.145090\n",
      "train loss:   0.767216\n",
      "train loss:   0.637628\n",
      "train loss:   1.227713\n",
      "train loss:   1.017716\n",
      "train loss:   0.944304\n",
      "train loss:   0.781442\n",
      "train loss:   0.887491\n",
      "train loss:   1.204662\n",
      "train loss:   0.672509\n",
      "train loss:   1.147784\n",
      "train loss:   0.729418\n",
      "########### epoch 79 ###########\n",
      "########### loop 14700 ###########\n",
      "test loss:   0.232032   test accuracy:   0.916667\n",
      "########### loop 14700 ###########\n",
      "train loss:   1.094521\n",
      "train loss:   0.923926\n",
      "train loss:   1.063405\n",
      "train loss:   0.747642\n",
      "train loss:   0.992045\n",
      "train loss:   1.036647\n",
      "train loss:   1.105266\n",
      "train loss:   1.058211\n",
      "train loss:   0.976865\n",
      "train loss:   1.152259\n",
      "train loss:   1.128836\n",
      "train loss:   0.887391\n",
      "train loss:   1.048755\n",
      "train loss:   1.310069\n",
      "train loss:   0.974081\n",
      "train loss:   0.869488\n",
      "train loss:   1.274715\n",
      "train loss:   1.170528\n",
      "train loss:   1.378199\n",
      "train loss:   1.474590\n",
      "train loss:   0.794202\n",
      "train loss:   0.898656\n",
      "train loss:   0.979119\n",
      "train loss:   1.037961\n",
      "train loss:   1.088647\n",
      "train loss:   1.274799\n",
      "train loss:   1.295101\n",
      "train loss:   0.740977\n",
      "train loss:   0.562352\n",
      "train loss:   0.832871\n",
      "train loss:   1.243025\n",
      "train loss:   1.197553\n",
      "train loss:   1.062342\n",
      "train loss:   0.925740\n",
      "train loss:   0.842024\n",
      "train loss:   1.192566\n",
      "train loss:   1.579185\n",
      "train loss:   0.795616\n",
      "train loss:   1.505369\n",
      "train loss:   1.188895\n",
      "train loss:   0.964520\n",
      "train loss:   0.849789\n",
      "train loss:   1.049808\n",
      "train loss:   0.746455\n",
      "train loss:   0.922982\n",
      "train loss:   1.055464\n",
      "train loss:   0.928353\n",
      "train loss:   1.422971\n",
      "train loss:   1.052338\n",
      "train loss:   0.811019\n",
      "########### epoch 79 ###########\n",
      "########### loop 14750 ###########\n",
      "test loss:   0.196257   test accuracy:   1.000000\n",
      "########### loop 14750 ###########\n",
      "train loss:   0.924965\n",
      "train loss:   0.999546\n",
      "train loss:   1.278105\n",
      "train loss:   0.911579\n",
      "train loss:   0.991498\n",
      "train loss:   1.216417\n",
      "train loss:   1.024634\n",
      "train loss:   0.897943\n",
      "train loss:   0.875862\n",
      "train loss:   1.108133\n",
      "train loss:   0.844881\n",
      "train loss:   0.516875\n",
      "train loss:   1.292484\n",
      "train loss:   0.780917\n",
      "train loss:   0.837805\n",
      "train loss:   0.854041\n",
      "train loss:   1.128710\n",
      "train loss:   0.858430\n",
      "train loss:   1.041845\n",
      "train loss:   0.957660\n",
      "train loss:   1.238426\n",
      "train loss:   1.314006\n",
      "train loss:   1.110593\n",
      "train loss:   0.777833\n",
      "train loss:   1.255387\n",
      "train loss:   0.966455\n",
      "train loss:   1.022672\n",
      "train loss:   0.915145\n",
      "train loss:   0.816242\n",
      "train loss:   0.919025\n",
      "train loss:   0.856027\n",
      "train loss:   1.163886\n",
      "train loss:   0.808259\n",
      "train loss:   0.828001\n",
      "train loss:   1.570470\n",
      "train loss:   1.209438\n",
      "train loss:   0.907292\n",
      "train loss:   1.181191\n",
      "train loss:   0.944652\n",
      "train loss:   1.042522\n",
      "train loss:   0.979601\n",
      "train loss:   0.832447\n",
      "train loss:   1.202745\n",
      "train loss:   1.004206\n",
      "train loss:   1.275461\n",
      "train loss:   0.897172\n",
      "train loss:   0.883362\n",
      "train loss:   1.064791\n",
      "train loss:   0.986335\n",
      "train loss:   1.045630\n",
      "########### epoch 79 ###########\n",
      "########### loop 14800 ###########\n",
      "test loss:   0.295352   test accuracy:   0.916667\n",
      "########### loop 14800 ###########\n",
      "train loss:   0.592686\n",
      "train loss:   0.893001\n",
      "train loss:   0.895792\n",
      "train loss:   0.878780\n",
      "train loss:   0.870924\n",
      "train loss:   0.847119\n",
      "train loss:   1.059000\n",
      "train loss:   1.024537\n",
      "train loss:   1.036885\n",
      "train loss:   1.281061\n",
      "train loss:   1.055829\n",
      "train loss:   0.671510\n",
      "train loss:   1.095711\n",
      "train loss:   0.917209\n",
      "train loss:   0.712458\n",
      "train loss:   0.989682\n",
      "train loss:   0.961912\n",
      "train loss:   0.758848\n",
      "train loss:   0.995755\n",
      "train loss:   0.940871\n",
      "train loss:   0.536293\n",
      "train loss:   0.866350\n",
      "train loss:   1.039801\n",
      "train loss:   1.000571\n",
      "train loss:   1.047789\n",
      "train loss:   1.144753\n",
      "train loss:   1.328618\n",
      "train loss:   1.189825\n",
      "train loss:   1.248159\n",
      "train loss:   0.942892\n",
      "train loss:   1.163724\n",
      "train loss:   0.954150\n",
      "train loss:   1.161247\n",
      "train loss:   1.126537\n",
      "train loss:   1.077317\n",
      "train loss:   1.232578\n",
      "train loss:   1.028358\n",
      "train loss:   0.807812\n",
      "train loss:   1.096377\n",
      "train loss:   0.774246\n",
      "train loss:   0.943160\n",
      "train loss:   1.164787\n",
      "train loss:   0.628048\n",
      "train loss:   0.879975\n",
      "train loss:   0.982115\n",
      "train loss:   0.766331\n",
      "train loss:   1.406406\n",
      "train loss:   1.035558\n",
      "train loss:   0.925946\n",
      "train loss:   1.272381\n",
      "########### epoch 79 ###########\n",
      "########### loop 14850 ###########\n",
      "test loss:   0.144663   test accuracy:   1.000000\n",
      "########### loop 14850 ###########\n",
      "train loss:   0.678527\n",
      "train loss:   1.210643\n",
      "train loss:   1.030693\n",
      "train loss:   1.144211\n",
      "train loss:   1.260473\n",
      "train loss:   0.902032\n",
      "train loss:   1.235449\n",
      "train loss:   0.926401\n",
      "train loss:   1.211455\n",
      "train loss:   0.594342\n",
      "train loss:   0.952916\n",
      "train loss:   0.988801\n",
      "train loss:   1.116009\n",
      "train loss:   0.847697\n",
      "train loss:   1.173346\n",
      "train loss:   1.318595\n",
      "train loss:   0.847555\n",
      "train loss:   0.847124\n",
      "train loss:   1.023299\n",
      "train loss:   0.684670\n",
      "train loss:   1.003093\n",
      "train loss:   0.968255\n",
      "train loss:   1.242566\n",
      "train loss:   1.249101\n",
      "train loss:   1.020042\n",
      "train loss:   0.998055\n",
      "train loss:   1.117090\n",
      "train loss:   0.697235\n",
      "train loss:   1.037919\n",
      "train loss:   0.677708\n",
      "train loss:   1.123561\n",
      "train loss:   1.006850\n",
      "train loss:   1.069360\n",
      "train loss:   1.060883\n",
      "train loss:   1.111582\n",
      "train loss:   0.981177\n",
      "train loss:   1.136274\n",
      "train loss:   1.151218\n",
      "train loss:   0.940690\n",
      "train loss:   1.140323\n",
      "train loss:   1.101177\n",
      "train loss:   0.880520\n",
      "train loss:   0.758333\n",
      "train loss:   1.067027\n",
      "train loss:   1.023714\n",
      "train loss:   0.615118\n",
      "train loss:   1.155727\n",
      "train loss:   1.069283\n",
      "train loss:   1.141742\n",
      "train loss:   0.972099\n",
      "########### epoch 80 ###########\n",
      "########### loop 14900 ###########\n",
      "test loss:   0.408914   test accuracy:   0.875000\n",
      "########### loop 14900 ###########\n",
      "train loss:   1.436431\n",
      "train loss:   1.276540\n",
      "train loss:   1.137638\n",
      "train loss:   0.943554\n",
      "train loss:   1.233496\n",
      "train loss:   0.981259\n",
      "train loss:   0.737290\n",
      "train loss:   1.000668\n",
      "train loss:   1.138793\n",
      "train loss:   1.170376\n",
      "train loss:   0.977518\n",
      "train loss:   0.948755\n",
      "train loss:   1.107849\n",
      "train loss:   0.918538\n",
      "train loss:   1.082393\n",
      "train loss:   0.972372\n",
      "train loss:   0.915326\n",
      "train loss:   0.971374\n",
      "train loss:   0.976367\n",
      "train loss:   0.964071\n",
      "train loss:   0.964527\n",
      "train loss:   1.122449\n",
      "train loss:   1.018173\n",
      "train loss:   0.856176\n",
      "train loss:   1.308896\n",
      "train loss:   1.264667\n",
      "train loss:   1.073301\n",
      "train loss:   0.835064\n",
      "train loss:   1.134171\n",
      "train loss:   0.914185\n",
      "train loss:   1.147990\n",
      "train loss:   0.928145\n",
      "train loss:   0.909405\n",
      "train loss:   0.906935\n",
      "train loss:   1.273366\n",
      "train loss:   1.000521\n",
      "train loss:   0.870647\n",
      "train loss:   1.336192\n",
      "train loss:   0.998361\n",
      "train loss:   0.951645\n",
      "train loss:   0.876762\n",
      "train loss:   0.992377\n",
      "train loss:   0.990542\n",
      "train loss:   1.289909\n",
      "train loss:   0.974615\n",
      "train loss:   1.022571\n",
      "train loss:   0.943867\n",
      "train loss:   0.678101\n",
      "train loss:   0.950086\n",
      "train loss:   0.919992\n",
      "########### epoch 80 ###########\n",
      "########### loop 14950 ###########\n",
      "test loss:   0.353181   test accuracy:   0.916667\n",
      "########### loop 14950 ###########\n",
      "train loss:   1.088037\n",
      "train loss:   0.796735\n",
      "train loss:   1.132499\n",
      "train loss:   1.140457\n",
      "train loss:   1.054523\n",
      "train loss:   0.927433\n",
      "train loss:   1.023779\n",
      "train loss:   1.045937\n",
      "train loss:   0.882527\n",
      "train loss:   0.879299\n",
      "train loss:   1.051276\n",
      "train loss:   1.152218\n",
      "train loss:   1.126762\n",
      "train loss:   1.067647\n",
      "train loss:   0.942128\n",
      "train loss:   0.999873\n",
      "train loss:   1.000017\n",
      "train loss:   1.270056\n",
      "train loss:   1.177403\n",
      "train loss:   1.325369\n",
      "train loss:   0.655759\n",
      "train loss:   1.000068\n",
      "train loss:   0.791833\n",
      "train loss:   0.973413\n",
      "train loss:   1.043485\n",
      "train loss:   0.712384\n",
      "train loss:   1.333780\n",
      "train loss:   1.088486\n",
      "train loss:   1.167303\n",
      "train loss:   0.743566\n",
      "train loss:   0.876590\n",
      "train loss:   0.919129\n",
      "train loss:   1.226263\n",
      "train loss:   1.090471\n",
      "train loss:   1.036580\n",
      "train loss:   1.001195\n",
      "train loss:   1.158499\n",
      "train loss:   1.173003\n",
      "train loss:   0.716211\n",
      "train loss:   1.015150\n",
      "train loss:   1.147713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.089710\n",
      "train loss:   0.863088\n",
      "train loss:   1.201963\n",
      "train loss:   0.895387\n",
      "train loss:   0.702574\n",
      "train loss:   1.263937\n",
      "train loss:   0.807987\n",
      "train loss:   0.699808\n",
      "train loss:   0.938298\n",
      "########### epoch 80 ###########\n",
      "########### loop 15000 ###########\n",
      "test loss:   0.279531   test accuracy:   0.958333\n",
      "########### loop 15000 ###########\n",
      "train loss:   0.850945\n",
      "train loss:   1.099171\n",
      "train loss:   1.005978\n",
      "train loss:   1.040383\n",
      "train loss:   0.938103\n",
      "train loss:   1.167014\n",
      "train loss:   1.237304\n",
      "train loss:   1.052203\n",
      "train loss:   0.935380\n",
      "train loss:   1.013178\n",
      "train loss:   0.933886\n",
      "train loss:   1.217554\n",
      "train loss:   0.824801\n",
      "train loss:   0.878239\n",
      "train loss:   1.137779\n",
      "train loss:   1.256354\n",
      "train loss:   0.931404\n",
      "train loss:   0.720497\n",
      "train loss:   0.840543\n",
      "train loss:   0.797296\n",
      "train loss:   0.878855\n",
      "train loss:   0.887202\n",
      "train loss:   1.156380\n",
      "train loss:   1.183799\n",
      "train loss:   0.798536\n",
      "train loss:   0.699322\n",
      "train loss:   0.908868\n",
      "train loss:   0.959606\n",
      "train loss:   0.816134\n",
      "train loss:   1.133274\n",
      "train loss:   0.949057\n",
      "train loss:   0.689525\n",
      "train loss:   0.943365\n",
      "train loss:   0.830749\n",
      "train loss:   1.032090\n",
      "train loss:   1.025300\n",
      "train loss:   0.927158\n",
      "train loss:   1.283667\n",
      "train loss:   0.516769\n",
      "train loss:   1.166908\n",
      "train loss:   0.924902\n",
      "train loss:   0.963742\n",
      "train loss:   0.861193\n",
      "train loss:   1.015779\n",
      "train loss:   0.920525\n",
      "train loss:   0.853440\n",
      "train loss:   0.869509\n",
      "train loss:   0.765282\n",
      "train loss:   1.012405\n",
      "train loss:   1.058869\n",
      "########### epoch 81 ###########\n",
      "########### loop 15050 ###########\n",
      "test loss:   0.238849   test accuracy:   0.958333\n",
      "########### loop 15050 ###########\n",
      "train loss:   0.901384\n",
      "train loss:   0.981453\n",
      "train loss:   1.139108\n",
      "train loss:   1.330788\n",
      "train loss:   0.848446\n",
      "train loss:   1.176795\n",
      "train loss:   0.831421\n",
      "train loss:   1.096341\n",
      "train loss:   0.969845\n",
      "train loss:   0.608425\n",
      "train loss:   0.993387\n",
      "train loss:   0.979345\n",
      "train loss:   0.966833\n",
      "train loss:   1.410549\n",
      "train loss:   1.202211\n",
      "train loss:   0.908474\n",
      "train loss:   1.105292\n",
      "train loss:   1.135283\n",
      "train loss:   1.057056\n",
      "train loss:   0.999158\n",
      "train loss:   0.769417\n",
      "train loss:   0.886184\n",
      "train loss:   1.010996\n",
      "train loss:   0.806733\n",
      "train loss:   1.019494\n",
      "train loss:   0.889345\n",
      "train loss:   0.702881\n",
      "train loss:   1.203489\n",
      "train loss:   0.760367\n",
      "train loss:   1.014246\n",
      "train loss:   0.864774\n",
      "train loss:   0.931211\n",
      "train loss:   1.088540\n",
      "train loss:   1.062153\n",
      "train loss:   1.099691\n",
      "train loss:   1.151325\n",
      "train loss:   0.845117\n",
      "train loss:   0.836125\n",
      "train loss:   1.184353\n",
      "train loss:   1.093548\n",
      "train loss:   0.987403\n",
      "train loss:   1.078652\n",
      "train loss:   1.129877\n",
      "train loss:   0.650811\n",
      "train loss:   1.180690\n",
      "train loss:   0.958222\n",
      "train loss:   1.002327\n",
      "train loss:   1.220161\n",
      "train loss:   1.159736\n",
      "train loss:   1.189022\n",
      "########### epoch 81 ###########\n",
      "########### loop 15100 ###########\n",
      "test loss:   0.229033   test accuracy:   0.916667\n",
      "########### loop 15100 ###########\n",
      "train loss:   0.596222\n",
      "train loss:   0.834112\n",
      "train loss:   1.238699\n",
      "train loss:   1.016505\n",
      "train loss:   0.946015\n",
      "train loss:   0.939541\n",
      "train loss:   1.142890\n",
      "train loss:   1.106491\n",
      "train loss:   0.592081\n",
      "train loss:   1.131753\n",
      "train loss:   0.898711\n",
      "train loss:   0.745651\n",
      "train loss:   1.127493\n",
      "train loss:   0.869521\n",
      "train loss:   0.686740\n",
      "train loss:   1.046069\n",
      "train loss:   1.289952\n",
      "train loss:   1.106149\n",
      "train loss:   1.081677\n",
      "train loss:   0.775266\n",
      "train loss:   1.194248\n",
      "train loss:   1.234013\n",
      "train loss:   1.005119\n",
      "train loss:   0.804956\n",
      "train loss:   1.064499\n",
      "train loss:   1.221942\n",
      "train loss:   0.927981\n",
      "train loss:   1.164564\n",
      "train loss:   1.161327\n",
      "train loss:   0.770010\n",
      "train loss:   1.027850\n",
      "train loss:   0.918042\n",
      "train loss:   0.922283\n",
      "train loss:   1.190884\n",
      "train loss:   0.772691\n",
      "train loss:   1.033427\n",
      "train loss:   1.089272\n",
      "train loss:   1.252299\n",
      "train loss:   1.204474\n",
      "train loss:   1.140841\n",
      "train loss:   1.177101\n",
      "train loss:   1.176185\n",
      "train loss:   0.991663\n",
      "train loss:   0.970565\n",
      "train loss:   0.765137\n",
      "train loss:   0.965855\n",
      "train loss:   0.932699\n",
      "train loss:   1.139190\n",
      "train loss:   0.989741\n",
      "train loss:   0.719473\n",
      "########### epoch 81 ###########\n",
      "########### loop 15150 ###########\n",
      "test loss:   0.128230   test accuracy:   1.000000\n",
      "########### loop 15150 ###########\n",
      "train loss:   1.087132\n",
      "train loss:   0.662291\n",
      "train loss:   1.436030\n",
      "train loss:   0.864711\n",
      "train loss:   1.216689\n",
      "train loss:   0.899272\n",
      "train loss:   1.250831\n",
      "train loss:   1.353777\n",
      "train loss:   1.284276\n",
      "train loss:   1.161962\n",
      "train loss:   1.133105\n",
      "train loss:   1.139362\n",
      "train loss:   0.833149\n",
      "train loss:   0.853871\n",
      "train loss:   1.153556\n",
      "train loss:   0.798427\n",
      "train loss:   1.422508\n",
      "train loss:   0.805996\n",
      "train loss:   0.905114\n",
      "train loss:   1.303197\n",
      "train loss:   0.982035\n",
      "train loss:   0.913713\n",
      "train loss:   1.176627\n",
      "train loss:   1.043137\n",
      "train loss:   1.032857\n",
      "train loss:   1.102275\n",
      "train loss:   1.197667\n",
      "train loss:   1.352417\n",
      "train loss:   1.191287\n",
      "train loss:   0.929879\n",
      "train loss:   1.090518\n",
      "train loss:   0.531467\n",
      "train loss:   0.721830\n",
      "train loss:   0.892089\n",
      "train loss:   1.182274\n",
      "train loss:   1.262227\n",
      "train loss:   0.866982\n",
      "train loss:   0.748380\n",
      "train loss:   1.218277\n",
      "train loss:   1.141527\n",
      "train loss:   0.921692\n",
      "train loss:   0.966270\n",
      "train loss:   0.972192\n",
      "train loss:   1.052608\n",
      "train loss:   1.026390\n",
      "train loss:   1.000141\n",
      "train loss:   0.602107\n",
      "train loss:   0.787848\n",
      "train loss:   1.025349\n",
      "train loss:   1.066878\n",
      "########### epoch 81 ###########\n",
      "########### loop 15200 ###########\n",
      "test loss:   0.250571   test accuracy:   0.958333\n",
      "########### loop 15200 ###########\n",
      "train loss:   0.733998\n",
      "train loss:   1.247305\n",
      "train loss:   0.876458\n",
      "train loss:   1.101024\n",
      "train loss:   0.842037\n",
      "train loss:   0.970540\n",
      "train loss:   1.281421\n",
      "train loss:   0.716473\n",
      "train loss:   0.704275\n",
      "train loss:   1.050202\n",
      "train loss:   0.904441\n",
      "train loss:   0.937975\n",
      "train loss:   0.927351\n",
      "train loss:   0.971962\n",
      "train loss:   0.942478\n",
      "train loss:   0.854679\n",
      "train loss:   0.826990\n",
      "train loss:   0.774950\n",
      "train loss:   1.255740\n",
      "train loss:   1.167004\n",
      "train loss:   1.262386\n",
      "train loss:   1.324501\n",
      "train loss:   1.045233\n",
      "train loss:   1.273313\n",
      "train loss:   0.980590\n",
      "train loss:   0.854043\n",
      "train loss:   0.708796\n",
      "train loss:   1.112571\n",
      "train loss:   1.066018\n",
      "train loss:   0.972088\n",
      "train loss:   0.599805\n",
      "train loss:   1.099181\n",
      "train loss:   0.975977\n",
      "train loss:   0.804839\n",
      "train loss:   0.884995\n",
      "train loss:   1.041511\n",
      "train loss:   0.939697\n",
      "train loss:   1.121646\n",
      "train loss:   1.112002\n",
      "train loss:   0.860913\n",
      "train loss:   0.791234\n",
      "train loss:   1.251102\n",
      "train loss:   1.044185\n",
      "train loss:   0.987007\n",
      "train loss:   1.044888\n",
      "train loss:   0.868208\n",
      "train loss:   1.047087\n",
      "train loss:   0.790527\n",
      "train loss:   1.080055\n",
      "train loss:   0.795752\n",
      "########### epoch 82 ###########\n",
      "########### loop 15250 ###########\n",
      "test loss:   0.116368   test accuracy:   1.000000\n",
      "########### loop 15250 ###########\n",
      "train loss:   1.312662\n",
      "train loss:   0.977520\n",
      "train loss:   0.762011\n",
      "train loss:   0.739069\n",
      "train loss:   1.117007\n",
      "train loss:   1.234183\n",
      "train loss:   0.669799\n",
      "train loss:   0.776513\n",
      "train loss:   1.071214\n",
      "train loss:   0.989070\n",
      "train loss:   0.999599\n",
      "train loss:   0.726587\n",
      "train loss:   0.789819\n",
      "train loss:   1.155515\n",
      "train loss:   1.063734\n",
      "train loss:   0.671680\n",
      "train loss:   1.000186\n",
      "train loss:   1.166857\n",
      "train loss:   0.924631\n",
      "train loss:   0.772860\n",
      "train loss:   0.858370\n",
      "train loss:   0.578877\n",
      "train loss:   1.066508\n",
      "train loss:   1.136476\n",
      "train loss:   1.052858\n",
      "train loss:   1.011370\n",
      "train loss:   1.115906\n",
      "train loss:   1.227768\n",
      "train loss:   1.166778\n",
      "train loss:   1.280370\n",
      "train loss:   0.930308\n",
      "train loss:   0.973948\n",
      "train loss:   0.935803\n",
      "train loss:   1.086698\n",
      "train loss:   0.496918\n",
      "train loss:   0.894685\n",
      "train loss:   0.742007\n",
      "train loss:   1.135247\n",
      "train loss:   0.869196\n",
      "train loss:   0.760967\n",
      "train loss:   1.490478\n",
      "train loss:   1.031934\n",
      "train loss:   0.959582\n",
      "train loss:   1.160611\n",
      "train loss:   0.886456\n",
      "train loss:   0.877760\n",
      "train loss:   0.717180\n",
      "train loss:   0.780288\n",
      "train loss:   0.899412\n",
      "train loss:   0.644232\n",
      "########### epoch 82 ###########\n",
      "########### loop 15300 ###########\n",
      "test loss:   0.254904   test accuracy:   0.916667\n",
      "########### loop 15300 ###########\n",
      "train loss:   1.176288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.907859\n",
      "train loss:   0.728071\n",
      "train loss:   1.041743\n",
      "train loss:   0.881680\n",
      "train loss:   1.173252\n",
      "train loss:   0.790384\n",
      "train loss:   0.979602\n",
      "train loss:   0.810961\n",
      "train loss:   1.203822\n",
      "train loss:   1.012646\n",
      "train loss:   1.215696\n",
      "train loss:   0.911364\n",
      "train loss:   1.270566\n",
      "train loss:   1.061332\n",
      "train loss:   1.226277\n",
      "train loss:   0.870568\n",
      "train loss:   1.266730\n",
      "train loss:   0.995211\n",
      "train loss:   0.978546\n",
      "train loss:   1.118221\n",
      "train loss:   0.597308\n",
      "train loss:   0.960852\n",
      "train loss:   0.992283\n",
      "train loss:   0.989727\n",
      "train loss:   0.982110\n",
      "train loss:   1.175211\n",
      "train loss:   0.751169\n",
      "train loss:   0.840953\n",
      "train loss:   1.106793\n",
      "train loss:   0.989278\n",
      "train loss:   0.966654\n",
      "train loss:   0.874648\n",
      "train loss:   0.703584\n",
      "train loss:   1.116801\n",
      "train loss:   1.015767\n",
      "train loss:   1.219555\n",
      "train loss:   0.942049\n",
      "train loss:   1.048263\n",
      "train loss:   1.166966\n",
      "train loss:   0.766437\n",
      "train loss:   0.862393\n",
      "train loss:   0.374188\n",
      "train loss:   0.801916\n",
      "train loss:   0.963192\n",
      "train loss:   0.786829\n",
      "train loss:   0.745247\n",
      "train loss:   1.009155\n",
      "train loss:   1.108675\n",
      "train loss:   0.545604\n",
      "########### epoch 82 ###########\n",
      "########### loop 15350 ###########\n",
      "test loss:   0.145288   test accuracy:   1.000000\n",
      "########### loop 15350 ###########\n",
      "train loss:   0.987699\n",
      "train loss:   0.588921\n",
      "train loss:   0.954102\n",
      "train loss:   0.744773\n",
      "train loss:   0.910830\n",
      "train loss:   1.025725\n",
      "train loss:   1.105002\n",
      "train loss:   0.972840\n",
      "train loss:   1.243300\n",
      "train loss:   1.007056\n",
      "train loss:   0.786955\n",
      "train loss:   0.731399\n",
      "train loss:   1.015146\n",
      "train loss:   0.971648\n",
      "train loss:   0.931421\n",
      "train loss:   0.924069\n",
      "train loss:   0.908071\n",
      "train loss:   1.244669\n",
      "train loss:   0.909382\n",
      "train loss:   0.809250\n",
      "train loss:   0.813081\n",
      "train loss:   1.169578\n",
      "train loss:   1.137686\n",
      "train loss:   0.854314\n",
      "train loss:   1.228964\n",
      "train loss:   1.043858\n",
      "train loss:   0.957002\n",
      "train loss:   0.946899\n",
      "train loss:   1.378883\n",
      "train loss:   1.112496\n",
      "train loss:   1.365256\n",
      "train loss:   0.802017\n",
      "train loss:   0.849838\n",
      "train loss:   0.776892\n",
      "train loss:   1.118933\n",
      "train loss:   1.223183\n",
      "train loss:   0.958628\n",
      "train loss:   1.135659\n",
      "train loss:   1.219680\n",
      "train loss:   0.757443\n",
      "train loss:   0.627285\n",
      "train loss:   0.914697\n",
      "train loss:   1.100840\n",
      "train loss:   1.145606\n",
      "train loss:   1.093881\n",
      "train loss:   1.042552\n",
      "train loss:   0.892096\n",
      "train loss:   0.678716\n",
      "train loss:   1.215887\n",
      "train loss:   0.735665\n",
      "########### epoch 82 ###########\n",
      "########### loop 15400 ###########\n",
      "test loss:   0.199818   test accuracy:   1.000000\n",
      "########### loop 15400 ###########\n",
      "train loss:   0.489409\n",
      "train loss:   0.966581\n",
      "train loss:   0.806689\n",
      "train loss:   0.914104\n",
      "train loss:   0.592354\n",
      "train loss:   1.079337\n",
      "train loss:   0.824707\n",
      "train loss:   0.835234\n",
      "train loss:   1.015650\n",
      "train loss:   1.287656\n",
      "train loss:   0.635599\n",
      "train loss:   0.896198\n",
      "train loss:   0.994053\n",
      "train loss:   1.117315\n",
      "train loss:   1.156357\n",
      "train loss:   0.586936\n",
      "train loss:   1.163011\n",
      "train loss:   1.275164\n",
      "train loss:   1.189173\n",
      "train loss:   0.707472\n",
      "train loss:   0.878084\n",
      "train loss:   0.846202\n",
      "train loss:   1.098024\n",
      "train loss:   1.317005\n",
      "train loss:   0.836023\n",
      "train loss:   0.917875\n",
      "train loss:   0.619577\n",
      "train loss:   1.091199\n",
      "train loss:   0.625650\n",
      "train loss:   0.976731\n",
      "train loss:   1.314970\n",
      "train loss:   1.106132\n",
      "train loss:   0.924725\n",
      "train loss:   1.073349\n",
      "train loss:   0.545119\n",
      "train loss:   0.938688\n",
      "train loss:   1.377902\n",
      "train loss:   1.129449\n",
      "train loss:   1.030756\n",
      "train loss:   0.966989\n",
      "train loss:   0.606137\n",
      "train loss:   1.296431\n",
      "train loss:   1.158171\n",
      "train loss:   1.143870\n",
      "train loss:   0.998041\n",
      "train loss:   1.189974\n",
      "train loss:   0.991859\n",
      "train loss:   1.074616\n",
      "train loss:   0.848179\n",
      "train loss:   0.936484\n",
      "########### epoch 83 ###########\n",
      "########### loop 15450 ###########\n",
      "test loss:   0.190054   test accuracy:   0.958333\n",
      "########### loop 15450 ###########\n",
      "train loss:   1.138466\n",
      "train loss:   0.551810\n",
      "train loss:   1.094261\n",
      "train loss:   1.311738\n",
      "train loss:   0.858789\n",
      "train loss:   1.102610\n",
      "train loss:   0.964033\n",
      "train loss:   0.840329\n",
      "train loss:   1.110466\n",
      "train loss:   1.130222\n",
      "train loss:   0.942410\n",
      "train loss:   1.057876\n",
      "train loss:   0.248066\n",
      "train loss:   1.129521\n",
      "train loss:   0.937301\n",
      "train loss:   0.911271\n",
      "train loss:   1.060474\n",
      "train loss:   1.115429\n",
      "train loss:   0.854717\n",
      "train loss:   0.780786\n",
      "train loss:   0.770476\n",
      "train loss:   0.790517\n",
      "train loss:   1.337718\n",
      "train loss:   1.122869\n",
      "train loss:   1.177384\n",
      "train loss:   1.046385\n",
      "train loss:   1.037176\n",
      "train loss:   0.888408\n",
      "train loss:   1.114097\n",
      "train loss:   1.065562\n",
      "train loss:   1.125290\n",
      "train loss:   0.936068\n",
      "train loss:   1.220186\n",
      "train loss:   1.401776\n",
      "train loss:   0.818501\n",
      "train loss:   1.270703\n",
      "train loss:   0.727057\n",
      "train loss:   1.140289\n",
      "train loss:   0.912404\n",
      "train loss:   1.344400\n",
      "train loss:   0.930625\n",
      "train loss:   1.045683\n",
      "train loss:   1.171557\n",
      "train loss:   0.821712\n",
      "train loss:   1.068332\n",
      "train loss:   0.808746\n",
      "train loss:   1.099329\n",
      "train loss:   1.352245\n",
      "train loss:   0.991223\n",
      "train loss:   1.048097\n",
      "########### epoch 83 ###########\n",
      "########### loop 15500 ###########\n",
      "test loss:   0.343811   test accuracy:   0.958333\n",
      "########### loop 15500 ###########\n",
      "train loss:   1.029260\n",
      "train loss:   0.758114\n",
      "train loss:   1.317900\n",
      "train loss:   0.981759\n",
      "train loss:   0.941841\n",
      "train loss:   1.347273\n",
      "train loss:   0.914298\n",
      "train loss:   0.951332\n",
      "train loss:   0.806831\n",
      "train loss:   0.986074\n",
      "train loss:   0.968931\n",
      "train loss:   0.716287\n",
      "train loss:   1.320250\n",
      "train loss:   0.852496\n",
      "train loss:   0.724192\n",
      "train loss:   0.935197\n",
      "train loss:   0.928804\n",
      "train loss:   0.819645\n",
      "train loss:   1.045123\n",
      "train loss:   0.959442\n",
      "train loss:   0.675642\n",
      "train loss:   1.138588\n",
      "train loss:   1.430439\n",
      "train loss:   0.980025\n",
      "train loss:   1.085866\n",
      "train loss:   1.294533\n",
      "train loss:   0.982196\n",
      "train loss:   0.841495\n",
      "train loss:   0.651417\n",
      "train loss:   0.709825\n",
      "train loss:   1.082577\n",
      "train loss:   0.834276\n",
      "train loss:   0.867725\n",
      "train loss:   0.829171\n",
      "train loss:   1.322334\n",
      "train loss:   0.786709\n",
      "train loss:   1.490824\n",
      "train loss:   1.339086\n",
      "train loss:   0.932556\n",
      "train loss:   1.115115\n",
      "train loss:   1.292190\n",
      "train loss:   1.052075\n",
      "train loss:   0.764140\n",
      "train loss:   0.959659\n",
      "train loss:   1.409248\n",
      "train loss:   1.272676\n",
      "train loss:   0.902531\n",
      "train loss:   1.046490\n",
      "train loss:   0.746684\n",
      "train loss:   0.610857\n",
      "########### epoch 83 ###########\n",
      "########### loop 15550 ###########\n",
      "test loss:   0.112240   test accuracy:   1.000000\n",
      "########### loop 15550 ###########\n",
      "train loss:   0.797848\n",
      "train loss:   0.661244\n",
      "train loss:   1.065496\n",
      "train loss:   1.299530\n",
      "train loss:   0.841657\n",
      "train loss:   1.068858\n",
      "train loss:   1.019388\n",
      "train loss:   1.060528\n",
      "train loss:   0.793359\n",
      "train loss:   0.526360\n",
      "train loss:   1.135670\n",
      "train loss:   1.085684\n",
      "train loss:   1.045707\n",
      "train loss:   0.826959\n",
      "train loss:   0.948303\n",
      "train loss:   0.740093\n",
      "train loss:   1.006443\n",
      "train loss:   1.009054\n",
      "train loss:   1.246862\n",
      "train loss:   0.836156\n",
      "train loss:   0.790404\n",
      "train loss:   0.726372\n",
      "train loss:   1.040832\n",
      "train loss:   0.917752\n",
      "train loss:   0.843707\n",
      "train loss:   0.939429\n",
      "train loss:   1.093609\n",
      "train loss:   1.228850\n",
      "train loss:   0.636119\n",
      "train loss:   1.095556\n",
      "train loss:   0.585115\n",
      "train loss:   1.090960\n",
      "train loss:   1.227649\n",
      "train loss:   0.886387\n",
      "train loss:   1.005969\n",
      "train loss:   1.162486\n",
      "train loss:   1.165909\n",
      "train loss:   0.670985\n",
      "train loss:   1.100797\n",
      "train loss:   1.159567\n",
      "train loss:   1.051684\n",
      "train loss:   1.546219\n",
      "train loss:   0.690457\n",
      "train loss:   0.769777\n",
      "train loss:   1.221746\n",
      "train loss:   1.105610\n",
      "train loss:   1.040657\n",
      "train loss:   0.932116\n",
      "train loss:   1.134118\n",
      "train loss:   0.817635\n",
      "########### epoch 83 ###########\n",
      "########### loop 15600 ###########\n",
      "test loss:   0.128544   test accuracy:   1.000000\n",
      "########### loop 15600 ###########\n",
      "train loss:   0.635645\n",
      "train loss:   0.887235\n",
      "train loss:   1.251210\n",
      "train loss:   1.320410\n",
      "train loss:   0.733568\n",
      "train loss:   0.773090\n",
      "train loss:   1.331059\n",
      "train loss:   0.724291\n",
      "train loss:   0.903786\n",
      "train loss:   0.871500\n",
      "train loss:   0.777154\n",
      "train loss:   0.690112\n",
      "train loss:   0.782854\n",
      "train loss:   1.119068\n",
      "train loss:   0.911744\n",
      "train loss:   0.867698\n",
      "train loss:   0.928661\n",
      "train loss:   0.635441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.178507\n",
      "train loss:   1.006894\n",
      "train loss:   1.091698\n",
      "train loss:   1.230095\n",
      "train loss:   0.867849\n",
      "train loss:   0.730474\n",
      "train loss:   1.006283\n",
      "train loss:   1.035584\n",
      "train loss:   0.620141\n",
      "train loss:   0.684337\n",
      "train loss:   0.736437\n",
      "train loss:   1.135752\n",
      "train loss:   0.916534\n",
      "train loss:   0.689123\n",
      "train loss:   0.858272\n",
      "train loss:   1.333751\n",
      "train loss:   0.729738\n",
      "train loss:   0.733084\n",
      "train loss:   1.087793\n",
      "train loss:   0.799515\n",
      "train loss:   1.124165\n",
      "train loss:   0.665139\n",
      "train loss:   1.002940\n",
      "train loss:   1.099535\n",
      "train loss:   0.849504\n",
      "train loss:   1.064577\n",
      "train loss:   1.061890\n",
      "train loss:   1.125983\n",
      "train loss:   1.041775\n",
      "train loss:   1.044747\n",
      "train loss:   1.044463\n",
      "train loss:   0.840309\n",
      "########### epoch 84 ###########\n",
      "########### loop 15650 ###########\n",
      "test loss:   0.318841   test accuracy:   0.916667\n",
      "########### loop 15650 ###########\n",
      "train loss:   0.670496\n",
      "train loss:   0.617493\n",
      "train loss:   1.042099\n",
      "train loss:   1.312485\n",
      "train loss:   0.916319\n",
      "train loss:   0.719496\n",
      "train loss:   0.888704\n",
      "train loss:   0.803167\n",
      "train loss:   1.036526\n",
      "train loss:   1.132800\n",
      "train loss:   0.898354\n",
      "train loss:   0.832723\n",
      "train loss:   1.080293\n",
      "train loss:   0.815022\n",
      "train loss:   0.795814\n",
      "train loss:   1.205401\n",
      "train loss:   0.851880\n",
      "train loss:   0.786238\n",
      "train loss:   1.112799\n",
      "train loss:   1.182046\n",
      "train loss:   0.848037\n",
      "train loss:   1.163771\n",
      "train loss:   1.070810\n",
      "train loss:   0.752725\n",
      "train loss:   0.850608\n",
      "train loss:   0.952616\n",
      "train loss:   1.019815\n",
      "train loss:   0.876370\n",
      "train loss:   0.954864\n",
      "train loss:   1.021631\n",
      "train loss:   1.151291\n",
      "train loss:   1.065288\n",
      "train loss:   0.866159\n",
      "train loss:   1.050037\n",
      "train loss:   1.352531\n",
      "train loss:   0.774040\n",
      "train loss:   1.024916\n",
      "train loss:   0.881557\n",
      "train loss:   0.708568\n",
      "train loss:   0.764603\n",
      "train loss:   0.774059\n",
      "train loss:   0.959688\n",
      "train loss:   0.898168\n",
      "train loss:   0.822489\n",
      "train loss:   0.792162\n",
      "train loss:   0.973479\n",
      "train loss:   1.016855\n",
      "train loss:   1.060310\n",
      "train loss:   1.035100\n",
      "train loss:   0.843514\n",
      "########### epoch 84 ###########\n",
      "########### loop 15700 ###########\n",
      "test loss:   0.110324   test accuracy:   1.000000\n",
      "########### loop 15700 ###########\n",
      "train loss:   0.952326\n",
      "train loss:   1.104600\n",
      "train loss:   1.252807\n",
      "train loss:   0.950879\n",
      "train loss:   0.782668\n",
      "train loss:   1.214195\n",
      "train loss:   1.262697\n",
      "train loss:   0.897160\n",
      "train loss:   1.134873\n",
      "train loss:   1.261050\n",
      "train loss:   1.014571\n",
      "train loss:   0.875087\n",
      "train loss:   1.133608\n",
      "train loss:   1.390253\n",
      "train loss:   0.686054\n",
      "train loss:   1.125697\n",
      "train loss:   0.993467\n",
      "train loss:   1.237627\n",
      "train loss:   1.006350\n",
      "train loss:   0.814414\n",
      "train loss:   1.016422\n",
      "train loss:   0.887960\n",
      "train loss:   1.288842\n",
      "train loss:   1.130792\n",
      "train loss:   1.281461\n",
      "train loss:   0.942519\n",
      "train loss:   0.891957\n",
      "train loss:   0.659710\n",
      "train loss:   1.171042\n",
      "train loss:   0.965516\n",
      "train loss:   1.157341\n",
      "train loss:   1.162096\n",
      "train loss:   1.166687\n",
      "train loss:   1.000289\n",
      "train loss:   0.547855\n",
      "train loss:   0.831885\n",
      "train loss:   1.086579\n",
      "train loss:   1.196177\n",
      "train loss:   0.848981\n",
      "train loss:   0.833809\n",
      "train loss:   1.057804\n",
      "train loss:   0.761558\n",
      "train loss:   1.178685\n",
      "train loss:   1.008566\n",
      "train loss:   1.424752\n",
      "train loss:   1.054241\n",
      "train loss:   1.052616\n",
      "train loss:   0.869804\n",
      "train loss:   0.950045\n",
      "train loss:   1.190899\n",
      "########### epoch 84 ###########\n",
      "########### loop 15750 ###########\n",
      "test loss:   0.117326   test accuracy:   1.000000\n",
      "########### loop 15750 ###########\n",
      "train loss:   1.332030\n",
      "train loss:   1.117423\n",
      "train loss:   0.572684\n",
      "train loss:   0.917303\n",
      "train loss:   1.014260\n",
      "train loss:   0.653991\n",
      "train loss:   0.807021\n",
      "train loss:   0.974458\n",
      "train loss:   0.795853\n",
      "train loss:   1.160673\n",
      "train loss:   0.968742\n",
      "train loss:   1.169525\n",
      "train loss:   0.910283\n",
      "train loss:   1.071938\n",
      "train loss:   1.038080\n",
      "train loss:   0.839738\n",
      "train loss:   1.180528\n",
      "train loss:   1.259881\n",
      "train loss:   0.591483\n",
      "train loss:   1.454354\n",
      "train loss:   1.030592\n",
      "train loss:   0.880190\n",
      "train loss:   0.739044\n",
      "train loss:   0.871077\n",
      "train loss:   1.010815\n",
      "train loss:   1.021324\n",
      "train loss:   0.862148\n",
      "train loss:   1.238884\n",
      "train loss:   0.751906\n",
      "train loss:   0.847908\n",
      "train loss:   1.591332\n",
      "train loss:   1.405165\n",
      "train loss:   0.834618\n",
      "train loss:   0.858478\n",
      "train loss:   0.897492\n",
      "train loss:   0.657233\n",
      "train loss:   1.135159\n",
      "train loss:   1.196009\n",
      "train loss:   0.797224\n",
      "train loss:   0.834896\n",
      "train loss:   1.130494\n",
      "train loss:   0.970369\n",
      "train loss:   0.829064\n",
      "train loss:   1.061898\n",
      "train loss:   0.945667\n",
      "train loss:   0.969788\n",
      "train loss:   0.754694\n",
      "train loss:   1.073140\n",
      "train loss:   0.848273\n",
      "train loss:   0.840196\n",
      "########### epoch 85 ###########\n",
      "########### loop 15800 ###########\n",
      "test loss:   0.248406   test accuracy:   1.000000\n",
      "########### loop 15800 ###########\n",
      "train loss:   0.984869\n",
      "train loss:   0.461131\n",
      "train loss:   0.950253\n",
      "train loss:   1.139449\n",
      "train loss:   0.853658\n",
      "train loss:   1.132243\n",
      "train loss:   1.058198\n",
      "train loss:   1.542399\n",
      "train loss:   1.142605\n",
      "train loss:   1.012957\n",
      "train loss:   1.259611\n",
      "train loss:   1.117673\n",
      "train loss:   1.064151\n",
      "train loss:   0.748318\n",
      "train loss:   0.933288\n",
      "train loss:   0.764365\n",
      "train loss:   0.913462\n",
      "train loss:   0.902998\n",
      "train loss:   1.054023\n",
      "train loss:   1.034977\n",
      "train loss:   1.104394\n",
      "train loss:   0.540513\n",
      "train loss:   1.233142\n",
      "train loss:   0.996249\n",
      "train loss:   1.068348\n",
      "train loss:   1.135911\n",
      "train loss:   1.044693\n",
      "train loss:   1.062865\n",
      "train loss:   0.935580\n",
      "train loss:   0.782973\n",
      "train loss:   0.951656\n",
      "train loss:   0.824484\n",
      "train loss:   1.134608\n",
      "train loss:   1.092273\n",
      "train loss:   1.544780\n",
      "train loss:   0.787290\n",
      "train loss:   0.832541\n",
      "train loss:   1.057014\n",
      "train loss:   1.010996\n",
      "train loss:   0.978913\n",
      "train loss:   0.890200\n",
      "train loss:   1.199344\n",
      "train loss:   1.034420\n",
      "train loss:   0.790886\n",
      "train loss:   0.947326\n",
      "train loss:   0.737342\n",
      "train loss:   1.358334\n",
      "train loss:   0.977991\n",
      "train loss:   0.845746\n",
      "train loss:   0.911383\n",
      "########### epoch 85 ###########\n",
      "########### loop 15850 ###########\n",
      "test loss:   0.315605   test accuracy:   0.833333\n",
      "########### loop 15850 ###########\n",
      "train loss:   1.038450\n",
      "train loss:   1.054997\n",
      "train loss:   1.107894\n",
      "train loss:   0.802875\n",
      "train loss:   1.069728\n",
      "train loss:   0.907135\n",
      "train loss:   1.173914\n",
      "train loss:   0.652065\n",
      "train loss:   1.069556\n",
      "train loss:   0.853234\n",
      "train loss:   0.988817\n",
      "train loss:   1.430857\n",
      "train loss:   0.615441\n",
      "train loss:   1.314818\n",
      "train loss:   0.870389\n",
      "train loss:   0.946027\n",
      "train loss:   0.843195\n",
      "train loss:   1.010447\n",
      "train loss:   0.920934\n",
      "train loss:   0.934405\n",
      "train loss:   0.737327\n",
      "train loss:   1.128130\n",
      "train loss:   1.375594\n",
      "train loss:   0.777225\n",
      "train loss:   1.152610\n",
      "train loss:   0.711834\n",
      "train loss:   1.042943\n",
      "train loss:   0.887213\n",
      "train loss:   1.150647\n",
      "train loss:   1.198157\n",
      "train loss:   0.721525\n",
      "train loss:   0.652640\n",
      "train loss:   0.915015\n",
      "train loss:   0.875500\n",
      "train loss:   1.264635\n",
      "train loss:   1.159605\n",
      "train loss:   0.952704\n",
      "train loss:   0.563331\n",
      "train loss:   0.836861\n",
      "train loss:   1.125288\n",
      "train loss:   0.997401\n",
      "train loss:   1.003694\n",
      "train loss:   1.245578\n",
      "train loss:   0.953075\n",
      "train loss:   1.346184\n",
      "train loss:   1.115420\n",
      "train loss:   1.129410\n",
      "train loss:   0.674482\n",
      "train loss:   1.002035\n",
      "train loss:   0.976303\n",
      "########### epoch 85 ###########\n",
      "########### loop 15900 ###########\n",
      "test loss:   0.292928   test accuracy:   0.916667\n",
      "########### loop 15900 ###########\n",
      "train loss:   1.062457\n",
      "train loss:   0.959522\n",
      "train loss:   0.840031\n",
      "train loss:   1.289102\n",
      "train loss:   1.082360\n",
      "train loss:   0.829179\n",
      "train loss:   0.965785\n",
      "train loss:   0.959510\n",
      "train loss:   0.868861\n",
      "train loss:   0.837415\n",
      "train loss:   0.901814\n",
      "train loss:   1.263328\n",
      "train loss:   1.148715\n",
      "train loss:   1.343903\n",
      "train loss:   0.908403\n",
      "train loss:   1.110130\n",
      "train loss:   0.748693\n",
      "train loss:   0.818013\n",
      "train loss:   1.049621\n",
      "train loss:   0.991904\n",
      "train loss:   0.948787\n",
      "train loss:   0.880057\n",
      "train loss:   0.893603\n",
      "train loss:   1.087361\n",
      "train loss:   0.990919\n",
      "train loss:   0.853424\n",
      "train loss:   0.933245\n",
      "train loss:   1.116532\n",
      "train loss:   1.344618\n",
      "train loss:   1.031089\n",
      "train loss:   1.439169\n",
      "train loss:   1.156355\n",
      "train loss:   1.164947\n",
      "train loss:   0.935393\n",
      "train loss:   1.091130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.960824\n",
      "train loss:   0.789926\n",
      "train loss:   0.583867\n",
      "train loss:   1.079026\n",
      "train loss:   1.109151\n",
      "train loss:   1.213376\n",
      "train loss:   0.699935\n",
      "train loss:   0.969707\n",
      "train loss:   1.239538\n",
      "train loss:   0.923966\n",
      "train loss:   0.919385\n",
      "train loss:   1.117421\n",
      "train loss:   0.838666\n",
      "train loss:   0.850822\n",
      "train loss:   1.059309\n",
      "########### epoch 85 ###########\n",
      "########### loop 15950 ###########\n",
      "test loss:   0.395966   test accuracy:   0.875000\n",
      "########### loop 15950 ###########\n",
      "train loss:   1.180216\n",
      "train loss:   0.771697\n",
      "train loss:   1.211573\n",
      "train loss:   1.132653\n",
      "train loss:   1.159409\n",
      "train loss:   1.164620\n",
      "train loss:   0.807085\n",
      "train loss:   1.045044\n",
      "train loss:   0.930900\n",
      "train loss:   0.952836\n",
      "train loss:   0.708489\n",
      "train loss:   1.177279\n",
      "train loss:   0.973443\n",
      "train loss:   1.104437\n",
      "train loss:   1.178209\n",
      "train loss:   1.008316\n",
      "train loss:   0.825349\n",
      "train loss:   0.917699\n",
      "train loss:   0.755957\n",
      "train loss:   1.135705\n",
      "train loss:   0.476265\n",
      "train loss:   1.246289\n",
      "train loss:   0.747595\n",
      "train loss:   0.760918\n",
      "train loss:   1.020821\n",
      "train loss:   1.409891\n",
      "train loss:   1.488702\n",
      "train loss:   0.826553\n",
      "train loss:   0.598123\n",
      "train loss:   1.124565\n",
      "train loss:   1.267088\n",
      "train loss:   0.977282\n",
      "train loss:   1.086086\n",
      "train loss:   0.967782\n",
      "train loss:   0.832302\n",
      "train loss:   1.098741\n",
      "train loss:   0.677873\n",
      "train loss:   0.728776\n",
      "train loss:   0.976889\n",
      "train loss:   0.719118\n",
      "train loss:   1.121659\n",
      "train loss:   0.972454\n",
      "train loss:   0.900299\n",
      "train loss:   1.037592\n",
      "train loss:   0.954584\n",
      "train loss:   1.196586\n",
      "train loss:   1.285632\n",
      "train loss:   1.003540\n",
      "train loss:   0.807460\n",
      "train loss:   1.203352\n",
      "########### epoch 86 ###########\n",
      "########### loop 16000 ###########\n",
      "test loss:   0.180770   test accuracy:   0.958333\n",
      "########### loop 16000 ###########\n",
      "train loss:   0.979602\n",
      "train loss:   0.807621\n",
      "train loss:   1.063012\n",
      "train loss:   0.849085\n",
      "train loss:   0.922327\n",
      "train loss:   1.086309\n",
      "train loss:   0.949987\n",
      "train loss:   1.205473\n",
      "train loss:   1.116948\n",
      "train loss:   1.410984\n",
      "train loss:   0.956890\n",
      "train loss:   1.102548\n",
      "train loss:   0.560893\n",
      "train loss:   1.109158\n",
      "train loss:   0.966385\n",
      "train loss:   1.146776\n",
      "train loss:   0.715677\n",
      "train loss:   0.995688\n",
      "train loss:   1.011745\n",
      "train loss:   1.506240\n",
      "train loss:   1.246307\n",
      "train loss:   0.794410\n",
      "train loss:   1.236234\n",
      "train loss:   1.162082\n",
      "train loss:   1.582930\n",
      "train loss:   0.787705\n",
      "train loss:   0.943069\n",
      "train loss:   1.027860\n",
      "train loss:   1.103262\n",
      "train loss:   1.008936\n",
      "train loss:   1.413004\n",
      "train loss:   1.340834\n",
      "train loss:   1.125976\n",
      "train loss:   1.400453\n",
      "train loss:   0.986300\n",
      "train loss:   0.859475\n",
      "train loss:   1.038741\n",
      "train loss:   0.997132\n",
      "train loss:   1.366533\n",
      "train loss:   1.061485\n",
      "train loss:   0.673074\n",
      "train loss:   0.954021\n",
      "train loss:   0.868881\n",
      "train loss:   0.498475\n",
      "train loss:   1.362363\n",
      "train loss:   0.912336\n",
      "train loss:   1.016425\n",
      "train loss:   1.052795\n",
      "train loss:   0.972393\n",
      "train loss:   1.281157\n",
      "########### epoch 86 ###########\n",
      "########### loop 16050 ###########\n",
      "test loss:   0.351506   test accuracy:   0.875000\n",
      "########### loop 16050 ###########\n",
      "train loss:   0.940695\n",
      "train loss:   1.051203\n",
      "train loss:   1.209439\n",
      "train loss:   0.956877\n",
      "train loss:   1.415164\n",
      "train loss:   1.055036\n",
      "train loss:   0.608860\n",
      "train loss:   1.097839\n",
      "train loss:   1.248584\n",
      "train loss:   1.201864\n",
      "train loss:   0.849296\n",
      "train loss:   0.909015\n",
      "train loss:   0.830638\n",
      "train loss:   0.954670\n",
      "train loss:   0.910594\n",
      "train loss:   0.804282\n",
      "train loss:   1.066264\n",
      "train loss:   1.118828\n",
      "train loss:   1.205185\n",
      "train loss:   0.856603\n",
      "train loss:   0.556281\n",
      "train loss:   0.771131\n",
      "train loss:   1.059662\n",
      "train loss:   1.125066\n",
      "train loss:   0.875919\n",
      "train loss:   1.184308\n",
      "train loss:   1.212409\n",
      "train loss:   1.155161\n",
      "train loss:   0.827991\n",
      "train loss:   1.132571\n",
      "train loss:   0.769049\n",
      "train loss:   0.751202\n",
      "train loss:   1.178102\n",
      "train loss:   0.722354\n",
      "train loss:   1.307175\n",
      "train loss:   0.895788\n",
      "train loss:   0.947227\n",
      "train loss:   0.870912\n",
      "train loss:   1.415661\n",
      "train loss:   0.646998\n",
      "train loss:   0.890565\n",
      "train loss:   0.838874\n",
      "train loss:   1.069243\n",
      "train loss:   0.767877\n",
      "train loss:   1.280221\n",
      "train loss:   1.062747\n",
      "train loss:   0.979912\n",
      "train loss:   0.633684\n",
      "train loss:   1.180845\n",
      "train loss:   0.910835\n",
      "########### epoch 86 ###########\n",
      "########### loop 16100 ###########\n",
      "test loss:   0.244306   test accuracy:   0.958333\n",
      "########### loop 16100 ###########\n",
      "train loss:   0.908818\n",
      "train loss:   1.101125\n",
      "train loss:   1.070442\n",
      "train loss:   1.184122\n",
      "train loss:   0.966242\n",
      "train loss:   1.048151\n",
      "train loss:   1.030809\n",
      "train loss:   1.108644\n",
      "train loss:   0.654499\n",
      "train loss:   1.140358\n",
      "train loss:   0.876704\n",
      "train loss:   0.817109\n",
      "train loss:   1.287223\n",
      "train loss:   0.808329\n",
      "train loss:   0.632136\n",
      "train loss:   1.228581\n",
      "train loss:   0.953755\n",
      "train loss:   1.167979\n",
      "train loss:   0.821529\n",
      "train loss:   0.707945\n",
      "train loss:   1.138980\n",
      "train loss:   1.058039\n",
      "train loss:   1.039790\n",
      "train loss:   1.279756\n",
      "train loss:   0.716781\n",
      "train loss:   0.991696\n",
      "train loss:   1.032593\n",
      "train loss:   1.431834\n",
      "train loss:   0.719729\n",
      "train loss:   1.017274\n",
      "train loss:   1.014658\n",
      "train loss:   1.001581\n",
      "train loss:   0.984755\n",
      "train loss:   0.546015\n",
      "train loss:   1.131170\n",
      "train loss:   0.957321\n",
      "train loss:   0.910941\n",
      "train loss:   0.796814\n",
      "train loss:   0.647985\n",
      "train loss:   0.936341\n",
      "train loss:   1.343330\n",
      "train loss:   0.877611\n",
      "train loss:   0.479318\n",
      "train loss:   0.953233\n",
      "train loss:   0.739404\n",
      "train loss:   0.878822\n",
      "train loss:   1.140071\n",
      "train loss:   1.140074\n",
      "train loss:   0.600676\n",
      "train loss:   1.308676\n",
      "########### epoch 86 ###########\n",
      "########### loop 16150 ###########\n",
      "test loss:   0.182111   test accuracy:   0.958333\n",
      "########### loop 16150 ###########\n",
      "train loss:   1.182193\n",
      "train loss:   0.991677\n",
      "train loss:   0.706296\n",
      "train loss:   0.675657\n",
      "train loss:   1.040260\n",
      "train loss:   0.717880\n",
      "train loss:   1.032881\n",
      "train loss:   1.380826\n",
      "train loss:   0.974486\n",
      "train loss:   1.049850\n",
      "train loss:   0.847702\n",
      "train loss:   0.873811\n",
      "train loss:   0.925917\n",
      "train loss:   0.862757\n",
      "train loss:   1.021384\n",
      "train loss:   1.033446\n",
      "train loss:   1.226901\n",
      "train loss:   1.293889\n",
      "train loss:   1.376433\n",
      "train loss:   0.960214\n",
      "train loss:   1.013858\n",
      "train loss:   1.007498\n",
      "train loss:   0.877976\n",
      "train loss:   0.668588\n",
      "train loss:   0.811396\n",
      "train loss:   0.804222\n",
      "train loss:   1.037769\n",
      "train loss:   1.181605\n",
      "train loss:   0.797413\n",
      "train loss:   1.144344\n",
      "train loss:   0.949618\n",
      "train loss:   0.882926\n",
      "train loss:   0.799318\n",
      "train loss:   0.806539\n",
      "train loss:   1.085862\n",
      "train loss:   0.920833\n",
      "train loss:   0.980664\n",
      "train loss:   0.670183\n",
      "train loss:   1.604785\n",
      "train loss:   0.896273\n",
      "train loss:   1.189728\n",
      "train loss:   0.902613\n",
      "train loss:   1.228076\n",
      "train loss:   1.025693\n",
      "train loss:   1.085678\n",
      "train loss:   1.020808\n",
      "train loss:   0.990741\n",
      "train loss:   1.033229\n",
      "train loss:   0.802845\n",
      "train loss:   1.248938\n",
      "########### epoch 87 ###########\n",
      "########### loop 16200 ###########\n",
      "test loss:   0.228202   test accuracy:   0.958333\n",
      "########### loop 16200 ###########\n",
      "train loss:   0.914406\n",
      "train loss:   1.003565\n",
      "train loss:   0.789335\n",
      "train loss:   0.757458\n",
      "train loss:   0.833450\n",
      "train loss:   1.259158\n",
      "train loss:   1.329730\n",
      "train loss:   0.930779\n",
      "train loss:   0.906545\n",
      "train loss:   0.943357\n",
      "train loss:   0.799717\n",
      "train loss:   1.371788\n",
      "train loss:   0.968327\n",
      "train loss:   0.771116\n",
      "train loss:   0.988741\n",
      "train loss:   1.098818\n",
      "train loss:   0.911052\n",
      "train loss:   0.879843\n",
      "train loss:   0.959641\n",
      "train loss:   1.604378\n",
      "train loss:   1.107144\n",
      "train loss:   0.786645\n",
      "train loss:   0.672842\n",
      "train loss:   0.906185\n",
      "train loss:   1.082694\n",
      "train loss:   0.997927\n",
      "train loss:   0.971227\n",
      "train loss:   1.417440\n",
      "train loss:   0.809503\n",
      "train loss:   1.097575\n",
      "train loss:   1.074214\n",
      "train loss:   1.282613\n",
      "train loss:   0.968008\n",
      "train loss:   1.246283\n",
      "train loss:   0.790052\n",
      "train loss:   1.247551\n",
      "train loss:   1.283269\n",
      "train loss:   0.869400\n",
      "train loss:   0.740448\n",
      "train loss:   0.994480\n",
      "train loss:   1.144583\n",
      "train loss:   0.642100\n",
      "train loss:   0.790977\n",
      "train loss:   0.892281\n",
      "train loss:   0.726372\n",
      "train loss:   0.865148\n",
      "train loss:   0.699347\n",
      "train loss:   1.046533\n",
      "train loss:   0.856722\n",
      "train loss:   0.803282\n",
      "########### epoch 87 ###########\n",
      "########### loop 16250 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.428445   test accuracy:   0.875000\n",
      "########### loop 16250 ###########\n",
      "train loss:   0.659636\n",
      "train loss:   0.685836\n",
      "train loss:   0.391394\n",
      "train loss:   0.719049\n",
      "train loss:   1.127224\n",
      "train loss:   0.793212\n",
      "train loss:   0.824449\n",
      "train loss:   1.244188\n",
      "train loss:   1.346246\n",
      "train loss:   1.084995\n",
      "train loss:   1.037517\n",
      "train loss:   1.141885\n",
      "train loss:   1.000000\n",
      "train loss:   0.452139\n",
      "train loss:   0.704446\n",
      "train loss:   0.919218\n",
      "train loss:   0.805159\n",
      "train loss:   1.019801\n",
      "train loss:   0.700802\n",
      "train loss:   0.922537\n",
      "train loss:   0.990189\n",
      "train loss:   1.035678\n",
      "train loss:   0.777677\n",
      "train loss:   1.225412\n",
      "train loss:   1.118073\n",
      "train loss:   0.932559\n",
      "train loss:   1.102118\n",
      "train loss:   1.000406\n",
      "train loss:   0.827806\n",
      "train loss:   1.021337\n",
      "train loss:   0.711157\n",
      "train loss:   1.007478\n",
      "train loss:   0.921859\n",
      "train loss:   0.912070\n",
      "train loss:   0.954004\n",
      "train loss:   1.012618\n",
      "train loss:   1.138146\n",
      "train loss:   1.164472\n",
      "train loss:   1.103567\n",
      "train loss:   1.040654\n",
      "train loss:   0.579446\n",
      "train loss:   0.950256\n",
      "train loss:   1.055556\n",
      "train loss:   1.066433\n",
      "train loss:   0.788184\n",
      "train loss:   1.071310\n",
      "train loss:   1.357582\n",
      "train loss:   1.070739\n",
      "train loss:   1.027378\n",
      "train loss:   0.994763\n",
      "########### epoch 87 ###########\n",
      "########### loop 16300 ###########\n",
      "test loss:   0.224055   test accuracy:   0.958333\n",
      "########### loop 16300 ###########\n",
      "train loss:   1.065310\n",
      "train loss:   0.849468\n",
      "train loss:   0.721724\n",
      "train loss:   0.995918\n",
      "train loss:   1.135296\n",
      "train loss:   0.969507\n",
      "train loss:   0.914809\n",
      "train loss:   0.919443\n",
      "train loss:   0.961865\n",
      "train loss:   0.820534\n",
      "train loss:   1.142399\n",
      "train loss:   0.941968\n",
      "train loss:   1.009189\n",
      "train loss:   1.152238\n",
      "train loss:   1.042675\n",
      "train loss:   1.231019\n",
      "train loss:   0.897306\n",
      "train loss:   1.258233\n",
      "train loss:   0.606487\n",
      "train loss:   0.957196\n",
      "train loss:   1.258115\n",
      "train loss:   1.204114\n",
      "train loss:   0.849264\n",
      "train loss:   0.916799\n",
      "train loss:   1.307209\n",
      "train loss:   0.771701\n",
      "train loss:   0.876180\n",
      "train loss:   0.813866\n",
      "train loss:   0.624652\n",
      "train loss:   0.985352\n",
      "train loss:   1.038491\n",
      "train loss:   0.861668\n",
      "train loss:   1.032662\n",
      "train loss:   0.632795\n",
      "train loss:   0.830193\n",
      "train loss:   1.107918\n",
      "train loss:   0.990976\n",
      "train loss:   0.522472\n",
      "train loss:   1.001946\n",
      "train loss:   1.288677\n",
      "train loss:   0.635100\n",
      "train loss:   1.032068\n",
      "train loss:   1.118300\n",
      "train loss:   1.011338\n",
      "train loss:   0.816060\n",
      "train loss:   1.042855\n",
      "train loss:   0.919303\n",
      "train loss:   1.120801\n",
      "train loss:   1.346511\n",
      "train loss:   1.190760\n",
      "########### epoch 87 ###########\n",
      "########### loop 16350 ###########\n",
      "test loss:   0.322029   test accuracy:   0.916667\n",
      "########### loop 16350 ###########\n",
      "train loss:   0.772645\n",
      "train loss:   0.901877\n",
      "train loss:   0.838161\n",
      "train loss:   1.119982\n",
      "train loss:   0.926429\n",
      "train loss:   0.956318\n",
      "train loss:   0.744296\n",
      "train loss:   0.812670\n",
      "train loss:   0.960256\n",
      "train loss:   0.343968\n",
      "train loss:   1.246905\n",
      "train loss:   1.240551\n",
      "train loss:   1.298197\n",
      "train loss:   1.219828\n",
      "train loss:   0.896664\n",
      "train loss:   0.835081\n",
      "train loss:   1.162537\n",
      "train loss:   0.966838\n",
      "train loss:   1.070305\n",
      "train loss:   0.675932\n",
      "train loss:   1.042037\n",
      "train loss:   1.092832\n",
      "train loss:   0.780915\n",
      "train loss:   1.233802\n",
      "train loss:   1.243154\n",
      "train loss:   0.894624\n",
      "train loss:   0.784640\n",
      "train loss:   0.851562\n",
      "train loss:   1.087517\n",
      "train loss:   0.759238\n",
      "train loss:   1.369180\n",
      "train loss:   1.138796\n",
      "train loss:   1.045832\n",
      "train loss:   1.445497\n",
      "train loss:   1.195989\n",
      "train loss:   1.298673\n",
      "train loss:   1.342938\n",
      "train loss:   1.307254\n",
      "train loss:   0.969013\n",
      "train loss:   0.929583\n",
      "train loss:   1.155922\n",
      "train loss:   0.473522\n",
      "train loss:   1.031669\n",
      "train loss:   0.830231\n",
      "train loss:   0.761927\n",
      "train loss:   0.910349\n",
      "train loss:   1.188239\n",
      "train loss:   0.767219\n",
      "train loss:   0.867693\n",
      "train loss:   0.707926\n",
      "########### epoch 88 ###########\n",
      "########### loop 16400 ###########\n",
      "test loss:   0.159801   test accuracy:   0.958333\n",
      "########### loop 16400 ###########\n",
      "train loss:   0.901092\n",
      "train loss:   1.023124\n",
      "train loss:   0.581482\n",
      "train loss:   1.114557\n",
      "train loss:   0.805381\n",
      "train loss:   0.873298\n",
      "train loss:   1.108919\n",
      "train loss:   0.948456\n",
      "train loss:   1.047480\n",
      "train loss:   0.814390\n",
      "train loss:   0.887595\n",
      "train loss:   1.186993\n",
      "train loss:   0.946637\n",
      "train loss:   1.139494\n",
      "train loss:   1.023168\n",
      "train loss:   0.968218\n",
      "train loss:   1.035036\n",
      "train loss:   0.867180\n",
      "train loss:   0.797443\n",
      "train loss:   1.004664\n",
      "train loss:   0.930091\n",
      "train loss:   1.142677\n",
      "train loss:   0.881737\n",
      "train loss:   0.816490\n",
      "train loss:   1.034693\n",
      "train loss:   0.992871\n",
      "train loss:   0.672178\n",
      "train loss:   1.149166\n",
      "train loss:   1.118187\n",
      "train loss:   1.295211\n",
      "train loss:   1.089622\n",
      "train loss:   1.261188\n",
      "train loss:   0.725486\n",
      "train loss:   0.668559\n",
      "train loss:   0.616399\n",
      "train loss:   1.042128\n",
      "train loss:   0.588827\n",
      "train loss:   0.875596\n",
      "train loss:   0.872389\n",
      "train loss:   1.201824\n",
      "train loss:   0.930076\n",
      "train loss:   0.671647\n",
      "train loss:   1.137119\n",
      "train loss:   1.184971\n",
      "train loss:   0.920584\n",
      "train loss:   1.252175\n",
      "train loss:   0.814809\n",
      "train loss:   0.821183\n",
      "train loss:   0.931129\n",
      "train loss:   0.783992\n",
      "########### epoch 88 ###########\n",
      "########### loop 16450 ###########\n",
      "test loss:   0.402617   test accuracy:   0.916667\n",
      "########### loop 16450 ###########\n",
      "train loss:   1.009740\n",
      "train loss:   0.740062\n",
      "train loss:   1.320599\n",
      "train loss:   0.967103\n",
      "train loss:   0.748411\n",
      "train loss:   0.782743\n",
      "train loss:   1.000163\n",
      "train loss:   0.969958\n",
      "train loss:   1.062755\n",
      "train loss:   1.073584\n",
      "train loss:   0.987653\n",
      "train loss:   1.249470\n",
      "train loss:   0.925337\n",
      "train loss:   1.210887\n",
      "train loss:   1.149344\n",
      "train loss:   0.757116\n",
      "train loss:   0.667723\n",
      "train loss:   0.865808\n",
      "train loss:   0.825043\n",
      "train loss:   0.570567\n",
      "train loss:   0.770138\n",
      "train loss:   1.405532\n",
      "train loss:   1.253257\n",
      "train loss:   1.002620\n",
      "train loss:   0.501793\n",
      "train loss:   1.104068\n",
      "train loss:   0.865417\n",
      "train loss:   0.949598\n",
      "train loss:   0.743642\n",
      "train loss:   0.563800\n",
      "train loss:   1.187419\n",
      "train loss:   0.701726\n",
      "train loss:   0.662321\n",
      "train loss:   1.018921\n",
      "train loss:   1.073269\n",
      "train loss:   1.216944\n",
      "train loss:   0.891443\n",
      "train loss:   0.972711\n",
      "train loss:   1.236826\n",
      "train loss:   1.269428\n",
      "train loss:   0.792481\n",
      "train loss:   0.610445\n",
      "train loss:   1.209936\n",
      "train loss:   0.848257\n",
      "train loss:   1.299581\n",
      "train loss:   1.039878\n",
      "train loss:   0.952476\n",
      "train loss:   0.923347\n",
      "train loss:   0.923146\n",
      "train loss:   0.914108\n",
      "########### epoch 88 ###########\n",
      "########### loop 16500 ###########\n",
      "test loss:   0.336379   test accuracy:   0.875000\n",
      "########### loop 16500 ###########\n",
      "train loss:   0.885026\n",
      "train loss:   0.964285\n",
      "train loss:   0.969666\n",
      "train loss:   0.934306\n",
      "train loss:   1.321691\n",
      "train loss:   1.229104\n",
      "train loss:   0.912612\n",
      "train loss:   1.194321\n",
      "train loss:   0.733911\n",
      "train loss:   1.108537\n",
      "train loss:   0.866394\n",
      "train loss:   1.074576\n",
      "train loss:   1.125974\n",
      "train loss:   1.253789\n",
      "train loss:   0.825897\n",
      "train loss:   1.170883\n",
      "train loss:   0.964024\n",
      "train loss:   0.762366\n",
      "train loss:   0.810758\n",
      "train loss:   0.900202\n",
      "train loss:   0.697397\n",
      "train loss:   1.263361\n",
      "train loss:   1.067979\n",
      "train loss:   0.714960\n",
      "train loss:   1.195689\n",
      "train loss:   1.064703\n",
      "train loss:   0.706896\n",
      "train loss:   0.843570\n",
      "train loss:   0.769797\n",
      "train loss:   0.813720\n",
      "train loss:   1.032005\n",
      "train loss:   1.139599\n",
      "train loss:   1.048922\n",
      "train loss:   1.116542\n",
      "train loss:   0.735611\n",
      "train loss:   0.882758\n",
      "train loss:   1.096825\n",
      "train loss:   1.063889\n",
      "train loss:   1.240286\n",
      "train loss:   1.136932\n",
      "train loss:   1.310760\n",
      "train loss:   0.688546\n",
      "train loss:   0.942757\n",
      "train loss:   1.033849\n",
      "train loss:   1.036989\n",
      "train loss:   0.859267\n",
      "train loss:   0.968391\n",
      "train loss:   1.314769\n",
      "train loss:   0.788809\n",
      "train loss:   0.994722\n",
      "########### epoch 89 ###########\n",
      "########### loop 16550 ###########\n",
      "test loss:   0.134355   test accuracy:   1.000000\n",
      "########### loop 16550 ###########\n",
      "train loss:   0.587700\n",
      "train loss:   1.119740\n",
      "train loss:   1.143657\n",
      "train loss:   1.044294\n",
      "train loss:   0.671555\n",
      "train loss:   1.089988\n",
      "train loss:   0.732914\n",
      "train loss:   1.033571\n",
      "train loss:   1.191513\n",
      "train loss:   0.990424\n",
      "train loss:   0.860156\n",
      "train loss:   1.358338\n",
      "train loss:   0.587786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.971436\n",
      "train loss:   0.980947\n",
      "train loss:   0.926552\n",
      "train loss:   0.919616\n",
      "train loss:   1.032909\n",
      "train loss:   1.107486\n",
      "train loss:   0.823040\n",
      "train loss:   1.098263\n",
      "train loss:   1.003131\n",
      "train loss:   1.043586\n",
      "train loss:   0.909498\n",
      "train loss:   0.807396\n",
      "train loss:   0.642527\n",
      "train loss:   0.701256\n",
      "train loss:   1.092736\n",
      "train loss:   1.323626\n",
      "train loss:   1.194100\n",
      "train loss:   1.028569\n",
      "train loss:   0.714479\n",
      "train loss:   1.122060\n",
      "train loss:   0.853101\n",
      "train loss:   1.143110\n",
      "train loss:   1.139622\n",
      "train loss:   0.902724\n",
      "train loss:   0.753395\n",
      "train loss:   1.289510\n",
      "train loss:   1.166522\n",
      "train loss:   1.008592\n",
      "train loss:   0.669813\n",
      "train loss:   0.589506\n",
      "train loss:   0.943674\n",
      "train loss:   0.602346\n",
      "train loss:   0.979479\n",
      "train loss:   1.072799\n",
      "train loss:   0.701144\n",
      "train loss:   1.104707\n",
      "train loss:   0.975167\n",
      "########### epoch 89 ###########\n",
      "########### loop 16600 ###########\n",
      "test loss:   0.269513   test accuracy:   0.916667\n",
      "########### loop 16600 ###########\n",
      "train loss:   0.664098\n",
      "train loss:   0.987325\n",
      "train loss:   1.192171\n",
      "train loss:   0.736526\n",
      "train loss:   0.771909\n",
      "train loss:   1.126226\n",
      "train loss:   1.211994\n",
      "train loss:   0.772016\n",
      "train loss:   0.876504\n",
      "train loss:   1.334721\n",
      "train loss:   0.995046\n",
      "train loss:   0.997975\n",
      "train loss:   1.018748\n",
      "train loss:   0.944514\n",
      "train loss:   1.313928\n",
      "train loss:   0.627094\n",
      "train loss:   0.774805\n",
      "train loss:   1.260811\n",
      "train loss:   1.384825\n",
      "train loss:   0.832252\n",
      "train loss:   0.766160\n",
      "train loss:   0.947787\n",
      "train loss:   0.972256\n",
      "train loss:   0.822186\n",
      "train loss:   1.393517\n",
      "train loss:   1.129025\n",
      "train loss:   1.028685\n",
      "train loss:   0.807787\n",
      "train loss:   0.730823\n",
      "train loss:   1.152331\n",
      "train loss:   1.049042\n",
      "train loss:   1.124762\n",
      "train loss:   0.909024\n",
      "train loss:   1.038484\n",
      "train loss:   1.145138\n",
      "train loss:   0.829119\n",
      "train loss:   0.803533\n",
      "train loss:   0.722498\n",
      "train loss:   0.858969\n",
      "train loss:   0.984775\n",
      "train loss:   0.704705\n",
      "train loss:   1.179201\n",
      "train loss:   0.805025\n",
      "train loss:   0.731753\n",
      "train loss:   1.205768\n",
      "train loss:   0.656588\n",
      "train loss:   0.806572\n",
      "train loss:   1.191276\n",
      "train loss:   0.968721\n",
      "train loss:   0.840565\n",
      "########### epoch 89 ###########\n",
      "########### loop 16650 ###########\n",
      "test loss:   0.362031   test accuracy:   0.833333\n",
      "########### loop 16650 ###########\n",
      "train loss:   1.165605\n",
      "train loss:   1.153666\n",
      "train loss:   0.946567\n",
      "train loss:   0.688555\n",
      "train loss:   0.735858\n",
      "train loss:   1.101152\n",
      "train loss:   1.013369\n",
      "train loss:   1.019609\n",
      "train loss:   0.841686\n",
      "train loss:   1.133937\n",
      "train loss:   0.636757\n",
      "train loss:   0.746825\n",
      "train loss:   1.032163\n",
      "train loss:   1.020186\n",
      "train loss:   1.373190\n",
      "train loss:   1.156735\n",
      "train loss:   0.947360\n",
      "train loss:   1.183228\n",
      "train loss:   0.778840\n",
      "train loss:   1.044851\n",
      "train loss:   0.901165\n",
      "train loss:   0.990725\n",
      "train loss:   1.247718\n",
      "train loss:   0.779586\n",
      "train loss:   0.883668\n",
      "train loss:   1.136353\n",
      "train loss:   0.962697\n",
      "train loss:   0.800544\n",
      "train loss:   0.902197\n",
      "train loss:   0.943017\n",
      "train loss:   0.976134\n",
      "train loss:   0.858661\n",
      "train loss:   1.056440\n",
      "train loss:   1.385673\n",
      "train loss:   1.310781\n",
      "train loss:   0.590803\n",
      "train loss:   1.080439\n",
      "train loss:   0.487461\n",
      "train loss:   0.877755\n",
      "train loss:   1.339252\n",
      "train loss:   1.096113\n",
      "train loss:   1.272863\n",
      "train loss:   0.707187\n",
      "train loss:   1.038611\n",
      "train loss:   1.003331\n",
      "train loss:   1.043914\n",
      "train loss:   1.064384\n",
      "train loss:   0.729365\n",
      "train loss:   0.604702\n",
      "train loss:   0.759378\n",
      "########### epoch 89 ###########\n",
      "########### loop 16700 ###########\n",
      "test loss:   0.695479   test accuracy:   0.833333\n",
      "########### loop 16700 ###########\n",
      "train loss:   1.097700\n",
      "train loss:   0.770515\n",
      "train loss:   1.149070\n",
      "train loss:   0.671745\n",
      "train loss:   1.056128\n",
      "train loss:   0.744127\n",
      "train loss:   0.885942\n",
      "train loss:   0.710835\n",
      "train loss:   1.070739\n",
      "train loss:   0.966883\n",
      "train loss:   1.225225\n",
      "train loss:   0.810845\n",
      "train loss:   0.847460\n",
      "train loss:   0.938014\n",
      "train loss:   0.915202\n",
      "train loss:   1.295149\n",
      "train loss:   0.500971\n",
      "train loss:   0.858030\n",
      "train loss:   1.244012\n",
      "train loss:   0.690227\n",
      "train loss:   0.906990\n",
      "train loss:   0.950460\n",
      "train loss:   1.151974\n",
      "train loss:   1.083275\n",
      "train loss:   1.409039\n",
      "train loss:   1.240497\n",
      "train loss:   1.083535\n",
      "train loss:   1.158454\n",
      "train loss:   0.809936\n",
      "train loss:   0.805118\n",
      "train loss:   0.892961\n",
      "train loss:   1.219627\n",
      "train loss:   1.184679\n",
      "train loss:   1.130346\n",
      "train loss:   0.818576\n",
      "train loss:   1.372144\n",
      "train loss:   1.461210\n",
      "train loss:   0.875943\n",
      "train loss:   0.732604\n",
      "train loss:   1.031799\n",
      "train loss:   1.224302\n",
      "train loss:   1.021617\n",
      "train loss:   0.555322\n",
      "train loss:   0.834977\n",
      "train loss:   0.563213\n",
      "train loss:   0.867921\n",
      "train loss:   0.788414\n",
      "train loss:   0.915907\n",
      "train loss:   1.017296\n",
      "train loss:   1.078891\n",
      "########### epoch 90 ###########\n",
      "########### loop 16750 ###########\n",
      "test loss:   0.252609   test accuracy:   0.958333\n",
      "########### loop 16750 ###########\n",
      "train loss:   1.061596\n",
      "train loss:   1.056866\n",
      "train loss:   1.117429\n",
      "train loss:   1.124712\n",
      "train loss:   1.029217\n",
      "train loss:   0.973299\n",
      "train loss:   0.989441\n",
      "train loss:   0.828991\n",
      "train loss:   0.886098\n",
      "train loss:   1.092068\n",
      "train loss:   1.026007\n",
      "train loss:   0.848101\n",
      "train loss:   1.255263\n",
      "train loss:   0.689727\n",
      "train loss:   1.024818\n",
      "train loss:   1.057255\n",
      "train loss:   1.102501\n",
      "train loss:   0.766013\n",
      "train loss:   1.034677\n",
      "train loss:   1.174278\n",
      "train loss:   0.887151\n",
      "train loss:   1.221300\n",
      "train loss:   0.915902\n",
      "train loss:   0.919372\n",
      "train loss:   0.800190\n",
      "train loss:   0.729579\n",
      "train loss:   0.733821\n",
      "train loss:   1.199495\n",
      "train loss:   1.287139\n",
      "train loss:   0.901864\n",
      "train loss:   1.164542\n",
      "train loss:   1.364535\n",
      "train loss:   0.896505\n",
      "train loss:   0.767352\n",
      "train loss:   0.954852\n",
      "train loss:   1.065140\n",
      "train loss:   1.195806\n",
      "train loss:   0.854461\n",
      "train loss:   0.817613\n",
      "train loss:   0.940513\n",
      "train loss:   0.653231\n",
      "train loss:   1.042841\n",
      "train loss:   0.909094\n",
      "train loss:   0.948949\n",
      "train loss:   0.913183\n",
      "train loss:   1.176302\n",
      "train loss:   1.055020\n",
      "train loss:   1.132170\n",
      "train loss:   1.083268\n",
      "train loss:   1.052491\n",
      "########### epoch 90 ###########\n",
      "########### loop 16800 ###########\n",
      "test loss:   0.276621   test accuracy:   0.916667\n",
      "########### loop 16800 ###########\n",
      "train loss:   0.920884\n",
      "train loss:   1.078796\n",
      "train loss:   0.512647\n",
      "train loss:   0.811847\n",
      "train loss:   0.914624\n",
      "train loss:   0.604731\n",
      "train loss:   0.894540\n",
      "train loss:   1.021988\n",
      "train loss:   0.848103\n",
      "train loss:   0.787872\n",
      "train loss:   1.076976\n",
      "train loss:   0.857880\n",
      "train loss:   1.153435\n",
      "train loss:   0.878241\n",
      "train loss:   1.458557\n",
      "train loss:   1.034248\n",
      "train loss:   0.806106\n",
      "train loss:   0.748031\n",
      "train loss:   1.096013\n",
      "train loss:   1.047789\n",
      "train loss:   0.948009\n",
      "train loss:   0.970182\n",
      "train loss:   0.963388\n",
      "train loss:   1.211550\n",
      "train loss:   1.334102\n",
      "train loss:   0.671956\n",
      "train loss:   0.433638\n",
      "train loss:   0.536664\n",
      "train loss:   0.940665\n",
      "train loss:   1.143141\n",
      "train loss:   1.297046\n",
      "train loss:   0.571778\n",
      "train loss:   0.985096\n",
      "train loss:   1.188135\n",
      "train loss:   1.066027\n",
      "train loss:   0.903183\n",
      "train loss:   0.954221\n",
      "train loss:   0.882726\n",
      "train loss:   0.703660\n",
      "train loss:   1.486093\n",
      "train loss:   0.998418\n",
      "train loss:   0.547446\n",
      "train loss:   0.689112\n",
      "train loss:   0.978241\n",
      "train loss:   0.761399\n",
      "train loss:   0.619483\n",
      "train loss:   1.051411\n",
      "train loss:   1.030635\n",
      "train loss:   0.996086\n",
      "train loss:   0.744929\n",
      "########### epoch 90 ###########\n",
      "########### loop 16850 ###########\n",
      "test loss:   0.478591   test accuracy:   0.833333\n",
      "########### loop 16850 ###########\n",
      "train loss:   0.839171\n",
      "train loss:   0.727811\n",
      "train loss:   1.111301\n",
      "train loss:   0.829461\n",
      "train loss:   0.954337\n",
      "train loss:   0.849385\n",
      "train loss:   0.755494\n",
      "train loss:   0.940006\n",
      "train loss:   1.188828\n",
      "train loss:   1.090991\n",
      "train loss:   0.598871\n",
      "train loss:   1.040401\n",
      "train loss:   0.923136\n",
      "train loss:   0.844836\n",
      "train loss:   1.086086\n",
      "train loss:   1.077145\n",
      "train loss:   0.957289\n",
      "train loss:   0.922776\n",
      "train loss:   1.206124\n",
      "train loss:   0.906808\n",
      "train loss:   1.264801\n",
      "train loss:   0.959824\n",
      "train loss:   0.971413\n",
      "train loss:   1.025065\n",
      "train loss:   1.000501\n",
      "train loss:   0.885268\n",
      "train loss:   1.258429\n",
      "train loss:   0.715594\n",
      "train loss:   0.635049\n",
      "train loss:   0.892145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.227244\n",
      "train loss:   0.837130\n",
      "train loss:   0.899337\n",
      "train loss:   1.026628\n",
      "train loss:   0.904795\n",
      "train loss:   0.607027\n",
      "train loss:   0.726907\n",
      "train loss:   1.129480\n",
      "train loss:   1.198658\n",
      "train loss:   0.840964\n",
      "train loss:   0.808447\n",
      "train loss:   0.968232\n",
      "train loss:   0.737961\n",
      "train loss:   1.014453\n",
      "train loss:   1.173086\n",
      "train loss:   1.044979\n",
      "train loss:   0.650462\n",
      "train loss:   0.885933\n",
      "train loss:   1.111042\n",
      "train loss:   0.748720\n",
      "########### epoch 90 ###########\n",
      "########### loop 16900 ###########\n",
      "test loss:   0.411695   test accuracy:   0.875000\n",
      "########### loop 16900 ###########\n",
      "train loss:   0.543583\n",
      "train loss:   0.925153\n",
      "train loss:   1.211719\n",
      "train loss:   1.473556\n",
      "train loss:   1.309592\n",
      "train loss:   0.965427\n",
      "train loss:   1.039093\n",
      "train loss:   1.157036\n",
      "train loss:   1.072277\n",
      "train loss:   1.013997\n",
      "train loss:   0.962991\n",
      "train loss:   1.095850\n",
      "train loss:   1.078739\n",
      "train loss:   0.791108\n",
      "train loss:   1.127952\n",
      "train loss:   0.939652\n",
      "train loss:   0.810212\n",
      "train loss:   1.014264\n",
      "train loss:   0.903167\n",
      "train loss:   0.743033\n",
      "train loss:   0.880945\n",
      "train loss:   0.902194\n",
      "train loss:   0.895427\n",
      "train loss:   0.993141\n",
      "train loss:   1.026305\n",
      "train loss:   0.981520\n",
      "train loss:   1.065192\n",
      "train loss:   1.010944\n",
      "train loss:   1.370385\n",
      "train loss:   1.201877\n",
      "train loss:   0.853277\n",
      "train loss:   1.303864\n",
      "train loss:   1.217180\n",
      "train loss:   1.090473\n",
      "train loss:   0.936881\n",
      "train loss:   1.274552\n",
      "train loss:   0.942465\n",
      "train loss:   0.851418\n",
      "train loss:   0.986014\n",
      "train loss:   0.452041\n",
      "train loss:   0.811257\n",
      "train loss:   0.895137\n",
      "train loss:   1.186184\n",
      "train loss:   1.003045\n",
      "train loss:   1.144511\n",
      "train loss:   0.750689\n",
      "train loss:   0.827272\n",
      "train loss:   0.653521\n",
      "train loss:   1.459949\n",
      "train loss:   0.621482\n",
      "########### epoch 91 ###########\n",
      "########### loop 16950 ###########\n",
      "test loss:   0.206674   test accuracy:   0.958333\n",
      "########### loop 16950 ###########\n",
      "train loss:   0.834840\n",
      "train loss:   1.021302\n",
      "train loss:   1.338072\n",
      "train loss:   0.911386\n",
      "train loss:   0.788117\n",
      "train loss:   1.001580\n",
      "train loss:   0.943134\n",
      "train loss:   1.126748\n",
      "train loss:   0.754535\n",
      "train loss:   1.027642\n",
      "train loss:   0.986297\n",
      "train loss:   1.014640\n",
      "train loss:   1.023829\n",
      "train loss:   0.701990\n",
      "train loss:   0.618749\n",
      "train loss:   1.214421\n",
      "train loss:   1.361689\n",
      "train loss:   0.792811\n",
      "train loss:   0.643842\n",
      "train loss:   1.149357\n",
      "train loss:   1.161766\n",
      "train loss:   0.875545\n",
      "train loss:   0.717361\n",
      "train loss:   0.744469\n",
      "train loss:   1.097909\n",
      "train loss:   1.195965\n",
      "train loss:   0.787316\n",
      "train loss:   0.918075\n",
      "train loss:   1.110603\n",
      "train loss:   1.030388\n",
      "train loss:   1.027482\n",
      "train loss:   0.916831\n",
      "train loss:   0.989351\n",
      "train loss:   1.210415\n",
      "train loss:   1.232647\n",
      "train loss:   1.314587\n",
      "train loss:   0.945716\n",
      "train loss:   1.147165\n",
      "train loss:   1.031929\n",
      "train loss:   1.303598\n",
      "train loss:   1.186452\n",
      "train loss:   0.732758\n",
      "train loss:   0.979968\n",
      "train loss:   0.968302\n",
      "train loss:   1.033834\n",
      "train loss:   0.908962\n",
      "train loss:   0.914684\n",
      "train loss:   1.141450\n",
      "train loss:   0.872023\n",
      "train loss:   1.055495\n",
      "########### epoch 91 ###########\n",
      "########### loop 17000 ###########\n",
      "test loss:   0.530199   test accuracy:   0.875000\n",
      "########### loop 17000 ###########\n",
      "train loss:   1.054101\n",
      "train loss:   1.059495\n",
      "train loss:   1.202341\n",
      "train loss:   0.919188\n",
      "train loss:   1.040701\n",
      "train loss:   0.753297\n",
      "train loss:   1.161062\n",
      "train loss:   0.694670\n",
      "train loss:   1.071854\n",
      "train loss:   0.646229\n",
      "train loss:   0.907362\n",
      "train loss:   0.726310\n",
      "train loss:   0.944359\n",
      "train loss:   0.465485\n",
      "train loss:   1.165192\n",
      "train loss:   0.978330\n",
      "train loss:   1.121556\n",
      "train loss:   0.949410\n",
      "train loss:   1.033911\n",
      "train loss:   1.308178\n",
      "train loss:   1.127225\n",
      "train loss:   1.189672\n",
      "train loss:   1.048076\n",
      "train loss:   1.131389\n",
      "train loss:   0.771528\n",
      "train loss:   1.217718\n",
      "train loss:   1.245355\n",
      "train loss:   0.858042\n",
      "train loss:   0.930971\n",
      "train loss:   0.481165\n",
      "train loss:   0.994828\n",
      "train loss:   1.084859\n",
      "train loss:   1.290605\n",
      "train loss:   0.854381\n",
      "train loss:   1.220628\n",
      "train loss:   1.169598\n",
      "train loss:   0.639723\n",
      "train loss:   0.818954\n",
      "train loss:   1.082903\n",
      "train loss:   0.869170\n",
      "train loss:   1.426245\n",
      "train loss:   1.065650\n",
      "train loss:   1.034500\n",
      "train loss:   0.977391\n",
      "train loss:   0.918664\n",
      "train loss:   1.198368\n",
      "train loss:   1.091395\n",
      "train loss:   1.159140\n",
      "train loss:   1.123700\n",
      "train loss:   1.334801\n",
      "########### epoch 91 ###########\n",
      "########### loop 17050 ###########\n",
      "test loss:   0.653905   test accuracy:   0.708333\n",
      "########### loop 17050 ###########\n",
      "train loss:   0.931418\n",
      "train loss:   1.110840\n",
      "train loss:   0.977348\n",
      "train loss:   0.718552\n",
      "train loss:   0.881300\n",
      "train loss:   1.061779\n",
      "train loss:   0.641771\n",
      "train loss:   1.055257\n",
      "train loss:   0.952665\n",
      "train loss:   0.872826\n",
      "train loss:   1.109616\n",
      "train loss:   1.108600\n",
      "train loss:   0.992610\n",
      "train loss:   0.837197\n",
      "train loss:   0.794833\n",
      "train loss:   0.836336\n",
      "train loss:   0.869857\n",
      "train loss:   0.603911\n",
      "train loss:   1.051167\n",
      "train loss:   1.114755\n",
      "train loss:   1.084788\n",
      "train loss:   0.497019\n",
      "train loss:   0.861739\n",
      "train loss:   1.528509\n",
      "train loss:   0.674314\n",
      "train loss:   1.272331\n",
      "train loss:   0.968795\n",
      "train loss:   1.135382\n",
      "train loss:   0.815526\n",
      "train loss:   1.172368\n",
      "train loss:   1.077389\n",
      "train loss:   0.789405\n",
      "train loss:   1.195789\n",
      "train loss:   1.181616\n",
      "train loss:   1.013984\n",
      "train loss:   1.173299\n",
      "train loss:   1.330375\n",
      "train loss:   0.757825\n",
      "train loss:   0.842656\n",
      "train loss:   0.699442\n",
      "train loss:   0.806637\n",
      "train loss:   0.929792\n",
      "train loss:   0.867642\n",
      "train loss:   0.825459\n",
      "train loss:   1.284147\n",
      "train loss:   1.132252\n",
      "train loss:   0.583096\n",
      "train loss:   0.729244\n",
      "train loss:   1.289248\n",
      "train loss:   1.014826\n",
      "########### epoch 91 ###########\n",
      "########### loop 17100 ###########\n",
      "test loss:   0.455734   test accuracy:   0.875000\n",
      "########### loop 17100 ###########\n",
      "train loss:   1.137536\n",
      "train loss:   1.055243\n",
      "train loss:   0.894102\n",
      "train loss:   1.096103\n",
      "train loss:   0.851377\n",
      "train loss:   0.695848\n",
      "train loss:   0.762719\n",
      "train loss:   0.752567\n",
      "train loss:   0.862302\n",
      "train loss:   0.860165\n",
      "train loss:   0.770390\n",
      "train loss:   0.839966\n",
      "train loss:   1.016197\n",
      "train loss:   0.862561\n",
      "train loss:   1.222191\n",
      "train loss:   0.753249\n",
      "train loss:   0.878226\n",
      "train loss:   0.724915\n",
      "train loss:   0.773766\n",
      "train loss:   1.017292\n",
      "train loss:   1.072595\n",
      "train loss:   0.832881\n",
      "train loss:   0.990343\n",
      "train loss:   0.851010\n",
      "train loss:   0.815248\n",
      "train loss:   0.921318\n",
      "train loss:   0.855781\n",
      "train loss:   0.867139\n",
      "train loss:   1.063571\n",
      "train loss:   0.970339\n",
      "train loss:   0.971984\n",
      "train loss:   1.222800\n",
      "train loss:   0.891218\n",
      "train loss:   0.893822\n",
      "train loss:   0.987851\n",
      "train loss:   0.678295\n",
      "train loss:   1.317932\n",
      "train loss:   1.150128\n",
      "train loss:   0.962096\n",
      "train loss:   1.120364\n",
      "train loss:   0.596548\n",
      "train loss:   0.898843\n",
      "train loss:   0.810342\n",
      "train loss:   0.852370\n",
      "train loss:   1.144501\n",
      "train loss:   1.226570\n",
      "train loss:   0.624997\n",
      "train loss:   0.801310\n",
      "train loss:   0.612936\n",
      "train loss:   1.029969\n",
      "########### epoch 92 ###########\n",
      "########### loop 17150 ###########\n",
      "test loss:   0.215732   test accuracy:   0.916667\n",
      "########### loop 17150 ###########\n",
      "train loss:   0.991475\n",
      "train loss:   1.046481\n",
      "train loss:   0.900073\n",
      "train loss:   0.738879\n",
      "train loss:   0.769975\n",
      "train loss:   0.905137\n",
      "train loss:   1.057244\n",
      "train loss:   0.895545\n",
      "train loss:   1.148792\n",
      "train loss:   0.944840\n",
      "train loss:   0.867803\n",
      "train loss:   0.858435\n",
      "train loss:   0.961763\n",
      "train loss:   0.936616\n",
      "train loss:   0.715408\n",
      "train loss:   0.820181\n",
      "train loss:   0.827367\n",
      "train loss:   0.734837\n",
      "train loss:   1.304739\n",
      "train loss:   0.865180\n",
      "train loss:   1.130189\n",
      "train loss:   0.721299\n",
      "train loss:   0.704121\n",
      "train loss:   0.849888\n",
      "train loss:   0.882500\n",
      "train loss:   0.863937\n",
      "train loss:   0.883101\n",
      "train loss:   0.926643\n",
      "train loss:   0.811683\n",
      "train loss:   0.881145\n",
      "train loss:   0.977443\n",
      "train loss:   1.295732\n",
      "train loss:   1.166232\n",
      "train loss:   1.296085\n",
      "train loss:   0.841217\n",
      "train loss:   1.497767\n",
      "train loss:   1.114165\n",
      "train loss:   1.041530\n",
      "train loss:   0.718432\n",
      "train loss:   0.585579\n",
      "train loss:   0.848082\n",
      "train loss:   0.838294\n",
      "train loss:   1.139358\n",
      "train loss:   1.060108\n",
      "train loss:   1.069942\n",
      "train loss:   0.629040\n",
      "train loss:   1.204909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.903232\n",
      "train loss:   1.132709\n",
      "train loss:   0.828086\n",
      "########### epoch 92 ###########\n",
      "########### loop 17200 ###########\n",
      "test loss:   0.340891   test accuracy:   0.916667\n",
      "########### loop 17200 ###########\n",
      "train loss:   0.907101\n",
      "train loss:   0.905441\n",
      "train loss:   1.001954\n",
      "train loss:   1.103417\n",
      "train loss:   0.760596\n",
      "train loss:   0.943431\n",
      "train loss:   0.830543\n",
      "train loss:   1.101435\n",
      "train loss:   0.977980\n",
      "train loss:   0.992785\n",
      "train loss:   0.919275\n",
      "train loss:   0.915576\n",
      "train loss:   1.039029\n",
      "train loss:   0.572808\n",
      "train loss:   0.725164\n",
      "train loss:   0.893783\n",
      "train loss:   0.931179\n",
      "train loss:   0.719019\n",
      "train loss:   1.127603\n",
      "train loss:   0.682895\n",
      "train loss:   0.851739\n",
      "train loss:   1.048547\n",
      "train loss:   1.063278\n",
      "train loss:   0.977007\n",
      "train loss:   0.999247\n",
      "train loss:   0.892513\n",
      "train loss:   1.162209\n",
      "train loss:   0.959350\n",
      "train loss:   1.103958\n",
      "train loss:   0.642953\n",
      "train loss:   1.012199\n",
      "train loss:   0.975297\n",
      "train loss:   1.082496\n",
      "train loss:   0.824084\n",
      "train loss:   1.191022\n",
      "train loss:   0.800255\n",
      "train loss:   1.287112\n",
      "train loss:   0.806696\n",
      "train loss:   0.528309\n",
      "train loss:   0.773347\n",
      "train loss:   0.745465\n",
      "train loss:   0.988047\n",
      "train loss:   0.858593\n",
      "train loss:   0.555814\n",
      "train loss:   0.815308\n",
      "train loss:   1.137436\n",
      "train loss:   0.806544\n",
      "train loss:   0.840039\n",
      "train loss:   0.867594\n",
      "train loss:   0.574880\n",
      "########### epoch 92 ###########\n",
      "########### loop 17250 ###########\n",
      "test loss:   0.154313   test accuracy:   1.000000\n",
      "########### loop 17250 ###########\n",
      "train loss:   0.886227\n",
      "train loss:   0.829831\n",
      "train loss:   0.873850\n",
      "train loss:   1.061096\n",
      "train loss:   1.005323\n",
      "train loss:   0.909586\n",
      "train loss:   1.124384\n",
      "train loss:   0.969478\n",
      "train loss:   1.039360\n",
      "train loss:   1.244712\n",
      "train loss:   0.501032\n",
      "train loss:   1.196190\n",
      "train loss:   1.463188\n",
      "train loss:   0.649656\n",
      "train loss:   0.861346\n",
      "train loss:   0.651708\n",
      "train loss:   0.797775\n",
      "train loss:   0.924819\n",
      "train loss:   1.218272\n",
      "train loss:   0.718048\n",
      "train loss:   0.963811\n",
      "train loss:   0.939775\n",
      "train loss:   0.767589\n",
      "train loss:   0.689690\n",
      "train loss:   0.719464\n",
      "train loss:   0.925887\n",
      "train loss:   1.164992\n",
      "train loss:   1.044894\n",
      "train loss:   0.933208\n",
      "train loss:   1.281346\n",
      "train loss:   1.089529\n",
      "train loss:   0.749382\n",
      "train loss:   1.315120\n",
      "train loss:   0.951084\n",
      "train loss:   0.935223\n",
      "train loss:   1.252534\n",
      "train loss:   1.117574\n",
      "train loss:   0.818401\n",
      "train loss:   0.792112\n",
      "train loss:   1.185182\n",
      "train loss:   0.886318\n",
      "train loss:   1.244524\n",
      "train loss:   0.829566\n",
      "train loss:   0.722364\n",
      "train loss:   0.989046\n",
      "train loss:   0.828432\n",
      "train loss:   0.942453\n",
      "train loss:   0.999117\n",
      "train loss:   1.184410\n",
      "train loss:   0.500307\n",
      "########### epoch 93 ###########\n",
      "########### loop 17300 ###########\n",
      "test loss:   0.350673   test accuracy:   0.916667\n",
      "########### loop 17300 ###########\n",
      "train loss:   0.878158\n",
      "train loss:   1.029100\n",
      "train loss:   1.081632\n",
      "train loss:   1.328874\n",
      "train loss:   1.066837\n",
      "train loss:   1.172999\n",
      "train loss:   1.287299\n",
      "train loss:   0.733918\n",
      "train loss:   0.514341\n",
      "train loss:   1.481703\n",
      "train loss:   1.342081\n",
      "train loss:   1.532438\n",
      "train loss:   1.092486\n",
      "train loss:   0.836965\n",
      "train loss:   0.794670\n",
      "train loss:   1.281468\n",
      "train loss:   0.902718\n",
      "train loss:   1.574573\n",
      "train loss:   0.985133\n",
      "train loss:   0.844531\n",
      "train loss:   1.109689\n",
      "train loss:   1.002466\n",
      "train loss:   0.720946\n",
      "train loss:   1.204630\n",
      "train loss:   1.479262\n",
      "train loss:   1.105867\n",
      "train loss:   0.896529\n",
      "train loss:   1.072457\n",
      "train loss:   0.522050\n",
      "train loss:   1.140585\n",
      "train loss:   0.863261\n",
      "train loss:   1.157520\n",
      "train loss:   0.895008\n",
      "train loss:   1.487686\n",
      "train loss:   1.482286\n",
      "train loss:   0.883567\n",
      "train loss:   0.874217\n",
      "train loss:   0.688503\n",
      "train loss:   1.129441\n",
      "train loss:   0.923857\n",
      "train loss:   1.012752\n",
      "train loss:   1.121798\n",
      "train loss:   0.995544\n",
      "train loss:   0.800097\n",
      "train loss:   0.786291\n",
      "train loss:   0.792164\n",
      "train loss:   0.702667\n",
      "train loss:   0.728597\n",
      "train loss:   0.901231\n",
      "train loss:   0.839725\n",
      "########### epoch 93 ###########\n",
      "########### loop 17350 ###########\n",
      "test loss:   0.214782   test accuracy:   0.958333\n",
      "########### loop 17350 ###########\n",
      "train loss:   1.112817\n",
      "train loss:   1.063989\n",
      "train loss:   0.779756\n",
      "train loss:   1.054348\n",
      "train loss:   0.903162\n",
      "train loss:   1.001508\n",
      "train loss:   1.314683\n",
      "train loss:   1.495079\n",
      "train loss:   0.656351\n",
      "train loss:   1.006602\n",
      "train loss:   0.889229\n",
      "train loss:   1.261012\n",
      "train loss:   1.124271\n",
      "train loss:   0.907073\n",
      "train loss:   0.895124\n",
      "train loss:   0.902034\n",
      "train loss:   1.050955\n",
      "train loss:   0.874348\n",
      "train loss:   0.962377\n",
      "train loss:   0.975797\n",
      "train loss:   1.056529\n",
      "train loss:   0.907858\n",
      "train loss:   1.359456\n",
      "train loss:   1.166382\n",
      "train loss:   1.125172\n",
      "train loss:   0.793431\n",
      "train loss:   0.728971\n",
      "train loss:   1.111108\n",
      "train loss:   1.182727\n",
      "train loss:   0.614474\n",
      "train loss:   1.097898\n",
      "train loss:   1.153810\n",
      "train loss:   1.159917\n",
      "train loss:   0.831870\n",
      "train loss:   0.767193\n",
      "train loss:   1.122117\n",
      "train loss:   1.154937\n",
      "train loss:   0.801583\n",
      "train loss:   0.997954\n",
      "train loss:   0.850379\n",
      "train loss:   1.311784\n",
      "train loss:   0.762697\n",
      "train loss:   0.980970\n",
      "train loss:   1.010156\n",
      "train loss:   0.726831\n",
      "train loss:   0.779810\n",
      "train loss:   1.242117\n",
      "train loss:   0.811556\n",
      "train loss:   0.975477\n",
      "train loss:   0.862397\n",
      "########### epoch 93 ###########\n",
      "########### loop 17400 ###########\n",
      "test loss:   0.247867   test accuracy:   0.875000\n",
      "########### loop 17400 ###########\n",
      "train loss:   0.934416\n",
      "train loss:   0.955799\n",
      "train loss:   1.355439\n",
      "train loss:   0.898229\n",
      "train loss:   1.039985\n",
      "train loss:   0.889263\n",
      "train loss:   0.957121\n",
      "train loss:   0.948240\n",
      "train loss:   0.895288\n",
      "train loss:   1.106013\n",
      "train loss:   0.981287\n",
      "train loss:   1.541463\n",
      "train loss:   0.434255\n",
      "train loss:   1.079681\n",
      "train loss:   1.132612\n",
      "train loss:   0.605780\n",
      "train loss:   1.293220\n",
      "train loss:   0.733784\n",
      "train loss:   0.743114\n",
      "train loss:   1.080191\n",
      "train loss:   0.801404\n",
      "train loss:   1.172493\n",
      "train loss:   1.029763\n",
      "train loss:   0.990753\n",
      "train loss:   0.833297\n",
      "train loss:   1.242548\n",
      "train loss:   1.053491\n",
      "train loss:   0.914863\n",
      "train loss:   1.233057\n",
      "train loss:   1.240468\n",
      "train loss:   1.025544\n",
      "train loss:   0.621382\n",
      "train loss:   1.129355\n",
      "train loss:   0.963859\n",
      "train loss:   1.312115\n",
      "train loss:   0.925643\n",
      "train loss:   0.774008\n",
      "train loss:   1.227064\n",
      "train loss:   1.139250\n",
      "train loss:   0.813296\n",
      "train loss:   0.960602\n",
      "train loss:   1.027650\n",
      "train loss:   0.527116\n",
      "train loss:   0.815720\n",
      "train loss:   0.872155\n",
      "train loss:   0.747702\n",
      "train loss:   1.038672\n",
      "train loss:   1.116941\n",
      "train loss:   1.285576\n",
      "train loss:   1.180075\n",
      "########### epoch 93 ###########\n",
      "########### loop 17450 ###########\n",
      "test loss:   0.268335   test accuracy:   0.916667\n",
      "########### loop 17450 ###########\n",
      "train loss:   1.020362\n",
      "train loss:   0.893852\n",
      "train loss:   0.796693\n",
      "train loss:   0.977934\n",
      "train loss:   1.080963\n",
      "train loss:   1.105491\n",
      "train loss:   0.929536\n",
      "train loss:   0.981959\n",
      "train loss:   1.161062\n",
      "train loss:   0.978686\n",
      "train loss:   0.650861\n",
      "train loss:   0.781138\n",
      "train loss:   0.764345\n",
      "train loss:   0.776031\n",
      "train loss:   0.744406\n",
      "train loss:   0.764352\n",
      "train loss:   1.085838\n",
      "train loss:   1.134962\n",
      "train loss:   0.935657\n",
      "train loss:   1.166710\n",
      "train loss:   0.890327\n",
      "train loss:   0.753329\n",
      "train loss:   1.042771\n",
      "train loss:   0.998376\n",
      "train loss:   1.047383\n",
      "train loss:   1.222356\n",
      "train loss:   1.001750\n",
      "train loss:   0.595902\n",
      "train loss:   1.029138\n",
      "train loss:   0.699300\n",
      "train loss:   0.948416\n",
      "train loss:   0.846123\n",
      "train loss:   0.822263\n",
      "train loss:   1.001522\n",
      "train loss:   0.980276\n",
      "train loss:   1.093280\n",
      "train loss:   0.994550\n",
      "train loss:   0.976175\n",
      "train loss:   0.917562\n",
      "train loss:   0.929355\n",
      "train loss:   0.831265\n",
      "train loss:   0.888311\n",
      "train loss:   0.986047\n",
      "train loss:   0.880411\n",
      "train loss:   0.635929\n",
      "train loss:   0.874972\n",
      "train loss:   1.013575\n",
      "train loss:   1.098872\n",
      "train loss:   0.738852\n",
      "train loss:   0.968294\n",
      "########### epoch 94 ###########\n",
      "########### loop 17500 ###########\n",
      "test loss:   0.163198   test accuracy:   1.000000\n",
      "########### loop 17500 ###########\n",
      "train loss:   1.245099\n",
      "train loss:   1.218580\n",
      "train loss:   0.995103\n",
      "train loss:   0.728600\n",
      "train loss:   0.995695\n",
      "train loss:   1.036723\n",
      "train loss:   0.554033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.803308\n",
      "train loss:   0.507198\n",
      "train loss:   0.705749\n",
      "train loss:   0.797259\n",
      "train loss:   0.991168\n",
      "train loss:   0.863540\n",
      "train loss:   0.910910\n",
      "train loss:   0.979836\n",
      "train loss:   1.399845\n",
      "train loss:   0.774672\n",
      "train loss:   1.189249\n",
      "train loss:   1.188959\n",
      "train loss:   0.520670\n",
      "train loss:   1.118632\n",
      "train loss:   1.041778\n",
      "train loss:   0.694392\n",
      "train loss:   1.117468\n",
      "train loss:   1.134614\n",
      "train loss:   1.338363\n",
      "train loss:   1.042230\n",
      "train loss:   0.663726\n",
      "train loss:   1.036845\n",
      "train loss:   1.089251\n",
      "train loss:   0.536213\n",
      "train loss:   1.265752\n",
      "train loss:   0.914072\n",
      "train loss:   0.887378\n",
      "train loss:   0.867776\n",
      "train loss:   0.974246\n",
      "train loss:   0.723316\n",
      "train loss:   0.825177\n",
      "train loss:   0.858859\n",
      "train loss:   1.073339\n",
      "train loss:   0.815013\n",
      "train loss:   0.842879\n",
      "train loss:   0.960381\n",
      "train loss:   0.812246\n",
      "train loss:   1.015689\n",
      "train loss:   1.478977\n",
      "train loss:   1.057474\n",
      "train loss:   0.699301\n",
      "train loss:   0.791104\n",
      "train loss:   1.083447\n",
      "########### epoch 94 ###########\n",
      "########### loop 17550 ###########\n",
      "test loss:   0.265556   test accuracy:   0.916667\n",
      "########### loop 17550 ###########\n",
      "train loss:   0.986557\n",
      "train loss:   0.942420\n",
      "train loss:   0.847115\n",
      "train loss:   1.272728\n",
      "train loss:   1.389738\n",
      "train loss:   1.266821\n",
      "train loss:   1.215119\n",
      "train loss:   0.985427\n",
      "train loss:   1.059226\n",
      "train loss:   1.314763\n",
      "train loss:   0.880020\n",
      "train loss:   0.989465\n",
      "train loss:   1.296958\n",
      "train loss:   0.790960\n",
      "train loss:   1.190795\n",
      "train loss:   1.023518\n",
      "train loss:   0.871674\n",
      "train loss:   1.064155\n",
      "train loss:   1.117168\n",
      "train loss:   0.652165\n",
      "train loss:   0.851295\n",
      "train loss:   0.828378\n",
      "train loss:   1.008205\n",
      "train loss:   0.685199\n",
      "train loss:   0.975354\n",
      "train loss:   1.102929\n",
      "train loss:   0.984618\n",
      "train loss:   1.222360\n",
      "train loss:   0.684586\n",
      "train loss:   1.014362\n",
      "train loss:   0.917132\n",
      "train loss:   0.984442\n",
      "train loss:   1.153592\n",
      "train loss:   0.820031\n",
      "train loss:   0.797557\n",
      "train loss:   1.057215\n",
      "train loss:   1.118412\n",
      "train loss:   0.963944\n",
      "train loss:   0.911245\n",
      "train loss:   0.632633\n",
      "train loss:   1.204512\n",
      "train loss:   1.011207\n",
      "train loss:   0.817212\n",
      "train loss:   1.100511\n",
      "train loss:   0.933398\n",
      "train loss:   1.107488\n",
      "train loss:   1.202338\n",
      "train loss:   0.879285\n",
      "train loss:   1.123626\n",
      "train loss:   0.611100\n",
      "########### epoch 94 ###########\n",
      "########### loop 17600 ###########\n",
      "test loss:   0.250874   test accuracy:   0.916667\n",
      "########### loop 17600 ###########\n",
      "train loss:   1.134920\n",
      "train loss:   0.958642\n",
      "train loss:   0.988704\n",
      "train loss:   1.066136\n",
      "train loss:   0.992838\n",
      "train loss:   1.049481\n",
      "train loss:   0.628510\n",
      "train loss:   0.880925\n",
      "train loss:   1.069435\n",
      "train loss:   0.923957\n",
      "train loss:   1.306666\n",
      "train loss:   0.706063\n",
      "train loss:   0.846738\n",
      "train loss:   0.984382\n",
      "train loss:   0.884605\n",
      "train loss:   0.929551\n",
      "train loss:   0.927527\n",
      "train loss:   1.001434\n",
      "train loss:   1.219337\n",
      "train loss:   1.242175\n",
      "train loss:   0.816074\n",
      "train loss:   1.020636\n",
      "train loss:   0.766848\n",
      "train loss:   0.926459\n",
      "train loss:   1.182749\n",
      "train loss:   1.245880\n",
      "train loss:   0.927846\n",
      "train loss:   0.742598\n",
      "train loss:   1.002794\n",
      "train loss:   0.462752\n",
      "train loss:   0.905727\n",
      "train loss:   0.430634\n",
      "train loss:   1.001202\n",
      "train loss:   1.010250\n",
      "train loss:   0.627426\n",
      "train loss:   1.199042\n",
      "train loss:   0.949102\n",
      "train loss:   0.668883\n",
      "train loss:   1.216148\n",
      "train loss:   0.842722\n",
      "train loss:   0.889330\n",
      "train loss:   0.661980\n",
      "train loss:   1.035139\n",
      "train loss:   1.096714\n",
      "train loss:   0.825967\n",
      "train loss:   0.873643\n",
      "train loss:   1.105968\n",
      "train loss:   1.114021\n",
      "train loss:   1.076205\n",
      "train loss:   0.997600\n",
      "########### epoch 94 ###########\n",
      "########### loop 17650 ###########\n",
      "test loss:   0.179531   test accuracy:   0.958333\n",
      "########### loop 17650 ###########\n",
      "train loss:   0.957517\n",
      "train loss:   1.047464\n",
      "train loss:   0.881728\n",
      "train loss:   1.107210\n",
      "train loss:   1.229382\n",
      "train loss:   0.836992\n",
      "train loss:   1.046947\n",
      "train loss:   1.067610\n",
      "train loss:   1.091974\n",
      "train loss:   0.775803\n",
      "train loss:   1.053037\n",
      "train loss:   0.766126\n",
      "train loss:   0.664467\n",
      "train loss:   1.072785\n",
      "train loss:   0.814551\n",
      "train loss:   0.965050\n",
      "train loss:   0.823709\n",
      "train loss:   0.921728\n",
      "train loss:   0.797482\n",
      "train loss:   1.170262\n",
      "train loss:   0.904624\n",
      "train loss:   0.837770\n",
      "train loss:   0.886377\n",
      "train loss:   0.872066\n",
      "train loss:   0.971185\n",
      "train loss:   1.173918\n",
      "train loss:   1.159987\n",
      "train loss:   0.444018\n",
      "train loss:   0.844292\n",
      "train loss:   0.871271\n",
      "train loss:   1.013795\n",
      "train loss:   1.420827\n",
      "train loss:   1.010809\n",
      "train loss:   1.105309\n",
      "train loss:   0.801869\n",
      "train loss:   1.065577\n",
      "train loss:   1.115542\n",
      "train loss:   1.069246\n",
      "train loss:   0.950803\n",
      "train loss:   1.295250\n",
      "train loss:   0.774598\n",
      "train loss:   1.129811\n",
      "train loss:   0.835355\n",
      "train loss:   1.110970\n",
      "train loss:   0.914212\n",
      "train loss:   0.605667\n",
      "train loss:   0.878494\n",
      "train loss:   0.723168\n",
      "train loss:   0.704961\n",
      "train loss:   0.800636\n",
      "########### epoch 95 ###########\n",
      "########### loop 17700 ###########\n",
      "test loss:   0.173852   test accuracy:   1.000000\n",
      "########### loop 17700 ###########\n",
      "train loss:   0.772209\n",
      "train loss:   0.875118\n",
      "train loss:   1.159284\n",
      "train loss:   0.908260\n",
      "train loss:   0.850278\n",
      "train loss:   0.704106\n",
      "train loss:   1.146779\n",
      "train loss:   0.790226\n",
      "train loss:   0.994652\n",
      "train loss:   0.950346\n",
      "train loss:   0.876569\n",
      "train loss:   0.598892\n",
      "train loss:   0.840491\n",
      "train loss:   0.720727\n",
      "train loss:   0.679886\n",
      "train loss:   0.865698\n",
      "train loss:   1.026110\n",
      "train loss:   1.170733\n",
      "train loss:   1.372363\n",
      "train loss:   0.843364\n",
      "train loss:   0.885868\n",
      "train loss:   1.024679\n",
      "train loss:   1.172312\n",
      "train loss:   0.999135\n",
      "train loss:   0.686595\n",
      "train loss:   1.254653\n",
      "train loss:   0.545134\n",
      "train loss:   0.912965\n",
      "train loss:   1.163206\n",
      "train loss:   0.912803\n",
      "train loss:   1.116853\n",
      "train loss:   1.154002\n",
      "train loss:   1.022952\n",
      "train loss:   1.140239\n",
      "train loss:   0.804864\n",
      "train loss:   1.055792\n",
      "train loss:   0.901064\n",
      "train loss:   1.192701\n",
      "train loss:   1.037466\n",
      "train loss:   0.935098\n",
      "train loss:   0.751557\n",
      "train loss:   0.804654\n",
      "train loss:   1.131770\n",
      "train loss:   0.773472\n",
      "train loss:   1.099594\n",
      "train loss:   1.013062\n",
      "train loss:   0.702012\n",
      "train loss:   0.695825\n",
      "train loss:   0.856107\n",
      "train loss:   0.704393\n",
      "########### epoch 95 ###########\n",
      "########### loop 17750 ###########\n",
      "test loss:   0.397535   test accuracy:   0.833333\n",
      "########### loop 17750 ###########\n",
      "train loss:   1.016038\n",
      "train loss:   0.942928\n",
      "train loss:   1.026462\n",
      "train loss:   0.856526\n",
      "train loss:   0.973264\n",
      "train loss:   1.088055\n",
      "train loss:   1.037638\n",
      "train loss:   1.069365\n",
      "train loss:   0.863076\n",
      "train loss:   0.625893\n",
      "train loss:   0.747374\n",
      "train loss:   0.661745\n",
      "train loss:   0.912421\n",
      "train loss:   1.120089\n",
      "train loss:   1.182785\n",
      "train loss:   0.797461\n",
      "train loss:   0.841383\n",
      "train loss:   1.240145\n",
      "train loss:   0.748997\n",
      "train loss:   1.183733\n",
      "train loss:   0.905364\n",
      "train loss:   0.726502\n",
      "train loss:   0.879336\n",
      "train loss:   1.060781\n",
      "train loss:   0.835424\n",
      "train loss:   1.269690\n",
      "train loss:   1.267990\n",
      "train loss:   0.644150\n",
      "train loss:   1.116259\n",
      "train loss:   0.980296\n",
      "train loss:   1.211871\n",
      "train loss:   0.922681\n",
      "train loss:   1.159935\n",
      "train loss:   0.966455\n",
      "train loss:   0.831174\n",
      "train loss:   0.814698\n",
      "train loss:   1.109119\n",
      "train loss:   0.929431\n",
      "train loss:   1.099487\n",
      "train loss:   1.072622\n",
      "train loss:   0.748355\n",
      "train loss:   0.769122\n",
      "train loss:   0.735671\n",
      "train loss:   0.779549\n",
      "train loss:   1.152826\n",
      "train loss:   0.964015\n",
      "train loss:   1.143073\n",
      "train loss:   0.944848\n",
      "train loss:   1.047755\n",
      "train loss:   0.655210\n",
      "########### epoch 95 ###########\n",
      "########### loop 17800 ###########\n",
      "test loss:   0.249723   test accuracy:   0.916667\n",
      "########### loop 17800 ###########\n",
      "train loss:   0.957515\n",
      "train loss:   1.161638\n",
      "train loss:   1.183865\n",
      "train loss:   1.031816\n",
      "train loss:   1.055284\n",
      "train loss:   1.027763\n",
      "train loss:   0.666437\n",
      "train loss:   0.805084\n",
      "train loss:   0.983553\n",
      "train loss:   0.780812\n",
      "train loss:   0.906022\n",
      "train loss:   0.896061\n",
      "train loss:   1.035525\n",
      "train loss:   1.256840\n",
      "train loss:   0.774799\n",
      "train loss:   1.065121\n",
      "train loss:   1.029538\n",
      "train loss:   1.031397\n",
      "train loss:   0.997517\n",
      "train loss:   0.665060\n",
      "train loss:   1.054970\n",
      "train loss:   1.348612\n",
      "train loss:   0.889925\n",
      "train loss:   0.907303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.959668\n",
      "train loss:   1.118291\n",
      "train loss:   0.865812\n",
      "train loss:   0.875055\n",
      "train loss:   0.507024\n",
      "train loss:   0.577641\n",
      "train loss:   1.053717\n",
      "train loss:   1.273175\n",
      "train loss:   0.736183\n",
      "train loss:   1.015387\n",
      "train loss:   0.635195\n",
      "train loss:   0.783908\n",
      "train loss:   0.710639\n",
      "train loss:   1.172096\n",
      "train loss:   1.261462\n",
      "train loss:   1.305240\n",
      "train loss:   1.214221\n",
      "train loss:   0.906986\n",
      "train loss:   1.082361\n",
      "train loss:   0.948496\n",
      "train loss:   1.076876\n",
      "train loss:   0.678686\n",
      "train loss:   0.969274\n",
      "train loss:   0.919308\n",
      "train loss:   0.808974\n",
      "train loss:   1.051343\n",
      "########### epoch 95 ###########\n",
      "########### loop 17850 ###########\n",
      "test loss:   0.123922   test accuracy:   1.000000\n",
      "########### loop 17850 ###########\n",
      "train loss:   0.735130\n",
      "train loss:   0.963265\n",
      "train loss:   1.178144\n",
      "train loss:   0.849298\n",
      "train loss:   1.037869\n",
      "train loss:   0.706132\n",
      "train loss:   0.875364\n",
      "train loss:   0.837514\n",
      "train loss:   1.037395\n",
      "train loss:   1.083025\n",
      "train loss:   0.719123\n",
      "train loss:   0.793438\n",
      "train loss:   1.089309\n",
      "train loss:   0.920368\n",
      "train loss:   1.047041\n",
      "train loss:   0.877296\n",
      "train loss:   0.866939\n",
      "train loss:   1.240614\n",
      "train loss:   0.730326\n",
      "train loss:   0.725149\n",
      "train loss:   0.660231\n",
      "train loss:   0.958249\n",
      "train loss:   0.754475\n",
      "train loss:   0.921084\n",
      "train loss:   1.086240\n",
      "train loss:   0.655375\n",
      "train loss:   1.000292\n",
      "train loss:   1.173785\n",
      "train loss:   0.948325\n",
      "train loss:   1.061787\n",
      "train loss:   0.893251\n",
      "train loss:   0.977563\n",
      "train loss:   1.155137\n",
      "train loss:   1.200190\n",
      "train loss:   0.979627\n",
      "train loss:   1.126783\n",
      "train loss:   1.019003\n",
      "train loss:   1.182044\n",
      "train loss:   0.789768\n",
      "train loss:   0.844957\n",
      "train loss:   1.152610\n",
      "train loss:   1.123626\n",
      "train loss:   1.371617\n",
      "train loss:   1.222397\n",
      "train loss:   0.574601\n",
      "train loss:   1.167981\n",
      "train loss:   1.114207\n",
      "train loss:   0.877036\n",
      "train loss:   0.560175\n",
      "train loss:   1.111113\n",
      "########### epoch 96 ###########\n",
      "########### loop 17900 ###########\n",
      "test loss:   0.109220   test accuracy:   1.000000\n",
      "########### loop 17900 ###########\n",
      "train loss:   0.748230\n",
      "train loss:   1.161944\n",
      "train loss:   0.617137\n",
      "train loss:   0.969231\n",
      "train loss:   1.027373\n",
      "train loss:   1.373830\n",
      "train loss:   1.217794\n",
      "train loss:   0.944190\n",
      "train loss:   0.866957\n",
      "train loss:   0.921496\n",
      "train loss:   1.152430\n",
      "train loss:   0.937744\n",
      "train loss:   0.635242\n",
      "train loss:   0.990222\n",
      "train loss:   0.952980\n",
      "train loss:   0.913541\n",
      "train loss:   0.752676\n",
      "train loss:   1.072005\n",
      "train loss:   0.609754\n",
      "train loss:   0.778584\n",
      "train loss:   1.208245\n",
      "train loss:   0.832836\n",
      "train loss:   1.304736\n",
      "train loss:   1.035958\n",
      "train loss:   1.191603\n",
      "train loss:   0.807472\n",
      "train loss:   0.934215\n",
      "train loss:   0.766132\n",
      "train loss:   0.857418\n",
      "train loss:   0.752840\n",
      "train loss:   1.346293\n",
      "train loss:   1.151355\n",
      "train loss:   0.924918\n",
      "train loss:   0.745451\n",
      "train loss:   0.761148\n",
      "train loss:   0.767845\n",
      "train loss:   1.037573\n",
      "train loss:   1.178083\n",
      "train loss:   0.847825\n",
      "train loss:   0.914348\n",
      "train loss:   1.038398\n",
      "train loss:   0.955876\n",
      "train loss:   0.897204\n",
      "train loss:   0.663586\n",
      "train loss:   0.856367\n",
      "train loss:   0.868149\n",
      "train loss:   1.118211\n",
      "train loss:   0.457837\n",
      "train loss:   0.843364\n",
      "train loss:   0.824771\n",
      "########### epoch 96 ###########\n",
      "########### loop 17950 ###########\n",
      "test loss:   0.168211   test accuracy:   0.958333\n",
      "########### loop 17950 ###########\n",
      "train loss:   0.837216\n",
      "train loss:   1.013936\n",
      "train loss:   1.081094\n",
      "train loss:   0.667828\n",
      "train loss:   1.403514\n",
      "train loss:   0.998245\n",
      "train loss:   0.921302\n",
      "train loss:   1.040626\n",
      "train loss:   0.809005\n",
      "train loss:   0.729185\n",
      "train loss:   0.909399\n",
      "train loss:   0.967291\n",
      "train loss:   0.905256\n",
      "train loss:   0.912807\n",
      "train loss:   1.497998\n",
      "train loss:   0.651909\n",
      "train loss:   1.131917\n",
      "train loss:   1.077494\n",
      "train loss:   0.896869\n",
      "train loss:   1.007493\n",
      "train loss:   0.734197\n",
      "train loss:   0.485373\n",
      "train loss:   0.876348\n",
      "train loss:   1.121186\n",
      "train loss:   0.819190\n",
      "train loss:   0.916615\n",
      "train loss:   1.249657\n",
      "train loss:   0.809484\n",
      "train loss:   0.942331\n",
      "train loss:   1.314033\n",
      "train loss:   0.936445\n",
      "train loss:   0.718181\n",
      "train loss:   0.931611\n",
      "train loss:   0.962356\n",
      "train loss:   0.792251\n",
      "train loss:   1.091341\n",
      "train loss:   0.844960\n",
      "train loss:   0.621287\n",
      "train loss:   1.211561\n",
      "train loss:   1.129774\n",
      "train loss:   1.164974\n",
      "train loss:   0.745182\n",
      "train loss:   0.895602\n",
      "train loss:   1.204531\n",
      "train loss:   1.148374\n",
      "train loss:   1.037033\n",
      "train loss:   1.220514\n",
      "train loss:   1.118082\n",
      "train loss:   0.807084\n",
      "train loss:   0.902616\n",
      "########### epoch 96 ###########\n",
      "########### loop 18000 ###########\n",
      "test loss:   0.355955   test accuracy:   0.916667\n",
      "########### loop 18000 ###########\n",
      "train loss:   0.814066\n",
      "train loss:   1.426744\n",
      "train loss:   1.322594\n",
      "train loss:   0.697912\n",
      "train loss:   1.138787\n",
      "train loss:   1.020083\n",
      "train loss:   1.038767\n",
      "train loss:   0.775620\n",
      "train loss:   0.919099\n",
      "train loss:   1.131381\n",
      "train loss:   1.188246\n",
      "train loss:   0.781224\n",
      "train loss:   0.817580\n",
      "train loss:   0.693202\n",
      "train loss:   0.927241\n",
      "train loss:   0.883127\n",
      "train loss:   1.218620\n",
      "train loss:   0.948589\n",
      "train loss:   1.153582\n",
      "train loss:   0.942301\n",
      "train loss:   1.107398\n",
      "train loss:   0.756141\n",
      "train loss:   0.940921\n",
      "train loss:   1.206357\n",
      "train loss:   0.803889\n",
      "train loss:   1.242242\n",
      "train loss:   0.897636\n",
      "train loss:   1.349757\n",
      "train loss:   0.934964\n",
      "train loss:   0.987396\n",
      "train loss:   1.025289\n",
      "train loss:   0.636917\n",
      "train loss:   1.197333\n",
      "train loss:   0.685941\n",
      "train loss:   1.004232\n",
      "train loss:   1.201492\n",
      "train loss:   1.063733\n",
      "train loss:   0.885359\n",
      "train loss:   1.244933\n",
      "train loss:   0.882480\n",
      "train loss:   1.003479\n",
      "train loss:   0.769989\n",
      "train loss:   1.238735\n",
      "train loss:   1.021862\n",
      "train loss:   0.871732\n",
      "train loss:   1.250666\n",
      "train loss:   0.893058\n",
      "train loss:   0.834055\n",
      "train loss:   1.009996\n",
      "train loss:   0.841448\n",
      "########### epoch 97 ###########\n",
      "########### loop 18050 ###########\n",
      "test loss:   0.324988   test accuracy:   0.875000\n",
      "########### loop 18050 ###########\n",
      "train loss:   0.906981\n",
      "train loss:   1.069559\n",
      "train loss:   1.255988\n",
      "train loss:   0.552478\n",
      "train loss:   1.025130\n",
      "train loss:   1.171995\n",
      "train loss:   0.806325\n",
      "train loss:   0.967439\n",
      "train loss:   0.934557\n",
      "train loss:   0.913017\n",
      "train loss:   1.388106\n",
      "train loss:   0.937822\n",
      "train loss:   0.962353\n",
      "train loss:   1.094529\n",
      "train loss:   0.775133\n",
      "train loss:   1.025091\n",
      "train loss:   0.892201\n",
      "train loss:   1.071678\n",
      "train loss:   0.936161\n",
      "train loss:   0.665216\n",
      "train loss:   0.585472\n",
      "train loss:   1.210526\n",
      "train loss:   0.817776\n",
      "train loss:   0.607453\n",
      "train loss:   0.849254\n",
      "train loss:   1.302576\n",
      "train loss:   1.183655\n",
      "train loss:   1.087102\n",
      "train loss:   1.240228\n",
      "train loss:   0.979317\n",
      "train loss:   1.155519\n",
      "train loss:   0.860177\n",
      "train loss:   1.286061\n",
      "train loss:   0.858870\n",
      "train loss:   0.839555\n",
      "train loss:   0.816518\n",
      "train loss:   1.359687\n",
      "train loss:   1.142624\n",
      "train loss:   1.281798\n",
      "train loss:   1.267419\n",
      "train loss:   0.693080\n",
      "train loss:   1.033729\n",
      "train loss:   0.731088\n",
      "train loss:   0.841188\n",
      "train loss:   0.909521\n",
      "train loss:   1.173027\n",
      "train loss:   0.878194\n",
      "train loss:   1.329520\n",
      "train loss:   1.285771\n",
      "train loss:   1.054681\n",
      "########### epoch 97 ###########\n",
      "########### loop 18100 ###########\n",
      "test loss:   0.282595   test accuracy:   0.875000\n",
      "########### loop 18100 ###########\n",
      "train loss:   1.134862\n",
      "train loss:   0.618639\n",
      "train loss:   0.897580\n",
      "train loss:   0.879529\n",
      "train loss:   0.878962\n",
      "train loss:   0.966635\n",
      "train loss:   1.214522\n",
      "train loss:   1.142013\n",
      "train loss:   0.788122\n",
      "train loss:   1.016357\n",
      "train loss:   0.648482\n",
      "train loss:   1.095391\n",
      "train loss:   1.259594\n",
      "train loss:   0.818908\n",
      "train loss:   1.219856\n",
      "train loss:   1.268161\n",
      "train loss:   1.141932\n",
      "train loss:   1.034184\n",
      "train loss:   0.760578\n",
      "train loss:   1.085515\n",
      "train loss:   0.718814\n",
      "train loss:   0.896839\n",
      "train loss:   0.961025\n",
      "train loss:   0.747863\n",
      "train loss:   1.122080\n",
      "train loss:   0.988458\n",
      "train loss:   1.099091\n",
      "train loss:   0.854963\n",
      "train loss:   1.140606\n",
      "train loss:   1.188587\n",
      "train loss:   0.726361\n",
      "train loss:   0.736246\n",
      "train loss:   1.207126\n",
      "train loss:   1.256157\n",
      "train loss:   1.161527\n",
      "train loss:   0.832882\n",
      "train loss:   1.095845\n",
      "train loss:   1.071109\n",
      "train loss:   1.366278\n",
      "train loss:   0.895920\n",
      "train loss:   1.097386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.062614\n",
      "train loss:   1.212809\n",
      "train loss:   1.099949\n",
      "train loss:   0.818863\n",
      "train loss:   0.713636\n",
      "train loss:   0.748871\n",
      "train loss:   0.790011\n",
      "train loss:   1.093575\n",
      "train loss:   1.297464\n",
      "########### epoch 97 ###########\n",
      "########### loop 18150 ###########\n",
      "test loss:   0.120021   test accuracy:   1.000000\n",
      "########### loop 18150 ###########\n",
      "train loss:   0.609860\n",
      "train loss:   0.658881\n",
      "train loss:   1.075334\n",
      "train loss:   0.766906\n",
      "train loss:   1.231513\n",
      "train loss:   0.959569\n",
      "train loss:   1.247729\n",
      "train loss:   1.100897\n",
      "train loss:   1.032028\n",
      "train loss:   0.923521\n",
      "train loss:   1.084337\n",
      "train loss:   1.100579\n",
      "train loss:   0.906527\n",
      "train loss:   1.249069\n",
      "train loss:   1.270720\n",
      "train loss:   0.871806\n",
      "train loss:   0.635487\n",
      "train loss:   1.055829\n",
      "train loss:   0.951069\n",
      "train loss:   1.074306\n",
      "train loss:   1.080201\n",
      "train loss:   1.101678\n",
      "train loss:   0.955675\n",
      "train loss:   1.062527\n",
      "train loss:   0.774473\n",
      "train loss:   0.869209\n",
      "train loss:   0.753017\n",
      "train loss:   1.239933\n",
      "train loss:   1.114998\n",
      "train loss:   1.002323\n",
      "train loss:   1.037251\n",
      "train loss:   0.959785\n",
      "train loss:   0.745029\n",
      "train loss:   0.786928\n",
      "train loss:   0.944923\n",
      "train loss:   0.840756\n",
      "train loss:   0.936672\n",
      "train loss:   1.093691\n",
      "train loss:   0.514315\n",
      "train loss:   0.717802\n",
      "train loss:   0.729523\n",
      "train loss:   0.980129\n",
      "train loss:   1.250068\n",
      "train loss:   0.872715\n",
      "train loss:   0.682408\n",
      "train loss:   0.865134\n",
      "train loss:   0.738603\n",
      "train loss:   0.934358\n",
      "train loss:   1.285898\n",
      "train loss:   0.821389\n",
      "########### epoch 97 ###########\n",
      "########### loop 18200 ###########\n",
      "test loss:   0.525928   test accuracy:   0.833333\n",
      "########### loop 18200 ###########\n",
      "train loss:   0.959571\n",
      "train loss:   1.177781\n",
      "train loss:   1.046393\n",
      "train loss:   1.113518\n",
      "train loss:   1.173930\n",
      "train loss:   1.300565\n",
      "train loss:   1.174343\n",
      "train loss:   0.828789\n",
      "train loss:   1.134442\n",
      "train loss:   1.065575\n",
      "train loss:   0.827828\n",
      "train loss:   1.131845\n",
      "train loss:   1.146907\n",
      "train loss:   1.074903\n",
      "train loss:   1.519367\n",
      "train loss:   1.242806\n",
      "train loss:   0.750308\n",
      "train loss:   0.792849\n",
      "train loss:   0.856801\n",
      "train loss:   1.264872\n",
      "train loss:   0.940903\n",
      "train loss:   0.638755\n",
      "train loss:   0.691520\n",
      "train loss:   0.840198\n",
      "train loss:   0.972513\n",
      "train loss:   1.188653\n",
      "train loss:   1.212444\n",
      "train loss:   0.654148\n",
      "train loss:   1.223309\n",
      "train loss:   1.021996\n",
      "train loss:   1.015063\n",
      "train loss:   0.759244\n",
      "train loss:   0.939556\n",
      "train loss:   1.281873\n",
      "train loss:   0.774323\n",
      "train loss:   1.295470\n",
      "train loss:   1.040844\n",
      "train loss:   0.740131\n",
      "train loss:   0.781322\n",
      "train loss:   0.929046\n",
      "train loss:   0.860213\n",
      "train loss:   0.766733\n",
      "train loss:   1.041132\n",
      "train loss:   1.137275\n",
      "train loss:   1.166103\n",
      "train loss:   0.747217\n",
      "train loss:   1.159025\n",
      "train loss:   0.928972\n",
      "train loss:   1.011340\n",
      "train loss:   1.109259\n",
      "########### epoch 98 ###########\n",
      "########### loop 18250 ###########\n",
      "test loss:   0.129673   test accuracy:   1.000000\n",
      "########### loop 18250 ###########\n",
      "train loss:   0.700994\n",
      "train loss:   1.106686\n",
      "train loss:   0.819898\n",
      "train loss:   1.058384\n",
      "train loss:   1.017184\n",
      "train loss:   0.635742\n",
      "train loss:   1.100569\n",
      "train loss:   0.966946\n",
      "train loss:   0.718548\n",
      "train loss:   1.018998\n",
      "train loss:   0.775771\n",
      "train loss:   0.808412\n",
      "train loss:   0.903730\n",
      "train loss:   0.699157\n",
      "train loss:   0.895052\n",
      "train loss:   1.099402\n",
      "train loss:   0.886884\n",
      "train loss:   1.235483\n",
      "train loss:   0.957291\n",
      "train loss:   1.068702\n",
      "train loss:   0.973323\n",
      "train loss:   0.852188\n",
      "train loss:   0.966434\n",
      "train loss:   0.785607\n",
      "train loss:   0.883442\n",
      "train loss:   1.067360\n",
      "train loss:   0.618083\n",
      "train loss:   1.246894\n",
      "train loss:   0.504554\n",
      "train loss:   1.129149\n",
      "train loss:   0.906826\n",
      "train loss:   1.122209\n",
      "train loss:   1.065189\n",
      "train loss:   1.085169\n",
      "train loss:   1.397639\n",
      "train loss:   1.072413\n",
      "train loss:   0.715080\n",
      "train loss:   1.067136\n",
      "train loss:   0.854409\n",
      "train loss:   1.062983\n",
      "train loss:   1.223054\n",
      "train loss:   1.179256\n",
      "train loss:   1.281069\n",
      "train loss:   1.240802\n",
      "train loss:   1.360551\n",
      "train loss:   1.032108\n",
      "train loss:   0.813935\n",
      "train loss:   0.721794\n",
      "train loss:   0.718960\n",
      "train loss:   0.861836\n",
      "########### epoch 98 ###########\n",
      "########### loop 18300 ###########\n",
      "test loss:   0.285032   test accuracy:   0.916667\n",
      "########### loop 18300 ###########\n",
      "train loss:   0.893105\n",
      "train loss:   0.925831\n",
      "train loss:   0.827126\n",
      "train loss:   1.151213\n",
      "train loss:   1.100222\n",
      "train loss:   1.125573\n",
      "train loss:   0.675948\n",
      "train loss:   0.774650\n",
      "train loss:   0.655366\n",
      "train loss:   1.408067\n",
      "train loss:   0.793143\n",
      "train loss:   1.279728\n",
      "train loss:   1.210286\n",
      "train loss:   0.976961\n",
      "train loss:   0.974594\n",
      "train loss:   0.845836\n",
      "train loss:   0.839093\n",
      "train loss:   1.185979\n",
      "train loss:   1.151775\n",
      "train loss:   0.894798\n",
      "train loss:   0.794344\n",
      "train loss:   1.003003\n",
      "train loss:   0.792211\n",
      "train loss:   0.892217\n",
      "train loss:   0.685663\n",
      "train loss:   0.745374\n",
      "train loss:   0.960073\n",
      "train loss:   0.839130\n",
      "train loss:   0.759953\n",
      "train loss:   1.190643\n",
      "train loss:   0.878834\n",
      "train loss:   0.849680\n",
      "train loss:   1.175445\n",
      "train loss:   0.994047\n",
      "train loss:   1.112343\n",
      "train loss:   1.264965\n",
      "train loss:   0.790013\n",
      "train loss:   0.685692\n",
      "train loss:   0.819965\n",
      "train loss:   1.228389\n",
      "train loss:   0.769703\n",
      "train loss:   1.072306\n",
      "train loss:   0.716160\n",
      "train loss:   1.291448\n",
      "train loss:   1.161754\n",
      "train loss:   0.757039\n",
      "train loss:   1.246019\n",
      "train loss:   1.237199\n",
      "train loss:   0.918039\n",
      "train loss:   1.049041\n",
      "########### epoch 98 ###########\n",
      "########### loop 18350 ###########\n",
      "test loss:   0.202323   test accuracy:   1.000000\n",
      "########### loop 18350 ###########\n",
      "train loss:   0.799608\n",
      "train loss:   1.120420\n",
      "train loss:   0.906130\n",
      "train loss:   0.909417\n",
      "train loss:   1.209373\n",
      "train loss:   1.188895\n",
      "train loss:   1.254113\n",
      "train loss:   1.154065\n",
      "train loss:   1.057673\n",
      "train loss:   1.054537\n",
      "train loss:   0.717540\n",
      "train loss:   1.005911\n",
      "train loss:   0.686594\n",
      "train loss:   1.088122\n",
      "train loss:   0.770039\n",
      "train loss:   1.125763\n",
      "train loss:   1.226763\n",
      "train loss:   0.760241\n",
      "train loss:   1.203754\n",
      "train loss:   1.145721\n",
      "train loss:   1.246837\n",
      "train loss:   1.306194\n",
      "train loss:   1.089825\n",
      "train loss:   1.091666\n",
      "train loss:   1.142557\n",
      "train loss:   0.758462\n",
      "train loss:   0.758223\n",
      "train loss:   0.812038\n",
      "train loss:   1.062311\n",
      "train loss:   1.001455\n",
      "train loss:   0.996111\n",
      "train loss:   0.743854\n",
      "train loss:   1.100703\n",
      "train loss:   1.140738\n",
      "train loss:   0.994852\n",
      "train loss:   0.894594\n",
      "train loss:   0.767837\n",
      "train loss:   0.985121\n",
      "train loss:   0.386839\n",
      "train loss:   0.609079\n",
      "train loss:   0.876202\n",
      "train loss:   1.079006\n",
      "train loss:   0.818771\n",
      "train loss:   1.074797\n",
      "train loss:   1.074206\n",
      "train loss:   0.986186\n",
      "train loss:   0.755027\n",
      "train loss:   1.090009\n",
      "train loss:   1.283547\n",
      "train loss:   1.143246\n",
      "########### epoch 98 ###########\n",
      "########### loop 18400 ###########\n",
      "test loss:   0.138649   test accuracy:   1.000000\n",
      "########### loop 18400 ###########\n",
      "train loss:   0.648383\n",
      "train loss:   0.982301\n",
      "train loss:   1.151299\n",
      "train loss:   1.392220\n",
      "train loss:   0.785581\n",
      "train loss:   0.761247\n",
      "train loss:   0.831460\n",
      "train loss:   1.098854\n",
      "train loss:   0.995366\n",
      "train loss:   1.136912\n",
      "train loss:   1.127599\n",
      "train loss:   1.095409\n",
      "train loss:   0.865757\n",
      "train loss:   0.903106\n",
      "train loss:   1.264238\n",
      "train loss:   1.026229\n",
      "train loss:   0.833814\n",
      "train loss:   0.812721\n",
      "train loss:   0.657723\n",
      "train loss:   0.855790\n",
      "train loss:   0.744357\n",
      "train loss:   0.810509\n",
      "train loss:   0.961681\n",
      "train loss:   0.910716\n",
      "train loss:   1.319353\n",
      "train loss:   0.995611\n",
      "train loss:   0.944934\n",
      "train loss:   1.030415\n",
      "train loss:   1.007598\n",
      "train loss:   0.860797\n",
      "train loss:   0.962056\n",
      "train loss:   1.293684\n",
      "train loss:   0.337286\n",
      "train loss:   0.881541\n",
      "train loss:   1.018278\n",
      "train loss:   0.705601\n",
      "train loss:   0.931033\n",
      "train loss:   1.036693\n",
      "train loss:   0.981201\n",
      "train loss:   0.511768\n",
      "train loss:   1.159012\n",
      "train loss:   1.040900\n",
      "train loss:   0.867508\n",
      "train loss:   0.602188\n",
      "train loss:   0.948746\n",
      "train loss:   1.191737\n",
      "train loss:   0.924569\n",
      "train loss:   1.094415\n",
      "train loss:   0.913057\n",
      "train loss:   1.003582\n",
      "########### epoch 99 ###########\n",
      "########### loop 18450 ###########\n",
      "test loss:   0.187937   test accuracy:   0.958333\n",
      "########### loop 18450 ###########\n",
      "train loss:   0.896260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.131006\n",
      "train loss:   1.496038\n",
      "train loss:   1.220406\n",
      "train loss:   0.853533\n",
      "train loss:   1.075952\n",
      "train loss:   1.037222\n",
      "train loss:   1.088729\n",
      "train loss:   0.883892\n",
      "train loss:   0.843911\n",
      "train loss:   0.613084\n",
      "train loss:   0.715455\n",
      "train loss:   0.875655\n",
      "train loss:   1.063247\n",
      "train loss:   1.026560\n",
      "train loss:   0.764158\n",
      "train loss:   0.895166\n",
      "train loss:   0.892355\n",
      "train loss:   0.829599\n",
      "train loss:   0.770147\n",
      "train loss:   1.377815\n",
      "train loss:   0.860152\n",
      "train loss:   1.122843\n",
      "train loss:   0.534294\n",
      "train loss:   1.052027\n",
      "train loss:   0.878302\n",
      "train loss:   0.737263\n",
      "train loss:   0.910387\n",
      "train loss:   0.750157\n",
      "train loss:   1.143828\n",
      "train loss:   0.983532\n",
      "train loss:   0.864561\n",
      "train loss:   1.084610\n",
      "train loss:   0.994483\n",
      "train loss:   0.923150\n",
      "train loss:   1.208364\n",
      "train loss:   0.932461\n",
      "train loss:   0.819206\n",
      "train loss:   1.004667\n",
      "train loss:   1.025737\n",
      "train loss:   1.005444\n",
      "train loss:   0.991840\n",
      "train loss:   0.742554\n",
      "train loss:   1.284934\n",
      "train loss:   0.941795\n",
      "train loss:   0.619588\n",
      "train loss:   1.079177\n",
      "train loss:   0.945569\n",
      "train loss:   0.717927\n",
      "train loss:   1.285576\n",
      "########### epoch 99 ###########\n",
      "########### loop 18500 ###########\n",
      "test loss:   0.258180   test accuracy:   0.875000\n",
      "########### loop 18500 ###########\n",
      "train loss:   0.601895\n",
      "train loss:   0.893140\n",
      "train loss:   1.129614\n",
      "train loss:   0.988953\n",
      "train loss:   0.807149\n",
      "train loss:   1.390652\n",
      "train loss:   1.370139\n",
      "train loss:   0.800483\n",
      "train loss:   0.871673\n",
      "train loss:   1.420870\n",
      "train loss:   0.831900\n",
      "train loss:   1.161648\n",
      "train loss:   0.968819\n",
      "train loss:   0.350156\n",
      "train loss:   1.048566\n",
      "train loss:   1.086915\n",
      "train loss:   0.922352\n",
      "train loss:   1.088902\n",
      "train loss:   1.006183\n",
      "train loss:   0.711558\n",
      "train loss:   0.520327\n",
      "train loss:   0.831379\n",
      "train loss:   0.961929\n",
      "train loss:   0.524297\n",
      "train loss:   0.898268\n",
      "train loss:   0.867925\n",
      "train loss:   0.904479\n",
      "train loss:   0.913905\n",
      "train loss:   0.985431\n",
      "train loss:   1.134759\n",
      "train loss:   0.817904\n",
      "train loss:   0.747173\n",
      "train loss:   0.989838\n",
      "train loss:   0.883000\n",
      "train loss:   1.199208\n",
      "train loss:   1.090819\n",
      "train loss:   0.676364\n",
      "train loss:   0.840125\n",
      "train loss:   0.921208\n",
      "train loss:   0.950570\n",
      "train loss:   0.762956\n",
      "train loss:   1.137632\n",
      "train loss:   0.711908\n",
      "train loss:   1.440607\n",
      "train loss:   0.742713\n",
      "train loss:   0.899286\n",
      "train loss:   1.158505\n",
      "train loss:   0.579981\n",
      "train loss:   0.862386\n",
      "train loss:   1.013144\n",
      "########### epoch 99 ###########\n",
      "########### loop 18550 ###########\n",
      "test loss:   0.198200   test accuracy:   1.000000\n",
      "########### loop 18550 ###########\n",
      "train loss:   1.132277\n",
      "train loss:   1.014824\n",
      "train loss:   0.841762\n",
      "train loss:   1.096185\n",
      "train loss:   1.152249\n",
      "train loss:   1.317327\n",
      "train loss:   0.806575\n",
      "train loss:   1.046788\n",
      "train loss:   0.748585\n",
      "train loss:   0.771101\n",
      "train loss:   0.986841\n",
      "train loss:   0.886896\n",
      "train loss:   0.808374\n",
      "train loss:   0.960700\n",
      "train loss:   0.707295\n",
      "train loss:   1.245105\n",
      "train loss:   0.993340\n",
      "train loss:   0.958410\n",
      "train loss:   1.023313\n",
      "train loss:   0.817791\n",
      "train loss:   0.892322\n",
      "train loss:   1.334593\n",
      "train loss:   1.048499\n",
      "train loss:   0.727222\n",
      "train loss:   0.910717\n",
      "train loss:   1.028436\n",
      "train loss:   1.047785\n",
      "train loss:   0.438710\n",
      "train loss:   0.867119\n",
      "train loss:   1.119802\n",
      "train loss:   0.855919\n",
      "train loss:   1.153144\n",
      "train loss:   0.900840\n",
      "train loss:   1.175027\n",
      "train loss:   0.919193\n",
      "train loss:   1.530892\n",
      "train loss:   0.625591\n",
      "train loss:   0.983595\n",
      "train loss:   1.243570\n",
      "train loss:   1.362516\n",
      "train loss:   1.297859\n",
      "train loss:   0.802590\n",
      "train loss:   1.347567\n",
      "train loss:   0.865121\n",
      "train loss:   0.802060\n",
      "train loss:   1.436030\n",
      "train loss:   1.125810\n",
      "train loss:   0.896709\n",
      "train loss:   0.756076\n",
      "train loss:   0.812434\n",
      "########### epoch 99 ###########\n",
      "########### loop 18600 ###########\n",
      "test loss:   0.145328   test accuracy:   1.000000\n",
      "########### loop 18600 ###########\n",
      "train loss:   0.788085\n",
      "train loss:   1.364584\n",
      "train loss:   0.860088\n",
      "train loss:   0.648432\n",
      "train loss:   0.892275\n",
      "train loss:   0.793803\n",
      "train loss:   0.852690\n",
      "train loss:   0.673345\n",
      "train loss:   0.986847\n",
      "train loss:   0.827766\n",
      "train loss:   1.005058\n",
      "train loss:   0.976136\n",
      "train loss:   1.026972\n",
      "train loss:   1.098973\n",
      "train loss:   0.771035\n",
      "train loss:   0.702243\n",
      "train loss:   1.235609\n",
      "train loss:   1.077454\n",
      "train loss:   0.885030\n",
      "train loss:   1.096045\n",
      "train loss:   1.025624\n",
      "train loss:   0.757607\n",
      "train loss:   0.929567\n",
      "train loss:   0.488548\n",
      "train loss:   0.961636\n",
      "train loss:   0.876515\n",
      "train loss:   0.761456\n",
      "train loss:   0.869628\n",
      "train loss:   0.920772\n",
      "train loss:   0.713011\n",
      "train loss:   0.751464\n",
      "train loss:   1.059832\n",
      "train loss:   0.696929\n",
      "train loss:   1.012591\n",
      "train loss:   1.038503\n",
      "train loss:   1.262343\n",
      "train loss:   0.997038\n",
      "train loss:   1.219533\n",
      "train loss:   0.629764\n",
      "train loss:   1.070685\n",
      "train loss:   0.800144\n",
      "train loss:   1.027105\n",
      "train loss:   1.046093\n",
      "train loss:   1.075380\n",
      "train loss:   1.166474\n",
      "train loss:   0.657875\n",
      "train loss:   0.969676\n",
      "train loss:   0.721341\n",
      "train loss:   1.061832\n",
      "train loss:   0.913001\n",
      "########### epoch 100 ###########\n",
      "########### loop 18650 ###########\n",
      "test loss:   0.339868   test accuracy:   0.875000\n",
      "########### loop 18650 ###########\n",
      "train loss:   0.947439\n",
      "train loss:   0.966624\n",
      "train loss:   0.963629\n",
      "train loss:   1.266590\n",
      "train loss:   1.297270\n",
      "train loss:   1.250546\n",
      "train loss:   1.095190\n",
      "train loss:   1.228909\n",
      "train loss:   1.043391\n",
      "train loss:   0.560655\n",
      "train loss:   1.414007\n",
      "train loss:   1.288975\n",
      "train loss:   1.152866\n",
      "train loss:   1.020334\n",
      "train loss:   1.075304\n",
      "train loss:   0.629402\n",
      "train loss:   1.252938\n",
      "train loss:   1.040781\n",
      "train loss:   1.236246\n",
      "train loss:   0.698640\n",
      "train loss:   0.753565\n",
      "train loss:   0.872685\n",
      "train loss:   0.794881\n",
      "train loss:   0.614439\n",
      "train loss:   0.768857\n",
      "train loss:   1.202011\n",
      "train loss:   0.761326\n",
      "train loss:   0.581342\n",
      "train loss:   0.885871\n",
      "train loss:   1.026174\n",
      "train loss:   1.105140\n",
      "train loss:   1.215570\n",
      "train loss:   1.002540\n",
      "train loss:   0.805297\n",
      "train loss:   1.242081\n",
      "train loss:   0.951314\n",
      "train loss:   0.540843\n",
      "train loss:   1.206779\n",
      "train loss:   0.871014\n",
      "train loss:   0.766987\n",
      "train loss:   0.768712\n",
      "train loss:   0.987150\n",
      "train loss:   1.444428\n",
      "train loss:   0.762336\n",
      "train loss:   1.151739\n",
      "train loss:   0.930267\n",
      "train loss:   1.049315\n",
      "train loss:   0.772887\n",
      "train loss:   1.061086\n",
      "train loss:   0.865870\n",
      "########### epoch 100 ###########\n",
      "########### loop 18700 ###########\n",
      "test loss:   0.164207   test accuracy:   0.958333\n",
      "########### loop 18700 ###########\n",
      "train loss:   1.226067\n",
      "train loss:   1.048893\n",
      "train loss:   1.365704\n",
      "train loss:   1.030090\n",
      "train loss:   1.393010\n",
      "train loss:   1.190463\n",
      "train loss:   1.148947\n",
      "train loss:   1.069473\n",
      "train loss:   0.686418\n",
      "train loss:   0.999036\n",
      "train loss:   0.817382\n",
      "train loss:   0.895819\n",
      "train loss:   0.722387\n",
      "train loss:   0.762684\n",
      "train loss:   1.028964\n",
      "train loss:   1.224921\n",
      "train loss:   0.926317\n",
      "train loss:   1.297839\n",
      "train loss:   1.060574\n",
      "train loss:   0.431625\n",
      "train loss:   0.940871\n",
      "train loss:   0.716450\n",
      "train loss:   0.594995\n",
      "train loss:   1.040450\n",
      "train loss:   1.273210\n",
      "train loss:   0.770541\n",
      "train loss:   0.620410\n",
      "train loss:   1.253040\n",
      "train loss:   1.227939\n",
      "train loss:   0.636250\n",
      "train loss:   1.062176\n",
      "train loss:   1.039460\n",
      "train loss:   1.072817\n",
      "train loss:   0.719693\n",
      "train loss:   1.188274\n",
      "train loss:   0.905793\n",
      "train loss:   0.947906\n",
      "train loss:   0.744129\n",
      "train loss:   0.735127\n",
      "train loss:   1.206853\n",
      "train loss:   1.208083\n",
      "train loss:   0.865306\n",
      "train loss:   0.830201\n",
      "train loss:   1.040140\n",
      "train loss:   0.844645\n",
      "train loss:   0.702427\n",
      "train loss:   1.293045\n",
      "train loss:   0.725718\n",
      "train loss:   1.279109\n",
      "train loss:   0.806392\n",
      "########### epoch 100 ###########\n",
      "########### loop 18750 ###########\n",
      "test loss:   0.227821   test accuracy:   0.958333\n",
      "########### loop 18750 ###########\n",
      "train loss:   0.560374\n",
      "train loss:   0.899859\n",
      "train loss:   0.868421\n",
      "train loss:   0.836510\n",
      "train loss:   1.016992\n",
      "train loss:   0.833394\n",
      "train loss:   0.892639\n",
      "train loss:   1.052735\n",
      "train loss:   0.759008\n",
      "train loss:   0.980356\n",
      "train loss:   0.896889\n",
      "train loss:   0.934380\n",
      "train loss:   0.793487\n",
      "train loss:   1.059762\n",
      "train loss:   0.810354\n",
      "train loss:   0.836934\n",
      "train loss:   0.798203\n",
      "train loss:   0.726528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.050775\n",
      "train loss:   1.291685\n",
      "train loss:   0.985783\n",
      "train loss:   0.733370\n",
      "train loss:   0.864644\n",
      "train loss:   0.654709\n",
      "train loss:   0.835608\n",
      "train loss:   1.136603\n",
      "train loss:   0.987597\n",
      "train loss:   0.748624\n",
      "train loss:   1.143472\n",
      "train loss:   1.042829\n",
      "train loss:   1.066316\n",
      "train loss:   1.041970\n",
      "train loss:   1.220761\n",
      "train loss:   1.084815\n",
      "train loss:   0.906665\n",
      "train loss:   0.807369\n",
      "train loss:   0.649175\n",
      "train loss:   0.933049\n",
      "train loss:   1.116009\n",
      "train loss:   1.148407\n",
      "train loss:   0.656799\n",
      "train loss:   0.851823\n",
      "train loss:   1.165947\n",
      "train loss:   0.923080\n",
      "train loss:   0.993325\n",
      "train loss:   0.972370\n",
      "train loss:   0.840023\n",
      "train loss:   1.239031\n",
      "train loss:   0.802285\n",
      "train loss:   0.896975\n",
      "########### epoch 101 ###########\n",
      "########### loop 18800 ###########\n",
      "test loss:   0.116208   test accuracy:   1.000000\n",
      "########### loop 18800 ###########\n",
      "train loss:   0.753787\n",
      "train loss:   1.077613\n",
      "train loss:   0.740080\n",
      "train loss:   1.148904\n",
      "train loss:   1.056380\n",
      "train loss:   1.356089\n",
      "train loss:   1.201421\n",
      "train loss:   1.258287\n",
      "train loss:   0.896395\n",
      "train loss:   0.795824\n",
      "train loss:   0.861492\n",
      "train loss:   1.118747\n",
      "train loss:   0.584024\n",
      "train loss:   0.534918\n",
      "train loss:   0.993186\n",
      "train loss:   1.205674\n",
      "train loss:   1.283145\n",
      "train loss:   0.997885\n",
      "train loss:   0.871683\n",
      "train loss:   0.944892\n",
      "train loss:   1.441012\n",
      "train loss:   1.050314\n",
      "train loss:   1.040745\n",
      "train loss:   1.079676\n",
      "train loss:   1.078964\n",
      "train loss:   0.777061\n",
      "train loss:   1.118399\n",
      "train loss:   1.230736\n",
      "train loss:   1.049209\n",
      "train loss:   1.076329\n",
      "train loss:   0.729197\n",
      "train loss:   1.049639\n",
      "train loss:   0.820561\n",
      "train loss:   0.966200\n",
      "train loss:   0.987071\n",
      "train loss:   0.919665\n",
      "train loss:   0.507515\n",
      "train loss:   1.006790\n",
      "train loss:   1.077469\n",
      "train loss:   1.019132\n",
      "train loss:   0.878499\n",
      "train loss:   1.154780\n",
      "train loss:   1.252446\n",
      "train loss:   0.404376\n",
      "train loss:   0.912816\n",
      "train loss:   0.750055\n",
      "train loss:   1.453108\n",
      "train loss:   0.945507\n",
      "train loss:   0.652799\n",
      "train loss:   1.628807\n",
      "########### epoch 101 ###########\n",
      "########### loop 18850 ###########\n",
      "test loss:   0.250689   test accuracy:   0.916667\n",
      "########### loop 18850 ###########\n",
      "train loss:   1.149476\n",
      "train loss:   1.000947\n",
      "train loss:   0.978411\n",
      "train loss:   0.745146\n",
      "train loss:   0.952643\n",
      "train loss:   1.041531\n",
      "train loss:   0.927200\n",
      "train loss:   0.895556\n",
      "train loss:   0.944494\n",
      "train loss:   0.619626\n",
      "train loss:   0.965221\n",
      "train loss:   1.059776\n",
      "train loss:   1.115980\n",
      "train loss:   1.017913\n",
      "train loss:   0.776009\n",
      "train loss:   0.974782\n",
      "train loss:   1.202695\n",
      "train loss:   1.097197\n",
      "train loss:   1.094230\n",
      "train loss:   1.359047\n",
      "train loss:   1.009250\n",
      "train loss:   1.213961\n",
      "train loss:   1.097377\n",
      "train loss:   0.989555\n",
      "train loss:   1.390301\n",
      "train loss:   1.067068\n",
      "train loss:   0.741759\n",
      "train loss:   1.076194\n",
      "train loss:   0.587712\n",
      "train loss:   1.089247\n",
      "train loss:   0.849331\n",
      "train loss:   0.833985\n",
      "train loss:   0.784071\n",
      "train loss:   0.981100\n",
      "train loss:   1.057887\n",
      "train loss:   1.157104\n",
      "train loss:   1.210450\n",
      "train loss:   1.256692\n",
      "train loss:   1.037192\n",
      "train loss:   1.288106\n",
      "train loss:   0.952407\n",
      "train loss:   1.141233\n",
      "train loss:   1.112844\n",
      "train loss:   0.799559\n",
      "train loss:   0.946501\n",
      "train loss:   0.779356\n",
      "train loss:   0.868376\n",
      "train loss:   1.013512\n",
      "train loss:   1.008497\n",
      "train loss:   0.790217\n",
      "########### epoch 101 ###########\n",
      "########### loop 18900 ###########\n",
      "test loss:   0.146629   test accuracy:   0.958333\n",
      "########### loop 18900 ###########\n",
      "train loss:   1.146168\n",
      "train loss:   1.006217\n",
      "train loss:   1.203454\n",
      "train loss:   0.832875\n",
      "train loss:   1.236401\n",
      "train loss:   0.723442\n",
      "train loss:   0.803689\n",
      "train loss:   0.927082\n",
      "train loss:   0.914292\n",
      "train loss:   1.357649\n",
      "train loss:   0.787277\n",
      "train loss:   0.789744\n",
      "train loss:   1.188823\n",
      "train loss:   1.095936\n",
      "train loss:   0.819745\n",
      "train loss:   0.892266\n",
      "train loss:   1.190548\n",
      "train loss:   1.230087\n",
      "train loss:   0.651108\n",
      "train loss:   1.203699\n",
      "train loss:   0.929414\n",
      "train loss:   0.578452\n",
      "train loss:   0.847066\n",
      "train loss:   1.156533\n",
      "train loss:   0.796684\n",
      "train loss:   1.267825\n",
      "train loss:   1.281568\n",
      "train loss:   0.591061\n",
      "train loss:   1.083443\n",
      "train loss:   1.064131\n",
      "train loss:   1.186977\n",
      "train loss:   1.385916\n",
      "train loss:   0.741043\n",
      "train loss:   1.123495\n",
      "train loss:   1.059951\n",
      "train loss:   0.540689\n",
      "train loss:   0.566192\n",
      "train loss:   0.781038\n",
      "train loss:   0.847319\n",
      "train loss:   0.804777\n",
      "train loss:   1.019278\n",
      "train loss:   1.055492\n",
      "train loss:   0.982876\n",
      "train loss:   1.018667\n",
      "train loss:   1.028909\n",
      "train loss:   0.823182\n",
      "train loss:   1.125180\n",
      "train loss:   1.033276\n",
      "train loss:   0.896661\n",
      "train loss:   1.026338\n",
      "########### epoch 101 ###########\n",
      "########### loop 18950 ###########\n",
      "test loss:   0.386475   test accuracy:   0.875000\n",
      "########### loop 18950 ###########\n",
      "train loss:   0.854861\n",
      "train loss:   0.985687\n",
      "train loss:   1.091012\n",
      "train loss:   0.759092\n",
      "train loss:   0.986971\n",
      "train loss:   0.941459\n",
      "train loss:   0.755316\n",
      "train loss:   0.831056\n",
      "train loss:   1.138707\n",
      "train loss:   1.210661\n",
      "train loss:   0.895417\n",
      "train loss:   0.818688\n",
      "train loss:   0.939854\n",
      "train loss:   1.146259\n",
      "train loss:   0.996940\n",
      "train loss:   0.584310\n",
      "train loss:   1.178077\n",
      "train loss:   1.188792\n",
      "train loss:   1.209683\n",
      "train loss:   0.952986\n",
      "train loss:   1.284116\n",
      "train loss:   0.843416\n",
      "train loss:   1.009787\n",
      "train loss:   1.269870\n",
      "train loss:   1.253849\n",
      "train loss:   0.886962\n",
      "train loss:   1.045976\n",
      "train loss:   1.214138\n",
      "train loss:   1.090564\n",
      "train loss:   0.961320\n",
      "train loss:   0.801802\n",
      "train loss:   0.784473\n",
      "train loss:   1.163917\n",
      "train loss:   0.917871\n",
      "train loss:   0.876344\n",
      "train loss:   0.880536\n",
      "train loss:   1.022563\n",
      "train loss:   0.982168\n",
      "train loss:   0.977036\n",
      "train loss:   0.874092\n",
      "train loss:   1.107875\n",
      "train loss:   0.958494\n",
      "train loss:   1.209938\n",
      "train loss:   1.085929\n",
      "train loss:   0.716225\n",
      "train loss:   0.791370\n",
      "train loss:   0.833419\n",
      "train loss:   0.604993\n",
      "train loss:   0.674455\n",
      "train loss:   1.010236\n",
      "########### epoch 102 ###########\n",
      "########### loop 19000 ###########\n",
      "test loss:   0.165892   test accuracy:   0.958333\n",
      "########### loop 19000 ###########\n",
      "train loss:   1.089455\n",
      "train loss:   0.881594\n",
      "train loss:   1.137745\n",
      "train loss:   1.170941\n",
      "train loss:   1.306365\n",
      "train loss:   0.862956\n",
      "train loss:   1.216907\n",
      "train loss:   0.789723\n",
      "train loss:   1.231175\n",
      "train loss:   1.033821\n",
      "train loss:   0.813607\n",
      "train loss:   1.023091\n",
      "train loss:   1.083820\n",
      "train loss:   0.830108\n",
      "train loss:   0.999182\n",
      "train loss:   0.871894\n",
      "train loss:   0.927230\n",
      "train loss:   1.179056\n",
      "train loss:   0.572515\n",
      "train loss:   1.011085\n",
      "train loss:   1.162912\n",
      "train loss:   0.983383\n",
      "train loss:   0.493597\n",
      "train loss:   0.915704\n",
      "train loss:   1.142789\n",
      "train loss:   1.266668\n",
      "train loss:   1.277801\n",
      "train loss:   0.615459\n",
      "train loss:   1.091118\n",
      "train loss:   0.611908\n",
      "train loss:   1.183925\n",
      "train loss:   0.574257\n",
      "train loss:   1.075549\n",
      "train loss:   1.092173\n",
      "train loss:   1.195426\n",
      "train loss:   1.245452\n",
      "train loss:   0.822219\n",
      "train loss:   1.170449\n",
      "train loss:   1.012841\n",
      "train loss:   1.309563\n",
      "train loss:   1.294435\n",
      "train loss:   1.160729\n",
      "train loss:   1.155395\n",
      "train loss:   1.068957\n",
      "train loss:   1.207460\n",
      "train loss:   0.848398\n",
      "train loss:   0.944365\n",
      "train loss:   0.769929\n",
      "train loss:   1.007631\n",
      "train loss:   1.036731\n",
      "########### epoch 102 ###########\n",
      "########### loop 19050 ###########\n",
      "test loss:   0.598215   test accuracy:   0.791667\n",
      "########### loop 19050 ###########\n",
      "train loss:   0.886048\n",
      "train loss:   1.052017\n",
      "train loss:   0.717440\n",
      "train loss:   1.063912\n",
      "train loss:   0.844947\n",
      "train loss:   0.953839\n",
      "train loss:   0.632582\n",
      "train loss:   0.988577\n",
      "train loss:   1.007142\n",
      "train loss:   0.892113\n",
      "train loss:   1.043154\n",
      "train loss:   1.318087\n",
      "train loss:   1.380577\n",
      "train loss:   0.891280\n",
      "train loss:   1.148494\n",
      "train loss:   0.687917\n",
      "train loss:   1.240359\n",
      "train loss:   0.810722\n",
      "train loss:   1.043460\n",
      "train loss:   1.168556\n",
      "train loss:   0.882883\n",
      "train loss:   1.133977\n",
      "train loss:   1.068608\n",
      "train loss:   1.073299\n",
      "train loss:   0.847758\n",
      "train loss:   1.206501\n",
      "train loss:   1.023002\n",
      "train loss:   0.944815\n",
      "train loss:   1.013073\n",
      "train loss:   1.228404\n",
      "train loss:   1.190849\n",
      "train loss:   0.846219\n",
      "train loss:   0.956772\n",
      "train loss:   1.055962\n",
      "train loss:   1.271279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.033548\n",
      "train loss:   0.960471\n",
      "train loss:   0.966041\n",
      "train loss:   1.135425\n",
      "train loss:   1.019112\n",
      "train loss:   1.252648\n",
      "train loss:   0.846909\n",
      "train loss:   0.595469\n",
      "train loss:   1.112326\n",
      "train loss:   1.171786\n",
      "train loss:   1.161640\n",
      "train loss:   0.570987\n",
      "train loss:   1.060340\n",
      "train loss:   1.405899\n",
      "train loss:   1.050475\n",
      "########### epoch 102 ###########\n",
      "########### loop 19100 ###########\n",
      "test loss:   0.311811   test accuracy:   0.916667\n",
      "########### loop 19100 ###########\n",
      "train loss:   0.796502\n",
      "train loss:   1.057205\n",
      "train loss:   0.882326\n",
      "train loss:   0.601800\n",
      "train loss:   1.093679\n",
      "train loss:   0.719622\n",
      "train loss:   0.960318\n",
      "train loss:   1.149060\n",
      "train loss:   0.982145\n",
      "train loss:   0.828656\n",
      "train loss:   1.062088\n",
      "train loss:   0.766996\n",
      "train loss:   0.918353\n",
      "train loss:   1.202044\n",
      "train loss:   0.890271\n",
      "train loss:   1.176322\n",
      "train loss:   1.177612\n",
      "train loss:   1.056475\n",
      "train loss:   1.238227\n",
      "train loss:   1.565564\n",
      "train loss:   0.956784\n",
      "train loss:   0.991086\n",
      "train loss:   0.844319\n",
      "train loss:   0.725900\n",
      "train loss:   0.983624\n",
      "train loss:   0.797881\n",
      "train loss:   0.861848\n",
      "train loss:   0.887053\n",
      "train loss:   1.111324\n",
      "train loss:   0.721525\n",
      "train loss:   0.798760\n",
      "train loss:   0.675254\n",
      "train loss:   0.414714\n",
      "train loss:   1.125290\n",
      "train loss:   0.966522\n",
      "train loss:   1.070966\n",
      "train loss:   0.878424\n",
      "train loss:   1.130204\n",
      "train loss:   0.862522\n",
      "train loss:   0.776608\n",
      "train loss:   0.953730\n",
      "train loss:   0.897444\n",
      "train loss:   0.716509\n",
      "train loss:   1.177449\n",
      "train loss:   0.809321\n",
      "train loss:   1.001574\n",
      "train loss:   1.055359\n",
      "train loss:   1.243393\n",
      "train loss:   0.790202\n",
      "train loss:   0.988594\n",
      "########### epoch 102 ###########\n",
      "########### loop 19150 ###########\n",
      "test loss:   0.314124   test accuracy:   0.875000\n",
      "########### loop 19150 ###########\n",
      "train loss:   0.867114\n",
      "train loss:   0.849223\n",
      "train loss:   0.720889\n",
      "train loss:   0.792977\n",
      "train loss:   1.131807\n",
      "train loss:   0.926066\n",
      "train loss:   1.298555\n",
      "train loss:   1.195270\n",
      "train loss:   0.777577\n",
      "train loss:   1.128802\n",
      "train loss:   0.813177\n",
      "train loss:   0.995132\n",
      "train loss:   0.763523\n",
      "train loss:   0.887447\n",
      "train loss:   1.302026\n",
      "train loss:   0.952715\n",
      "train loss:   1.033636\n",
      "train loss:   1.187126\n",
      "train loss:   0.948798\n",
      "train loss:   0.868545\n",
      "train loss:   0.729299\n",
      "train loss:   0.974139\n",
      "train loss:   1.136411\n",
      "train loss:   0.716261\n",
      "train loss:   0.942379\n",
      "train loss:   0.945543\n",
      "train loss:   0.952882\n",
      "train loss:   0.864402\n",
      "train loss:   0.868435\n",
      "train loss:   0.893648\n",
      "train loss:   0.999447\n",
      "train loss:   0.826609\n",
      "train loss:   0.845877\n",
      "train loss:   0.736606\n",
      "train loss:   0.808147\n",
      "train loss:   1.249026\n",
      "train loss:   1.035842\n",
      "train loss:   1.006296\n",
      "train loss:   0.572406\n",
      "train loss:   1.080115\n",
      "train loss:   0.932544\n",
      "train loss:   1.027790\n",
      "train loss:   1.151483\n",
      "train loss:   1.101141\n",
      "train loss:   0.918455\n",
      "train loss:   1.690510\n",
      "train loss:   0.833212\n",
      "train loss:   0.817135\n",
      "train loss:   0.691781\n",
      "train loss:   1.205881\n",
      "########### epoch 103 ###########\n",
      "########### loop 19200 ###########\n",
      "test loss:   0.364439   test accuracy:   0.875000\n",
      "########### loop 19200 ###########\n",
      "train loss:   0.842451\n",
      "train loss:   1.099836\n",
      "train loss:   0.998541\n",
      "train loss:   0.790150\n",
      "train loss:   0.829727\n",
      "train loss:   1.013848\n",
      "train loss:   0.872525\n",
      "train loss:   0.759119\n",
      "train loss:   1.384520\n",
      "train loss:   1.136252\n",
      "train loss:   1.083363\n",
      "train loss:   0.792206\n",
      "train loss:   0.813408\n",
      "train loss:   0.919204\n",
      "train loss:   0.963545\n",
      "train loss:   0.759211\n",
      "train loss:   0.864804\n",
      "train loss:   1.081530\n",
      "train loss:   0.878626\n",
      "train loss:   1.197387\n",
      "train loss:   1.214070\n",
      "train loss:   0.998872\n",
      "train loss:   0.880905\n",
      "train loss:   0.895075\n",
      "train loss:   0.723781\n",
      "train loss:   1.128661\n",
      "train loss:   0.873842\n",
      "train loss:   0.989973\n",
      "train loss:   1.095162\n",
      "train loss:   0.641432\n",
      "train loss:   0.830294\n",
      "train loss:   0.990711\n",
      "train loss:   1.060039\n",
      "train loss:   0.928123\n",
      "train loss:   0.960093\n",
      "train loss:   0.817622\n",
      "train loss:   0.863612\n",
      "train loss:   0.797635\n",
      "train loss:   0.782010\n",
      "train loss:   0.715439\n",
      "train loss:   1.031225\n",
      "train loss:   0.946983\n",
      "train loss:   0.850351\n",
      "train loss:   0.707444\n",
      "train loss:   0.882368\n",
      "train loss:   0.922533\n",
      "train loss:   1.162984\n",
      "train loss:   0.782455\n",
      "train loss:   0.887350\n",
      "train loss:   1.058369\n",
      "########### epoch 103 ###########\n",
      "########### loop 19250 ###########\n",
      "test loss:   0.475736   test accuracy:   0.875000\n",
      "########### loop 19250 ###########\n",
      "train loss:   0.689937\n",
      "train loss:   0.946429\n",
      "train loss:   0.925284\n",
      "train loss:   1.034436\n",
      "train loss:   0.694695\n",
      "train loss:   1.491385\n",
      "train loss:   1.208191\n",
      "train loss:   0.991639\n",
      "train loss:   0.884782\n",
      "train loss:   1.095802\n",
      "train loss:   0.676911\n",
      "train loss:   1.046211\n",
      "train loss:   0.817914\n",
      "train loss:   0.945007\n",
      "train loss:   1.104885\n",
      "train loss:   0.998206\n",
      "train loss:   1.120816\n",
      "train loss:   0.825949\n",
      "train loss:   1.349312\n",
      "train loss:   1.161222\n",
      "train loss:   1.091335\n",
      "train loss:   0.989959\n",
      "train loss:   0.937682\n",
      "train loss:   1.112100\n",
      "train loss:   0.993855\n",
      "train loss:   0.565485\n",
      "train loss:   0.498451\n",
      "train loss:   1.077342\n",
      "train loss:   0.684177\n",
      "train loss:   0.420908\n",
      "train loss:   0.987461\n",
      "train loss:   0.864782\n",
      "train loss:   0.902683\n",
      "train loss:   0.832704\n",
      "train loss:   0.797345\n",
      "train loss:   1.343290\n",
      "train loss:   0.861113\n",
      "train loss:   1.107829\n",
      "train loss:   0.715704\n",
      "train loss:   1.198978\n",
      "train loss:   0.986808\n",
      "train loss:   0.818845\n",
      "train loss:   0.819462\n",
      "train loss:   1.074050\n",
      "train loss:   0.655105\n",
      "train loss:   0.778401\n",
      "train loss:   0.866749\n",
      "train loss:   1.280751\n",
      "train loss:   1.113826\n",
      "train loss:   0.851445\n",
      "########### epoch 103 ###########\n",
      "########### loop 19300 ###########\n",
      "test loss:   0.126156   test accuracy:   1.000000\n",
      "########### loop 19300 ###########\n",
      "train loss:   1.292760\n",
      "train loss:   0.654580\n",
      "train loss:   0.728176\n",
      "train loss:   0.856751\n",
      "train loss:   0.754969\n",
      "train loss:   1.046080\n",
      "train loss:   0.662138\n",
      "train loss:   1.127149\n",
      "train loss:   1.049528\n",
      "train loss:   0.920298\n",
      "train loss:   0.740618\n",
      "train loss:   0.981702\n",
      "train loss:   1.143442\n",
      "train loss:   0.678034\n",
      "train loss:   0.903216\n",
      "train loss:   1.048928\n",
      "train loss:   1.351885\n",
      "train loss:   0.992863\n",
      "train loss:   0.764924\n",
      "train loss:   1.044914\n",
      "train loss:   0.970948\n",
      "train loss:   0.690487\n",
      "train loss:   1.146562\n",
      "train loss:   0.807554\n",
      "train loss:   0.749100\n",
      "train loss:   0.819920\n",
      "train loss:   0.554814\n",
      "train loss:   1.046688\n",
      "train loss:   1.339859\n",
      "train loss:   0.713136\n",
      "train loss:   0.984354\n",
      "train loss:   0.861675\n",
      "train loss:   0.770020\n",
      "train loss:   0.985964\n",
      "train loss:   1.140526\n",
      "train loss:   0.860773\n",
      "train loss:   1.024799\n",
      "train loss:   1.154055\n",
      "train loss:   0.840828\n",
      "train loss:   0.802592\n",
      "train loss:   0.753888\n",
      "train loss:   1.235844\n",
      "train loss:   1.199438\n",
      "train loss:   1.068136\n",
      "train loss:   1.062229\n",
      "train loss:   0.908231\n",
      "train loss:   0.925873\n",
      "train loss:   0.962874\n",
      "train loss:   0.854775\n",
      "train loss:   1.178446\n",
      "########### epoch 103 ###########\n",
      "########### loop 19350 ###########\n",
      "test loss:   0.171225   test accuracy:   0.958333\n",
      "########### loop 19350 ###########\n",
      "train loss:   1.039359\n",
      "train loss:   0.976349\n",
      "train loss:   1.058061\n",
      "train loss:   1.084300\n",
      "train loss:   0.855088\n",
      "train loss:   0.770351\n",
      "train loss:   1.119612\n",
      "train loss:   1.008101\n",
      "train loss:   0.945842\n",
      "train loss:   0.735214\n",
      "train loss:   0.736296\n",
      "train loss:   1.095066\n",
      "train loss:   0.749500\n",
      "train loss:   0.956674\n",
      "train loss:   1.386082\n",
      "train loss:   0.861687\n",
      "train loss:   1.054705\n",
      "train loss:   0.734579\n",
      "train loss:   0.895474\n",
      "train loss:   1.071549\n",
      "train loss:   0.871363\n",
      "train loss:   0.968618\n",
      "train loss:   0.967167\n",
      "train loss:   0.758480\n",
      "train loss:   0.985119\n",
      "train loss:   1.299391\n",
      "train loss:   0.783009\n",
      "train loss:   0.866817\n",
      "train loss:   1.185135\n",
      "train loss:   1.188887\n",
      "train loss:   0.761980\n",
      "train loss:   0.944534\n",
      "train loss:   0.597716\n",
      "train loss:   0.750752\n",
      "train loss:   0.834773\n",
      "train loss:   1.071862\n",
      "train loss:   0.629525\n",
      "train loss:   0.897117\n",
      "train loss:   1.397767\n",
      "train loss:   0.718567\n",
      "train loss:   0.645748\n",
      "train loss:   0.667833\n",
      "train loss:   0.769472\n",
      "train loss:   1.001780\n",
      "train loss:   0.639593\n",
      "train loss:   1.017535\n",
      "train loss:   1.123854\n",
      "train loss:   0.814644\n",
      "train loss:   1.125942\n",
      "train loss:   0.967438\n",
      "########### epoch 104 ###########\n",
      "########### loop 19400 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.167234   test accuracy:   1.000000\n",
      "########### loop 19400 ###########\n",
      "train loss:   1.004210\n",
      "train loss:   0.690628\n",
      "train loss:   1.143300\n",
      "train loss:   1.451946\n",
      "train loss:   1.051751\n",
      "train loss:   0.939402\n",
      "train loss:   0.997088\n",
      "train loss:   1.348728\n",
      "train loss:   1.264783\n",
      "train loss:   1.259160\n",
      "train loss:   1.099759\n",
      "train loss:   1.361251\n",
      "train loss:   1.298169\n",
      "train loss:   0.430084\n",
      "train loss:   0.973034\n",
      "train loss:   0.887627\n",
      "train loss:   0.807451\n",
      "train loss:   0.772086\n",
      "train loss:   0.758708\n",
      "train loss:   0.993421\n",
      "train loss:   0.668983\n",
      "train loss:   1.125481\n",
      "train loss:   0.932228\n",
      "train loss:   0.997104\n",
      "train loss:   0.663271\n",
      "train loss:   0.904014\n",
      "train loss:   1.222614\n",
      "train loss:   1.173783\n",
      "train loss:   1.066053\n",
      "train loss:   1.078675\n",
      "train loss:   0.698398\n",
      "train loss:   1.044397\n",
      "train loss:   0.769182\n",
      "train loss:   0.835676\n",
      "train loss:   1.012868\n",
      "train loss:   1.086971\n",
      "train loss:   1.275323\n",
      "train loss:   0.901609\n",
      "train loss:   1.183708\n",
      "train loss:   1.383387\n",
      "train loss:   0.840674\n",
      "train loss:   0.917782\n",
      "train loss:   1.101676\n",
      "train loss:   0.888810\n",
      "train loss:   1.058071\n",
      "train loss:   0.999669\n",
      "train loss:   1.188363\n",
      "train loss:   0.993158\n",
      "train loss:   0.631690\n",
      "train loss:   1.334318\n",
      "########### epoch 104 ###########\n",
      "########### loop 19450 ###########\n",
      "test loss:   0.311559   test accuracy:   0.958333\n",
      "########### loop 19450 ###########\n",
      "train loss:   1.089785\n",
      "train loss:   0.837807\n",
      "train loss:   0.591848\n",
      "train loss:   1.149526\n",
      "train loss:   1.171519\n",
      "train loss:   0.813250\n",
      "train loss:   1.013742\n",
      "train loss:   0.984955\n",
      "train loss:   0.536640\n",
      "train loss:   0.934542\n",
      "train loss:   1.064990\n",
      "train loss:   0.842751\n",
      "train loss:   0.957927\n",
      "train loss:   0.931157\n",
      "train loss:   1.068247\n",
      "train loss:   0.890216\n",
      "train loss:   1.006371\n",
      "train loss:   0.799960\n",
      "train loss:   0.949596\n",
      "train loss:   1.168872\n",
      "train loss:   1.254455\n",
      "train loss:   1.393330\n",
      "train loss:   1.285811\n",
      "train loss:   0.723833\n",
      "train loss:   1.026764\n",
      "train loss:   1.330816\n",
      "train loss:   0.975039\n",
      "train loss:   0.682972\n",
      "train loss:   1.040758\n",
      "train loss:   1.135301\n",
      "train loss:   1.029281\n",
      "train loss:   1.101115\n",
      "train loss:   1.199884\n",
      "train loss:   0.994119\n",
      "train loss:   1.320904\n",
      "train loss:   0.904561\n",
      "train loss:   1.289875\n",
      "train loss:   1.147295\n",
      "train loss:   1.142212\n",
      "train loss:   1.319087\n",
      "train loss:   0.784911\n",
      "train loss:   0.944267\n",
      "train loss:   1.027647\n",
      "train loss:   1.112238\n",
      "train loss:   0.829412\n",
      "train loss:   1.041126\n",
      "train loss:   0.782455\n",
      "train loss:   1.120015\n",
      "train loss:   0.942734\n",
      "train loss:   0.699525\n",
      "########### epoch 104 ###########\n",
      "########### loop 19500 ###########\n",
      "test loss:   0.283483   test accuracy:   0.958333\n",
      "########### loop 19500 ###########\n",
      "train loss:   0.927318\n",
      "train loss:   0.891526\n",
      "train loss:   0.738864\n",
      "train loss:   0.994997\n",
      "train loss:   1.242081\n",
      "train loss:   0.658478\n",
      "train loss:   0.736310\n",
      "train loss:   0.822197\n",
      "train loss:   0.979503\n",
      "train loss:   0.836219\n",
      "train loss:   1.133028\n",
      "train loss:   0.891835\n",
      "train loss:   0.474679\n",
      "train loss:   0.908276\n",
      "train loss:   0.811046\n",
      "train loss:   0.952742\n",
      "train loss:   1.111153\n",
      "train loss:   0.800830\n",
      "train loss:   0.977432\n",
      "train loss:   1.211184\n",
      "train loss:   1.042074\n",
      "train loss:   0.862676\n",
      "train loss:   0.410376\n",
      "train loss:   1.277304\n",
      "train loss:   0.857118\n",
      "train loss:   0.816207\n",
      "train loss:   1.116681\n",
      "train loss:   0.892295\n",
      "train loss:   1.044456\n",
      "train loss:   1.347774\n",
      "train loss:   1.175748\n",
      "train loss:   1.044450\n",
      "train loss:   1.039145\n",
      "train loss:   1.140155\n",
      "train loss:   0.873603\n",
      "train loss:   0.522900\n",
      "train loss:   0.721200\n",
      "train loss:   1.041068\n",
      "train loss:   1.121631\n",
      "train loss:   0.755012\n",
      "train loss:   0.652264\n",
      "train loss:   0.954307\n",
      "train loss:   0.453976\n",
      "train loss:   0.864144\n",
      "train loss:   0.927594\n",
      "train loss:   0.906078\n",
      "train loss:   1.128909\n",
      "train loss:   0.922632\n",
      "train loss:   0.644853\n",
      "train loss:   1.063284\n",
      "########### epoch 104 ###########\n",
      "########### loop 19550 ###########\n",
      "test loss:   0.232010   test accuracy:   0.958333\n",
      "########### loop 19550 ###########\n",
      "train loss:   0.794532\n",
      "train loss:   0.842512\n",
      "train loss:   1.019625\n",
      "train loss:   1.135498\n",
      "train loss:   0.884614\n",
      "train loss:   0.948332\n",
      "train loss:   1.089241\n",
      "train loss:   1.294632\n",
      "train loss:   0.881076\n",
      "train loss:   0.906231\n",
      "train loss:   0.840557\n",
      "train loss:   1.077465\n",
      "train loss:   0.892237\n",
      "train loss:   0.635696\n",
      "train loss:   1.473204\n",
      "train loss:   1.033891\n",
      "train loss:   1.089933\n",
      "train loss:   0.782194\n",
      "train loss:   1.176761\n",
      "train loss:   1.546285\n",
      "train loss:   0.768506\n",
      "train loss:   1.078282\n",
      "train loss:   1.046857\n",
      "train loss:   0.953770\n",
      "train loss:   1.285930\n",
      "train loss:   0.900333\n",
      "train loss:   0.957985\n",
      "train loss:   0.934197\n",
      "train loss:   0.937764\n",
      "train loss:   1.146579\n",
      "train loss:   1.265192\n",
      "train loss:   0.875041\n",
      "train loss:   0.792425\n",
      "train loss:   1.323744\n",
      "train loss:   0.933960\n",
      "train loss:   0.626017\n",
      "train loss:   0.855237\n",
      "train loss:   0.605402\n",
      "train loss:   0.958693\n",
      "train loss:   0.954077\n",
      "train loss:   0.760355\n",
      "train loss:   1.235070\n",
      "train loss:   0.912620\n",
      "train loss:   0.996686\n",
      "train loss:   1.346266\n",
      "train loss:   0.732906\n",
      "train loss:   0.802610\n",
      "train loss:   1.262657\n",
      "train loss:   0.984711\n",
      "train loss:   0.855510\n",
      "########### epoch 105 ###########\n",
      "########### loop 19600 ###########\n",
      "test loss:   0.339713   test accuracy:   0.916667\n",
      "########### loop 19600 ###########\n",
      "train loss:   0.924892\n",
      "train loss:   1.091942\n",
      "train loss:   1.041005\n",
      "train loss:   1.284113\n",
      "train loss:   1.166809\n",
      "train loss:   0.995774\n",
      "train loss:   0.942916\n",
      "train loss:   0.967240\n",
      "train loss:   0.779467\n",
      "train loss:   1.255352\n",
      "train loss:   1.055482\n",
      "train loss:   0.881202\n",
      "train loss:   0.930402\n",
      "train loss:   1.023422\n",
      "train loss:   0.908464\n",
      "train loss:   0.970614\n",
      "train loss:   1.068860\n",
      "train loss:   1.020151\n",
      "train loss:   1.070137\n",
      "train loss:   0.960595\n",
      "train loss:   1.042370\n",
      "train loss:   1.089036\n",
      "train loss:   0.817684\n",
      "train loss:   0.510206\n",
      "train loss:   1.119205\n",
      "train loss:   0.846675\n",
      "train loss:   0.911634\n",
      "train loss:   0.883380\n",
      "train loss:   0.568630\n",
      "train loss:   0.876940\n",
      "train loss:   0.884021\n",
      "train loss:   0.865363\n",
      "train loss:   1.182048\n",
      "train loss:   1.016121\n",
      "train loss:   1.010829\n",
      "train loss:   1.037813\n",
      "train loss:   1.012098\n",
      "train loss:   0.964988\n",
      "train loss:   1.054395\n",
      "train loss:   1.093928\n",
      "train loss:   0.735209\n",
      "train loss:   1.086197\n",
      "train loss:   1.017460\n",
      "train loss:   0.945026\n",
      "train loss:   0.734871\n",
      "train loss:   0.929359\n",
      "train loss:   0.559207\n",
      "train loss:   1.018915\n",
      "train loss:   1.137841\n",
      "train loss:   0.820445\n",
      "########### epoch 105 ###########\n",
      "########### loop 19650 ###########\n",
      "test loss:   0.470974   test accuracy:   0.833333\n",
      "########### loop 19650 ###########\n",
      "train loss:   0.608730\n",
      "train loss:   0.995123\n",
      "train loss:   0.654898\n",
      "train loss:   0.901006\n",
      "train loss:   1.025657\n",
      "train loss:   1.192541\n",
      "train loss:   0.947029\n",
      "train loss:   0.905458\n",
      "train loss:   1.068537\n",
      "train loss:   1.078591\n",
      "train loss:   0.983124\n",
      "train loss:   0.597494\n",
      "train loss:   1.179749\n",
      "train loss:   0.758501\n",
      "train loss:   1.000828\n",
      "train loss:   1.527071\n",
      "train loss:   1.462795\n",
      "train loss:   0.692845\n",
      "train loss:   0.963391\n",
      "train loss:   1.168028\n",
      "train loss:   0.808665\n",
      "train loss:   1.041832\n",
      "train loss:   1.290761\n",
      "train loss:   1.157333\n",
      "train loss:   1.140516\n",
      "train loss:   0.797175\n",
      "train loss:   1.051037\n",
      "train loss:   0.852724\n",
      "train loss:   0.690664\n",
      "train loss:   1.030059\n",
      "train loss:   1.135418\n",
      "train loss:   0.793399\n",
      "train loss:   0.899002\n",
      "train loss:   0.972680\n",
      "train loss:   1.021433\n",
      "train loss:   0.823654\n",
      "train loss:   1.106259\n",
      "train loss:   1.257464\n",
      "train loss:   1.162750\n",
      "train loss:   0.963718\n",
      "train loss:   1.077888\n",
      "train loss:   0.967884\n",
      "train loss:   1.200780\n",
      "train loss:   0.944593\n",
      "train loss:   0.968649\n",
      "train loss:   1.186062\n",
      "train loss:   0.731302\n",
      "train loss:   0.832299\n",
      "train loss:   1.190746\n",
      "train loss:   0.760859\n",
      "########### epoch 105 ###########\n",
      "########### loop 19700 ###########\n",
      "test loss:   0.484245   test accuracy:   0.833333\n",
      "########### loop 19700 ###########\n",
      "train loss:   0.887477\n",
      "train loss:   0.944383\n",
      "train loss:   0.849524\n",
      "train loss:   1.225079\n",
      "train loss:   0.995817\n",
      "train loss:   0.870597\n",
      "train loss:   0.830676\n",
      "train loss:   1.115316\n",
      "train loss:   0.984247\n",
      "train loss:   1.207799\n",
      "train loss:   0.470322\n",
      "train loss:   0.799990\n",
      "train loss:   0.942962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.886208\n",
      "train loss:   1.068321\n",
      "train loss:   0.853126\n",
      "train loss:   1.060180\n",
      "train loss:   0.904402\n",
      "train loss:   0.777646\n",
      "train loss:   1.068067\n",
      "train loss:   0.924602\n",
      "train loss:   0.875524\n",
      "train loss:   1.012097\n",
      "train loss:   0.744725\n",
      "train loss:   0.688765\n",
      "train loss:   0.986881\n",
      "train loss:   1.216628\n",
      "train loss:   1.304128\n",
      "train loss:   0.795298\n",
      "train loss:   1.058505\n",
      "train loss:   0.951200\n",
      "train loss:   1.002837\n",
      "train loss:   0.700379\n",
      "train loss:   1.015992\n",
      "train loss:   0.754464\n",
      "train loss:   1.064326\n",
      "train loss:   0.984733\n",
      "train loss:   1.565866\n",
      "train loss:   0.720779\n",
      "train loss:   0.800811\n",
      "train loss:   1.041196\n",
      "train loss:   0.861500\n",
      "train loss:   1.041568\n",
      "train loss:   1.272848\n",
      "train loss:   0.851366\n",
      "train loss:   0.950224\n",
      "train loss:   1.200417\n",
      "train loss:   0.731390\n",
      "train loss:   1.215123\n",
      "train loss:   1.213045\n",
      "########### epoch 106 ###########\n",
      "########### loop 19750 ###########\n",
      "test loss:   0.407003   test accuracy:   0.875000\n",
      "########### loop 19750 ###########\n",
      "train loss:   1.242725\n",
      "train loss:   1.270885\n",
      "train loss:   1.346012\n",
      "train loss:   0.803308\n",
      "train loss:   0.792771\n",
      "train loss:   1.074917\n",
      "train loss:   0.818064\n",
      "train loss:   1.248982\n",
      "train loss:   0.842814\n",
      "train loss:   0.851650\n",
      "train loss:   1.005570\n",
      "train loss:   1.064621\n",
      "train loss:   0.949414\n",
      "train loss:   1.216335\n",
      "train loss:   0.492925\n",
      "train loss:   1.012576\n",
      "train loss:   1.008576\n",
      "train loss:   0.831499\n",
      "train loss:   0.717334\n",
      "train loss:   0.871384\n",
      "train loss:   1.190873\n",
      "train loss:   1.046731\n",
      "train loss:   1.338127\n",
      "train loss:   1.042850\n",
      "train loss:   0.887245\n",
      "train loss:   0.907867\n",
      "train loss:   1.279326\n",
      "train loss:   0.829302\n",
      "train loss:   1.161610\n",
      "train loss:   1.379845\n",
      "train loss:   0.797390\n",
      "train loss:   0.699370\n",
      "train loss:   0.908155\n",
      "train loss:   0.932080\n",
      "train loss:   1.171543\n",
      "train loss:   1.108847\n",
      "train loss:   0.863642\n",
      "train loss:   1.014813\n",
      "train loss:   0.849334\n",
      "train loss:   0.926455\n",
      "train loss:   0.971200\n",
      "train loss:   0.918701\n",
      "train loss:   1.043831\n",
      "train loss:   1.056369\n",
      "train loss:   1.013145\n",
      "train loss:   1.085164\n",
      "train loss:   1.230863\n",
      "train loss:   0.847319\n",
      "train loss:   1.436206\n",
      "train loss:   0.876249\n",
      "########### epoch 106 ###########\n",
      "########### loop 19800 ###########\n",
      "test loss:   0.180984   test accuracy:   0.958333\n",
      "########### loop 19800 ###########\n",
      "train loss:   1.202664\n",
      "train loss:   0.837930\n",
      "train loss:   1.036707\n",
      "train loss:   0.649233\n",
      "train loss:   1.211480\n",
      "train loss:   0.839375\n",
      "train loss:   0.969117\n",
      "train loss:   1.042852\n",
      "train loss:   0.976208\n",
      "train loss:   1.077464\n",
      "train loss:   0.861853\n",
      "train loss:   0.596605\n",
      "train loss:   0.961673\n",
      "train loss:   1.027355\n",
      "train loss:   0.671452\n",
      "train loss:   1.330747\n",
      "train loss:   1.142799\n",
      "train loss:   1.191066\n",
      "train loss:   1.015981\n",
      "train loss:   1.461984\n",
      "train loss:   0.568737\n",
      "train loss:   1.188146\n",
      "train loss:   0.814074\n",
      "train loss:   0.633678\n",
      "train loss:   0.921345\n",
      "train loss:   0.770728\n",
      "train loss:   0.805411\n",
      "train loss:   1.117545\n",
      "train loss:   0.939611\n",
      "train loss:   1.069858\n",
      "train loss:   0.984855\n",
      "train loss:   0.819296\n",
      "train loss:   0.815476\n",
      "train loss:   1.113848\n",
      "train loss:   1.116422\n",
      "train loss:   1.053100\n",
      "train loss:   1.056670\n",
      "train loss:   0.839754\n",
      "train loss:   1.078366\n",
      "train loss:   1.035724\n",
      "train loss:   0.685803\n",
      "train loss:   0.747382\n",
      "train loss:   1.006801\n",
      "train loss:   1.132063\n",
      "train loss:   0.176842\n",
      "train loss:   0.936004\n",
      "train loss:   1.309225\n",
      "train loss:   1.096052\n",
      "train loss:   1.224113\n",
      "train loss:   0.844581\n",
      "########### epoch 106 ###########\n",
      "########### loop 19850 ###########\n",
      "test loss:   0.167774   test accuracy:   0.958333\n",
      "########### loop 19850 ###########\n",
      "train loss:   1.104235\n",
      "train loss:   0.928597\n",
      "train loss:   0.916407\n",
      "train loss:   1.059121\n",
      "train loss:   0.897984\n",
      "train loss:   0.603617\n",
      "train loss:   0.852955\n",
      "train loss:   0.933329\n",
      "train loss:   1.100508\n",
      "train loss:   0.944598\n",
      "train loss:   0.737870\n",
      "train loss:   0.982538\n",
      "train loss:   0.986776\n",
      "train loss:   0.913036\n",
      "train loss:   1.069995\n",
      "train loss:   1.042316\n",
      "train loss:   0.864329\n",
      "train loss:   0.996678\n",
      "train loss:   0.943637\n",
      "train loss:   1.332789\n",
      "train loss:   1.023092\n",
      "train loss:   0.657849\n",
      "train loss:   1.245987\n",
      "train loss:   0.936012\n",
      "train loss:   0.933125\n",
      "train loss:   1.049535\n",
      "train loss:   0.769705\n",
      "train loss:   0.997607\n",
      "train loss:   0.976724\n",
      "train loss:   1.171431\n",
      "train loss:   1.085889\n",
      "train loss:   0.803853\n",
      "train loss:   0.864021\n",
      "train loss:   1.299488\n",
      "train loss:   0.815347\n",
      "train loss:   1.317610\n",
      "train loss:   1.038642\n",
      "train loss:   1.174895\n",
      "train loss:   1.059732\n",
      "train loss:   0.878614\n",
      "train loss:   0.939055\n",
      "train loss:   0.947520\n",
      "train loss:   0.998608\n",
      "train loss:   0.661639\n",
      "train loss:   1.103790\n",
      "train loss:   0.988576\n",
      "train loss:   1.097591\n",
      "train loss:   1.008422\n",
      "train loss:   1.408610\n",
      "train loss:   0.827613\n",
      "########### epoch 106 ###########\n",
      "########### loop 19900 ###########\n",
      "test loss:   0.154646   test accuracy:   0.958333\n",
      "########### loop 19900 ###########\n",
      "train loss:   1.278726\n",
      "train loss:   0.868252\n",
      "train loss:   0.943942\n",
      "train loss:   0.979181\n",
      "train loss:   1.110938\n",
      "train loss:   0.678946\n",
      "train loss:   0.909827\n",
      "train loss:   1.137245\n",
      "train loss:   0.865890\n",
      "train loss:   0.862908\n",
      "train loss:   0.816899\n",
      "train loss:   1.152717\n",
      "train loss:   0.816631\n",
      "train loss:   1.004652\n",
      "train loss:   0.795237\n",
      "train loss:   0.977587\n",
      "train loss:   1.188057\n",
      "train loss:   0.966024\n",
      "train loss:   1.026925\n",
      "train loss:   0.962679\n",
      "train loss:   1.138146\n",
      "train loss:   1.154764\n",
      "train loss:   0.920490\n",
      "train loss:   0.569504\n",
      "train loss:   0.973192\n",
      "train loss:   1.292277\n",
      "train loss:   1.079025\n",
      "train loss:   0.867400\n",
      "train loss:   0.592780\n",
      "train loss:   0.997414\n",
      "train loss:   1.059180\n",
      "train loss:   1.052966\n",
      "train loss:   0.689495\n",
      "train loss:   1.369384\n",
      "train loss:   0.854395\n",
      "train loss:   1.019083\n",
      "train loss:   0.929236\n",
      "train loss:   1.213151\n",
      "train loss:   1.425616\n",
      "train loss:   1.010249\n",
      "train loss:   0.842004\n",
      "train loss:   1.071189\n",
      "train loss:   1.031370\n",
      "train loss:   0.785365\n",
      "train loss:   0.956061\n",
      "train loss:   1.208496\n",
      "train loss:   1.029510\n",
      "train loss:   1.199817\n",
      "train loss:   0.770340\n",
      "train loss:   1.068237\n",
      "########### epoch 107 ###########\n",
      "########### loop 19950 ###########\n",
      "test loss:   0.076325   test accuracy:   1.000000\n",
      "########### loop 19950 ###########\n",
      "train loss:   1.127979\n",
      "train loss:   1.069051\n",
      "train loss:   0.968772\n",
      "train loss:   1.052814\n",
      "train loss:   1.226247\n",
      "train loss:   1.083277\n",
      "train loss:   1.069684\n",
      "train loss:   1.217481\n",
      "train loss:   1.131858\n",
      "train loss:   0.947060\n",
      "train loss:   0.878902\n",
      "train loss:   0.864513\n",
      "train loss:   0.944616\n",
      "train loss:   1.051848\n",
      "train loss:   0.889013\n",
      "train loss:   1.160687\n",
      "train loss:   1.159824\n",
      "train loss:   1.707449\n",
      "train loss:   0.893833\n",
      "train loss:   0.733742\n",
      "train loss:   0.968547\n",
      "train loss:   1.168609\n",
      "train loss:   1.256147\n",
      "train loss:   1.025501\n",
      "train loss:   1.267654\n",
      "train loss:   0.724259\n",
      "train loss:   0.991778\n",
      "train loss:   0.993741\n",
      "train loss:   0.837580\n",
      "train loss:   1.215853\n",
      "train loss:   0.940740\n",
      "train loss:   0.834915\n",
      "train loss:   0.923513\n",
      "train loss:   1.088256\n",
      "train loss:   0.940195\n",
      "train loss:   0.816529\n",
      "train loss:   0.551659\n",
      "train loss:   1.063102\n",
      "train loss:   0.753146\n",
      "train loss:   0.576378\n",
      "train loss:   1.071144\n",
      "train loss:   0.763658\n",
      "train loss:   1.203118\n",
      "train loss:   1.052725\n",
      "train loss:   1.238390\n",
      "train loss:   0.917533\n",
      "train loss:   1.070743\n",
      "train loss:   0.459804\n",
      "train loss:   1.048211\n",
      "train loss:   0.686447\n",
      "########### epoch 107 ###########\n",
      "########### loop 20000 ###########\n",
      "test loss:   0.170762   test accuracy:   1.000000\n",
      "########### loop 20000 ###########\n",
      "train loss:   1.018593\n",
      "train loss:   0.649574\n",
      "train loss:   1.028228\n",
      "train loss:   0.760859\n",
      "train loss:   1.375578\n",
      "train loss:   1.223574\n",
      "train loss:   0.792619\n",
      "train loss:   0.730556\n",
      "train loss:   0.987715\n",
      "train loss:   0.959707\n",
      "train loss:   0.877022\n",
      "train loss:   1.193378\n",
      "train loss:   0.958987\n",
      "train loss:   0.759850\n",
      "train loss:   0.761073\n",
      "train loss:   0.763258\n",
      "train loss:   0.866176\n",
      "train loss:   0.855697\n",
      "train loss:   1.037076\n",
      "train loss:   0.818205\n",
      "train loss:   1.198593\n",
      "train loss:   0.810267\n",
      "train loss:   0.828455\n",
      "train loss:   1.006237\n",
      "train loss:   0.934514\n",
      "train loss:   0.790396\n",
      "train loss:   0.593002\n",
      "train loss:   0.670426\n",
      "train loss:   0.964516\n",
      "train loss:   1.219668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.943923\n",
      "train loss:   1.458184\n",
      "train loss:   0.933889\n",
      "train loss:   0.975327\n",
      "train loss:   0.780657\n",
      "train loss:   1.027502\n",
      "train loss:   1.230732\n",
      "train loss:   0.826878\n",
      "train loss:   1.441450\n",
      "train loss:   0.902124\n",
      "train loss:   0.662727\n",
      "train loss:   0.896990\n",
      "train loss:   1.611086\n",
      "train loss:   1.255069\n",
      "train loss:   1.015616\n",
      "train loss:   1.083647\n",
      "train loss:   0.780642\n",
      "train loss:   0.778994\n",
      "train loss:   0.762238\n",
      "train loss:   0.859359\n",
      "########### epoch 107 ###########\n",
      "########### loop 20050 ###########\n",
      "test loss:   0.223294   test accuracy:   0.958333\n",
      "########### loop 20050 ###########\n",
      "train loss:   1.142806\n",
      "train loss:   0.806180\n",
      "train loss:   0.845481\n",
      "train loss:   0.962595\n",
      "train loss:   1.167117\n",
      "train loss:   0.971999\n",
      "train loss:   1.220351\n",
      "train loss:   1.252410\n",
      "train loss:   1.015686\n",
      "train loss:   0.920333\n",
      "train loss:   1.207749\n",
      "train loss:   0.966931\n",
      "train loss:   1.404812\n",
      "train loss:   1.193968\n",
      "train loss:   0.976207\n",
      "train loss:   0.936389\n",
      "train loss:   0.823584\n",
      "train loss:   1.060966\n",
      "train loss:   0.781716\n",
      "train loss:   0.918236\n",
      "train loss:   0.985855\n",
      "train loss:   0.796094\n",
      "train loss:   0.841359\n",
      "train loss:   1.034223\n",
      "train loss:   1.170399\n",
      "train loss:   0.640424\n",
      "train loss:   0.801155\n",
      "train loss:   0.963536\n",
      "train loss:   1.475702\n",
      "train loss:   1.088550\n",
      "train loss:   0.812318\n",
      "train loss:   0.976513\n",
      "train loss:   0.946588\n",
      "train loss:   0.707142\n",
      "train loss:   1.107884\n",
      "train loss:   1.139501\n",
      "train loss:   1.074143\n",
      "train loss:   0.971084\n",
      "train loss:   0.828903\n",
      "train loss:   0.727211\n",
      "train loss:   1.088913\n",
      "train loss:   0.889243\n",
      "train loss:   0.862744\n",
      "train loss:   0.807206\n",
      "train loss:   0.870039\n",
      "train loss:   0.906514\n",
      "train loss:   1.221875\n",
      "train loss:   0.950155\n",
      "train loss:   1.012231\n",
      "train loss:   0.950370\n",
      "########### epoch 107 ###########\n",
      "########### loop 20100 ###########\n",
      "test loss:   0.195822   test accuracy:   0.916667\n",
      "########### loop 20100 ###########\n",
      "train loss:   0.904205\n",
      "train loss:   0.686314\n",
      "train loss:   0.762642\n",
      "train loss:   1.209731\n",
      "train loss:   0.976634\n",
      "train loss:   1.289173\n",
      "train loss:   0.659501\n",
      "train loss:   0.938947\n",
      "train loss:   0.740361\n",
      "train loss:   1.263879\n",
      "train loss:   0.689999\n",
      "train loss:   0.811892\n",
      "train loss:   0.723675\n",
      "train loss:   0.975694\n",
      "train loss:   1.108902\n",
      "train loss:   0.763354\n",
      "train loss:   1.266717\n",
      "train loss:   0.858624\n",
      "train loss:   1.170954\n",
      "train loss:   1.097739\n",
      "train loss:   1.054036\n",
      "train loss:   0.822265\n",
      "train loss:   0.995486\n",
      "train loss:   1.053532\n",
      "train loss:   1.059846\n",
      "train loss:   1.127409\n",
      "train loss:   1.113376\n",
      "train loss:   1.068257\n",
      "train loss:   0.844968\n",
      "train loss:   1.173442\n",
      "train loss:   0.747946\n",
      "train loss:   1.163622\n",
      "train loss:   1.079905\n",
      "train loss:   0.710219\n",
      "train loss:   0.940127\n",
      "train loss:   1.057380\n",
      "train loss:   1.036151\n",
      "train loss:   1.214125\n",
      "train loss:   1.355523\n",
      "train loss:   0.995269\n",
      "train loss:   0.810908\n",
      "train loss:   1.418578\n",
      "train loss:   0.766809\n",
      "train loss:   0.733927\n",
      "train loss:   0.815453\n",
      "train loss:   1.118541\n",
      "train loss:   0.913718\n",
      "train loss:   0.712990\n",
      "train loss:   0.982292\n",
      "train loss:   0.692966\n",
      "########### epoch 108 ###########\n",
      "########### loop 20150 ###########\n",
      "test loss:   0.402657   test accuracy:   0.833333\n",
      "########### loop 20150 ###########\n",
      "train loss:   0.848388\n",
      "train loss:   1.137392\n",
      "train loss:   0.806724\n",
      "train loss:   1.083841\n",
      "train loss:   0.852279\n",
      "train loss:   0.958860\n",
      "train loss:   0.882092\n",
      "train loss:   1.189697\n",
      "train loss:   0.843595\n",
      "train loss:   0.994680\n",
      "train loss:   1.182793\n",
      "train loss:   0.882515\n",
      "train loss:   1.135368\n",
      "train loss:   0.913437\n",
      "train loss:   0.748402\n",
      "train loss:   1.145008\n",
      "train loss:   0.941638\n",
      "train loss:   0.864331\n",
      "train loss:   1.403296\n",
      "train loss:   0.983082\n",
      "train loss:   1.266303\n",
      "train loss:   0.909557\n",
      "train loss:   1.040184\n",
      "train loss:   1.213125\n",
      "train loss:   0.696465\n",
      "train loss:   0.606645\n",
      "train loss:   0.824186\n",
      "train loss:   0.706096\n",
      "train loss:   0.678159\n",
      "train loss:   1.010061\n",
      "train loss:   1.112999\n",
      "train loss:   0.735729\n",
      "train loss:   0.857751\n",
      "train loss:   1.084825\n",
      "train loss:   1.287836\n",
      "train loss:   1.028508\n",
      "train loss:   0.929372\n",
      "train loss:   0.823381\n",
      "train loss:   1.109769\n",
      "train loss:   0.960969\n",
      "train loss:   1.195468\n",
      "train loss:   0.907589\n",
      "train loss:   0.644150\n",
      "train loss:   0.994326\n",
      "train loss:   1.198671\n",
      "train loss:   1.143454\n",
      "train loss:   0.473524\n",
      "train loss:   0.805528\n",
      "train loss:   0.943331\n",
      "train loss:   0.942508\n",
      "########### epoch 108 ###########\n",
      "########### loop 20200 ###########\n",
      "test loss:   0.342643   test accuracy:   0.916667\n",
      "########### loop 20200 ###########\n",
      "train loss:   0.690526\n",
      "train loss:   0.814566\n",
      "train loss:   1.080564\n",
      "train loss:   0.989385\n",
      "train loss:   0.877342\n",
      "train loss:   1.125153\n",
      "train loss:   0.979285\n",
      "train loss:   1.313312\n",
      "train loss:   0.876390\n",
      "train loss:   1.214617\n",
      "train loss:   1.112226\n",
      "train loss:   1.133083\n",
      "train loss:   0.931184\n",
      "train loss:   0.875090\n",
      "train loss:   0.898515\n",
      "train loss:   0.904522\n",
      "train loss:   1.087988\n",
      "train loss:   1.081582\n",
      "train loss:   1.365988\n",
      "train loss:   0.984612\n",
      "train loss:   1.123793\n",
      "train loss:   0.886289\n",
      "train loss:   0.843309\n",
      "train loss:   1.115130\n",
      "train loss:   0.742326\n",
      "train loss:   1.314332\n",
      "train loss:   1.148763\n",
      "train loss:   1.269281\n",
      "train loss:   1.013204\n",
      "train loss:   0.822676\n",
      "train loss:   0.974538\n",
      "train loss:   1.150694\n",
      "train loss:   1.373848\n",
      "train loss:   0.893143\n",
      "train loss:   1.044944\n",
      "train loss:   1.108119\n",
      "train loss:   1.066724\n",
      "train loss:   1.189608\n",
      "train loss:   0.920814\n",
      "train loss:   1.495647\n",
      "train loss:   0.835682\n",
      "train loss:   1.098790\n",
      "train loss:   1.080670\n",
      "train loss:   1.122038\n",
      "train loss:   0.872916\n",
      "train loss:   0.937961\n",
      "train loss:   1.167319\n",
      "train loss:   0.956837\n",
      "train loss:   1.258443\n",
      "train loss:   0.905926\n",
      "########### epoch 108 ###########\n",
      "########### loop 20250 ###########\n",
      "test loss:   0.143482   test accuracy:   0.958333\n",
      "########### loop 20250 ###########\n",
      "train loss:   1.068540\n",
      "train loss:   0.978348\n",
      "train loss:   0.924333\n",
      "train loss:   0.821437\n",
      "train loss:   0.764921\n",
      "train loss:   1.158930\n",
      "train loss:   1.204962\n",
      "train loss:   0.872782\n",
      "train loss:   1.068133\n",
      "train loss:   1.137745\n",
      "train loss:   0.880740\n",
      "train loss:   0.740866\n",
      "train loss:   0.846852\n",
      "train loss:   0.934140\n",
      "train loss:   0.995866\n",
      "train loss:   0.865268\n",
      "train loss:   1.061793\n",
      "train loss:   0.940446\n",
      "train loss:   0.501373\n",
      "train loss:   0.931043\n",
      "train loss:   1.270195\n",
      "train loss:   1.331081\n",
      "train loss:   0.945033\n",
      "train loss:   1.084950\n",
      "train loss:   1.161921\n",
      "train loss:   1.179317\n",
      "train loss:   1.197861\n",
      "train loss:   0.812045\n",
      "train loss:   0.829268\n",
      "train loss:   0.777545\n",
      "train loss:   0.982260\n",
      "train loss:   0.946334\n",
      "train loss:   0.917166\n",
      "train loss:   0.898067\n",
      "train loss:   1.056144\n",
      "train loss:   1.136537\n",
      "train loss:   1.061298\n",
      "train loss:   0.793689\n",
      "train loss:   0.838147\n",
      "train loss:   0.759002\n",
      "train loss:   0.881485\n",
      "train loss:   0.910812\n",
      "train loss:   1.261591\n",
      "train loss:   1.167930\n",
      "train loss:   1.454224\n",
      "train loss:   0.802040\n",
      "train loss:   1.023751\n",
      "train loss:   1.189425\n",
      "train loss:   0.954563\n",
      "train loss:   1.132276\n",
      "########### epoch 108 ###########\n",
      "########### loop 20300 ###########\n",
      "test loss:   0.547864   test accuracy:   0.833333\n",
      "########### loop 20300 ###########\n",
      "train loss:   1.069849\n",
      "train loss:   0.908068\n",
      "train loss:   0.825936\n",
      "train loss:   0.948117\n",
      "train loss:   0.948967\n",
      "train loss:   1.102269\n",
      "train loss:   0.948476\n",
      "train loss:   1.047320\n",
      "train loss:   0.930041\n",
      "train loss:   0.990334\n",
      "train loss:   0.883161\n",
      "train loss:   1.023550\n",
      "train loss:   1.122074\n",
      "train loss:   0.867301\n",
      "train loss:   0.797253\n",
      "train loss:   0.777130\n",
      "train loss:   1.126127\n",
      "train loss:   1.119634\n",
      "train loss:   0.744666\n",
      "train loss:   0.948105\n",
      "train loss:   1.462386\n",
      "train loss:   1.390481\n",
      "train loss:   1.079029\n",
      "train loss:   1.028980\n",
      "train loss:   1.138862\n",
      "train loss:   0.648768\n",
      "train loss:   0.806112\n",
      "train loss:   1.155247\n",
      "train loss:   1.103800\n",
      "train loss:   1.063056\n",
      "train loss:   1.110100\n",
      "train loss:   0.948454\n",
      "train loss:   1.261929\n",
      "train loss:   1.113383\n",
      "train loss:   0.938287\n",
      "train loss:   0.961246\n",
      "train loss:   1.034472\n",
      "train loss:   0.947825\n",
      "train loss:   0.569811\n",
      "train loss:   0.799010\n",
      "train loss:   1.006202\n",
      "train loss:   1.202135\n",
      "train loss:   1.204907\n",
      "train loss:   1.053933\n",
      "train loss:   1.064909\n",
      "train loss:   1.069831\n",
      "train loss:   0.808990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.234717\n",
      "train loss:   0.930285\n",
      "train loss:   0.922018\n",
      "########### epoch 109 ###########\n",
      "########### loop 20350 ###########\n",
      "test loss:   0.163550   test accuracy:   0.958333\n",
      "########### loop 20350 ###########\n",
      "train loss:   0.982653\n",
      "train loss:   1.284563\n",
      "train loss:   0.594255\n",
      "train loss:   0.997763\n",
      "train loss:   1.138701\n",
      "train loss:   1.119597\n",
      "train loss:   1.019265\n",
      "train loss:   0.585523\n",
      "train loss:   0.757617\n",
      "train loss:   0.724971\n",
      "train loss:   1.143286\n",
      "train loss:   0.878636\n",
      "train loss:   1.037897\n",
      "train loss:   0.810602\n",
      "train loss:   0.666050\n",
      "train loss:   0.797880\n",
      "train loss:   1.069989\n",
      "train loss:   0.882018\n",
      "train loss:   1.407955\n",
      "train loss:   1.282976\n",
      "train loss:   0.813264\n",
      "train loss:   0.995683\n",
      "train loss:   0.788480\n",
      "train loss:   0.669817\n",
      "train loss:   0.684097\n",
      "train loss:   0.983630\n",
      "train loss:   1.122542\n",
      "train loss:   0.856248\n",
      "train loss:   0.973724\n",
      "train loss:   0.940869\n",
      "train loss:   0.980384\n",
      "train loss:   1.070552\n",
      "train loss:   0.978058\n",
      "train loss:   0.711598\n",
      "train loss:   0.912570\n",
      "train loss:   0.899576\n",
      "train loss:   0.701826\n",
      "train loss:   1.026505\n",
      "train loss:   0.753264\n",
      "train loss:   1.213925\n",
      "train loss:   0.905061\n",
      "train loss:   0.736922\n",
      "train loss:   1.166508\n",
      "train loss:   0.313394\n",
      "train loss:   1.061589\n",
      "train loss:   0.979822\n",
      "train loss:   0.955500\n",
      "train loss:   1.226225\n",
      "train loss:   0.862569\n",
      "train loss:   0.473778\n",
      "########### epoch 109 ###########\n",
      "########### loop 20400 ###########\n",
      "test loss:   0.165447   test accuracy:   1.000000\n",
      "########### loop 20400 ###########\n",
      "train loss:   1.364364\n",
      "train loss:   1.059709\n",
      "train loss:   0.724243\n",
      "train loss:   1.090088\n",
      "train loss:   1.081782\n",
      "train loss:   0.967149\n",
      "train loss:   0.827140\n",
      "train loss:   0.683415\n",
      "train loss:   0.933255\n",
      "train loss:   1.163209\n",
      "train loss:   0.822709\n",
      "train loss:   1.173673\n",
      "train loss:   1.113117\n",
      "train loss:   0.852681\n",
      "train loss:   0.996254\n",
      "train loss:   0.739649\n",
      "train loss:   1.089987\n",
      "train loss:   0.981327\n",
      "train loss:   1.561771\n",
      "train loss:   1.043941\n",
      "train loss:   1.246030\n",
      "train loss:   1.048474\n",
      "train loss:   0.650207\n",
      "train loss:   0.941047\n",
      "train loss:   0.823632\n",
      "train loss:   0.827540\n",
      "train loss:   0.959795\n",
      "train loss:   0.824340\n",
      "train loss:   1.047030\n",
      "train loss:   1.113457\n",
      "train loss:   1.182118\n",
      "train loss:   0.707008\n",
      "train loss:   0.571717\n",
      "train loss:   1.017586\n",
      "train loss:   0.875341\n",
      "train loss:   0.765206\n",
      "train loss:   1.090697\n",
      "train loss:   1.074071\n",
      "train loss:   0.955160\n",
      "train loss:   0.901690\n",
      "train loss:   0.977929\n",
      "train loss:   1.101382\n",
      "train loss:   0.993660\n",
      "train loss:   1.100489\n",
      "train loss:   0.826823\n",
      "train loss:   0.958367\n",
      "train loss:   1.001354\n",
      "train loss:   1.323941\n",
      "train loss:   0.845587\n",
      "train loss:   1.253589\n",
      "########### epoch 109 ###########\n",
      "########### loop 20450 ###########\n",
      "test loss:   0.368046   test accuracy:   0.833333\n",
      "########### loop 20450 ###########\n",
      "train loss:   0.896416\n",
      "train loss:   1.118495\n",
      "train loss:   0.929796\n",
      "train loss:   1.040079\n",
      "train loss:   0.821581\n",
      "train loss:   0.974339\n",
      "train loss:   1.074034\n",
      "train loss:   0.661111\n",
      "train loss:   1.283749\n",
      "train loss:   1.488182\n",
      "train loss:   0.747765\n",
      "train loss:   1.000123\n",
      "train loss:   1.180259\n",
      "train loss:   0.972112\n",
      "train loss:   1.013770\n",
      "train loss:   1.025540\n",
      "train loss:   0.702683\n",
      "train loss:   0.831468\n",
      "train loss:   1.033308\n",
      "train loss:   0.935780\n",
      "train loss:   0.825187\n",
      "train loss:   0.995119\n",
      "train loss:   0.990131\n",
      "train loss:   0.971880\n",
      "train loss:   0.991225\n",
      "train loss:   0.749647\n",
      "train loss:   0.766191\n",
      "train loss:   0.864255\n",
      "train loss:   1.350551\n",
      "train loss:   1.011662\n",
      "train loss:   1.000944\n",
      "train loss:   1.040958\n",
      "train loss:   1.111921\n",
      "train loss:   1.268607\n",
      "train loss:   0.965357\n",
      "train loss:   0.876222\n",
      "train loss:   1.099595\n",
      "train loss:   1.496430\n",
      "train loss:   0.718415\n",
      "train loss:   0.622722\n",
      "train loss:   1.139917\n",
      "train loss:   1.108399\n",
      "train loss:   1.148920\n",
      "train loss:   1.008497\n",
      "train loss:   0.783100\n",
      "train loss:   1.011094\n",
      "train loss:   1.202409\n",
      "train loss:   1.052379\n",
      "train loss:   0.932891\n",
      "train loss:   0.979582\n",
      "########### epoch 110 ###########\n",
      "########### loop 20500 ###########\n",
      "test loss:   0.353296   test accuracy:   0.875000\n",
      "########### loop 20500 ###########\n",
      "train loss:   0.935812\n",
      "train loss:   1.052677\n",
      "train loss:   0.929044\n",
      "train loss:   0.647803\n",
      "train loss:   1.126718\n",
      "train loss:   1.286182\n",
      "train loss:   1.340162\n",
      "train loss:   1.042895\n",
      "train loss:   1.144890\n",
      "train loss:   0.934264\n",
      "train loss:   0.887400\n",
      "train loss:   1.019307\n",
      "train loss:   1.025404\n",
      "train loss:   0.657189\n",
      "train loss:   1.050456\n",
      "train loss:   1.233567\n",
      "train loss:   0.953006\n",
      "train loss:   0.575675\n",
      "train loss:   1.088999\n",
      "train loss:   0.813480\n",
      "train loss:   0.747966\n",
      "train loss:   0.710442\n",
      "train loss:   0.809529\n",
      "train loss:   1.259112\n",
      "train loss:   1.032200\n",
      "train loss:   0.696700\n",
      "train loss:   0.733378\n",
      "train loss:   0.970268\n",
      "train loss:   0.882242\n",
      "train loss:   1.257660\n",
      "train loss:   0.829311\n",
      "train loss:   0.896785\n",
      "train loss:   0.790139\n",
      "train loss:   0.914011\n",
      "train loss:   1.099355\n",
      "train loss:   0.819425\n",
      "train loss:   0.736411\n",
      "train loss:   1.164444\n",
      "train loss:   0.815925\n",
      "train loss:   1.115669\n",
      "train loss:   0.521601\n",
      "train loss:   0.847669\n",
      "train loss:   1.225411\n",
      "train loss:   1.310427\n",
      "train loss:   0.620194\n",
      "train loss:   1.076911\n",
      "train loss:   0.916181\n",
      "train loss:   1.197661\n",
      "train loss:   0.818816\n",
      "train loss:   0.790173\n",
      "########### epoch 110 ###########\n",
      "########### loop 20550 ###########\n",
      "test loss:   0.287467   test accuracy:   0.875000\n",
      "########### loop 20550 ###########\n",
      "train loss:   1.044226\n",
      "train loss:   1.176529\n",
      "train loss:   0.807602\n",
      "train loss:   0.872278\n",
      "train loss:   1.046182\n",
      "train loss:   0.862553\n",
      "train loss:   0.818189\n",
      "train loss:   0.947614\n",
      "train loss:   1.407472\n",
      "train loss:   1.131993\n",
      "train loss:   0.855576\n",
      "train loss:   0.634113\n",
      "train loss:   1.038250\n",
      "train loss:   1.151206\n",
      "train loss:   1.120950\n",
      "train loss:   1.373522\n",
      "train loss:   0.998948\n",
      "train loss:   1.070705\n",
      "train loss:   0.878706\n",
      "train loss:   0.559831\n",
      "train loss:   0.945390\n",
      "train loss:   1.278009\n",
      "train loss:   0.803162\n",
      "train loss:   1.068915\n",
      "train loss:   0.686297\n",
      "train loss:   0.842037\n",
      "train loss:   1.337268\n",
      "train loss:   0.979941\n",
      "train loss:   0.931411\n",
      "train loss:   0.672103\n",
      "train loss:   0.915087\n",
      "train loss:   0.722738\n",
      "train loss:   1.135496\n",
      "train loss:   1.442625\n",
      "train loss:   1.303508\n",
      "train loss:   0.948197\n",
      "train loss:   0.898667\n",
      "train loss:   0.852245\n",
      "train loss:   0.899388\n",
      "train loss:   1.433613\n",
      "train loss:   0.936519\n",
      "train loss:   0.984511\n",
      "train loss:   0.968003\n",
      "train loss:   0.934280\n",
      "train loss:   0.854221\n",
      "train loss:   0.818596\n",
      "train loss:   1.278940\n",
      "train loss:   1.096409\n",
      "train loss:   1.084038\n",
      "train loss:   1.136352\n",
      "########### epoch 110 ###########\n",
      "########### loop 20600 ###########\n",
      "test loss:   0.224605   test accuracy:   0.916667\n",
      "########### loop 20600 ###########\n",
      "train loss:   1.068095\n",
      "train loss:   0.946583\n",
      "train loss:   1.013669\n",
      "train loss:   1.263459\n",
      "train loss:   1.194714\n",
      "train loss:   1.037077\n",
      "train loss:   0.980142\n",
      "train loss:   1.424553\n",
      "train loss:   1.334996\n",
      "train loss:   1.075162\n",
      "train loss:   0.968344\n",
      "train loss:   1.024456\n",
      "train loss:   0.917606\n",
      "train loss:   1.227365\n",
      "train loss:   0.767671\n",
      "train loss:   0.767659\n",
      "train loss:   0.788590\n",
      "train loss:   1.057379\n",
      "train loss:   0.748301\n",
      "train loss:   0.733942\n",
      "train loss:   0.946211\n",
      "train loss:   1.308527\n",
      "train loss:   0.832725\n",
      "train loss:   0.862405\n",
      "train loss:   0.576834\n",
      "train loss:   0.761494\n",
      "train loss:   0.817300\n",
      "train loss:   1.097150\n",
      "train loss:   0.799638\n",
      "train loss:   0.972484\n",
      "train loss:   0.958168\n",
      "train loss:   0.822863\n",
      "train loss:   0.843474\n",
      "train loss:   0.745340\n",
      "train loss:   0.983407\n",
      "train loss:   1.012776\n",
      "train loss:   1.131798\n",
      "train loss:   1.398143\n",
      "train loss:   0.893124\n",
      "train loss:   1.145093\n",
      "train loss:   0.846257\n",
      "train loss:   0.637100\n",
      "train loss:   0.856538\n",
      "train loss:   1.156497\n",
      "train loss:   0.645549\n",
      "train loss:   0.869107\n",
      "train loss:   1.073149\n",
      "train loss:   1.103338\n",
      "train loss:   1.016189\n",
      "train loss:   1.031535\n",
      "########### epoch 110 ###########\n",
      "########### loop 20650 ###########\n",
      "test loss:   0.243750   test accuracy:   0.916667\n",
      "########### loop 20650 ###########\n",
      "train loss:   0.964744\n",
      "train loss:   0.880822\n",
      "train loss:   1.121515\n",
      "train loss:   0.746303\n",
      "train loss:   0.822130\n",
      "train loss:   0.874931\n",
      "train loss:   1.027813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.038618\n",
      "train loss:   0.933159\n",
      "train loss:   1.383648\n",
      "train loss:   1.050328\n",
      "train loss:   1.029892\n",
      "train loss:   0.978806\n",
      "train loss:   1.133142\n",
      "train loss:   1.432978\n",
      "train loss:   0.838666\n",
      "train loss:   1.211676\n",
      "train loss:   1.003960\n",
      "train loss:   1.052920\n",
      "train loss:   0.852455\n",
      "train loss:   1.060458\n",
      "train loss:   1.341222\n",
      "train loss:   1.020378\n",
      "train loss:   1.069870\n",
      "train loss:   0.922606\n",
      "train loss:   0.663615\n",
      "train loss:   1.221782\n",
      "train loss:   0.979476\n",
      "train loss:   0.715200\n",
      "train loss:   1.083284\n",
      "train loss:   0.832514\n",
      "train loss:   0.853002\n",
      "train loss:   1.027082\n",
      "train loss:   0.661739\n",
      "train loss:   0.634057\n",
      "train loss:   0.837691\n",
      "train loss:   1.086938\n",
      "train loss:   0.872976\n",
      "train loss:   1.182790\n",
      "train loss:   0.952711\n",
      "train loss:   0.752971\n",
      "train loss:   0.720747\n",
      "train loss:   0.834682\n",
      "train loss:   0.807585\n",
      "train loss:   0.972498\n",
      "train loss:   0.586789\n",
      "train loss:   0.953296\n",
      "train loss:   0.835030\n",
      "train loss:   1.002866\n",
      "train loss:   0.957426\n",
      "########### epoch 111 ###########\n",
      "########### loop 20700 ###########\n",
      "test loss:   0.431365   test accuracy:   0.916667\n",
      "########### loop 20700 ###########\n",
      "train loss:   1.102080\n",
      "train loss:   0.881621\n",
      "train loss:   1.219936\n",
      "train loss:   0.903586\n",
      "train loss:   0.999454\n",
      "train loss:   0.906738\n",
      "train loss:   0.913529\n",
      "train loss:   1.131691\n",
      "train loss:   1.185673\n",
      "train loss:   1.026146\n",
      "train loss:   0.699721\n",
      "train loss:   1.086226\n",
      "train loss:   1.369664\n",
      "train loss:   1.070868\n",
      "train loss:   0.916628\n",
      "train loss:   1.045418\n",
      "train loss:   1.016429\n",
      "train loss:   1.018598\n",
      "train loss:   0.925035\n",
      "train loss:   0.807059\n",
      "train loss:   1.081556\n",
      "train loss:   0.845819\n",
      "train loss:   0.660561\n",
      "train loss:   1.005300\n",
      "train loss:   1.156536\n",
      "train loss:   0.862261\n",
      "train loss:   1.089721\n",
      "train loss:   0.766212\n",
      "train loss:   1.027418\n",
      "train loss:   0.882625\n",
      "train loss:   0.818903\n",
      "train loss:   1.231557\n",
      "train loss:   1.018306\n",
      "train loss:   1.101659\n",
      "train loss:   0.533534\n",
      "train loss:   0.906402\n",
      "train loss:   1.287492\n",
      "train loss:   0.986668\n",
      "train loss:   0.825113\n",
      "train loss:   1.282836\n",
      "train loss:   0.881601\n",
      "train loss:   0.914417\n",
      "train loss:   0.669953\n",
      "train loss:   1.136042\n",
      "train loss:   0.637012\n",
      "train loss:   1.179644\n",
      "train loss:   0.549045\n",
      "train loss:   0.920206\n",
      "train loss:   1.027640\n",
      "train loss:   0.993080\n",
      "########### epoch 111 ###########\n",
      "########### loop 20750 ###########\n",
      "test loss:   0.133534   test accuracy:   1.000000\n",
      "########### loop 20750 ###########\n",
      "train loss:   0.899068\n",
      "train loss:   0.713502\n",
      "train loss:   1.121586\n",
      "train loss:   0.958532\n",
      "train loss:   1.085752\n",
      "train loss:   0.846354\n",
      "train loss:   0.535891\n",
      "train loss:   0.911807\n",
      "train loss:   0.970784\n",
      "train loss:   0.872375\n",
      "train loss:   1.027249\n",
      "train loss:   0.859316\n",
      "train loss:   0.937450\n",
      "train loss:   1.154630\n",
      "train loss:   0.762039\n",
      "train loss:   1.002545\n",
      "train loss:   1.060183\n",
      "train loss:   0.871218\n",
      "train loss:   0.640231\n",
      "train loss:   1.049809\n",
      "train loss:   1.239383\n",
      "train loss:   0.727058\n",
      "train loss:   1.140592\n",
      "train loss:   0.879153\n",
      "train loss:   1.195654\n",
      "train loss:   1.069031\n",
      "train loss:   0.773804\n",
      "train loss:   0.894106\n",
      "train loss:   0.688672\n",
      "train loss:   1.018567\n",
      "train loss:   0.866465\n",
      "train loss:   0.866697\n",
      "train loss:   0.853862\n",
      "train loss:   0.782713\n",
      "train loss:   0.691137\n",
      "train loss:   0.562235\n",
      "train loss:   1.212918\n",
      "train loss:   0.718080\n",
      "train loss:   1.012564\n",
      "train loss:   1.034863\n",
      "train loss:   1.129311\n",
      "train loss:   1.222637\n",
      "train loss:   1.144551\n",
      "train loss:   0.683307\n",
      "train loss:   1.101510\n",
      "train loss:   0.948262\n",
      "train loss:   0.863496\n",
      "train loss:   1.064831\n",
      "train loss:   0.732968\n",
      "train loss:   1.024407\n",
      "########### epoch 111 ###########\n",
      "########### loop 20800 ###########\n",
      "test loss:   0.407851   test accuracy:   0.875000\n",
      "########### loop 20800 ###########\n",
      "train loss:   1.141413\n",
      "train loss:   0.760121\n",
      "train loss:   0.927007\n",
      "train loss:   0.929191\n",
      "train loss:   1.013613\n",
      "train loss:   0.797699\n",
      "train loss:   0.845906\n",
      "train loss:   1.171889\n",
      "train loss:   0.960661\n",
      "train loss:   1.033723\n",
      "train loss:   0.806734\n",
      "train loss:   0.914446\n",
      "train loss:   0.784915\n",
      "train loss:   1.043199\n",
      "train loss:   1.173241\n",
      "train loss:   0.681805\n",
      "train loss:   0.916362\n",
      "train loss:   1.637806\n",
      "train loss:   0.836005\n",
      "train loss:   0.893160\n",
      "train loss:   0.913298\n",
      "train loss:   0.680016\n",
      "train loss:   0.873917\n",
      "train loss:   0.675225\n",
      "train loss:   0.815109\n",
      "train loss:   1.012673\n",
      "train loss:   1.058367\n",
      "train loss:   0.921622\n",
      "train loss:   0.926898\n",
      "train loss:   1.164871\n",
      "train loss:   0.661696\n",
      "train loss:   0.892722\n",
      "train loss:   1.113694\n",
      "train loss:   0.656827\n",
      "train loss:   0.888838\n",
      "train loss:   0.776723\n",
      "train loss:   1.163465\n",
      "train loss:   1.316334\n",
      "train loss:   0.940272\n",
      "train loss:   0.561740\n",
      "train loss:   0.817668\n",
      "train loss:   1.047398\n",
      "train loss:   0.880339\n",
      "train loss:   1.076581\n",
      "train loss:   1.219436\n",
      "train loss:   0.930606\n",
      "train loss:   1.288007\n",
      "train loss:   0.707400\n",
      "train loss:   0.960054\n",
      "train loss:   0.878654\n",
      "########### epoch 111 ###########\n",
      "########### loop 20850 ###########\n",
      "test loss:   0.165108   test accuracy:   1.000000\n",
      "########### loop 20850 ###########\n",
      "train loss:   0.953880\n",
      "train loss:   0.665305\n",
      "train loss:   0.947376\n",
      "train loss:   1.147628\n",
      "train loss:   0.818544\n",
      "train loss:   1.007910\n",
      "train loss:   1.124397\n",
      "train loss:   1.184095\n",
      "train loss:   0.807710\n",
      "train loss:   0.857974\n",
      "train loss:   0.877579\n",
      "train loss:   1.171327\n",
      "train loss:   1.111903\n",
      "train loss:   0.797266\n",
      "train loss:   1.077457\n",
      "train loss:   1.052037\n",
      "train loss:   1.084319\n",
      "train loss:   1.310621\n",
      "train loss:   0.894403\n",
      "train loss:   0.791210\n",
      "train loss:   0.575743\n",
      "train loss:   0.845104\n",
      "train loss:   0.677839\n",
      "train loss:   1.132117\n",
      "train loss:   0.897631\n",
      "train loss:   0.999911\n",
      "train loss:   0.751878\n",
      "train loss:   0.948633\n",
      "train loss:   0.807002\n",
      "train loss:   1.105844\n",
      "train loss:   0.772246\n",
      "train loss:   0.810338\n",
      "train loss:   0.923066\n",
      "train loss:   0.932718\n",
      "train loss:   0.741289\n",
      "train loss:   1.143135\n",
      "train loss:   1.223845\n",
      "train loss:   0.941173\n",
      "train loss:   0.697207\n",
      "train loss:   1.031319\n",
      "train loss:   0.957023\n",
      "train loss:   0.961453\n",
      "train loss:   1.007326\n",
      "train loss:   1.341273\n",
      "train loss:   1.222777\n",
      "train loss:   0.816496\n",
      "train loss:   1.230276\n",
      "train loss:   0.637899\n",
      "train loss:   0.827593\n",
      "train loss:   1.110009\n",
      "########### epoch 112 ###########\n",
      "########### loop 20900 ###########\n",
      "test loss:   0.350050   test accuracy:   0.875000\n",
      "########### loop 20900 ###########\n",
      "train loss:   0.867833\n",
      "train loss:   1.138011\n",
      "train loss:   1.071988\n",
      "train loss:   1.074139\n",
      "train loss:   0.830301\n",
      "train loss:   1.030058\n",
      "train loss:   0.759757\n",
      "train loss:   0.753755\n",
      "train loss:   1.568941\n",
      "train loss:   1.079797\n",
      "train loss:   1.176877\n",
      "train loss:   0.628571\n",
      "train loss:   1.006350\n",
      "train loss:   1.173135\n",
      "train loss:   1.339942\n",
      "train loss:   1.187410\n",
      "train loss:   1.263690\n",
      "train loss:   0.721432\n",
      "train loss:   0.950779\n",
      "train loss:   0.892937\n",
      "train loss:   1.328613\n",
      "train loss:   0.668510\n",
      "train loss:   0.873562\n",
      "train loss:   0.754400\n",
      "train loss:   0.803475\n",
      "train loss:   0.900731\n",
      "train loss:   1.304668\n",
      "train loss:   1.009016\n",
      "train loss:   1.256652\n",
      "train loss:   1.126202\n",
      "train loss:   1.029061\n",
      "train loss:   1.255909\n",
      "train loss:   1.008019\n",
      "train loss:   0.991725\n",
      "train loss:   1.045348\n",
      "train loss:   0.864751\n",
      "train loss:   0.860149\n",
      "train loss:   0.881485\n",
      "train loss:   0.964558\n",
      "train loss:   0.858162\n",
      "train loss:   1.004689\n",
      "train loss:   1.160286\n",
      "train loss:   0.958859\n",
      "train loss:   0.944727\n",
      "train loss:   1.048789\n",
      "train loss:   0.913639\n",
      "train loss:   0.854738\n",
      "train loss:   0.871044\n",
      "train loss:   1.197036\n",
      "train loss:   1.182924\n",
      "########### epoch 112 ###########\n",
      "########### loop 20950 ###########\n",
      "test loss:   0.128083   test accuracy:   1.000000\n",
      "########### loop 20950 ###########\n",
      "train loss:   1.077518\n",
      "train loss:   0.989409\n",
      "train loss:   0.528687\n",
      "train loss:   1.186853\n",
      "train loss:   1.001401\n",
      "train loss:   1.249488\n",
      "train loss:   0.643589\n",
      "train loss:   0.672290\n",
      "train loss:   1.226138\n",
      "train loss:   0.781117\n",
      "train loss:   0.774199\n",
      "train loss:   1.111888\n",
      "train loss:   1.080363\n",
      "train loss:   0.860622\n",
      "train loss:   0.736683\n",
      "train loss:   1.230800\n",
      "train loss:   0.743967\n",
      "train loss:   1.285142\n",
      "train loss:   0.677605\n",
      "train loss:   0.804350\n",
      "train loss:   0.481860\n",
      "train loss:   1.016066\n",
      "train loss:   0.599522\n",
      "train loss:   0.804878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.003758\n",
      "train loss:   0.949999\n",
      "train loss:   0.580780\n",
      "train loss:   0.787340\n",
      "train loss:   0.927052\n",
      "train loss:   0.655027\n",
      "train loss:   0.624314\n",
      "train loss:   1.238109\n",
      "train loss:   0.543884\n",
      "train loss:   0.965286\n",
      "train loss:   0.970968\n",
      "train loss:   0.834749\n",
      "train loss:   1.036093\n",
      "train loss:   0.950742\n",
      "train loss:   1.323257\n",
      "train loss:   0.806715\n",
      "train loss:   0.810134\n",
      "train loss:   0.952048\n",
      "train loss:   0.891002\n",
      "train loss:   0.897347\n",
      "train loss:   1.070058\n",
      "train loss:   1.034653\n",
      "train loss:   0.680847\n",
      "train loss:   1.314825\n",
      "train loss:   0.910789\n",
      "train loss:   0.943747\n",
      "########### epoch 112 ###########\n",
      "########### loop 21000 ###########\n",
      "test loss:   0.224188   test accuracy:   0.958333\n",
      "########### loop 21000 ###########\n",
      "train loss:   0.933375\n",
      "train loss:   1.204300\n",
      "train loss:   0.869811\n",
      "train loss:   0.901730\n",
      "train loss:   1.023973\n",
      "train loss:   1.309286\n",
      "train loss:   0.778902\n",
      "train loss:   0.776090\n",
      "train loss:   0.835489\n",
      "train loss:   0.774226\n",
      "train loss:   0.969822\n",
      "train loss:   1.144363\n",
      "train loss:   0.813942\n",
      "train loss:   0.961775\n",
      "train loss:   1.127869\n",
      "train loss:   1.208359\n",
      "train loss:   0.958853\n",
      "train loss:   1.118340\n",
      "train loss:   0.768254\n",
      "train loss:   0.898003\n",
      "train loss:   0.872258\n",
      "train loss:   0.969675\n",
      "train loss:   0.848085\n",
      "train loss:   0.998442\n",
      "train loss:   0.925249\n",
      "train loss:   1.352623\n",
      "train loss:   1.003297\n",
      "train loss:   0.963122\n",
      "train loss:   0.728258\n",
      "train loss:   0.567696\n",
      "train loss:   0.667304\n",
      "train loss:   0.805050\n",
      "train loss:   0.753183\n",
      "train loss:   0.777755\n",
      "train loss:   1.298959\n",
      "train loss:   1.188627\n",
      "train loss:   0.858200\n",
      "train loss:   0.911329\n",
      "train loss:   0.889010\n",
      "train loss:   0.839302\n",
      "train loss:   1.321654\n",
      "train loss:   0.671255\n",
      "train loss:   0.941140\n",
      "train loss:   1.325202\n",
      "train loss:   1.107682\n",
      "train loss:   0.873773\n",
      "train loss:   0.934752\n",
      "train loss:   1.099056\n",
      "train loss:   1.084652\n",
      "train loss:   0.940643\n",
      "########### epoch 112 ###########\n",
      "########### loop 21050 ###########\n",
      "test loss:   0.205699   test accuracy:   0.958333\n",
      "########### loop 21050 ###########\n",
      "train loss:   1.168842\n",
      "train loss:   1.277252\n",
      "train loss:   1.113543\n",
      "train loss:   0.853236\n",
      "train loss:   1.000066\n",
      "train loss:   0.689249\n",
      "train loss:   0.970126\n",
      "train loss:   1.026752\n",
      "train loss:   0.766150\n",
      "train loss:   0.959958\n",
      "train loss:   1.477176\n",
      "train loss:   0.550210\n",
      "train loss:   0.891602\n",
      "train loss:   1.347588\n",
      "train loss:   1.027836\n",
      "train loss:   0.941878\n",
      "train loss:   0.964208\n",
      "train loss:   0.812110\n",
      "train loss:   1.009081\n",
      "train loss:   0.615478\n",
      "train loss:   0.993644\n",
      "train loss:   1.036601\n",
      "train loss:   0.646110\n",
      "train loss:   1.305312\n",
      "train loss:   0.694849\n",
      "train loss:   0.910765\n",
      "train loss:   0.878691\n",
      "train loss:   0.951923\n",
      "train loss:   0.901350\n",
      "train loss:   0.730706\n",
      "train loss:   1.105819\n",
      "train loss:   0.658502\n",
      "train loss:   0.633938\n",
      "train loss:   1.134884\n",
      "train loss:   1.158712\n",
      "train loss:   0.798593\n",
      "train loss:   1.011013\n",
      "train loss:   0.684468\n",
      "train loss:   1.088052\n",
      "train loss:   0.873468\n",
      "train loss:   0.959485\n",
      "train loss:   0.925308\n",
      "train loss:   0.965477\n",
      "train loss:   0.773904\n",
      "train loss:   0.799362\n",
      "train loss:   1.189302\n",
      "train loss:   0.783245\n",
      "train loss:   0.862190\n",
      "train loss:   0.924090\n",
      "train loss:   0.995524\n",
      "########### epoch 113 ###########\n",
      "########### loop 21100 ###########\n",
      "test loss:   0.171393   test accuracy:   0.958333\n",
      "########### loop 21100 ###########\n",
      "train loss:   0.662450\n",
      "train loss:   0.818736\n",
      "train loss:   1.085889\n",
      "train loss:   1.061262\n",
      "train loss:   0.952079\n",
      "train loss:   1.113795\n",
      "train loss:   0.964402\n",
      "train loss:   0.991534\n",
      "train loss:   1.178352\n",
      "train loss:   0.961764\n",
      "train loss:   0.980154\n",
      "train loss:   0.622049\n",
      "train loss:   0.746074\n",
      "train loss:   1.190754\n",
      "train loss:   1.225104\n",
      "train loss:   0.993628\n",
      "train loss:   1.150871\n",
      "train loss:   0.928074\n",
      "train loss:   1.415999\n",
      "train loss:   0.677728\n",
      "train loss:   0.958277\n",
      "train loss:   1.111250\n",
      "train loss:   1.002518\n",
      "train loss:   0.619886\n",
      "train loss:   0.911527\n",
      "train loss:   0.955462\n",
      "train loss:   1.238964\n",
      "train loss:   0.619869\n",
      "train loss:   1.099328\n",
      "train loss:   0.927427\n",
      "train loss:   0.913707\n",
      "train loss:   1.057965\n",
      "train loss:   0.849500\n",
      "train loss:   1.043857\n",
      "train loss:   0.837205\n",
      "train loss:   1.026999\n",
      "train loss:   0.695344\n",
      "train loss:   1.090027\n",
      "train loss:   0.505647\n",
      "train loss:   1.250450\n",
      "train loss:   0.779834\n",
      "train loss:   0.988242\n",
      "train loss:   0.639536\n",
      "train loss:   1.179279\n",
      "train loss:   0.953339\n",
      "train loss:   1.040303\n",
      "train loss:   0.717029\n",
      "train loss:   1.148135\n",
      "train loss:   0.555912\n",
      "train loss:   0.979328\n",
      "########### epoch 113 ###########\n",
      "########### loop 21150 ###########\n",
      "test loss:   0.275072   test accuracy:   0.916667\n",
      "########### loop 21150 ###########\n",
      "train loss:   0.866166\n",
      "train loss:   0.666895\n",
      "train loss:   1.300507\n",
      "train loss:   1.011402\n",
      "train loss:   0.523000\n",
      "train loss:   0.998935\n",
      "train loss:   1.090596\n",
      "train loss:   0.798726\n",
      "train loss:   0.647730\n",
      "train loss:   1.193263\n",
      "train loss:   1.098641\n",
      "train loss:   1.047922\n",
      "train loss:   0.934074\n",
      "train loss:   0.967074\n",
      "train loss:   0.921975\n",
      "train loss:   0.774365\n",
      "train loss:   0.923844\n",
      "train loss:   0.950446\n",
      "train loss:   0.872309\n",
      "train loss:   1.082744\n",
      "train loss:   1.003813\n",
      "train loss:   1.066707\n",
      "train loss:   0.881167\n",
      "train loss:   1.059242\n",
      "train loss:   0.927537\n",
      "train loss:   1.025519\n",
      "train loss:   1.130489\n",
      "train loss:   1.378532\n",
      "train loss:   1.083150\n",
      "train loss:   0.853872\n",
      "train loss:   1.230403\n",
      "train loss:   0.577806\n",
      "train loss:   1.264025\n",
      "train loss:   0.920052\n",
      "train loss:   0.943360\n",
      "train loss:   0.625193\n",
      "train loss:   0.668507\n",
      "train loss:   0.970404\n",
      "train loss:   0.938090\n",
      "train loss:   1.021825\n",
      "train loss:   1.156178\n",
      "train loss:   0.970325\n",
      "train loss:   0.989253\n",
      "train loss:   0.935020\n",
      "train loss:   1.095551\n",
      "train loss:   1.088114\n",
      "train loss:   1.378884\n",
      "train loss:   0.676305\n",
      "train loss:   0.775631\n",
      "train loss:   0.716173\n",
      "########### epoch 113 ###########\n",
      "########### loop 21200 ###########\n",
      "test loss:   0.230608   test accuracy:   0.916667\n",
      "########### loop 21200 ###########\n",
      "train loss:   1.054535\n",
      "train loss:   1.139769\n",
      "train loss:   0.674996\n",
      "train loss:   1.001071\n",
      "train loss:   1.278859\n",
      "train loss:   1.280202\n",
      "train loss:   1.164194\n",
      "train loss:   0.949910\n",
      "train loss:   1.098465\n",
      "train loss:   0.624036\n",
      "train loss:   1.325391\n",
      "train loss:   1.034327\n",
      "train loss:   0.997743\n",
      "train loss:   0.913362\n",
      "train loss:   1.081980\n",
      "train loss:   0.657039\n",
      "train loss:   0.889413\n",
      "train loss:   0.778282\n",
      "train loss:   0.990272\n",
      "train loss:   0.914253\n",
      "train loss:   1.362336\n",
      "train loss:   1.025290\n",
      "train loss:   0.976087\n",
      "train loss:   0.928866\n",
      "train loss:   0.915956\n",
      "train loss:   1.022845\n",
      "train loss:   0.694495\n",
      "train loss:   0.908312\n",
      "train loss:   0.723252\n",
      "train loss:   0.589289\n",
      "train loss:   0.860314\n",
      "train loss:   0.438761\n",
      "train loss:   0.751865\n",
      "train loss:   0.990567\n",
      "train loss:   1.433782\n",
      "train loss:   0.578143\n",
      "train loss:   1.148021\n",
      "train loss:   0.909009\n",
      "train loss:   0.930530\n",
      "train loss:   0.616003\n",
      "train loss:   1.005384\n",
      "train loss:   0.961078\n",
      "train loss:   0.885258\n",
      "train loss:   0.897687\n",
      "train loss:   0.898822\n",
      "train loss:   0.969717\n",
      "train loss:   1.124663\n",
      "train loss:   1.072572\n",
      "train loss:   0.708501\n",
      "train loss:   0.906957\n",
      "########### epoch 114 ###########\n",
      "########### loop 21250 ###########\n",
      "test loss:   0.185751   test accuracy:   1.000000\n",
      "########### loop 21250 ###########\n",
      "train loss:   0.792977\n",
      "train loss:   1.450508\n",
      "train loss:   1.227398\n",
      "train loss:   0.689351\n",
      "train loss:   0.956802\n",
      "train loss:   1.310486\n",
      "train loss:   0.754026\n",
      "train loss:   0.604792\n",
      "train loss:   0.695098\n",
      "train loss:   0.843043\n",
      "train loss:   0.775483\n",
      "train loss:   0.734236\n",
      "train loss:   0.958313\n",
      "train loss:   1.200774\n",
      "train loss:   0.735975\n",
      "train loss:   0.826055\n",
      "train loss:   0.933044\n",
      "train loss:   0.861754\n",
      "train loss:   0.910008\n",
      "train loss:   0.629512\n",
      "train loss:   1.037925\n",
      "train loss:   1.138947\n",
      "train loss:   1.140611\n",
      "train loss:   0.971027\n",
      "train loss:   0.898655\n",
      "train loss:   0.893979\n",
      "train loss:   0.851685\n",
      "train loss:   1.091196\n",
      "train loss:   1.360601\n",
      "train loss:   1.157118\n",
      "train loss:   0.746137\n",
      "train loss:   0.887514\n",
      "train loss:   1.313395\n",
      "train loss:   0.877789\n",
      "train loss:   0.939404\n",
      "train loss:   1.176277\n",
      "train loss:   1.017636\n",
      "train loss:   1.262983\n",
      "train loss:   1.027958\n",
      "train loss:   1.112784\n",
      "train loss:   1.125902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.730928\n",
      "train loss:   1.157337\n",
      "train loss:   0.961307\n",
      "train loss:   1.057022\n",
      "train loss:   1.388868\n",
      "train loss:   0.913922\n",
      "train loss:   1.218647\n",
      "train loss:   0.719638\n",
      "train loss:   0.984105\n",
      "########### epoch 114 ###########\n",
      "########### loop 21300 ###########\n",
      "test loss:   0.227576   test accuracy:   0.916667\n",
      "########### loop 21300 ###########\n",
      "train loss:   1.075619\n",
      "train loss:   1.069689\n",
      "train loss:   1.387586\n",
      "train loss:   1.314412\n",
      "train loss:   0.662984\n",
      "train loss:   0.748343\n",
      "train loss:   0.802800\n",
      "train loss:   0.988858\n",
      "train loss:   1.327677\n",
      "train loss:   1.048774\n",
      "train loss:   1.065915\n",
      "train loss:   1.135051\n",
      "train loss:   1.126259\n",
      "train loss:   0.988825\n",
      "train loss:   1.404508\n",
      "train loss:   0.885059\n",
      "train loss:   0.844463\n",
      "train loss:   1.017189\n",
      "train loss:   0.737956\n",
      "train loss:   0.905155\n",
      "train loss:   1.151491\n",
      "train loss:   1.148836\n",
      "train loss:   1.201807\n",
      "train loss:   0.959643\n",
      "train loss:   1.001671\n",
      "train loss:   0.957717\n",
      "train loss:   0.909303\n",
      "train loss:   0.835749\n",
      "train loss:   1.134383\n",
      "train loss:   0.781500\n",
      "train loss:   0.846839\n",
      "train loss:   0.910005\n",
      "train loss:   1.276373\n",
      "train loss:   1.236507\n",
      "train loss:   0.714461\n",
      "train loss:   0.952120\n",
      "train loss:   0.990678\n",
      "train loss:   1.193173\n",
      "train loss:   1.026012\n",
      "train loss:   1.098421\n",
      "train loss:   1.120615\n",
      "train loss:   0.512508\n",
      "train loss:   0.949235\n",
      "train loss:   1.000269\n",
      "train loss:   0.708441\n",
      "train loss:   0.956684\n",
      "train loss:   1.171522\n",
      "train loss:   1.036304\n",
      "train loss:   0.946068\n",
      "train loss:   0.965322\n",
      "########### epoch 114 ###########\n",
      "########### loop 21350 ###########\n",
      "test loss:   0.206110   test accuracy:   0.958333\n",
      "########### loop 21350 ###########\n",
      "train loss:   1.141896\n",
      "train loss:   0.670896\n",
      "train loss:   0.933127\n",
      "train loss:   0.782692\n",
      "train loss:   1.415733\n",
      "train loss:   0.901345\n",
      "train loss:   0.780644\n",
      "train loss:   0.992184\n",
      "train loss:   1.350830\n",
      "train loss:   0.707888\n",
      "train loss:   0.823284\n",
      "train loss:   0.998132\n",
      "train loss:   1.001417\n",
      "train loss:   1.277187\n",
      "train loss:   1.098152\n",
      "train loss:   0.890362\n",
      "train loss:   0.947243\n",
      "train loss:   1.172433\n",
      "train loss:   0.943930\n",
      "train loss:   0.772345\n",
      "train loss:   0.980578\n",
      "train loss:   0.585470\n",
      "train loss:   0.570445\n",
      "train loss:   1.233354\n",
      "train loss:   1.147635\n",
      "train loss:   1.028880\n",
      "train loss:   0.613791\n",
      "train loss:   1.068585\n",
      "train loss:   1.224117\n",
      "train loss:   0.628854\n",
      "train loss:   1.028375\n",
      "train loss:   1.012474\n",
      "train loss:   0.978653\n",
      "train loss:   0.970251\n",
      "train loss:   1.049216\n",
      "train loss:   1.009939\n",
      "train loss:   0.607463\n",
      "train loss:   0.699002\n",
      "train loss:   1.306558\n",
      "train loss:   0.781919\n",
      "train loss:   1.024534\n",
      "train loss:   1.005303\n",
      "train loss:   0.903740\n",
      "train loss:   0.798821\n",
      "train loss:   0.984336\n",
      "train loss:   0.937232\n",
      "train loss:   1.072520\n",
      "train loss:   0.863773\n",
      "train loss:   1.287218\n",
      "train loss:   0.827873\n",
      "########### epoch 114 ###########\n",
      "########### loop 21400 ###########\n",
      "test loss:   0.254769   test accuracy:   0.958333\n",
      "########### loop 21400 ###########\n",
      "train loss:   1.298835\n",
      "train loss:   1.006323\n",
      "train loss:   0.703121\n",
      "train loss:   0.683208\n",
      "train loss:   1.082796\n",
      "train loss:   1.044987\n",
      "train loss:   0.969626\n",
      "train loss:   1.064948\n",
      "train loss:   0.994411\n",
      "train loss:   1.021950\n",
      "train loss:   0.807435\n",
      "train loss:   0.881043\n",
      "train loss:   0.686601\n",
      "train loss:   1.094087\n",
      "train loss:   1.144207\n",
      "train loss:   0.918548\n",
      "train loss:   0.719151\n",
      "train loss:   0.777936\n",
      "train loss:   1.114533\n",
      "train loss:   0.977150\n",
      "train loss:   0.786288\n",
      "train loss:   1.261201\n",
      "train loss:   0.743351\n",
      "train loss:   0.885466\n",
      "train loss:   0.644739\n",
      "train loss:   1.083148\n",
      "train loss:   1.091809\n",
      "train loss:   0.877566\n",
      "train loss:   0.944812\n",
      "train loss:   1.030698\n",
      "train loss:   0.592966\n",
      "train loss:   1.072744\n",
      "train loss:   0.822893\n",
      "train loss:   0.766358\n",
      "train loss:   1.400151\n",
      "train loss:   1.182816\n",
      "train loss:   0.969365\n",
      "train loss:   0.576099\n",
      "train loss:   1.150846\n",
      "train loss:   1.150128\n",
      "train loss:   1.111946\n",
      "train loss:   0.854609\n",
      "train loss:   0.973333\n",
      "train loss:   0.849752\n",
      "train loss:   1.017809\n",
      "train loss:   0.769059\n",
      "train loss:   0.846230\n",
      "train loss:   0.868773\n",
      "train loss:   1.060323\n",
      "train loss:   0.878548\n",
      "########### epoch 115 ###########\n",
      "########### loop 21450 ###########\n",
      "test loss:   0.236378   test accuracy:   0.916667\n",
      "########### loop 21450 ###########\n",
      "train loss:   1.011132\n",
      "train loss:   0.985655\n",
      "train loss:   0.795712\n",
      "train loss:   1.172709\n",
      "train loss:   1.163310\n",
      "train loss:   0.894066\n",
      "train loss:   0.762845\n",
      "train loss:   0.931281\n",
      "train loss:   0.874029\n",
      "train loss:   1.166883\n",
      "train loss:   0.661011\n",
      "train loss:   1.208253\n",
      "train loss:   1.056566\n",
      "train loss:   0.818374\n",
      "train loss:   0.681070\n",
      "train loss:   1.025229\n",
      "train loss:   1.028931\n",
      "train loss:   0.893374\n",
      "train loss:   1.180290\n",
      "train loss:   1.072035\n",
      "train loss:   1.075216\n",
      "train loss:   1.142386\n",
      "train loss:   0.841025\n",
      "train loss:   1.291669\n",
      "train loss:   0.668104\n",
      "train loss:   0.790444\n",
      "train loss:   0.949112\n",
      "train loss:   1.215411\n",
      "train loss:   1.266892\n",
      "train loss:   1.252024\n",
      "train loss:   1.083530\n",
      "train loss:   0.885551\n",
      "train loss:   0.922227\n",
      "train loss:   0.907199\n",
      "train loss:   1.290515\n",
      "train loss:   0.862889\n",
      "train loss:   0.734066\n",
      "train loss:   0.886097\n",
      "train loss:   0.687705\n",
      "train loss:   1.380829\n",
      "train loss:   1.084447\n",
      "train loss:   0.867912\n",
      "train loss:   0.437675\n",
      "train loss:   0.962215\n",
      "train loss:   0.743451\n",
      "train loss:   0.866390\n",
      "train loss:   1.029329\n",
      "train loss:   0.986450\n",
      "train loss:   1.128437\n",
      "train loss:   0.832725\n",
      "########### epoch 115 ###########\n",
      "########### loop 21500 ###########\n",
      "test loss:   0.214205   test accuracy:   0.958333\n",
      "########### loop 21500 ###########\n",
      "train loss:   1.197663\n",
      "train loss:   0.724715\n",
      "train loss:   1.128412\n",
      "train loss:   1.077127\n",
      "train loss:   0.779173\n",
      "train loss:   0.928263\n",
      "train loss:   0.952473\n",
      "train loss:   0.805158\n",
      "train loss:   1.339667\n",
      "train loss:   0.953295\n",
      "train loss:   0.908807\n",
      "train loss:   0.636813\n",
      "train loss:   1.124487\n",
      "train loss:   0.880337\n",
      "train loss:   0.912184\n",
      "train loss:   0.862693\n",
      "train loss:   1.176437\n",
      "train loss:   0.686523\n",
      "train loss:   0.621498\n",
      "train loss:   1.050535\n",
      "train loss:   0.937007\n",
      "train loss:   1.091487\n",
      "train loss:   1.386194\n",
      "train loss:   0.857498\n",
      "train loss:   0.924497\n",
      "train loss:   0.985820\n",
      "train loss:   0.840816\n",
      "train loss:   0.789702\n",
      "train loss:   1.103631\n",
      "train loss:   1.028279\n",
      "train loss:   0.699650\n",
      "train loss:   0.845578\n",
      "train loss:   0.978983\n",
      "train loss:   0.638221\n",
      "train loss:   1.082370\n",
      "train loss:   0.814652\n",
      "train loss:   1.422846\n",
      "train loss:   0.849799\n",
      "train loss:   0.778054\n",
      "train loss:   0.934472\n",
      "train loss:   0.973584\n",
      "train loss:   1.267357\n",
      "train loss:   0.880809\n",
      "train loss:   1.398124\n",
      "train loss:   0.845351\n",
      "train loss:   0.924722\n",
      "train loss:   0.538619\n",
      "train loss:   1.031160\n",
      "train loss:   1.014032\n",
      "train loss:   1.180223\n",
      "########### epoch 115 ###########\n",
      "########### loop 21550 ###########\n",
      "test loss:   0.178116   test accuracy:   0.958333\n",
      "########### loop 21550 ###########\n",
      "train loss:   0.648520\n",
      "train loss:   0.745739\n",
      "train loss:   1.063328\n",
      "train loss:   0.954735\n",
      "train loss:   1.235833\n",
      "train loss:   1.073474\n",
      "train loss:   1.071310\n",
      "train loss:   1.323270\n",
      "train loss:   0.935782\n",
      "train loss:   0.795316\n",
      "train loss:   1.057315\n",
      "train loss:   0.815293\n",
      "train loss:   1.000353\n",
      "train loss:   0.746667\n",
      "train loss:   1.447573\n",
      "train loss:   0.503480\n",
      "train loss:   1.120125\n",
      "train loss:   0.909114\n",
      "train loss:   1.031595\n",
      "train loss:   0.991678\n",
      "train loss:   0.980494\n",
      "train loss:   0.881206\n",
      "train loss:   0.748674\n",
      "train loss:   1.213303\n",
      "train loss:   1.371240\n",
      "train loss:   0.891959\n",
      "train loss:   0.861423\n",
      "train loss:   0.901913\n",
      "train loss:   0.886944\n",
      "train loss:   0.994559\n",
      "train loss:   0.912938\n",
      "train loss:   0.698109\n",
      "train loss:   1.093087\n",
      "train loss:   1.102378\n",
      "train loss:   1.136898\n",
      "train loss:   0.850778\n",
      "train loss:   0.944276\n",
      "train loss:   1.114442\n",
      "train loss:   0.859493\n",
      "train loss:   1.023289\n",
      "train loss:   1.120258\n",
      "train loss:   0.558830\n",
      "train loss:   0.878069\n",
      "train loss:   0.729951\n",
      "train loss:   0.718902\n",
      "train loss:   0.891891\n",
      "train loss:   0.963361\n",
      "train loss:   0.961082\n",
      "train loss:   0.961872\n",
      "train loss:   0.937828\n",
      "########### epoch 115 ###########\n",
      "########### loop 21600 ###########\n",
      "test loss:   0.324571   test accuracy:   0.875000\n",
      "########### loop 21600 ###########\n",
      "train loss:   0.920395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.859289\n",
      "train loss:   0.916550\n",
      "train loss:   1.276054\n",
      "train loss:   1.092362\n",
      "train loss:   1.052284\n",
      "train loss:   1.061236\n",
      "train loss:   0.837101\n",
      "train loss:   1.218517\n",
      "train loss:   1.048098\n",
      "train loss:   1.362200\n",
      "train loss:   1.286769\n",
      "train loss:   1.379568\n",
      "train loss:   1.194173\n",
      "train loss:   1.137329\n",
      "train loss:   1.186664\n",
      "train loss:   1.117115\n",
      "train loss:   0.994024\n",
      "train loss:   1.034868\n",
      "train loss:   1.217714\n",
      "train loss:   0.693416\n",
      "train loss:   0.890254\n",
      "train loss:   1.162088\n",
      "train loss:   1.319904\n",
      "train loss:   0.953653\n",
      "train loss:   0.817303\n",
      "train loss:   1.048123\n",
      "train loss:   0.935951\n",
      "train loss:   1.114728\n",
      "train loss:   0.974593\n",
      "train loss:   0.875989\n",
      "train loss:   1.072506\n",
      "train loss:   1.229307\n",
      "train loss:   1.256335\n",
      "train loss:   0.784148\n",
      "train loss:   1.425061\n",
      "train loss:   0.838625\n",
      "train loss:   1.093488\n",
      "train loss:   1.055393\n",
      "train loss:   0.806666\n",
      "train loss:   1.240275\n",
      "train loss:   1.149508\n",
      "train loss:   0.989662\n",
      "train loss:   1.111887\n",
      "train loss:   1.181541\n",
      "train loss:   1.054418\n",
      "train loss:   0.897142\n",
      "train loss:   1.174101\n",
      "train loss:   1.156571\n",
      "train loss:   0.801482\n",
      "########### epoch 116 ###########\n",
      "########### loop 21650 ###########\n",
      "test loss:   0.233839   test accuracy:   0.916667\n",
      "########### loop 21650 ###########\n",
      "train loss:   1.077538\n",
      "train loss:   0.874964\n",
      "train loss:   0.467911\n",
      "train loss:   0.841259\n",
      "train loss:   1.191658\n",
      "train loss:   1.306389\n",
      "train loss:   0.774858\n",
      "train loss:   1.202423\n",
      "train loss:   0.741836\n",
      "train loss:   0.998829\n",
      "train loss:   0.891267\n",
      "train loss:   0.682597\n",
      "train loss:   0.966912\n",
      "train loss:   0.932342\n",
      "train loss:   1.115058\n",
      "train loss:   1.060890\n",
      "train loss:   0.712247\n",
      "train loss:   0.714532\n",
      "train loss:   0.676626\n",
      "train loss:   0.888059\n",
      "train loss:   0.934035\n",
      "train loss:   0.727547\n",
      "train loss:   1.034537\n",
      "train loss:   0.983191\n",
      "train loss:   0.764742\n",
      "train loss:   0.969821\n",
      "train loss:   1.149183\n",
      "train loss:   1.080948\n",
      "train loss:   0.594322\n",
      "train loss:   0.562465\n",
      "train loss:   1.365889\n",
      "train loss:   0.809169\n",
      "train loss:   0.734606\n",
      "train loss:   0.973216\n",
      "train loss:   1.075695\n",
      "train loss:   0.545242\n",
      "train loss:   0.578363\n",
      "train loss:   1.115596\n",
      "train loss:   0.541571\n",
      "train loss:   0.839324\n",
      "train loss:   1.016072\n",
      "train loss:   0.932726\n",
      "train loss:   0.312952\n",
      "train loss:   1.264010\n",
      "train loss:   1.221962\n",
      "train loss:   0.963612\n",
      "train loss:   0.715528\n",
      "train loss:   1.275201\n",
      "train loss:   1.000856\n",
      "train loss:   0.961085\n",
      "########### epoch 116 ###########\n",
      "########### loop 21700 ###########\n",
      "test loss:   0.182508   test accuracy:   0.958333\n",
      "########### loop 21700 ###########\n",
      "train loss:   0.987415\n",
      "train loss:   0.957725\n",
      "train loss:   1.201126\n",
      "train loss:   0.784903\n",
      "train loss:   0.428891\n",
      "train loss:   0.934699\n",
      "train loss:   1.194652\n",
      "train loss:   0.817515\n",
      "train loss:   1.038300\n",
      "train loss:   1.045985\n",
      "train loss:   0.947993\n",
      "train loss:   0.962917\n",
      "train loss:   0.779153\n",
      "train loss:   0.925364\n",
      "train loss:   0.895566\n",
      "train loss:   1.180903\n",
      "train loss:   1.292554\n",
      "train loss:   1.000244\n",
      "train loss:   0.577578\n",
      "train loss:   0.867559\n",
      "train loss:   1.216920\n",
      "train loss:   0.999960\n",
      "train loss:   0.824084\n",
      "train loss:   1.286798\n",
      "train loss:   1.128108\n",
      "train loss:   1.048167\n",
      "train loss:   1.104065\n",
      "train loss:   0.870695\n",
      "train loss:   1.275867\n",
      "train loss:   0.983599\n",
      "train loss:   0.720657\n",
      "train loss:   0.747593\n",
      "train loss:   1.332862\n",
      "train loss:   0.625352\n",
      "train loss:   0.554249\n",
      "train loss:   0.972904\n",
      "train loss:   0.956735\n",
      "train loss:   1.063098\n",
      "train loss:   0.751411\n",
      "train loss:   1.181833\n",
      "train loss:   0.694287\n",
      "train loss:   0.999416\n",
      "train loss:   0.830283\n",
      "train loss:   0.663970\n",
      "train loss:   1.364951\n",
      "train loss:   0.899675\n",
      "train loss:   0.766228\n",
      "train loss:   1.317580\n",
      "train loss:   0.528875\n",
      "train loss:   1.027114\n",
      "########### epoch 116 ###########\n",
      "########### loop 21750 ###########\n",
      "test loss:   0.232758   test accuracy:   0.958333\n",
      "########### loop 21750 ###########\n",
      "train loss:   0.989981\n",
      "train loss:   0.929522\n",
      "train loss:   1.202158\n",
      "train loss:   1.049481\n",
      "train loss:   0.863310\n",
      "train loss:   1.011297\n",
      "train loss:   1.120438\n",
      "train loss:   1.027259\n",
      "train loss:   0.994703\n",
      "train loss:   0.753419\n",
      "train loss:   0.782326\n",
      "train loss:   0.702753\n",
      "train loss:   1.158911\n",
      "train loss:   1.065827\n",
      "train loss:   0.859208\n",
      "train loss:   0.917331\n",
      "train loss:   0.720523\n",
      "train loss:   1.121124\n",
      "train loss:   0.759986\n",
      "train loss:   0.945181\n",
      "train loss:   1.129301\n",
      "train loss:   0.668968\n",
      "train loss:   0.583155\n",
      "train loss:   1.125572\n",
      "train loss:   1.152685\n",
      "train loss:   0.894845\n",
      "train loss:   0.631424\n",
      "train loss:   0.912285\n",
      "train loss:   0.843853\n",
      "train loss:   1.294803\n",
      "train loss:   1.161098\n",
      "train loss:   0.855251\n",
      "train loss:   0.919180\n",
      "train loss:   0.840335\n",
      "train loss:   1.090364\n",
      "train loss:   0.939308\n",
      "train loss:   1.378318\n",
      "train loss:   1.180586\n",
      "train loss:   1.240807\n",
      "train loss:   1.187364\n",
      "train loss:   1.060748\n",
      "train loss:   1.057940\n",
      "train loss:   1.020111\n",
      "train loss:   1.025314\n",
      "train loss:   0.804047\n",
      "train loss:   1.004840\n",
      "train loss:   1.226492\n",
      "train loss:   1.139928\n",
      "train loss:   0.861967\n",
      "train loss:   1.029872\n",
      "########### epoch 116 ###########\n",
      "########### loop 21800 ###########\n",
      "test loss:   0.244581   test accuracy:   0.958333\n",
      "########### loop 21800 ###########\n",
      "train loss:   1.135373\n",
      "train loss:   0.684013\n",
      "train loss:   0.821464\n",
      "train loss:   1.134222\n",
      "train loss:   1.074344\n",
      "train loss:   0.904287\n",
      "train loss:   0.773774\n",
      "train loss:   1.103397\n",
      "train loss:   1.204402\n",
      "train loss:   0.847854\n",
      "train loss:   1.159005\n",
      "train loss:   1.189331\n",
      "train loss:   0.825416\n",
      "train loss:   0.795266\n",
      "train loss:   0.781763\n",
      "train loss:   0.745044\n",
      "train loss:   1.051067\n",
      "train loss:   1.133904\n",
      "train loss:   0.899651\n",
      "train loss:   0.798678\n",
      "train loss:   1.309915\n",
      "train loss:   1.136349\n",
      "train loss:   0.948946\n",
      "train loss:   0.781758\n",
      "train loss:   0.939742\n",
      "train loss:   1.072198\n",
      "train loss:   1.024235\n",
      "train loss:   1.144029\n",
      "train loss:   0.917756\n",
      "train loss:   1.187587\n",
      "train loss:   1.208872\n",
      "train loss:   0.992031\n",
      "train loss:   1.303951\n",
      "train loss:   1.158567\n",
      "train loss:   0.749084\n",
      "train loss:   0.947540\n",
      "train loss:   1.043964\n",
      "train loss:   1.039042\n",
      "train loss:   1.031760\n",
      "train loss:   1.244688\n",
      "train loss:   1.087754\n",
      "train loss:   0.959731\n",
      "train loss:   1.096679\n",
      "train loss:   1.059070\n",
      "train loss:   0.851291\n",
      "train loss:   1.384264\n",
      "train loss:   0.831677\n",
      "train loss:   1.301258\n",
      "train loss:   0.715838\n",
      "train loss:   0.791120\n",
      "########### epoch 117 ###########\n",
      "########### loop 21850 ###########\n",
      "test loss:   0.437236   test accuracy:   0.833333\n",
      "########### loop 21850 ###########\n",
      "train loss:   0.689710\n",
      "train loss:   1.002193\n",
      "train loss:   0.808429\n",
      "train loss:   1.098801\n",
      "train loss:   0.905617\n",
      "train loss:   0.905186\n",
      "train loss:   0.755949\n",
      "train loss:   0.834923\n",
      "train loss:   1.067391\n",
      "train loss:   0.835782\n",
      "train loss:   1.041717\n",
      "train loss:   0.820736\n",
      "train loss:   0.928750\n",
      "train loss:   0.831167\n",
      "train loss:   0.812038\n",
      "train loss:   0.863057\n",
      "train loss:   0.776095\n",
      "train loss:   1.027439\n",
      "train loss:   1.166932\n",
      "train loss:   1.136952\n",
      "train loss:   0.827017\n",
      "train loss:   0.821258\n",
      "train loss:   1.117859\n",
      "train loss:   0.888767\n",
      "train loss:   0.993858\n",
      "train loss:   0.793335\n",
      "train loss:   0.816568\n",
      "train loss:   0.943619\n",
      "train loss:   1.103795\n",
      "train loss:   0.694065\n",
      "train loss:   1.120230\n",
      "train loss:   0.925583\n",
      "train loss:   1.318813\n",
      "train loss:   0.978727\n",
      "train loss:   1.069222\n",
      "train loss:   0.713058\n",
      "train loss:   0.721546\n",
      "train loss:   1.051141\n",
      "train loss:   1.096041\n",
      "train loss:   1.058944\n",
      "train loss:   1.073100\n",
      "train loss:   0.702558\n",
      "train loss:   1.114305\n",
      "train loss:   0.881359\n",
      "train loss:   0.844760\n",
      "train loss:   1.196978\n",
      "train loss:   0.851824\n",
      "train loss:   1.009764\n",
      "train loss:   1.211105\n",
      "train loss:   0.744317\n",
      "########### epoch 117 ###########\n",
      "########### loop 21900 ###########\n",
      "test loss:   0.253751   test accuracy:   0.916667\n",
      "########### loop 21900 ###########\n",
      "train loss:   1.224372\n",
      "train loss:   1.042361\n",
      "train loss:   0.978159\n",
      "train loss:   1.110490\n",
      "train loss:   0.865395\n",
      "train loss:   0.835617\n",
      "train loss:   1.099113\n",
      "train loss:   1.064571\n",
      "train loss:   0.924843\n",
      "train loss:   0.993113\n",
      "train loss:   1.165492\n",
      "train loss:   0.676060\n",
      "train loss:   0.989109\n",
      "train loss:   0.709603\n",
      "train loss:   0.925451\n",
      "train loss:   1.138221\n",
      "train loss:   0.841336\n",
      "train loss:   1.246583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.251833\n",
      "train loss:   1.126040\n",
      "train loss:   1.063515\n",
      "train loss:   1.003676\n",
      "train loss:   0.658504\n",
      "train loss:   1.261038\n",
      "train loss:   0.856361\n",
      "train loss:   0.889373\n",
      "train loss:   0.813698\n",
      "train loss:   1.125350\n",
      "train loss:   0.900342\n",
      "train loss:   0.902076\n",
      "train loss:   1.116284\n",
      "train loss:   1.073813\n",
      "train loss:   1.044247\n",
      "train loss:   0.806137\n",
      "train loss:   1.158285\n",
      "train loss:   0.992864\n",
      "train loss:   1.035731\n",
      "train loss:   0.677234\n",
      "train loss:   1.302777\n",
      "train loss:   1.093427\n",
      "train loss:   1.037601\n",
      "train loss:   0.559567\n",
      "train loss:   0.922890\n",
      "train loss:   1.054447\n",
      "train loss:   1.081695\n",
      "train loss:   0.891124\n",
      "train loss:   1.270636\n",
      "train loss:   1.001412\n",
      "train loss:   0.679286\n",
      "train loss:   0.981038\n",
      "########### epoch 117 ###########\n",
      "########### loop 21950 ###########\n",
      "test loss:   0.291890   test accuracy:   0.916667\n",
      "########### loop 21950 ###########\n",
      "train loss:   0.949578\n",
      "train loss:   1.134396\n",
      "train loss:   1.097927\n",
      "train loss:   0.797055\n",
      "train loss:   0.475756\n",
      "train loss:   1.033661\n",
      "train loss:   0.743611\n",
      "train loss:   0.904831\n",
      "train loss:   0.565915\n",
      "train loss:   0.883513\n",
      "train loss:   1.098623\n",
      "train loss:   1.153852\n",
      "train loss:   1.176139\n",
      "train loss:   1.227444\n",
      "train loss:   0.947488\n",
      "train loss:   1.071324\n",
      "train loss:   1.156444\n",
      "train loss:   1.071305\n",
      "train loss:   1.134974\n",
      "train loss:   0.808739\n",
      "train loss:   1.001467\n",
      "train loss:   0.762367\n",
      "train loss:   1.235684\n",
      "train loss:   0.904492\n",
      "train loss:   1.219092\n",
      "train loss:   1.054794\n",
      "train loss:   0.918356\n",
      "train loss:   1.111370\n",
      "train loss:   1.251109\n",
      "train loss:   0.925802\n",
      "train loss:   1.157404\n",
      "train loss:   0.729203\n",
      "train loss:   0.849452\n",
      "train loss:   0.849482\n",
      "train loss:   0.653068\n",
      "train loss:   0.814777\n",
      "train loss:   0.923543\n",
      "train loss:   0.709017\n",
      "train loss:   1.073005\n",
      "train loss:   0.917704\n",
      "train loss:   1.157141\n",
      "train loss:   0.857361\n",
      "train loss:   0.748416\n",
      "train loss:   0.751155\n",
      "train loss:   0.872624\n",
      "train loss:   1.204973\n",
      "train loss:   1.021549\n",
      "train loss:   1.184629\n",
      "train loss:   1.233960\n",
      "train loss:   0.780643\n",
      "########### epoch 118 ###########\n",
      "########### loop 22000 ###########\n",
      "test loss:   0.163867   test accuracy:   0.958333\n",
      "########### loop 22000 ###########\n",
      "train loss:   1.276157\n",
      "train loss:   0.690077\n",
      "train loss:   0.757063\n",
      "train loss:   1.119081\n",
      "train loss:   0.837407\n",
      "train loss:   0.986357\n",
      "train loss:   1.210828\n",
      "train loss:   0.937842\n",
      "train loss:   0.714214\n",
      "train loss:   0.770774\n",
      "train loss:   0.750083\n",
      "train loss:   1.220261\n",
      "train loss:   1.066720\n",
      "train loss:   0.832694\n",
      "train loss:   0.933500\n",
      "train loss:   0.881843\n",
      "train loss:   0.741752\n",
      "train loss:   1.261820\n",
      "train loss:   0.897455\n",
      "train loss:   0.912403\n",
      "train loss:   1.547264\n",
      "train loss:   0.829334\n",
      "train loss:   1.240711\n",
      "train loss:   1.156832\n",
      "train loss:   1.034380\n",
      "train loss:   0.905473\n",
      "train loss:   0.850279\n",
      "train loss:   1.252439\n",
      "train loss:   0.817329\n",
      "train loss:   0.903776\n",
      "train loss:   1.060317\n",
      "train loss:   0.846264\n",
      "train loss:   1.066400\n",
      "train loss:   1.095368\n",
      "train loss:   0.983389\n",
      "train loss:   1.212016\n",
      "train loss:   1.012145\n",
      "train loss:   0.501248\n",
      "train loss:   0.869430\n",
      "train loss:   0.903688\n",
      "train loss:   0.920861\n",
      "train loss:   1.028419\n",
      "train loss:   1.022724\n",
      "train loss:   1.208078\n",
      "train loss:   0.790300\n",
      "train loss:   0.860212\n",
      "train loss:   0.812756\n",
      "train loss:   0.652672\n",
      "train loss:   0.832422\n",
      "train loss:   0.991141\n",
      "########### epoch 118 ###########\n",
      "########### loop 22050 ###########\n",
      "test loss:   0.219353   test accuracy:   0.916667\n",
      "########### loop 22050 ###########\n",
      "train loss:   1.062273\n",
      "train loss:   1.013439\n",
      "train loss:   0.626112\n",
      "train loss:   1.062431\n",
      "train loss:   0.629676\n",
      "train loss:   0.873213\n",
      "train loss:   0.982436\n",
      "train loss:   0.806449\n",
      "train loss:   0.942868\n",
      "train loss:   1.089910\n",
      "train loss:   0.930404\n",
      "train loss:   0.837916\n",
      "train loss:   1.124014\n",
      "train loss:   1.310468\n",
      "train loss:   1.104672\n",
      "train loss:   0.950495\n",
      "train loss:   0.816370\n",
      "train loss:   1.006508\n",
      "train loss:   0.922795\n",
      "train loss:   0.832776\n",
      "train loss:   0.862020\n",
      "train loss:   0.966500\n",
      "train loss:   1.222911\n",
      "train loss:   1.199767\n",
      "train loss:   0.877167\n",
      "train loss:   1.067338\n",
      "train loss:   1.046577\n",
      "train loss:   0.883774\n",
      "train loss:   1.211326\n",
      "train loss:   0.972348\n",
      "train loss:   1.306054\n",
      "train loss:   0.852989\n",
      "train loss:   1.094340\n",
      "train loss:   1.139778\n",
      "train loss:   1.080727\n",
      "train loss:   1.115825\n",
      "train loss:   0.995332\n",
      "train loss:   0.865692\n",
      "train loss:   1.038582\n",
      "train loss:   1.082286\n",
      "train loss:   0.681668\n",
      "train loss:   1.083975\n",
      "train loss:   0.985819\n",
      "train loss:   0.774675\n",
      "train loss:   0.910551\n",
      "train loss:   1.528171\n",
      "train loss:   0.740626\n",
      "train loss:   0.613174\n",
      "train loss:   1.286875\n",
      "train loss:   1.016184\n",
      "########### epoch 118 ###########\n",
      "########### loop 22100 ###########\n",
      "test loss:   0.207053   test accuracy:   0.958333\n",
      "########### loop 22100 ###########\n",
      "train loss:   0.959252\n",
      "train loss:   0.963001\n",
      "train loss:   1.123347\n",
      "train loss:   1.283841\n",
      "train loss:   0.943234\n",
      "train loss:   1.073806\n",
      "train loss:   0.537607\n",
      "train loss:   1.017940\n",
      "train loss:   0.956868\n",
      "train loss:   1.016757\n",
      "train loss:   0.894276\n",
      "train loss:   1.187814\n",
      "train loss:   1.242517\n",
      "train loss:   0.853161\n",
      "train loss:   0.922437\n",
      "train loss:   0.969499\n",
      "train loss:   1.186818\n",
      "train loss:   0.965475\n",
      "train loss:   1.015459\n",
      "train loss:   0.840474\n",
      "train loss:   0.866689\n",
      "train loss:   1.395887\n",
      "train loss:   1.113009\n",
      "train loss:   0.907679\n",
      "train loss:   0.858232\n",
      "train loss:   0.899050\n",
      "train loss:   0.945514\n",
      "train loss:   0.696159\n",
      "train loss:   0.729071\n",
      "train loss:   1.358300\n",
      "train loss:   0.508249\n",
      "train loss:   1.058780\n",
      "train loss:   1.372088\n",
      "train loss:   0.812509\n",
      "train loss:   0.978487\n",
      "train loss:   1.037602\n",
      "train loss:   0.895809\n",
      "train loss:   0.463065\n",
      "train loss:   1.185401\n",
      "train loss:   0.798623\n",
      "train loss:   1.112351\n",
      "train loss:   0.797798\n",
      "train loss:   0.960155\n",
      "train loss:   0.956199\n",
      "train loss:   0.941798\n",
      "train loss:   1.277336\n",
      "train loss:   1.068563\n",
      "train loss:   0.987748\n",
      "train loss:   1.071895\n",
      "train loss:   0.942434\n",
      "########### epoch 118 ###########\n",
      "########### loop 22150 ###########\n",
      "test loss:   0.282873   test accuracy:   0.916667\n",
      "########### loop 22150 ###########\n",
      "train loss:   1.033739\n",
      "train loss:   0.915179\n",
      "train loss:   1.186388\n",
      "train loss:   0.980260\n",
      "train loss:   0.948055\n",
      "train loss:   1.047619\n",
      "train loss:   1.089713\n",
      "train loss:   0.802648\n",
      "train loss:   0.600856\n",
      "train loss:   1.263798\n",
      "train loss:   1.184070\n",
      "train loss:   0.807055\n",
      "train loss:   1.298364\n",
      "train loss:   1.360531\n",
      "train loss:   1.185978\n",
      "train loss:   1.387058\n",
      "train loss:   1.037259\n",
      "train loss:   0.777472\n",
      "train loss:   0.926289\n",
      "train loss:   0.850585\n",
      "train loss:   0.864997\n",
      "train loss:   1.082768\n",
      "train loss:   0.785472\n",
      "train loss:   1.051451\n",
      "train loss:   0.900382\n",
      "train loss:   1.067926\n",
      "train loss:   1.319625\n",
      "train loss:   0.981847\n",
      "train loss:   1.253809\n",
      "train loss:   0.699747\n",
      "train loss:   0.977653\n",
      "train loss:   1.262396\n",
      "train loss:   1.597496\n",
      "train loss:   0.796310\n",
      "train loss:   1.205285\n",
      "train loss:   0.958336\n",
      "train loss:   1.279673\n",
      "train loss:   0.526546\n",
      "train loss:   1.108503\n",
      "train loss:   0.928280\n",
      "train loss:   0.908545\n",
      "train loss:   0.808236\n",
      "train loss:   0.762083\n",
      "train loss:   0.926544\n",
      "train loss:   0.859344\n",
      "train loss:   0.924008\n",
      "train loss:   1.157246\n",
      "train loss:   0.969878\n",
      "train loss:   0.952999\n",
      "train loss:   0.674693\n",
      "########### epoch 119 ###########\n",
      "########### loop 22200 ###########\n",
      "test loss:   0.377608   test accuracy:   0.791667\n",
      "########### loop 22200 ###########\n",
      "train loss:   1.163767\n",
      "train loss:   0.867157\n",
      "train loss:   0.623753\n",
      "train loss:   1.136240\n",
      "train loss:   0.739714\n",
      "train loss:   0.918037\n",
      "train loss:   0.727526\n",
      "train loss:   0.793327\n",
      "train loss:   1.062274\n",
      "train loss:   0.934438\n",
      "train loss:   0.956662\n",
      "train loss:   1.282043\n",
      "train loss:   1.009010\n",
      "train loss:   1.214395\n",
      "train loss:   0.971120\n",
      "train loss:   0.766477\n",
      "train loss:   0.586770\n",
      "train loss:   0.895912\n",
      "train loss:   1.021957\n",
      "train loss:   1.012310\n",
      "train loss:   1.101142\n",
      "train loss:   1.114191\n",
      "train loss:   1.334846\n",
      "train loss:   0.487283\n",
      "train loss:   1.079298\n",
      "train loss:   0.765376\n",
      "train loss:   0.617296\n",
      "train loss:   0.985916\n",
      "train loss:   0.908725\n",
      "train loss:   1.151970\n",
      "train loss:   1.244269\n",
      "train loss:   0.551401\n",
      "train loss:   1.014446\n",
      "train loss:   0.886648\n",
      "train loss:   0.833794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.067591\n",
      "train loss:   1.036944\n",
      "train loss:   1.179532\n",
      "train loss:   0.822168\n",
      "train loss:   1.043094\n",
      "train loss:   0.887812\n",
      "train loss:   0.884931\n",
      "train loss:   0.626262\n",
      "train loss:   1.337382\n",
      "train loss:   0.792917\n",
      "train loss:   1.168810\n",
      "train loss:   1.019685\n",
      "train loss:   0.570369\n",
      "train loss:   1.047376\n",
      "train loss:   0.854402\n",
      "########### epoch 119 ###########\n",
      "########### loop 22250 ###########\n",
      "test loss:   0.433580   test accuracy:   0.833333\n",
      "########### loop 22250 ###########\n",
      "train loss:   0.476670\n",
      "train loss:   1.026626\n",
      "train loss:   1.018965\n",
      "train loss:   1.099826\n",
      "train loss:   0.963644\n",
      "train loss:   0.785888\n",
      "train loss:   0.739734\n",
      "train loss:   1.181430\n",
      "train loss:   1.269634\n",
      "train loss:   1.286877\n",
      "train loss:   0.782190\n",
      "train loss:   1.051952\n",
      "train loss:   1.076355\n",
      "train loss:   1.102522\n",
      "train loss:   0.552758\n",
      "train loss:   1.265786\n",
      "train loss:   0.802812\n",
      "train loss:   0.420395\n",
      "train loss:   1.044189\n",
      "train loss:   0.834318\n",
      "train loss:   0.992495\n",
      "train loss:   0.954434\n",
      "train loss:   1.103483\n",
      "train loss:   1.076551\n",
      "train loss:   0.665488\n",
      "train loss:   1.230949\n",
      "train loss:   0.601197\n",
      "train loss:   0.993908\n",
      "train loss:   1.182504\n",
      "train loss:   0.787817\n",
      "train loss:   0.793154\n",
      "train loss:   0.664208\n",
      "train loss:   1.055090\n",
      "train loss:   1.136901\n",
      "train loss:   0.804855\n",
      "train loss:   0.824963\n",
      "train loss:   1.069276\n",
      "train loss:   0.941128\n",
      "train loss:   0.864735\n",
      "train loss:   0.878138\n",
      "train loss:   1.190612\n",
      "train loss:   1.077926\n",
      "train loss:   0.997441\n",
      "train loss:   0.801192\n",
      "train loss:   0.931094\n",
      "train loss:   0.986015\n",
      "train loss:   1.233139\n",
      "train loss:   1.268210\n",
      "train loss:   1.047377\n",
      "train loss:   0.923631\n",
      "########### epoch 119 ###########\n",
      "########### loop 22300 ###########\n",
      "test loss:   0.176327   test accuracy:   0.958333\n",
      "########### loop 22300 ###########\n",
      "train loss:   0.410162\n",
      "train loss:   0.424132\n",
      "train loss:   0.830315\n",
      "train loss:   1.040935\n",
      "train loss:   0.870197\n",
      "train loss:   0.854558\n",
      "train loss:   0.821892\n",
      "train loss:   1.067306\n",
      "train loss:   0.997236\n",
      "train loss:   1.029513\n",
      "train loss:   1.032565\n",
      "train loss:   0.830138\n",
      "train loss:   1.020590\n",
      "train loss:   0.661888\n",
      "train loss:   1.051001\n",
      "train loss:   0.751505\n",
      "train loss:   0.907835\n",
      "train loss:   1.164546\n",
      "train loss:   1.224904\n",
      "train loss:   1.006590\n",
      "train loss:   1.193393\n",
      "train loss:   0.433731\n",
      "train loss:   0.943624\n",
      "train loss:   0.946702\n",
      "train loss:   1.003144\n",
      "train loss:   0.885607\n",
      "train loss:   0.800547\n",
      "train loss:   1.000527\n",
      "train loss:   1.114541\n",
      "train loss:   1.171939\n",
      "train loss:   0.832941\n",
      "train loss:   1.563104\n",
      "train loss:   1.159927\n",
      "train loss:   1.224121\n",
      "train loss:   0.968245\n",
      "train loss:   1.190065\n",
      "train loss:   1.209959\n",
      "train loss:   0.875664\n",
      "train loss:   0.765666\n",
      "train loss:   1.234878\n",
      "train loss:   1.140913\n",
      "train loss:   1.062825\n",
      "train loss:   0.978676\n",
      "train loss:   0.950393\n",
      "train loss:   1.072703\n",
      "train loss:   1.025036\n",
      "train loss:   1.007117\n",
      "train loss:   0.808628\n",
      "train loss:   0.801848\n",
      "train loss:   0.816247\n",
      "########### epoch 119 ###########\n",
      "########### loop 22350 ###########\n",
      "test loss:   0.185077   test accuracy:   0.958333\n",
      "########### loop 22350 ###########\n",
      "train loss:   0.952813\n",
      "train loss:   0.982405\n",
      "train loss:   0.976529\n",
      "train loss:   0.777162\n",
      "train loss:   0.982065\n",
      "train loss:   1.260240\n",
      "train loss:   0.724381\n",
      "train loss:   0.760770\n",
      "train loss:   0.736711\n",
      "train loss:   1.136236\n",
      "train loss:   0.930360\n",
      "train loss:   1.117566\n",
      "train loss:   1.009817\n",
      "train loss:   0.959366\n",
      "train loss:   0.873690\n",
      "train loss:   1.093138\n",
      "train loss:   0.865029\n",
      "train loss:   0.869342\n",
      "train loss:   0.961437\n",
      "train loss:   0.903027\n",
      "train loss:   0.607862\n",
      "train loss:   0.824883\n",
      "train loss:   1.274043\n",
      "train loss:   0.958358\n",
      "train loss:   0.746135\n",
      "train loss:   0.878331\n",
      "train loss:   1.203825\n",
      "train loss:   1.051348\n",
      "train loss:   1.141422\n",
      "train loss:   0.751976\n",
      "train loss:   0.863581\n",
      "train loss:   0.888032\n",
      "train loss:   1.159500\n",
      "train loss:   0.873356\n",
      "train loss:   0.960057\n",
      "train loss:   0.805940\n",
      "train loss:   0.953026\n",
      "train loss:   0.925306\n",
      "train loss:   1.149749\n",
      "train loss:   1.199835\n",
      "train loss:   1.076804\n",
      "train loss:   0.748277\n",
      "train loss:   0.872720\n",
      "train loss:   0.980609\n",
      "train loss:   1.138095\n",
      "train loss:   0.864879\n",
      "train loss:   1.112407\n",
      "train loss:   1.190696\n",
      "train loss:   0.666325\n",
      "train loss:   1.089379\n",
      "########### epoch 120 ###########\n",
      "########### loop 22400 ###########\n",
      "test loss:   0.410546   test accuracy:   0.875000\n",
      "########### loop 22400 ###########\n",
      "train loss:   0.852478\n",
      "train loss:   0.957641\n",
      "train loss:   1.075567\n",
      "train loss:   1.140869\n",
      "train loss:   0.910907\n",
      "train loss:   0.806025\n",
      "train loss:   0.786150\n",
      "train loss:   0.758596\n",
      "train loss:   1.232865\n",
      "train loss:   0.942092\n",
      "train loss:   0.855218\n",
      "train loss:   0.789796\n",
      "train loss:   1.200969\n",
      "train loss:   0.974708\n",
      "train loss:   0.767791\n",
      "train loss:   0.978819\n",
      "train loss:   1.086635\n",
      "train loss:   1.172219\n",
      "train loss:   0.921388\n",
      "train loss:   1.218520\n",
      "train loss:   0.717512\n",
      "train loss:   1.161060\n",
      "train loss:   1.204170\n",
      "train loss:   0.960776\n",
      "train loss:   1.141243\n",
      "train loss:   0.898758\n",
      "train loss:   1.261216\n",
      "train loss:   1.083609\n",
      "train loss:   0.784874\n",
      "train loss:   0.959676\n",
      "train loss:   1.014360\n",
      "train loss:   0.834249\n",
      "train loss:   1.045052\n",
      "train loss:   1.227029\n",
      "train loss:   1.088237\n",
      "train loss:   1.048994\n",
      "train loss:   1.478669\n",
      "train loss:   0.635014\n",
      "train loss:   1.033222\n",
      "train loss:   1.032493\n",
      "train loss:   1.039648\n",
      "train loss:   0.874658\n",
      "train loss:   1.099707\n",
      "train loss:   0.689597\n",
      "train loss:   1.051443\n",
      "train loss:   0.953575\n",
      "train loss:   0.912375\n",
      "train loss:   1.014819\n",
      "train loss:   1.182555\n",
      "train loss:   1.120863\n",
      "########### epoch 120 ###########\n",
      "########### loop 22450 ###########\n",
      "test loss:   0.242616   test accuracy:   0.958333\n",
      "########### loop 22450 ###########\n",
      "train loss:   1.052150\n",
      "train loss:   1.190132\n",
      "train loss:   1.000548\n",
      "train loss:   0.843426\n",
      "train loss:   0.621661\n",
      "train loss:   1.197581\n",
      "train loss:   0.765637\n",
      "train loss:   0.973810\n",
      "train loss:   0.865509\n",
      "train loss:   0.676640\n",
      "train loss:   0.766629\n",
      "train loss:   1.067172\n",
      "train loss:   0.805332\n",
      "train loss:   0.971131\n",
      "train loss:   1.132758\n",
      "train loss:   1.142223\n",
      "train loss:   0.855338\n",
      "train loss:   1.049533\n",
      "train loss:   0.701079\n",
      "train loss:   1.053708\n",
      "train loss:   0.820439\n",
      "train loss:   1.086421\n",
      "train loss:   0.658645\n",
      "train loss:   1.187696\n",
      "train loss:   0.912474\n",
      "train loss:   0.865433\n",
      "train loss:   1.449377\n",
      "train loss:   0.678724\n",
      "train loss:   0.653765\n",
      "train loss:   0.973825\n",
      "train loss:   0.963780\n",
      "train loss:   0.991976\n",
      "train loss:   1.152584\n",
      "train loss:   0.962725\n",
      "train loss:   0.909696\n",
      "train loss:   0.392852\n",
      "train loss:   0.675009\n",
      "train loss:   1.320117\n",
      "train loss:   1.293300\n",
      "train loss:   0.998011\n",
      "train loss:   0.864039\n",
      "train loss:   0.836511\n",
      "train loss:   0.979078\n",
      "train loss:   0.714741\n",
      "train loss:   1.162772\n",
      "train loss:   1.286999\n",
      "train loss:   1.069457\n",
      "train loss:   0.685630\n",
      "train loss:   1.262678\n",
      "train loss:   1.287989\n",
      "########### epoch 120 ###########\n",
      "########### loop 22500 ###########\n",
      "test loss:   0.284099   test accuracy:   0.916667\n",
      "########### loop 22500 ###########\n",
      "train loss:   1.217445\n",
      "train loss:   0.928348\n",
      "train loss:   0.778706\n",
      "train loss:   1.001842\n",
      "train loss:   1.062982\n",
      "train loss:   1.031609\n",
      "train loss:   1.006209\n",
      "train loss:   1.176795\n",
      "train loss:   1.463539\n",
      "train loss:   0.969452\n",
      "train loss:   1.170588\n",
      "train loss:   0.961556\n",
      "train loss:   0.919554\n",
      "train loss:   1.355913\n",
      "train loss:   0.871264\n",
      "train loss:   0.866114\n",
      "train loss:   0.846935\n",
      "train loss:   0.876096\n",
      "train loss:   0.992493\n",
      "train loss:   0.889280\n",
      "train loss:   0.983083\n",
      "train loss:   1.109666\n",
      "train loss:   0.807520\n",
      "train loss:   0.968982\n",
      "train loss:   1.309918\n",
      "train loss:   0.647197\n",
      "train loss:   0.820709\n",
      "train loss:   1.170023\n",
      "train loss:   0.900986\n",
      "train loss:   0.960046\n",
      "train loss:   0.754559\n",
      "train loss:   0.960829\n",
      "train loss:   1.062088\n",
      "train loss:   1.027412\n",
      "train loss:   1.143096\n",
      "train loss:   0.823557\n",
      "train loss:   0.542541\n",
      "train loss:   0.887221\n",
      "train loss:   0.829672\n",
      "train loss:   0.687539\n",
      "train loss:   0.975088\n",
      "train loss:   0.679261\n",
      "train loss:   0.899122\n",
      "train loss:   1.334713\n",
      "train loss:   1.105755\n",
      "train loss:   0.924482\n",
      "train loss:   0.834249\n",
      "train loss:   0.685623\n",
      "train loss:   1.136617\n",
      "train loss:   0.653597\n",
      "########### epoch 120 ###########\n",
      "########### loop 22550 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.078200   test accuracy:   1.000000\n",
      "########### loop 22550 ###########\n",
      "train loss:   0.875426\n",
      "train loss:   0.868068\n",
      "train loss:   0.933785\n",
      "train loss:   0.929302\n",
      "train loss:   1.284247\n",
      "train loss:   0.777685\n",
      "train loss:   0.626766\n",
      "train loss:   1.331469\n",
      "train loss:   1.042879\n",
      "train loss:   0.888185\n",
      "train loss:   0.979124\n",
      "train loss:   0.794303\n",
      "train loss:   1.139050\n",
      "train loss:   0.473455\n",
      "train loss:   0.790418\n",
      "train loss:   1.004356\n",
      "train loss:   0.960877\n",
      "train loss:   1.384597\n",
      "train loss:   0.649278\n",
      "train loss:   0.842164\n",
      "train loss:   0.717068\n",
      "train loss:   0.992539\n",
      "train loss:   0.977050\n",
      "train loss:   0.890537\n",
      "train loss:   0.985746\n",
      "train loss:   0.725749\n",
      "train loss:   1.246130\n",
      "train loss:   1.181041\n",
      "train loss:   1.017733\n",
      "train loss:   1.177970\n",
      "train loss:   0.982231\n",
      "train loss:   1.240026\n",
      "train loss:   0.785893\n",
      "train loss:   0.800059\n",
      "train loss:   0.976150\n",
      "train loss:   1.171874\n",
      "train loss:   1.039926\n",
      "train loss:   0.694576\n",
      "train loss:   1.272717\n",
      "train loss:   0.830605\n",
      "train loss:   1.311915\n",
      "train loss:   0.757967\n",
      "train loss:   0.815994\n",
      "train loss:   1.245262\n",
      "train loss:   0.748222\n",
      "train loss:   1.502175\n",
      "train loss:   1.309854\n",
      "train loss:   0.940984\n",
      "train loss:   0.792549\n",
      "train loss:   0.906948\n",
      "########### epoch 121 ###########\n",
      "########### loop 22600 ###########\n",
      "test loss:   0.190837   test accuracy:   0.958333\n",
      "########### loop 22600 ###########\n",
      "train loss:   0.791616\n",
      "train loss:   1.045206\n",
      "train loss:   1.234026\n",
      "train loss:   0.804354\n",
      "train loss:   0.739058\n",
      "train loss:   0.950127\n",
      "train loss:   0.649379\n",
      "train loss:   0.979349\n",
      "train loss:   1.032930\n",
      "train loss:   1.040949\n",
      "train loss:   0.969693\n",
      "train loss:   1.393630\n",
      "train loss:   0.500645\n",
      "train loss:   0.914879\n",
      "train loss:   0.919548\n",
      "train loss:   0.863226\n",
      "train loss:   1.040667\n",
      "train loss:   1.188170\n",
      "train loss:   1.032379\n",
      "train loss:   0.883083\n",
      "train loss:   1.176058\n",
      "train loss:   0.811764\n",
      "train loss:   1.016140\n",
      "train loss:   0.757088\n",
      "train loss:   0.761279\n",
      "train loss:   1.001828\n",
      "train loss:   0.782370\n",
      "train loss:   0.877117\n",
      "train loss:   0.625769\n",
      "train loss:   0.718731\n",
      "train loss:   1.025841\n",
      "train loss:   0.936831\n",
      "train loss:   0.893412\n",
      "train loss:   1.061480\n",
      "train loss:   0.808856\n",
      "train loss:   1.012881\n",
      "train loss:   0.957562\n",
      "train loss:   0.982367\n",
      "train loss:   0.663489\n",
      "train loss:   1.037571\n",
      "train loss:   0.831511\n",
      "train loss:   0.997090\n",
      "train loss:   1.003441\n",
      "train loss:   0.811950\n",
      "train loss:   0.752389\n",
      "train loss:   0.732711\n",
      "train loss:   0.897516\n",
      "train loss:   1.041193\n",
      "train loss:   0.684759\n",
      "train loss:   1.040772\n",
      "########### epoch 121 ###########\n",
      "########### loop 22650 ###########\n",
      "test loss:   0.140390   test accuracy:   1.000000\n",
      "########### loop 22650 ###########\n",
      "train loss:   1.263762\n",
      "train loss:   0.884750\n",
      "train loss:   0.979993\n",
      "train loss:   1.212067\n",
      "train loss:   1.571346\n",
      "train loss:   0.999516\n",
      "train loss:   1.180732\n",
      "train loss:   0.921409\n",
      "train loss:   1.038040\n",
      "train loss:   0.955588\n",
      "train loss:   1.149565\n",
      "train loss:   0.915749\n",
      "train loss:   0.910215\n",
      "train loss:   1.069221\n",
      "train loss:   1.041390\n",
      "train loss:   0.968347\n",
      "train loss:   1.147605\n",
      "train loss:   1.082280\n",
      "train loss:   1.195270\n",
      "train loss:   0.851626\n",
      "train loss:   0.884219\n",
      "train loss:   1.008868\n",
      "train loss:   0.964411\n",
      "train loss:   0.739295\n",
      "train loss:   0.952052\n",
      "train loss:   0.850187\n",
      "train loss:   0.941196\n",
      "train loss:   1.068269\n",
      "train loss:   0.941254\n",
      "train loss:   0.921721\n",
      "train loss:   0.767656\n",
      "train loss:   1.108809\n",
      "train loss:   0.949313\n",
      "train loss:   0.910950\n",
      "train loss:   0.629273\n",
      "train loss:   0.876826\n",
      "train loss:   0.908733\n",
      "train loss:   1.187822\n",
      "train loss:   1.170349\n",
      "train loss:   0.641888\n",
      "train loss:   1.022230\n",
      "train loss:   0.993856\n",
      "train loss:   0.779040\n",
      "train loss:   0.787674\n",
      "train loss:   1.334348\n",
      "train loss:   0.570872\n",
      "train loss:   1.070069\n",
      "train loss:   0.611922\n",
      "train loss:   0.960284\n",
      "train loss:   1.015656\n",
      "########### epoch 121 ###########\n",
      "########### loop 22700 ###########\n",
      "test loss:   0.612985   test accuracy:   0.833333\n",
      "########### loop 22700 ###########\n",
      "train loss:   0.939426\n",
      "train loss:   0.852202\n",
      "train loss:   1.257543\n",
      "train loss:   1.193914\n",
      "train loss:   1.102555\n",
      "train loss:   1.156173\n",
      "train loss:   0.715492\n",
      "train loss:   1.163608\n",
      "train loss:   0.776250\n",
      "train loss:   0.832576\n",
      "train loss:   0.840698\n",
      "train loss:   1.391557\n",
      "train loss:   1.081073\n",
      "train loss:   0.915061\n",
      "train loss:   1.175154\n",
      "train loss:   0.998517\n",
      "train loss:   1.138131\n",
      "train loss:   0.955270\n",
      "train loss:   1.001961\n",
      "train loss:   0.820060\n",
      "train loss:   1.058612\n",
      "train loss:   1.046527\n",
      "train loss:   1.289060\n",
      "train loss:   0.973049\n",
      "train loss:   1.063795\n",
      "train loss:   1.448686\n",
      "train loss:   0.762456\n",
      "train loss:   0.948371\n",
      "train loss:   1.197253\n",
      "train loss:   1.072448\n",
      "train loss:   0.950913\n",
      "train loss:   0.927390\n",
      "train loss:   0.776986\n",
      "train loss:   0.911277\n",
      "train loss:   1.106498\n",
      "train loss:   0.980461\n",
      "train loss:   0.522811\n",
      "train loss:   0.659278\n",
      "train loss:   0.741160\n",
      "train loss:   1.281851\n",
      "train loss:   0.909924\n",
      "train loss:   1.055941\n",
      "train loss:   1.622547\n",
      "train loss:   1.226086\n",
      "train loss:   0.951259\n",
      "train loss:   1.097881\n",
      "train loss:   0.949781\n",
      "train loss:   0.624255\n",
      "train loss:   0.668472\n",
      "train loss:   0.923865\n",
      "########### epoch 122 ###########\n",
      "########### loop 22750 ###########\n",
      "test loss:   0.301828   test accuracy:   0.875000\n",
      "########### loop 22750 ###########\n",
      "train loss:   0.669813\n",
      "train loss:   0.990536\n",
      "train loss:   1.016935\n",
      "train loss:   1.258185\n",
      "train loss:   1.022352\n",
      "train loss:   1.088005\n",
      "train loss:   0.652053\n",
      "train loss:   0.918514\n",
      "train loss:   0.726443\n",
      "train loss:   1.106707\n",
      "train loss:   1.049950\n",
      "train loss:   0.536805\n",
      "train loss:   1.158567\n",
      "train loss:   0.746159\n",
      "train loss:   0.716694\n",
      "train loss:   0.994681\n",
      "train loss:   1.058367\n",
      "train loss:   1.072382\n",
      "train loss:   0.777800\n",
      "train loss:   0.869708\n",
      "train loss:   0.733578\n",
      "train loss:   0.891699\n",
      "train loss:   1.012000\n",
      "train loss:   1.133812\n",
      "train loss:   0.896325\n",
      "train loss:   1.195386\n",
      "train loss:   0.904183\n",
      "train loss:   1.097593\n",
      "train loss:   1.135267\n",
      "train loss:   0.878702\n",
      "train loss:   0.856984\n",
      "train loss:   0.912023\n",
      "train loss:   0.789869\n",
      "train loss:   1.101060\n",
      "train loss:   0.740931\n",
      "train loss:   1.046218\n",
      "train loss:   0.853366\n",
      "train loss:   0.674453\n",
      "train loss:   0.920769\n",
      "train loss:   1.070545\n",
      "train loss:   0.747602\n",
      "train loss:   0.814180\n",
      "train loss:   1.005849\n",
      "train loss:   0.863950\n",
      "train loss:   1.031857\n",
      "train loss:   1.069791\n",
      "train loss:   0.915240\n",
      "train loss:   0.844183\n",
      "train loss:   0.734672\n",
      "train loss:   0.663287\n",
      "########### epoch 122 ###########\n",
      "########### loop 22800 ###########\n",
      "test loss:   0.443425   test accuracy:   0.875000\n",
      "########### loop 22800 ###########\n",
      "train loss:   1.122466\n",
      "train loss:   1.213549\n",
      "train loss:   0.890317\n",
      "train loss:   0.746524\n",
      "train loss:   0.622588\n",
      "train loss:   1.387770\n",
      "train loss:   1.066031\n",
      "train loss:   1.214061\n",
      "train loss:   1.055818\n",
      "train loss:   0.674440\n",
      "train loss:   0.698218\n",
      "train loss:   0.830883\n",
      "train loss:   0.854400\n",
      "train loss:   1.131023\n",
      "train loss:   1.132879\n",
      "train loss:   1.339585\n",
      "train loss:   0.841385\n",
      "train loss:   1.121566\n",
      "train loss:   0.982214\n",
      "train loss:   1.038124\n",
      "train loss:   0.448733\n",
      "train loss:   1.121919\n",
      "train loss:   1.151807\n",
      "train loss:   0.767239\n",
      "train loss:   0.929003\n",
      "train loss:   0.994048\n",
      "train loss:   1.090381\n",
      "train loss:   0.840976\n",
      "train loss:   0.728191\n",
      "train loss:   0.904317\n",
      "train loss:   0.917999\n",
      "train loss:   1.046990\n",
      "train loss:   0.787015\n",
      "train loss:   1.039690\n",
      "train loss:   0.794315\n",
      "train loss:   1.108385\n",
      "train loss:   0.722510\n",
      "train loss:   1.210233\n",
      "train loss:   0.736283\n",
      "train loss:   0.835099\n",
      "train loss:   1.118766\n",
      "train loss:   0.998184\n",
      "train loss:   1.058236\n",
      "train loss:   1.728219\n",
      "train loss:   1.005737\n",
      "train loss:   0.876930\n",
      "train loss:   0.885130\n",
      "train loss:   1.017246\n",
      "train loss:   0.813784\n",
      "train loss:   0.885052\n",
      "########### epoch 122 ###########\n",
      "########### loop 22850 ###########\n",
      "test loss:   0.419745   test accuracy:   0.875000\n",
      "########### loop 22850 ###########\n",
      "train loss:   0.918894\n",
      "train loss:   1.028327\n",
      "train loss:   1.055504\n",
      "train loss:   1.005237\n",
      "train loss:   1.166459\n",
      "train loss:   1.255241\n",
      "train loss:   1.042742\n",
      "train loss:   1.130382\n",
      "train loss:   0.742101\n",
      "train loss:   1.044530\n",
      "train loss:   0.674634\n",
      "train loss:   0.907583\n",
      "train loss:   0.989469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.601410\n",
      "train loss:   0.863295\n",
      "train loss:   1.418872\n",
      "train loss:   0.746122\n",
      "train loss:   0.762332\n",
      "train loss:   0.997382\n",
      "train loss:   1.156810\n",
      "train loss:   0.702930\n",
      "train loss:   0.876413\n",
      "train loss:   0.729765\n",
      "train loss:   0.928425\n",
      "train loss:   0.801117\n",
      "train loss:   0.835533\n",
      "train loss:   1.176333\n",
      "train loss:   0.927218\n",
      "train loss:   1.310985\n",
      "train loss:   1.038536\n",
      "train loss:   0.830687\n",
      "train loss:   1.134063\n",
      "train loss:   0.828370\n",
      "train loss:   1.098345\n",
      "train loss:   0.999917\n",
      "train loss:   1.058734\n",
      "train loss:   0.977537\n",
      "train loss:   1.152453\n",
      "train loss:   0.816282\n",
      "train loss:   0.943674\n",
      "train loss:   0.874771\n",
      "train loss:   0.813823\n",
      "train loss:   1.022946\n",
      "train loss:   1.072795\n",
      "train loss:   0.895569\n",
      "train loss:   1.153103\n",
      "train loss:   1.021824\n",
      "train loss:   1.156432\n",
      "train loss:   0.892655\n",
      "train loss:   1.177774\n",
      "########### epoch 122 ###########\n",
      "########### loop 22900 ###########\n",
      "test loss:   0.382453   test accuracy:   0.833333\n",
      "########### loop 22900 ###########\n",
      "train loss:   1.021407\n",
      "train loss:   1.089420\n",
      "train loss:   0.995838\n",
      "train loss:   0.852306\n",
      "train loss:   0.843093\n",
      "train loss:   0.784271\n",
      "train loss:   1.340079\n",
      "train loss:   0.753628\n",
      "train loss:   0.558027\n",
      "train loss:   1.340950\n",
      "train loss:   0.875309\n",
      "train loss:   1.073874\n",
      "train loss:   1.072083\n",
      "train loss:   1.116401\n",
      "train loss:   1.258204\n",
      "train loss:   1.011419\n",
      "train loss:   0.885394\n",
      "train loss:   1.146272\n",
      "train loss:   1.098445\n",
      "train loss:   0.959800\n",
      "train loss:   1.214811\n",
      "train loss:   0.887158\n",
      "train loss:   0.494770\n",
      "train loss:   0.995750\n",
      "train loss:   1.306803\n",
      "train loss:   0.932497\n",
      "train loss:   1.220046\n",
      "train loss:   0.737889\n",
      "train loss:   0.965035\n",
      "train loss:   1.252311\n",
      "train loss:   0.775795\n",
      "train loss:   1.262424\n",
      "train loss:   1.072800\n",
      "train loss:   0.753691\n",
      "train loss:   0.804504\n",
      "train loss:   0.670089\n",
      "train loss:   1.227515\n",
      "train loss:   0.681282\n",
      "train loss:   1.117987\n",
      "train loss:   0.904548\n",
      "train loss:   1.132638\n",
      "train loss:   1.470475\n",
      "train loss:   0.992439\n",
      "train loss:   1.171346\n",
      "train loss:   0.770474\n",
      "train loss:   0.725111\n",
      "train loss:   0.937804\n",
      "train loss:   1.193146\n",
      "train loss:   1.110639\n",
      "train loss:   0.698985\n",
      "########### epoch 123 ###########\n",
      "########### loop 22950 ###########\n",
      "test loss:   0.282857   test accuracy:   0.916667\n",
      "########### loop 22950 ###########\n",
      "train loss:   0.794772\n",
      "train loss:   1.361296\n",
      "train loss:   0.852184\n",
      "train loss:   1.461313\n",
      "train loss:   0.609348\n",
      "train loss:   1.039695\n",
      "train loss:   0.872352\n",
      "train loss:   1.036946\n",
      "train loss:   1.118027\n",
      "train loss:   1.327263\n",
      "train loss:   1.168128\n",
      "train loss:   1.064591\n",
      "train loss:   1.064554\n",
      "train loss:   0.528550\n",
      "train loss:   0.735952\n",
      "train loss:   0.830280\n",
      "train loss:   0.580605\n",
      "train loss:   0.868640\n",
      "train loss:   0.775247\n",
      "train loss:   0.840918\n",
      "train loss:   1.158258\n",
      "train loss:   0.822925\n",
      "train loss:   1.032328\n",
      "train loss:   1.018386\n",
      "train loss:   0.982140\n",
      "train loss:   0.797721\n",
      "train loss:   0.925538\n",
      "train loss:   0.978579\n",
      "train loss:   1.037031\n",
      "train loss:   0.794212\n",
      "train loss:   0.712445\n",
      "train loss:   1.342596\n",
      "train loss:   1.098001\n",
      "train loss:   0.669838\n",
      "train loss:   0.901386\n",
      "train loss:   0.728007\n",
      "train loss:   1.142667\n",
      "train loss:   1.217604\n",
      "train loss:   0.773523\n",
      "train loss:   0.779380\n",
      "train loss:   1.204872\n",
      "train loss:   0.982132\n",
      "train loss:   0.912506\n",
      "train loss:   0.996543\n",
      "train loss:   0.969457\n",
      "train loss:   0.905401\n",
      "train loss:   0.713992\n",
      "train loss:   0.944633\n",
      "train loss:   1.038367\n",
      "train loss:   0.879187\n",
      "########### epoch 123 ###########\n",
      "########### loop 23000 ###########\n",
      "test loss:   0.333705   test accuracy:   0.916667\n",
      "########### loop 23000 ###########\n",
      "train loss:   1.195782\n",
      "train loss:   0.800106\n",
      "train loss:   0.638511\n",
      "train loss:   0.913752\n",
      "train loss:   0.911005\n",
      "train loss:   0.998799\n",
      "train loss:   1.106573\n",
      "train loss:   1.076838\n",
      "train loss:   0.913811\n",
      "train loss:   1.042982\n",
      "train loss:   1.227567\n",
      "train loss:   0.789788\n",
      "train loss:   1.340985\n",
      "train loss:   0.788487\n",
      "train loss:   1.024865\n",
      "train loss:   1.127841\n",
      "train loss:   1.238711\n",
      "train loss:   1.354265\n",
      "train loss:   0.798602\n",
      "train loss:   1.151826\n",
      "train loss:   0.857535\n",
      "train loss:   0.704222\n",
      "train loss:   1.053787\n",
      "train loss:   0.508356\n",
      "train loss:   0.783211\n",
      "train loss:   1.106508\n",
      "train loss:   0.744739\n",
      "train loss:   1.270607\n",
      "train loss:   0.833064\n",
      "train loss:   0.946889\n",
      "train loss:   0.938481\n",
      "train loss:   0.890971\n",
      "train loss:   0.973308\n",
      "train loss:   0.984671\n",
      "train loss:   0.988933\n",
      "train loss:   0.659573\n",
      "train loss:   0.830915\n",
      "train loss:   0.962627\n",
      "train loss:   0.774776\n",
      "train loss:   0.696731\n",
      "train loss:   1.176166\n",
      "train loss:   1.123997\n",
      "train loss:   1.062658\n",
      "train loss:   0.627818\n",
      "train loss:   0.833040\n",
      "train loss:   1.020010\n",
      "train loss:   1.125025\n",
      "train loss:   0.942155\n",
      "train loss:   0.401713\n",
      "train loss:   0.699331\n",
      "########### epoch 123 ###########\n",
      "########### loop 23050 ###########\n",
      "test loss:   0.430889   test accuracy:   0.916667\n",
      "########### loop 23050 ###########\n",
      "train loss:   0.877075\n",
      "train loss:   0.994954\n",
      "train loss:   1.221958\n",
      "train loss:   0.758212\n",
      "train loss:   1.097048\n",
      "train loss:   1.194916\n",
      "train loss:   1.214056\n",
      "train loss:   0.880039\n",
      "train loss:   0.753991\n",
      "train loss:   0.827449\n",
      "train loss:   0.540344\n",
      "train loss:   0.477154\n",
      "train loss:   1.208792\n",
      "train loss:   0.971540\n",
      "train loss:   1.053233\n",
      "train loss:   1.118773\n",
      "train loss:   0.888397\n",
      "train loss:   0.915684\n",
      "train loss:   0.673356\n",
      "train loss:   0.993402\n",
      "train loss:   1.007716\n",
      "train loss:   0.851687\n",
      "train loss:   1.018778\n",
      "train loss:   0.894302\n",
      "train loss:   0.827056\n",
      "train loss:   1.036541\n",
      "train loss:   1.102748\n",
      "train loss:   1.195226\n",
      "train loss:   0.951960\n",
      "train loss:   1.229578\n",
      "train loss:   0.626667\n",
      "train loss:   1.075336\n",
      "train loss:   0.749302\n",
      "train loss:   1.021772\n",
      "train loss:   1.186171\n",
      "train loss:   0.975984\n",
      "train loss:   0.980579\n",
      "train loss:   1.168024\n",
      "train loss:   0.637876\n",
      "train loss:   0.888356\n",
      "train loss:   1.242719\n",
      "train loss:   0.611006\n",
      "train loss:   0.918561\n",
      "train loss:   1.098161\n",
      "train loss:   0.794109\n",
      "train loss:   0.943891\n",
      "train loss:   1.288045\n",
      "train loss:   1.212775\n",
      "train loss:   1.481998\n",
      "train loss:   1.002944\n",
      "########### epoch 123 ###########\n",
      "########### loop 23100 ###########\n",
      "test loss:   0.143354   test accuracy:   1.000000\n",
      "########### loop 23100 ###########\n",
      "train loss:   0.875186\n",
      "train loss:   0.893983\n",
      "train loss:   0.997054\n",
      "train loss:   1.127231\n",
      "train loss:   0.782339\n",
      "train loss:   1.110019\n",
      "train loss:   0.850261\n",
      "train loss:   0.778372\n",
      "train loss:   1.055276\n",
      "train loss:   0.485793\n",
      "train loss:   0.825521\n",
      "train loss:   1.060567\n",
      "train loss:   1.138147\n",
      "train loss:   0.910991\n",
      "train loss:   1.088373\n",
      "train loss:   1.162002\n",
      "train loss:   0.869401\n",
      "train loss:   1.075127\n",
      "train loss:   1.086698\n",
      "train loss:   0.519870\n",
      "train loss:   1.073030\n",
      "train loss:   0.444460\n",
      "train loss:   0.847746\n",
      "train loss:   1.040781\n",
      "train loss:   1.165541\n",
      "train loss:   0.442462\n",
      "train loss:   1.131184\n",
      "train loss:   1.042717\n",
      "train loss:   0.864741\n",
      "train loss:   1.247941\n",
      "train loss:   0.953432\n",
      "train loss:   0.999988\n",
      "train loss:   1.007527\n",
      "train loss:   0.767272\n",
      "train loss:   0.715271\n",
      "train loss:   0.729147\n",
      "train loss:   1.039597\n",
      "train loss:   0.621840\n",
      "train loss:   0.645603\n",
      "train loss:   0.585515\n",
      "train loss:   1.074613\n",
      "train loss:   0.886540\n",
      "train loss:   1.051206\n",
      "train loss:   1.082364\n",
      "train loss:   1.010984\n",
      "train loss:   0.753823\n",
      "train loss:   0.848499\n",
      "train loss:   0.806707\n",
      "train loss:   1.040970\n",
      "train loss:   1.159609\n",
      "########### epoch 124 ###########\n",
      "########### loop 23150 ###########\n",
      "test loss:   0.181067   test accuracy:   0.958333\n",
      "########### loop 23150 ###########\n",
      "train loss:   0.980268\n",
      "train loss:   0.877651\n",
      "train loss:   0.878057\n",
      "train loss:   0.830555\n",
      "train loss:   0.685338\n",
      "train loss:   0.874573\n",
      "train loss:   1.253788\n",
      "train loss:   0.962776\n",
      "train loss:   0.896187\n",
      "train loss:   1.311667\n",
      "train loss:   1.115013\n",
      "train loss:   1.032101\n",
      "train loss:   0.721556\n",
      "train loss:   0.923556\n",
      "train loss:   1.274697\n",
      "train loss:   0.900656\n",
      "train loss:   0.775574\n",
      "train loss:   0.890605\n",
      "train loss:   1.044112\n",
      "train loss:   1.062516\n",
      "train loss:   1.119610\n",
      "train loss:   0.893971\n",
      "train loss:   1.290003\n",
      "train loss:   0.724043\n",
      "train loss:   0.892034\n",
      "train loss:   1.272886\n",
      "train loss:   0.681768\n",
      "train loss:   0.996175\n",
      "train loss:   1.057778\n",
      "train loss:   0.853451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.811934\n",
      "train loss:   1.284852\n",
      "train loss:   0.766996\n",
      "train loss:   0.835109\n",
      "train loss:   0.609396\n",
      "train loss:   0.996154\n",
      "train loss:   0.819637\n",
      "train loss:   1.211102\n",
      "train loss:   0.974460\n",
      "train loss:   0.774690\n",
      "train loss:   0.908831\n",
      "train loss:   0.864745\n",
      "train loss:   1.122162\n",
      "train loss:   1.430438\n",
      "train loss:   0.951633\n",
      "train loss:   1.011854\n",
      "train loss:   1.198767\n",
      "train loss:   0.994813\n",
      "train loss:   0.832946\n",
      "train loss:   0.842426\n",
      "########### epoch 124 ###########\n",
      "########### loop 23200 ###########\n",
      "test loss:   0.270001   test accuracy:   0.875000\n",
      "########### loop 23200 ###########\n",
      "train loss:   0.830868\n",
      "train loss:   1.028778\n",
      "train loss:   0.949914\n",
      "train loss:   1.028749\n",
      "train loss:   0.761523\n",
      "train loss:   0.750372\n",
      "train loss:   1.419563\n",
      "train loss:   0.873725\n",
      "train loss:   1.128217\n",
      "train loss:   1.043251\n",
      "train loss:   1.123722\n",
      "train loss:   1.032886\n",
      "train loss:   0.896440\n",
      "train loss:   0.997628\n",
      "train loss:   0.962943\n",
      "train loss:   1.003430\n",
      "train loss:   1.201948\n",
      "train loss:   1.082309\n",
      "train loss:   0.757545\n",
      "train loss:   0.668594\n",
      "train loss:   1.009333\n",
      "train loss:   0.623910\n",
      "train loss:   0.922156\n",
      "train loss:   0.944084\n",
      "train loss:   0.971551\n",
      "train loss:   0.775178\n",
      "train loss:   1.240853\n",
      "train loss:   0.805613\n",
      "train loss:   1.025515\n",
      "train loss:   1.039398\n",
      "train loss:   1.065549\n",
      "train loss:   1.181501\n",
      "train loss:   1.015778\n",
      "train loss:   1.024997\n",
      "train loss:   1.041938\n",
      "train loss:   0.877304\n",
      "train loss:   1.353512\n",
      "train loss:   0.641534\n",
      "train loss:   0.881750\n",
      "train loss:   0.861263\n",
      "train loss:   0.995543\n",
      "train loss:   1.036657\n",
      "train loss:   0.773718\n",
      "train loss:   0.924989\n",
      "train loss:   1.365157\n",
      "train loss:   0.992068\n",
      "train loss:   0.784631\n",
      "train loss:   0.857496\n",
      "train loss:   1.202178\n",
      "train loss:   1.175398\n",
      "########### epoch 124 ###########\n",
      "########### loop 23250 ###########\n",
      "test loss:   0.238314   test accuracy:   0.958333\n",
      "########### loop 23250 ###########\n",
      "train loss:   0.747181\n",
      "train loss:   0.979806\n",
      "train loss:   1.041499\n",
      "train loss:   0.961078\n",
      "train loss:   1.010984\n",
      "train loss:   0.881683\n",
      "train loss:   0.824611\n",
      "train loss:   0.801605\n",
      "train loss:   0.861742\n",
      "train loss:   1.204451\n",
      "train loss:   0.876948\n",
      "train loss:   1.227835\n",
      "train loss:   0.890259\n",
      "train loss:   0.991502\n",
      "train loss:   0.985121\n",
      "train loss:   1.048314\n",
      "train loss:   1.046436\n",
      "train loss:   1.214615\n",
      "train loss:   0.509210\n",
      "train loss:   1.641713\n",
      "train loss:   0.845354\n",
      "train loss:   0.977089\n",
      "train loss:   0.940936\n",
      "train loss:   0.669647\n",
      "train loss:   0.843754\n",
      "train loss:   1.142917\n",
      "train loss:   1.002316\n",
      "train loss:   0.868728\n",
      "train loss:   0.892235\n",
      "train loss:   1.260796\n",
      "train loss:   1.006891\n",
      "train loss:   0.763802\n",
      "train loss:   1.011039\n",
      "train loss:   0.976932\n",
      "train loss:   1.153563\n",
      "train loss:   1.224739\n",
      "train loss:   1.223404\n",
      "train loss:   1.064978\n",
      "train loss:   1.112036\n",
      "train loss:   0.564212\n",
      "train loss:   1.075554\n",
      "train loss:   1.126837\n",
      "train loss:   1.008790\n",
      "train loss:   1.042405\n",
      "train loss:   1.148477\n",
      "train loss:   0.876213\n",
      "train loss:   0.993275\n",
      "train loss:   1.436100\n",
      "train loss:   0.910270\n",
      "train loss:   1.114894\n",
      "########### epoch 124 ###########\n",
      "########### loop 23300 ###########\n",
      "test loss:   0.252201   test accuracy:   0.958333\n",
      "########### loop 23300 ###########\n",
      "train loss:   1.033605\n",
      "train loss:   0.709352\n",
      "train loss:   0.903189\n",
      "train loss:   1.105901\n",
      "train loss:   1.112701\n",
      "train loss:   1.067578\n",
      "train loss:   1.165794\n",
      "train loss:   1.207529\n",
      "train loss:   1.068903\n",
      "train loss:   1.100598\n",
      "train loss:   0.873990\n",
      "train loss:   0.945837\n",
      "train loss:   1.107147\n",
      "train loss:   0.882501\n",
      "train loss:   0.796264\n",
      "train loss:   0.739367\n",
      "train loss:   1.299878\n",
      "train loss:   1.043806\n",
      "train loss:   1.024506\n",
      "train loss:   0.718356\n",
      "train loss:   0.910825\n",
      "train loss:   1.055909\n",
      "train loss:   1.173221\n",
      "train loss:   0.862253\n",
      "train loss:   1.320276\n",
      "train loss:   0.888490\n",
      "train loss:   0.695211\n",
      "train loss:   1.026585\n",
      "train loss:   0.920489\n",
      "train loss:   0.926190\n",
      "train loss:   1.405747\n",
      "train loss:   0.970355\n",
      "train loss:   0.884650\n",
      "train loss:   0.936050\n",
      "train loss:   1.012290\n",
      "train loss:   0.596563\n",
      "train loss:   0.932214\n",
      "train loss:   1.112101\n",
      "train loss:   0.857996\n",
      "train loss:   1.124011\n",
      "train loss:   0.841208\n",
      "train loss:   1.237773\n",
      "train loss:   0.822242\n",
      "train loss:   0.837283\n",
      "train loss:   1.192149\n",
      "train loss:   0.777562\n",
      "train loss:   1.059632\n",
      "train loss:   0.842309\n",
      "train loss:   1.287103\n",
      "train loss:   1.078036\n",
      "########### epoch 125 ###########\n",
      "########### loop 23350 ###########\n",
      "test loss:   0.378062   test accuracy:   0.833333\n",
      "########### loop 23350 ###########\n",
      "train loss:   1.023754\n",
      "train loss:   0.574464\n",
      "train loss:   1.100894\n",
      "train loss:   0.937590\n",
      "train loss:   1.089914\n",
      "train loss:   0.841968\n",
      "train loss:   1.002342\n",
      "train loss:   0.712908\n",
      "train loss:   0.822428\n",
      "train loss:   0.753240\n",
      "train loss:   1.042288\n",
      "train loss:   0.866642\n",
      "train loss:   1.312558\n",
      "train loss:   0.925687\n",
      "train loss:   0.943440\n",
      "train loss:   0.930641\n",
      "train loss:   1.037846\n",
      "train loss:   1.067390\n",
      "train loss:   1.235193\n",
      "train loss:   1.030717\n",
      "train loss:   1.171873\n",
      "train loss:   1.037320\n",
      "train loss:   0.746221\n",
      "train loss:   0.998657\n",
      "train loss:   1.313201\n",
      "train loss:   1.034879\n",
      "train loss:   0.949883\n",
      "train loss:   1.024726\n",
      "train loss:   0.823045\n",
      "train loss:   0.772065\n",
      "train loss:   1.020687\n",
      "train loss:   0.791585\n",
      "train loss:   0.914408\n",
      "train loss:   1.166514\n",
      "train loss:   0.846406\n",
      "train loss:   1.001385\n",
      "train loss:   1.239757\n",
      "train loss:   0.912059\n",
      "train loss:   0.955356\n",
      "train loss:   0.990883\n",
      "train loss:   0.853999\n",
      "train loss:   0.761355\n",
      "train loss:   1.126979\n",
      "train loss:   0.995079\n",
      "train loss:   0.680036\n",
      "train loss:   1.231479\n",
      "train loss:   1.222396\n",
      "train loss:   0.909164\n",
      "train loss:   0.940704\n",
      "train loss:   0.943781\n",
      "########### epoch 125 ###########\n",
      "########### loop 23400 ###########\n",
      "test loss:   0.423793   test accuracy:   0.916667\n",
      "########### loop 23400 ###########\n",
      "train loss:   0.935579\n",
      "train loss:   0.859656\n",
      "train loss:   1.278333\n",
      "train loss:   0.806403\n",
      "train loss:   0.810098\n",
      "train loss:   1.077891\n",
      "train loss:   0.825402\n",
      "train loss:   0.997599\n",
      "train loss:   1.023334\n",
      "train loss:   0.978720\n",
      "train loss:   1.048901\n",
      "train loss:   0.655858\n",
      "train loss:   1.073858\n",
      "train loss:   0.878719\n",
      "train loss:   1.048140\n",
      "train loss:   0.542342\n",
      "train loss:   0.922115\n",
      "train loss:   1.063103\n",
      "train loss:   1.115009\n",
      "train loss:   0.722003\n",
      "train loss:   0.839082\n",
      "train loss:   0.885006\n",
      "train loss:   1.311932\n",
      "train loss:   0.819080\n",
      "train loss:   0.711894\n",
      "train loss:   0.989034\n",
      "train loss:   1.112882\n",
      "train loss:   0.986127\n",
      "train loss:   0.692134\n",
      "train loss:   1.016979\n",
      "train loss:   1.287128\n",
      "train loss:   1.041829\n",
      "train loss:   0.651884\n",
      "train loss:   1.132118\n",
      "train loss:   0.968760\n",
      "train loss:   1.139817\n",
      "train loss:   0.876169\n",
      "train loss:   0.956908\n",
      "train loss:   0.884068\n",
      "train loss:   0.842153\n",
      "train loss:   0.489062\n",
      "train loss:   0.968165\n",
      "train loss:   0.852656\n",
      "train loss:   0.805844\n",
      "train loss:   1.398836\n",
      "train loss:   1.006443\n",
      "train loss:   0.774905\n",
      "train loss:   1.289197\n",
      "train loss:   0.809042\n",
      "train loss:   0.816582\n",
      "########### epoch 125 ###########\n",
      "########### loop 23450 ###########\n",
      "test loss:   0.372216   test accuracy:   0.833333\n",
      "########### loop 23450 ###########\n",
      "train loss:   1.056779\n",
      "train loss:   1.201148\n",
      "train loss:   1.002102\n",
      "train loss:   0.663695\n",
      "train loss:   1.525990\n",
      "train loss:   1.025195\n",
      "train loss:   0.742787\n",
      "train loss:   0.879022\n",
      "train loss:   1.112379\n",
      "train loss:   0.945563\n",
      "train loss:   0.673481\n",
      "train loss:   0.847940\n",
      "train loss:   1.046437\n",
      "train loss:   0.526332\n",
      "train loss:   1.153853\n",
      "train loss:   1.088247\n",
      "train loss:   1.119896\n",
      "train loss:   0.789204\n",
      "train loss:   0.917060\n",
      "train loss:   0.750268\n",
      "train loss:   0.836890\n",
      "train loss:   0.715685\n",
      "train loss:   0.824004\n",
      "train loss:   1.254352\n",
      "train loss:   0.561696\n",
      "train loss:   0.852410\n",
      "train loss:   1.071260\n",
      "train loss:   0.822686\n",
      "train loss:   1.011591\n",
      "train loss:   0.786452\n",
      "train loss:   0.813845\n",
      "train loss:   0.983831\n",
      "train loss:   1.234068\n",
      "train loss:   1.080573\n",
      "train loss:   1.559376\n",
      "train loss:   1.079782\n",
      "train loss:   1.115895\n",
      "train loss:   1.077657\n",
      "train loss:   0.806769\n",
      "train loss:   1.366914\n",
      "train loss:   1.047000\n",
      "train loss:   0.915965\n",
      "train loss:   0.719843\n",
      "train loss:   1.195456\n",
      "train loss:   0.787780\n",
      "train loss:   1.061552\n",
      "train loss:   0.906073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.589494\n",
      "train loss:   1.197610\n",
      "train loss:   0.935715\n",
      "########### epoch 126 ###########\n",
      "########### loop 23500 ###########\n",
      "test loss:   0.201733   test accuracy:   0.958333\n",
      "########### loop 23500 ###########\n",
      "train loss:   1.015087\n",
      "train loss:   1.080198\n",
      "train loss:   1.253099\n",
      "train loss:   0.731382\n",
      "train loss:   1.046178\n",
      "train loss:   1.114760\n",
      "train loss:   0.980689\n",
      "train loss:   1.350999\n",
      "train loss:   0.848699\n",
      "train loss:   0.910882\n",
      "train loss:   1.124985\n",
      "train loss:   1.189510\n",
      "train loss:   1.330468\n",
      "train loss:   0.999541\n",
      "train loss:   1.055885\n",
      "train loss:   0.918389\n",
      "train loss:   1.226504\n",
      "train loss:   0.961753\n",
      "train loss:   0.624120\n",
      "train loss:   0.957959\n",
      "train loss:   0.976077\n",
      "train loss:   1.221662\n",
      "train loss:   1.280953\n",
      "train loss:   0.899377\n",
      "train loss:   1.221470\n",
      "train loss:   0.942710\n",
      "train loss:   1.037600\n",
      "train loss:   0.848081\n",
      "train loss:   0.995719\n",
      "train loss:   1.004198\n",
      "train loss:   0.774190\n",
      "train loss:   0.699690\n",
      "train loss:   1.084063\n",
      "train loss:   0.597364\n",
      "train loss:   0.991620\n",
      "train loss:   1.179897\n",
      "train loss:   1.366175\n",
      "train loss:   1.145936\n",
      "train loss:   0.712734\n",
      "train loss:   0.843324\n",
      "train loss:   0.778944\n",
      "train loss:   1.209515\n",
      "train loss:   1.208422\n",
      "train loss:   1.182269\n",
      "train loss:   0.806573\n",
      "train loss:   1.027647\n",
      "train loss:   1.309541\n",
      "train loss:   1.008091\n",
      "train loss:   0.935000\n",
      "train loss:   1.161420\n",
      "########### epoch 126 ###########\n",
      "########### loop 23550 ###########\n",
      "test loss:   0.136302   test accuracy:   1.000000\n",
      "########### loop 23550 ###########\n",
      "train loss:   0.777442\n",
      "train loss:   0.978447\n",
      "train loss:   0.906466\n",
      "train loss:   0.855762\n",
      "train loss:   1.206159\n",
      "train loss:   0.855795\n",
      "train loss:   0.943905\n",
      "train loss:   1.005113\n",
      "train loss:   0.750152\n",
      "train loss:   0.542577\n",
      "train loss:   0.699022\n",
      "train loss:   1.009830\n",
      "train loss:   1.241454\n",
      "train loss:   1.144522\n",
      "train loss:   1.253838\n",
      "train loss:   0.976708\n",
      "train loss:   0.862241\n",
      "train loss:   1.176438\n",
      "train loss:   0.663857\n",
      "train loss:   0.857544\n",
      "train loss:   0.907468\n",
      "train loss:   0.998888\n",
      "train loss:   0.900894\n",
      "train loss:   0.747538\n",
      "train loss:   1.111454\n",
      "train loss:   1.177001\n",
      "train loss:   1.328940\n",
      "train loss:   0.968319\n",
      "train loss:   0.963949\n",
      "train loss:   0.778215\n",
      "train loss:   0.855948\n",
      "train loss:   0.786024\n",
      "train loss:   0.824257\n",
      "train loss:   0.568131\n",
      "train loss:   0.962285\n",
      "train loss:   1.057314\n",
      "train loss:   0.863328\n",
      "train loss:   1.074276\n",
      "train loss:   1.195354\n",
      "train loss:   1.051771\n",
      "train loss:   0.994224\n",
      "train loss:   0.922445\n",
      "train loss:   0.808340\n",
      "train loss:   1.091708\n",
      "train loss:   0.877988\n",
      "train loss:   1.294647\n",
      "train loss:   0.933680\n",
      "train loss:   0.878266\n",
      "train loss:   1.123622\n",
      "train loss:   1.102418\n",
      "########### epoch 126 ###########\n",
      "########### loop 23600 ###########\n",
      "test loss:   0.155444   test accuracy:   1.000000\n",
      "########### loop 23600 ###########\n",
      "train loss:   0.745319\n",
      "train loss:   0.894831\n",
      "train loss:   1.011408\n",
      "train loss:   1.088156\n",
      "train loss:   0.744624\n",
      "train loss:   0.857578\n",
      "train loss:   0.814670\n",
      "train loss:   1.039230\n",
      "train loss:   0.802152\n",
      "train loss:   0.759671\n",
      "train loss:   0.638408\n",
      "train loss:   1.377578\n",
      "train loss:   1.202415\n",
      "train loss:   0.534140\n",
      "train loss:   1.413381\n",
      "train loss:   0.980735\n",
      "train loss:   0.890094\n",
      "train loss:   0.907142\n",
      "train loss:   0.988461\n",
      "train loss:   0.721495\n",
      "train loss:   1.331631\n",
      "train loss:   0.894178\n",
      "train loss:   1.264087\n",
      "train loss:   0.582620\n",
      "train loss:   0.855526\n",
      "train loss:   0.904458\n",
      "train loss:   1.023462\n",
      "train loss:   0.874392\n",
      "train loss:   0.657351\n",
      "train loss:   1.016896\n",
      "train loss:   0.991048\n",
      "train loss:   0.741915\n",
      "train loss:   0.880323\n",
      "train loss:   0.844141\n",
      "train loss:   0.713016\n",
      "train loss:   1.072274\n",
      "train loss:   0.957390\n",
      "train loss:   1.044627\n",
      "train loss:   1.130019\n",
      "train loss:   1.116911\n",
      "train loss:   0.878860\n",
      "train loss:   0.824567\n",
      "train loss:   0.836529\n",
      "train loss:   1.062131\n",
      "train loss:   0.729949\n",
      "train loss:   0.883672\n",
      "train loss:   0.782770\n",
      "train loss:   0.857077\n",
      "train loss:   0.714949\n",
      "train loss:   0.906154\n",
      "########### epoch 126 ###########\n",
      "########### loop 23650 ###########\n",
      "test loss:   0.181479   test accuracy:   0.958333\n",
      "########### loop 23650 ###########\n",
      "train loss:   0.700161\n",
      "train loss:   1.129989\n",
      "train loss:   0.993750\n",
      "train loss:   1.309867\n",
      "train loss:   1.252610\n",
      "train loss:   0.663873\n",
      "train loss:   0.957722\n",
      "train loss:   0.952160\n",
      "train loss:   1.154273\n",
      "train loss:   1.138650\n",
      "train loss:   1.015999\n",
      "train loss:   1.180766\n",
      "train loss:   1.213395\n",
      "train loss:   1.211489\n",
      "train loss:   1.126731\n",
      "train loss:   0.902679\n",
      "train loss:   1.017969\n",
      "train loss:   0.795593\n",
      "train loss:   0.937210\n",
      "train loss:   0.899920\n",
      "train loss:   0.915834\n",
      "train loss:   0.797367\n",
      "train loss:   0.693594\n",
      "train loss:   1.130647\n",
      "train loss:   1.352232\n",
      "train loss:   1.232069\n",
      "train loss:   1.089216\n",
      "train loss:   1.186374\n",
      "train loss:   0.707583\n",
      "train loss:   0.982123\n",
      "train loss:   0.869801\n",
      "train loss:   1.120857\n",
      "train loss:   0.938643\n",
      "train loss:   0.944353\n",
      "train loss:   0.816320\n",
      "train loss:   0.974858\n",
      "train loss:   0.948434\n",
      "train loss:   1.025670\n",
      "train loss:   1.047481\n",
      "train loss:   1.164047\n",
      "train loss:   1.019864\n",
      "train loss:   0.866857\n",
      "train loss:   0.890977\n",
      "train loss:   1.223523\n",
      "train loss:   0.749636\n",
      "train loss:   0.856071\n",
      "train loss:   0.741555\n",
      "train loss:   1.377962\n",
      "train loss:   1.180840\n",
      "train loss:   0.828686\n",
      "########### epoch 127 ###########\n",
      "########### loop 23700 ###########\n",
      "test loss:   0.306393   test accuracy:   0.916667\n",
      "########### loop 23700 ###########\n",
      "train loss:   0.549814\n",
      "train loss:   1.043049\n",
      "train loss:   0.869665\n",
      "train loss:   0.740575\n",
      "train loss:   0.718206\n",
      "train loss:   1.001009\n",
      "train loss:   1.131165\n",
      "train loss:   0.954631\n",
      "train loss:   0.759054\n",
      "train loss:   1.061941\n",
      "train loss:   0.836959\n",
      "train loss:   1.209219\n",
      "train loss:   0.849770\n",
      "train loss:   0.927414\n",
      "train loss:   1.019043\n",
      "train loss:   1.048520\n",
      "train loss:   0.873552\n",
      "train loss:   0.926272\n",
      "train loss:   1.114490\n",
      "train loss:   0.397522\n",
      "train loss:   0.942373\n",
      "train loss:   1.062219\n",
      "train loss:   1.000601\n",
      "train loss:   1.127063\n",
      "train loss:   1.035973\n",
      "train loss:   1.249671\n",
      "train loss:   0.658934\n",
      "train loss:   0.968373\n",
      "train loss:   0.504967\n",
      "train loss:   1.015176\n",
      "train loss:   1.047989\n",
      "train loss:   0.770573\n",
      "train loss:   1.179320\n",
      "train loss:   0.984076\n",
      "train loss:   0.965568\n",
      "train loss:   0.711844\n",
      "train loss:   1.460682\n",
      "train loss:   0.930768\n",
      "train loss:   0.824568\n",
      "train loss:   1.023911\n",
      "train loss:   1.215070\n",
      "train loss:   0.703279\n",
      "train loss:   0.598774\n",
      "train loss:   0.960380\n",
      "train loss:   0.959375\n",
      "train loss:   0.943889\n",
      "train loss:   0.805958\n",
      "train loss:   0.870855\n",
      "train loss:   0.609050\n",
      "train loss:   0.827715\n",
      "########### epoch 127 ###########\n",
      "########### loop 23750 ###########\n",
      "test loss:   0.228180   test accuracy:   0.958333\n",
      "########### loop 23750 ###########\n",
      "train loss:   1.097106\n",
      "train loss:   1.080839\n",
      "train loss:   0.814661\n",
      "train loss:   1.113582\n",
      "train loss:   0.748032\n",
      "train loss:   0.967961\n",
      "train loss:   0.985593\n",
      "train loss:   0.943984\n",
      "train loss:   1.131794\n",
      "train loss:   1.087474\n",
      "train loss:   1.206172\n",
      "train loss:   0.985437\n",
      "train loss:   0.885970\n",
      "train loss:   0.948104\n",
      "train loss:   0.847845\n",
      "train loss:   1.056100\n",
      "train loss:   0.994344\n",
      "train loss:   0.880141\n",
      "train loss:   1.335787\n",
      "train loss:   0.882611\n",
      "train loss:   1.260281\n",
      "train loss:   0.916741\n",
      "train loss:   0.944437\n",
      "train loss:   0.718919\n",
      "train loss:   0.864187\n",
      "train loss:   0.816764\n",
      "train loss:   1.310213\n",
      "train loss:   1.013843\n",
      "train loss:   0.998737\n",
      "train loss:   1.539822\n",
      "train loss:   0.911221\n",
      "train loss:   1.087387\n",
      "train loss:   1.025561\n",
      "train loss:   0.744192\n",
      "train loss:   1.091732\n",
      "train loss:   0.622762\n",
      "train loss:   0.954023\n",
      "train loss:   1.235584\n",
      "train loss:   1.052540\n",
      "train loss:   0.958992\n",
      "train loss:   0.740757\n",
      "train loss:   1.344633\n",
      "train loss:   0.959775\n",
      "train loss:   0.872908\n",
      "train loss:   1.101895\n",
      "train loss:   1.013066\n",
      "train loss:   0.938978\n",
      "train loss:   1.114151\n",
      "train loss:   0.916310\n",
      "train loss:   0.817656\n",
      "########### epoch 127 ###########\n",
      "########### loop 23800 ###########\n",
      "test loss:   0.272551   test accuracy:   0.916667\n",
      "########### loop 23800 ###########\n",
      "train loss:   1.339437\n",
      "train loss:   0.964843\n",
      "train loss:   0.822929\n",
      "train loss:   0.922099\n",
      "train loss:   0.841555\n",
      "train loss:   0.722211\n",
      "train loss:   1.072746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.926279\n",
      "train loss:   1.057984\n",
      "train loss:   0.874227\n",
      "train loss:   1.259368\n",
      "train loss:   1.278161\n",
      "train loss:   0.788610\n",
      "train loss:   1.119518\n",
      "train loss:   0.695043\n",
      "train loss:   1.077638\n",
      "train loss:   1.011410\n",
      "train loss:   0.944985\n",
      "train loss:   0.880085\n",
      "train loss:   0.938818\n",
      "train loss:   0.732186\n",
      "train loss:   0.924934\n",
      "train loss:   0.575997\n",
      "train loss:   1.002292\n",
      "train loss:   0.886857\n",
      "train loss:   0.648717\n",
      "train loss:   0.700852\n",
      "train loss:   0.990517\n",
      "train loss:   0.777445\n",
      "train loss:   0.824567\n",
      "train loss:   1.189670\n",
      "train loss:   1.194729\n",
      "train loss:   0.655737\n",
      "train loss:   1.346407\n",
      "train loss:   1.144801\n",
      "train loss:   0.850085\n",
      "train loss:   1.187277\n",
      "train loss:   0.836697\n",
      "train loss:   1.081781\n",
      "train loss:   0.818505\n",
      "train loss:   0.887829\n",
      "train loss:   0.935747\n",
      "train loss:   1.005179\n",
      "train loss:   1.076548\n",
      "train loss:   0.474470\n",
      "train loss:   0.887474\n",
      "train loss:   0.763187\n",
      "train loss:   0.876540\n",
      "train loss:   1.019999\n",
      "train loss:   1.086535\n",
      "########### epoch 127 ###########\n",
      "########### loop 23850 ###########\n",
      "test loss:   0.191517   test accuracy:   1.000000\n",
      "########### loop 23850 ###########\n",
      "train loss:   1.020320\n",
      "train loss:   1.278689\n",
      "train loss:   0.855407\n",
      "train loss:   1.273063\n",
      "train loss:   1.036585\n",
      "train loss:   1.151487\n",
      "train loss:   1.140023\n",
      "train loss:   1.023547\n",
      "train loss:   0.943022\n",
      "train loss:   0.992327\n",
      "train loss:   1.015456\n",
      "train loss:   0.488486\n",
      "train loss:   0.851795\n",
      "train loss:   1.054243\n",
      "train loss:   0.912322\n",
      "train loss:   0.876847\n",
      "train loss:   0.879863\n",
      "train loss:   0.902407\n",
      "train loss:   0.805070\n",
      "train loss:   1.012026\n",
      "train loss:   0.932278\n",
      "train loss:   1.320250\n",
      "train loss:   1.069082\n",
      "train loss:   0.869674\n",
      "train loss:   0.976532\n",
      "train loss:   0.817880\n",
      "train loss:   0.969051\n",
      "train loss:   0.944379\n",
      "train loss:   0.998637\n",
      "train loss:   1.253173\n",
      "train loss:   0.948865\n",
      "train loss:   0.890758\n",
      "train loss:   0.955879\n",
      "train loss:   1.206002\n",
      "train loss:   0.947059\n",
      "train loss:   0.840290\n",
      "train loss:   1.101324\n",
      "train loss:   1.345042\n",
      "train loss:   1.241748\n",
      "train loss:   1.010310\n",
      "train loss:   0.877586\n",
      "train loss:   0.953807\n",
      "train loss:   0.907135\n",
      "train loss:   1.039816\n",
      "train loss:   1.107178\n",
      "train loss:   0.619777\n",
      "train loss:   1.084710\n",
      "train loss:   0.630348\n",
      "train loss:   1.156358\n",
      "train loss:   1.230315\n",
      "########### epoch 128 ###########\n",
      "########### loop 23900 ###########\n",
      "test loss:   0.229501   test accuracy:   0.916667\n",
      "########### loop 23900 ###########\n",
      "train loss:   1.019130\n",
      "train loss:   0.803308\n",
      "train loss:   1.017745\n",
      "train loss:   0.906016\n",
      "train loss:   0.664935\n",
      "train loss:   0.883366\n",
      "train loss:   1.002723\n",
      "train loss:   1.158175\n",
      "train loss:   0.642516\n",
      "train loss:   0.909628\n",
      "train loss:   0.816229\n",
      "train loss:   0.531360\n",
      "train loss:   1.222817\n",
      "train loss:   0.997718\n",
      "train loss:   0.584277\n",
      "train loss:   0.730926\n",
      "train loss:   1.212796\n",
      "train loss:   0.693981\n",
      "train loss:   1.018450\n",
      "train loss:   1.377092\n",
      "train loss:   0.854042\n",
      "train loss:   0.951176\n",
      "train loss:   1.014477\n",
      "train loss:   0.990564\n",
      "train loss:   1.167076\n",
      "train loss:   0.907089\n",
      "train loss:   1.031654\n",
      "train loss:   0.997664\n",
      "train loss:   0.965811\n",
      "train loss:   0.970980\n",
      "train loss:   1.448396\n",
      "train loss:   0.733864\n",
      "train loss:   0.886547\n",
      "train loss:   1.046423\n",
      "train loss:   1.090769\n",
      "train loss:   0.925390\n",
      "train loss:   0.889289\n",
      "train loss:   0.573729\n",
      "train loss:   1.158941\n",
      "train loss:   0.650345\n",
      "train loss:   1.038962\n",
      "train loss:   0.992551\n",
      "train loss:   1.190275\n",
      "train loss:   1.085322\n",
      "train loss:   0.902838\n",
      "train loss:   1.020927\n",
      "train loss:   1.081674\n",
      "train loss:   1.103573\n",
      "train loss:   0.944080\n",
      "train loss:   0.803650\n",
      "########### epoch 128 ###########\n",
      "########### loop 23950 ###########\n",
      "test loss:   0.085461   test accuracy:   1.000000\n",
      "########### loop 23950 ###########\n",
      "train loss:   0.788701\n",
      "train loss:   1.055912\n",
      "train loss:   0.880006\n",
      "train loss:   1.079760\n",
      "train loss:   0.701246\n",
      "train loss:   0.648614\n",
      "train loss:   1.146523\n",
      "train loss:   1.076665\n",
      "train loss:   0.812471\n",
      "train loss:   0.898073\n",
      "train loss:   0.854316\n",
      "train loss:   1.070594\n",
      "train loss:   0.836432\n",
      "train loss:   1.162607\n",
      "train loss:   1.030061\n",
      "train loss:   1.048467\n",
      "train loss:   0.900884\n",
      "train loss:   0.583494\n",
      "train loss:   0.783281\n",
      "train loss:   0.858692\n",
      "train loss:   0.697884\n",
      "train loss:   1.087153\n",
      "train loss:   0.830548\n",
      "train loss:   0.817518\n",
      "train loss:   1.253635\n",
      "train loss:   0.748313\n",
      "train loss:   1.058769\n",
      "train loss:   1.043103\n",
      "train loss:   0.898908\n",
      "train loss:   0.644317\n",
      "train loss:   1.116279\n",
      "train loss:   0.891473\n",
      "train loss:   0.695342\n",
      "train loss:   0.937949\n",
      "train loss:   0.953328\n",
      "train loss:   0.939560\n",
      "train loss:   0.760724\n",
      "train loss:   0.688418\n",
      "train loss:   1.191909\n",
      "train loss:   0.729704\n",
      "train loss:   0.683023\n",
      "train loss:   1.317482\n",
      "train loss:   0.889189\n",
      "train loss:   0.612341\n",
      "train loss:   0.793907\n",
      "train loss:   0.772521\n",
      "train loss:   0.965900\n",
      "train loss:   0.484814\n",
      "train loss:   0.429531\n",
      "train loss:   0.765722\n",
      "########### epoch 128 ###########\n",
      "########### loop 24000 ###########\n",
      "test loss:   0.610153   test accuracy:   0.833333\n",
      "########### loop 24000 ###########\n",
      "train loss:   0.927927\n",
      "train loss:   1.011617\n",
      "train loss:   1.168126\n",
      "train loss:   0.691754\n",
      "train loss:   0.946289\n",
      "train loss:   0.985430\n",
      "train loss:   0.796230\n",
      "train loss:   0.866896\n",
      "train loss:   0.818340\n",
      "train loss:   0.978034\n",
      "train loss:   0.838700\n",
      "train loss:   1.000320\n",
      "train loss:   1.060365\n",
      "train loss:   0.783301\n",
      "train loss:   1.060009\n",
      "train loss:   1.040630\n",
      "train loss:   1.176284\n",
      "train loss:   1.054421\n",
      "train loss:   0.930292\n",
      "train loss:   1.006621\n",
      "train loss:   1.108586\n",
      "train loss:   0.607051\n",
      "train loss:   0.714549\n",
      "train loss:   1.086512\n",
      "train loss:   0.880547\n",
      "train loss:   1.015303\n",
      "train loss:   0.781751\n",
      "train loss:   1.087499\n",
      "train loss:   1.062512\n",
      "train loss:   0.600497\n",
      "train loss:   0.802781\n",
      "train loss:   1.042579\n",
      "train loss:   1.014250\n",
      "train loss:   0.991726\n",
      "train loss:   0.958202\n",
      "train loss:   1.063871\n",
      "train loss:   0.910306\n",
      "train loss:   0.878567\n",
      "train loss:   1.253471\n",
      "train loss:   1.472529\n",
      "train loss:   0.840046\n",
      "train loss:   1.338375\n",
      "train loss:   1.283862\n",
      "train loss:   0.824555\n",
      "train loss:   0.526955\n",
      "train loss:   0.991183\n",
      "train loss:   0.785775\n",
      "train loss:   1.103515\n",
      "train loss:   1.057044\n",
      "train loss:   1.274738\n",
      "########### epoch 128 ###########\n",
      "########### loop 24050 ###########\n",
      "test loss:   0.383935   test accuracy:   0.833333\n",
      "########### loop 24050 ###########\n",
      "train loss:   0.891758\n",
      "train loss:   0.431207\n",
      "train loss:   0.667296\n",
      "train loss:   0.810695\n",
      "train loss:   0.988412\n",
      "train loss:   1.008232\n",
      "train loss:   0.991359\n",
      "train loss:   1.092171\n",
      "train loss:   0.578539\n",
      "train loss:   1.039914\n",
      "train loss:   1.044806\n",
      "train loss:   0.911051\n",
      "train loss:   0.858594\n",
      "train loss:   1.312672\n",
      "train loss:   0.953601\n",
      "train loss:   0.804380\n",
      "train loss:   0.662929\n",
      "train loss:   1.025737\n",
      "train loss:   0.861115\n",
      "train loss:   1.348121\n",
      "train loss:   0.850970\n",
      "train loss:   1.214492\n",
      "train loss:   0.957945\n",
      "train loss:   1.128583\n",
      "train loss:   1.111178\n",
      "train loss:   1.021347\n",
      "train loss:   0.999723\n",
      "train loss:   0.769158\n",
      "train loss:   1.094805\n",
      "train loss:   0.849043\n",
      "train loss:   0.782222\n",
      "train loss:   0.985365\n",
      "train loss:   0.716927\n",
      "train loss:   0.920675\n",
      "train loss:   1.221959\n",
      "train loss:   0.830037\n",
      "train loss:   0.946048\n",
      "train loss:   1.262634\n",
      "train loss:   0.758584\n",
      "train loss:   0.820082\n",
      "train loss:   0.823585\n",
      "train loss:   0.999296\n",
      "train loss:   1.118371\n",
      "train loss:   0.629766\n",
      "train loss:   1.300571\n",
      "train loss:   1.243056\n",
      "train loss:   1.206185\n",
      "train loss:   0.960192\n",
      "train loss:   0.874844\n",
      "train loss:   0.952285\n",
      "########### epoch 129 ###########\n",
      "########### loop 24100 ###########\n",
      "test loss:   0.222033   test accuracy:   0.958333\n",
      "########### loop 24100 ###########\n",
      "train loss:   0.997429\n",
      "train loss:   0.474339\n",
      "train loss:   0.852375\n",
      "train loss:   1.088829\n",
      "train loss:   0.819570\n",
      "train loss:   1.123544\n",
      "train loss:   0.878543\n",
      "train loss:   0.862870\n",
      "train loss:   1.044623\n",
      "train loss:   1.034460\n",
      "train loss:   1.168603\n",
      "train loss:   0.949931\n",
      "train loss:   1.012047\n",
      "train loss:   1.049077\n",
      "train loss:   1.102414\n",
      "train loss:   1.206852\n",
      "train loss:   0.789216\n",
      "train loss:   0.808453\n",
      "train loss:   1.358929\n",
      "train loss:   0.851818\n",
      "train loss:   0.780613\n",
      "train loss:   0.894003\n",
      "train loss:   1.042400\n",
      "train loss:   0.908077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.699116\n",
      "train loss:   0.978240\n",
      "train loss:   1.062476\n",
      "train loss:   1.039967\n",
      "train loss:   0.886195\n",
      "train loss:   0.573272\n",
      "train loss:   0.750367\n",
      "train loss:   0.892367\n",
      "train loss:   0.730607\n",
      "train loss:   1.032614\n",
      "train loss:   0.999533\n",
      "train loss:   0.698120\n",
      "train loss:   0.870596\n",
      "train loss:   0.857823\n",
      "train loss:   1.133474\n",
      "train loss:   0.923876\n",
      "train loss:   0.938022\n",
      "train loss:   0.840735\n",
      "train loss:   0.652859\n",
      "train loss:   0.960946\n",
      "train loss:   1.206499\n",
      "train loss:   0.651137\n",
      "train loss:   0.919134\n",
      "train loss:   0.923073\n",
      "train loss:   1.196627\n",
      "train loss:   1.018692\n",
      "########### epoch 129 ###########\n",
      "########### loop 24150 ###########\n",
      "test loss:   0.263387   test accuracy:   0.875000\n",
      "########### loop 24150 ###########\n",
      "train loss:   1.219695\n",
      "train loss:   0.936455\n",
      "train loss:   1.106758\n",
      "train loss:   0.907086\n",
      "train loss:   0.885425\n",
      "train loss:   0.952002\n",
      "train loss:   0.920451\n",
      "train loss:   1.018094\n",
      "train loss:   0.962833\n",
      "train loss:   1.191864\n",
      "train loss:   0.877424\n",
      "train loss:   0.909687\n",
      "train loss:   1.061056\n",
      "train loss:   0.878206\n",
      "train loss:   0.934331\n",
      "train loss:   1.038269\n",
      "train loss:   0.984108\n",
      "train loss:   0.839858\n",
      "train loss:   0.960492\n",
      "train loss:   1.056173\n",
      "train loss:   0.680957\n",
      "train loss:   0.896636\n",
      "train loss:   0.902835\n",
      "train loss:   1.030600\n",
      "train loss:   1.028019\n",
      "train loss:   1.368359\n",
      "train loss:   1.066701\n",
      "train loss:   0.963464\n",
      "train loss:   0.843447\n",
      "train loss:   1.235539\n",
      "train loss:   0.944706\n",
      "train loss:   0.727872\n",
      "train loss:   0.958685\n",
      "train loss:   1.017486\n",
      "train loss:   0.709693\n",
      "train loss:   1.045592\n",
      "train loss:   1.027085\n",
      "train loss:   0.935931\n",
      "train loss:   0.952509\n",
      "train loss:   0.651438\n",
      "train loss:   1.048274\n",
      "train loss:   0.847877\n",
      "train loss:   0.593202\n",
      "train loss:   1.380101\n",
      "train loss:   0.634373\n",
      "train loss:   0.405615\n",
      "train loss:   1.398325\n",
      "train loss:   1.081492\n",
      "train loss:   0.892218\n",
      "train loss:   0.945015\n",
      "########### epoch 129 ###########\n",
      "########### loop 24200 ###########\n",
      "test loss:   0.237816   test accuracy:   0.916667\n",
      "########### loop 24200 ###########\n",
      "train loss:   0.964646\n",
      "train loss:   0.805550\n",
      "train loss:   0.679472\n",
      "train loss:   0.543525\n",
      "train loss:   0.989418\n",
      "train loss:   1.052911\n",
      "train loss:   0.778968\n",
      "train loss:   0.978201\n",
      "train loss:   1.080937\n",
      "train loss:   0.946272\n",
      "train loss:   0.693508\n",
      "train loss:   0.903038\n",
      "train loss:   0.961371\n",
      "train loss:   1.327770\n",
      "train loss:   1.052135\n",
      "train loss:   1.023139\n",
      "train loss:   1.057930\n",
      "train loss:   0.847633\n",
      "train loss:   0.959501\n",
      "train loss:   0.848709\n",
      "train loss:   1.159808\n",
      "train loss:   0.637723\n",
      "train loss:   0.676367\n",
      "train loss:   0.961643\n",
      "train loss:   1.060757\n",
      "train loss:   0.534193\n",
      "train loss:   0.819964\n",
      "train loss:   0.867982\n",
      "train loss:   0.819572\n",
      "train loss:   1.084648\n",
      "train loss:   0.927983\n",
      "train loss:   1.124825\n",
      "train loss:   1.035031\n",
      "train loss:   0.811204\n",
      "train loss:   0.981629\n",
      "train loss:   0.789864\n",
      "train loss:   0.875995\n",
      "train loss:   1.064324\n",
      "train loss:   1.126982\n",
      "train loss:   1.203423\n",
      "train loss:   0.743946\n",
      "train loss:   0.796185\n",
      "train loss:   1.142484\n",
      "train loss:   0.618448\n",
      "train loss:   0.984962\n",
      "train loss:   1.057679\n",
      "train loss:   1.056303\n",
      "train loss:   1.056614\n",
      "train loss:   0.987574\n",
      "train loss:   0.938121\n",
      "########### epoch 129 ###########\n",
      "########### loop 24250 ###########\n",
      "test loss:   0.375778   test accuracy:   0.875000\n",
      "########### loop 24250 ###########\n",
      "train loss:   0.823560\n",
      "train loss:   0.682546\n",
      "train loss:   0.784384\n",
      "train loss:   0.854169\n",
      "train loss:   1.194052\n",
      "train loss:   0.661408\n",
      "train loss:   0.899851\n",
      "train loss:   0.698649\n",
      "train loss:   1.379339\n",
      "train loss:   0.687635\n",
      "train loss:   1.088487\n",
      "train loss:   1.130053\n",
      "train loss:   1.079759\n",
      "train loss:   0.743672\n",
      "train loss:   0.743558\n",
      "train loss:   0.650277\n",
      "train loss:   1.253126\n",
      "train loss:   0.938447\n",
      "train loss:   1.122105\n",
      "train loss:   0.849589\n",
      "train loss:   0.966335\n",
      "train loss:   1.122692\n",
      "train loss:   0.872588\n",
      "train loss:   0.930292\n",
      "train loss:   0.869128\n",
      "train loss:   1.050734\n",
      "train loss:   0.930576\n",
      "train loss:   1.379652\n",
      "train loss:   1.192092\n",
      "train loss:   0.830016\n",
      "train loss:   0.805887\n",
      "train loss:   1.266880\n",
      "train loss:   0.639416\n",
      "train loss:   1.050744\n",
      "train loss:   1.459809\n",
      "train loss:   1.019864\n",
      "train loss:   0.991736\n",
      "train loss:   0.729895\n",
      "train loss:   0.960417\n",
      "train loss:   0.729458\n",
      "train loss:   0.813164\n",
      "train loss:   0.943423\n",
      "train loss:   0.703863\n",
      "train loss:   1.169648\n",
      "train loss:   0.818942\n",
      "train loss:   1.084392\n",
      "train loss:   0.656557\n",
      "train loss:   0.892096\n",
      "train loss:   0.915773\n",
      "train loss:   1.045102\n",
      "########### epoch 130 ###########\n",
      "########### loop 24300 ###########\n",
      "test loss:   0.098029   test accuracy:   1.000000\n",
      "########### loop 24300 ###########\n",
      "train loss:   0.991596\n",
      "train loss:   1.226498\n",
      "train loss:   1.216749\n",
      "train loss:   0.927439\n",
      "train loss:   1.210345\n",
      "train loss:   0.745460\n",
      "train loss:   0.909907\n",
      "train loss:   1.199614\n",
      "train loss:   0.694602\n",
      "train loss:   1.132664\n",
      "train loss:   0.891140\n",
      "train loss:   0.882544\n",
      "train loss:   0.973754\n",
      "train loss:   1.058858\n",
      "train loss:   0.979525\n",
      "train loss:   0.744281\n",
      "train loss:   0.913295\n",
      "train loss:   1.139713\n",
      "train loss:   1.208347\n",
      "train loss:   0.709201\n",
      "train loss:   0.876116\n",
      "train loss:   1.264059\n",
      "train loss:   1.162225\n",
      "train loss:   0.955019\n",
      "train loss:   1.107019\n",
      "train loss:   0.878439\n",
      "train loss:   1.056247\n",
      "train loss:   1.129799\n",
      "train loss:   0.924419\n",
      "train loss:   1.106723\n",
      "train loss:   1.149135\n",
      "train loss:   0.807825\n",
      "train loss:   0.886498\n",
      "train loss:   0.883286\n",
      "train loss:   1.330888\n",
      "train loss:   0.968149\n",
      "train loss:   0.799220\n",
      "train loss:   0.737701\n",
      "train loss:   0.962765\n",
      "train loss:   0.913011\n",
      "train loss:   1.106753\n",
      "train loss:   1.079734\n",
      "train loss:   0.872729\n",
      "train loss:   0.879228\n",
      "train loss:   0.864258\n",
      "train loss:   1.128320\n",
      "train loss:   0.953189\n",
      "train loss:   1.076946\n",
      "train loss:   1.158285\n",
      "train loss:   1.252494\n",
      "########### epoch 130 ###########\n",
      "########### loop 24350 ###########\n",
      "test loss:   0.324430   test accuracy:   0.916667\n",
      "########### loop 24350 ###########\n",
      "train loss:   0.911981\n",
      "train loss:   0.892391\n",
      "train loss:   0.959893\n",
      "train loss:   1.185522\n",
      "train loss:   1.291258\n",
      "train loss:   0.942991\n",
      "train loss:   0.968691\n",
      "train loss:   0.718100\n",
      "train loss:   1.048878\n",
      "train loss:   0.928631\n",
      "train loss:   0.960802\n",
      "train loss:   1.111257\n",
      "train loss:   1.114268\n",
      "train loss:   1.060171\n",
      "train loss:   0.970423\n",
      "train loss:   0.873791\n",
      "train loss:   1.080633\n",
      "train loss:   1.277576\n",
      "train loss:   1.086955\n",
      "train loss:   1.108077\n",
      "train loss:   0.976686\n",
      "train loss:   0.941500\n",
      "train loss:   1.158499\n",
      "train loss:   0.711294\n",
      "train loss:   0.642309\n",
      "train loss:   0.815041\n",
      "train loss:   0.881281\n",
      "train loss:   0.744674\n",
      "train loss:   1.103224\n",
      "train loss:   1.185114\n",
      "train loss:   0.976828\n",
      "train loss:   0.998323\n",
      "train loss:   1.140529\n",
      "train loss:   0.967638\n",
      "train loss:   1.035195\n",
      "train loss:   0.582733\n",
      "train loss:   0.733394\n",
      "train loss:   0.954843\n",
      "train loss:   0.631850\n",
      "train loss:   1.113539\n",
      "train loss:   1.110767\n",
      "train loss:   1.068974\n",
      "train loss:   1.320755\n",
      "train loss:   0.813823\n",
      "train loss:   1.000331\n",
      "train loss:   0.863684\n",
      "train loss:   0.780838\n",
      "train loss:   0.755085\n",
      "train loss:   0.853319\n",
      "train loss:   0.928751\n",
      "########### epoch 130 ###########\n",
      "########### loop 24400 ###########\n",
      "test loss:   0.370495   test accuracy:   0.875000\n",
      "########### loop 24400 ###########\n",
      "train loss:   0.968969\n",
      "train loss:   1.057148\n",
      "train loss:   0.394368\n",
      "train loss:   0.830110\n",
      "train loss:   0.663326\n",
      "train loss:   0.965583\n",
      "train loss:   0.920583\n",
      "train loss:   1.056275\n",
      "train loss:   0.639953\n",
      "train loss:   0.812918\n",
      "train loss:   1.224429\n",
      "train loss:   0.875708\n",
      "train loss:   0.825277\n",
      "train loss:   1.198840\n",
      "train loss:   0.585688\n",
      "train loss:   0.893945\n",
      "train loss:   0.932872\n",
      "train loss:   1.063347\n",
      "train loss:   0.929535\n",
      "train loss:   1.240112\n",
      "train loss:   0.982531\n",
      "train loss:   0.999393\n",
      "train loss:   0.897987\n",
      "train loss:   1.340375\n",
      "train loss:   0.934210\n",
      "train loss:   0.891195\n",
      "train loss:   1.075107\n",
      "train loss:   0.877306\n",
      "train loss:   0.978064\n",
      "train loss:   1.018261\n",
      "train loss:   0.835250\n",
      "train loss:   1.127681\n",
      "train loss:   0.925723\n",
      "train loss:   0.641472\n",
      "train loss:   0.971283\n",
      "train loss:   1.090332\n",
      "train loss:   0.987033\n",
      "train loss:   0.897759\n",
      "train loss:   0.936904\n",
      "train loss:   0.922428\n",
      "train loss:   1.012907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.386489\n",
      "train loss:   0.824798\n",
      "train loss:   1.204575\n",
      "train loss:   1.024614\n",
      "train loss:   0.887118\n",
      "train loss:   1.114957\n",
      "train loss:   1.286416\n",
      "train loss:   0.904159\n",
      "train loss:   0.747381\n",
      "########### epoch 131 ###########\n",
      "########### loop 24450 ###########\n",
      "test loss:   0.183864   test accuracy:   0.958333\n",
      "########### loop 24450 ###########\n",
      "train loss:   0.815972\n",
      "train loss:   0.723251\n",
      "train loss:   0.915587\n",
      "train loss:   1.017779\n",
      "train loss:   0.480953\n",
      "train loss:   1.068414\n",
      "train loss:   1.286245\n",
      "train loss:   0.878259\n",
      "train loss:   1.198823\n",
      "train loss:   0.911791\n",
      "train loss:   0.983217\n",
      "train loss:   1.019965\n",
      "train loss:   0.937178\n",
      "train loss:   0.839195\n",
      "train loss:   0.869548\n",
      "train loss:   1.616187\n",
      "train loss:   1.043917\n",
      "train loss:   0.795401\n",
      "train loss:   0.613204\n",
      "train loss:   1.009484\n",
      "train loss:   0.952752\n",
      "train loss:   0.808652\n",
      "train loss:   0.679895\n",
      "train loss:   1.204327\n",
      "train loss:   1.125523\n",
      "train loss:   0.953838\n",
      "train loss:   1.124819\n",
      "train loss:   0.722942\n",
      "train loss:   0.813873\n",
      "train loss:   1.149118\n",
      "train loss:   0.981932\n",
      "train loss:   0.922119\n",
      "train loss:   0.747749\n",
      "train loss:   0.860022\n",
      "train loss:   1.138026\n",
      "train loss:   1.000053\n",
      "train loss:   0.990567\n",
      "train loss:   1.059692\n",
      "train loss:   1.202647\n",
      "train loss:   1.167596\n",
      "train loss:   1.238552\n",
      "train loss:   0.918670\n",
      "train loss:   1.044338\n",
      "train loss:   0.916450\n",
      "train loss:   0.726790\n",
      "train loss:   0.778185\n",
      "train loss:   0.886134\n",
      "train loss:   1.013359\n",
      "train loss:   0.806579\n",
      "train loss:   1.339080\n",
      "########### epoch 131 ###########\n",
      "########### loop 24500 ###########\n",
      "test loss:   0.381516   test accuracy:   0.833333\n",
      "########### loop 24500 ###########\n",
      "train loss:   1.009683\n",
      "train loss:   1.151101\n",
      "train loss:   1.015997\n",
      "train loss:   1.214229\n",
      "train loss:   0.988416\n",
      "train loss:   0.935101\n",
      "train loss:   0.852004\n",
      "train loss:   1.121126\n",
      "train loss:   0.839851\n",
      "train loss:   1.282403\n",
      "train loss:   1.062257\n",
      "train loss:   0.897113\n",
      "train loss:   0.794005\n",
      "train loss:   0.886020\n",
      "train loss:   0.891320\n",
      "train loss:   0.921512\n",
      "train loss:   0.937862\n",
      "train loss:   1.068597\n",
      "train loss:   0.606795\n",
      "train loss:   0.972662\n",
      "train loss:   0.645716\n",
      "train loss:   1.113572\n",
      "train loss:   0.749712\n",
      "train loss:   0.810009\n",
      "train loss:   0.998168\n",
      "train loss:   1.261848\n",
      "train loss:   1.082652\n",
      "train loss:   1.316238\n",
      "train loss:   1.035697\n",
      "train loss:   0.980472\n",
      "train loss:   0.828464\n",
      "train loss:   0.854247\n",
      "train loss:   0.953328\n",
      "train loss:   0.626975\n",
      "train loss:   1.131577\n",
      "train loss:   1.212543\n",
      "train loss:   1.055601\n",
      "train loss:   0.839325\n",
      "train loss:   1.002737\n",
      "train loss:   0.758726\n",
      "train loss:   1.132909\n",
      "train loss:   1.060765\n",
      "train loss:   1.503367\n",
      "train loss:   1.101429\n",
      "train loss:   1.021608\n",
      "train loss:   0.780323\n",
      "train loss:   0.959092\n",
      "train loss:   1.100171\n",
      "train loss:   1.087394\n",
      "train loss:   0.809960\n",
      "########### epoch 131 ###########\n",
      "########### loop 24550 ###########\n",
      "test loss:   0.115666   test accuracy:   1.000000\n",
      "########### loop 24550 ###########\n",
      "train loss:   1.171872\n",
      "train loss:   1.029101\n",
      "train loss:   0.780403\n",
      "train loss:   1.031825\n",
      "train loss:   0.908784\n",
      "train loss:   0.847850\n",
      "train loss:   0.984622\n",
      "train loss:   1.282758\n",
      "train loss:   1.238419\n",
      "train loss:   0.758650\n",
      "train loss:   0.912032\n",
      "train loss:   0.635081\n",
      "train loss:   1.240400\n",
      "train loss:   1.436414\n",
      "train loss:   1.079477\n",
      "train loss:   0.787093\n",
      "train loss:   0.814096\n",
      "train loss:   1.050979\n",
      "train loss:   0.911807\n",
      "train loss:   0.903784\n",
      "train loss:   0.915916\n",
      "train loss:   0.939207\n",
      "train loss:   1.098790\n",
      "train loss:   0.739639\n",
      "train loss:   0.919014\n",
      "train loss:   0.471511\n",
      "train loss:   1.233484\n",
      "train loss:   0.756218\n",
      "train loss:   0.861296\n",
      "train loss:   0.898098\n",
      "train loss:   1.180891\n",
      "train loss:   0.718847\n",
      "train loss:   0.872401\n",
      "train loss:   0.647481\n",
      "train loss:   0.840586\n",
      "train loss:   0.815274\n",
      "train loss:   0.827274\n",
      "train loss:   0.928376\n",
      "train loss:   1.041366\n",
      "train loss:   0.942848\n",
      "train loss:   1.072314\n",
      "train loss:   0.812437\n",
      "train loss:   1.202792\n",
      "train loss:   1.169467\n",
      "train loss:   0.525662\n",
      "train loss:   0.953544\n",
      "train loss:   0.915884\n",
      "train loss:   0.903678\n",
      "train loss:   0.959333\n",
      "train loss:   0.974196\n",
      "########### epoch 131 ###########\n",
      "########### loop 24600 ###########\n",
      "test loss:   0.201905   test accuracy:   0.916667\n",
      "########### loop 24600 ###########\n",
      "train loss:   0.680734\n",
      "train loss:   1.153313\n",
      "train loss:   1.024951\n",
      "train loss:   1.257707\n",
      "train loss:   0.866628\n",
      "train loss:   0.982671\n",
      "train loss:   1.162175\n",
      "train loss:   0.868493\n",
      "train loss:   0.932457\n",
      "train loss:   1.233547\n",
      "train loss:   1.113029\n",
      "train loss:   0.857484\n",
      "train loss:   0.881777\n",
      "train loss:   0.678341\n",
      "train loss:   1.085165\n",
      "train loss:   0.831996\n",
      "train loss:   0.702334\n",
      "train loss:   0.714831\n",
      "train loss:   1.032753\n",
      "train loss:   0.977562\n",
      "train loss:   1.077017\n",
      "train loss:   0.750330\n",
      "train loss:   0.798825\n",
      "train loss:   1.572938\n",
      "train loss:   1.025886\n",
      "train loss:   0.741174\n",
      "train loss:   0.989571\n",
      "train loss:   0.990580\n",
      "train loss:   0.659135\n",
      "train loss:   0.688093\n",
      "train loss:   0.877632\n",
      "train loss:   1.195143\n",
      "train loss:   1.034865\n",
      "train loss:   1.211886\n",
      "train loss:   0.807388\n",
      "train loss:   0.983738\n",
      "train loss:   1.132498\n",
      "train loss:   0.832374\n",
      "train loss:   0.941789\n",
      "train loss:   0.755331\n",
      "train loss:   1.232056\n",
      "train loss:   0.792535\n",
      "train loss:   0.739821\n",
      "train loss:   1.154375\n",
      "train loss:   1.211617\n",
      "train loss:   1.053305\n",
      "train loss:   1.127719\n",
      "train loss:   0.711131\n",
      "train loss:   1.218073\n",
      "train loss:   0.815595\n",
      "########### epoch 132 ###########\n",
      "########### loop 24650 ###########\n",
      "test loss:   0.168082   test accuracy:   1.000000\n",
      "########### loop 24650 ###########\n",
      "train loss:   1.022003\n",
      "train loss:   1.056506\n",
      "train loss:   0.681061\n",
      "train loss:   1.093875\n",
      "train loss:   0.843632\n",
      "train loss:   1.051772\n",
      "train loss:   0.747869\n",
      "train loss:   0.945780\n",
      "train loss:   0.723741\n",
      "train loss:   0.913028\n",
      "train loss:   1.276462\n",
      "train loss:   0.827932\n",
      "train loss:   0.883672\n",
      "train loss:   0.947774\n",
      "train loss:   1.239369\n",
      "train loss:   1.211179\n",
      "train loss:   0.949828\n",
      "train loss:   1.030954\n",
      "train loss:   0.767768\n",
      "train loss:   0.912825\n",
      "train loss:   1.041790\n",
      "train loss:   0.977662\n",
      "train loss:   1.705589\n",
      "train loss:   0.928061\n",
      "train loss:   1.383048\n",
      "train loss:   0.887293\n",
      "train loss:   0.902676\n",
      "train loss:   1.091339\n",
      "train loss:   1.074405\n",
      "train loss:   1.183861\n",
      "train loss:   0.634693\n",
      "train loss:   0.846915\n",
      "train loss:   0.970967\n",
      "train loss:   0.698154\n",
      "train loss:   0.945803\n",
      "train loss:   1.025786\n",
      "train loss:   1.134251\n",
      "train loss:   1.039579\n",
      "train loss:   1.166125\n",
      "train loss:   1.193306\n",
      "train loss:   0.771885\n",
      "train loss:   1.222113\n",
      "train loss:   0.551627\n",
      "train loss:   0.726176\n",
      "train loss:   0.984094\n",
      "train loss:   1.123647\n",
      "train loss:   0.786184\n",
      "train loss:   0.858450\n",
      "train loss:   0.738979\n",
      "train loss:   0.897175\n",
      "########### epoch 132 ###########\n",
      "########### loop 24700 ###########\n",
      "test loss:   0.192653   test accuracy:   0.958333\n",
      "########### loop 24700 ###########\n",
      "train loss:   0.873761\n",
      "train loss:   0.983647\n",
      "train loss:   1.331525\n",
      "train loss:   1.088100\n",
      "train loss:   0.714845\n",
      "train loss:   1.067650\n",
      "train loss:   1.013502\n",
      "train loss:   0.975050\n",
      "train loss:   0.776646\n",
      "train loss:   1.405321\n",
      "train loss:   0.779301\n",
      "train loss:   0.978479\n",
      "train loss:   0.962147\n",
      "train loss:   0.829990\n",
      "train loss:   1.077564\n",
      "train loss:   0.928053\n",
      "train loss:   0.672996\n",
      "train loss:   1.146180\n",
      "train loss:   0.986192\n",
      "train loss:   1.273421\n",
      "train loss:   0.868104\n",
      "train loss:   0.646610\n",
      "train loss:   1.010267\n",
      "train loss:   0.929055\n",
      "train loss:   1.149757\n",
      "train loss:   0.978019\n",
      "train loss:   0.999061\n",
      "train loss:   0.669818\n",
      "train loss:   1.466713\n",
      "train loss:   0.733182\n",
      "train loss:   1.110211\n",
      "train loss:   1.127186\n",
      "train loss:   0.860711\n",
      "train loss:   1.013106\n",
      "train loss:   1.173699\n",
      "train loss:   1.050676\n",
      "train loss:   1.034930\n",
      "train loss:   0.793868\n",
      "train loss:   0.953125\n",
      "train loss:   0.640177\n",
      "train loss:   0.827682\n",
      "train loss:   0.744495\n",
      "train loss:   0.918904\n",
      "train loss:   0.962172\n",
      "train loss:   0.734261\n",
      "train loss:   0.991636\n",
      "train loss:   0.874312\n",
      "train loss:   1.016649\n",
      "train loss:   1.145819\n",
      "train loss:   1.320411\n",
      "########### epoch 132 ###########\n",
      "########### loop 24750 ###########\n",
      "test loss:   0.435944   test accuracy:   0.875000\n",
      "########### loop 24750 ###########\n",
      "train loss:   0.958273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.977987\n",
      "train loss:   0.923843\n",
      "train loss:   0.692105\n",
      "train loss:   0.755618\n",
      "train loss:   1.075806\n",
      "train loss:   1.260194\n",
      "train loss:   0.783837\n",
      "train loss:   0.933394\n",
      "train loss:   0.772152\n",
      "train loss:   0.980898\n",
      "train loss:   1.235615\n",
      "train loss:   0.986436\n",
      "train loss:   1.273088\n",
      "train loss:   1.180661\n",
      "train loss:   1.190908\n",
      "train loss:   1.002423\n",
      "train loss:   0.846209\n",
      "train loss:   0.755021\n",
      "train loss:   1.155571\n",
      "train loss:   1.014240\n",
      "train loss:   0.797395\n",
      "train loss:   0.822166\n",
      "train loss:   0.925044\n",
      "train loss:   0.718501\n",
      "train loss:   0.958070\n",
      "train loss:   1.068719\n",
      "train loss:   0.796526\n",
      "train loss:   0.774072\n",
      "train loss:   1.302416\n",
      "train loss:   0.903117\n",
      "train loss:   1.209567\n",
      "train loss:   0.889404\n",
      "train loss:   0.869007\n",
      "train loss:   0.844194\n",
      "train loss:   0.482880\n",
      "train loss:   1.063196\n",
      "train loss:   1.094417\n",
      "train loss:   1.198480\n",
      "train loss:   1.220174\n",
      "train loss:   1.062409\n",
      "train loss:   0.573863\n",
      "train loss:   1.066856\n",
      "train loss:   0.898154\n",
      "train loss:   1.119049\n",
      "train loss:   1.162918\n",
      "train loss:   0.967250\n",
      "train loss:   0.634765\n",
      "train loss:   0.928385\n",
      "train loss:   1.041420\n",
      "########### epoch 132 ###########\n",
      "########### loop 24800 ###########\n",
      "test loss:   0.155335   test accuracy:   1.000000\n",
      "########### loop 24800 ###########\n",
      "train loss:   0.973277\n",
      "train loss:   0.752555\n",
      "train loss:   1.431932\n",
      "train loss:   0.649188\n",
      "train loss:   1.255101\n",
      "train loss:   0.939601\n",
      "train loss:   0.928782\n",
      "train loss:   0.870475\n",
      "train loss:   1.142429\n",
      "train loss:   0.780895\n",
      "train loss:   0.936462\n",
      "train loss:   0.674107\n",
      "train loss:   0.829332\n",
      "train loss:   0.815286\n",
      "train loss:   0.741790\n",
      "train loss:   1.050826\n",
      "train loss:   0.843417\n",
      "train loss:   1.165974\n",
      "train loss:   1.167903\n",
      "train loss:   1.014067\n",
      "train loss:   0.841971\n",
      "train loss:   0.855661\n",
      "train loss:   0.814861\n",
      "train loss:   0.795920\n",
      "train loss:   1.376076\n",
      "train loss:   1.192119\n",
      "train loss:   1.025926\n",
      "train loss:   0.769255\n",
      "train loss:   1.126476\n",
      "train loss:   1.066638\n",
      "train loss:   0.889820\n",
      "train loss:   0.724337\n",
      "train loss:   1.351009\n",
      "train loss:   0.951900\n",
      "train loss:   0.979553\n",
      "train loss:   1.100737\n",
      "train loss:   0.727072\n",
      "train loss:   0.696336\n",
      "train loss:   0.390866\n",
      "train loss:   1.280832\n",
      "train loss:   0.974692\n",
      "train loss:   0.985569\n",
      "train loss:   1.067099\n",
      "train loss:   0.672915\n",
      "train loss:   0.953024\n",
      "train loss:   0.658986\n",
      "train loss:   1.056668\n",
      "train loss:   0.696766\n",
      "train loss:   0.825195\n",
      "train loss:   1.148795\n",
      "########### epoch 133 ###########\n",
      "########### loop 24850 ###########\n",
      "test loss:   0.214522   test accuracy:   0.958333\n",
      "########### loop 24850 ###########\n",
      "train loss:   1.120190\n",
      "train loss:   0.990092\n",
      "train loss:   0.771000\n",
      "train loss:   1.163967\n",
      "train loss:   1.058033\n",
      "train loss:   0.882414\n",
      "train loss:   1.188397\n",
      "train loss:   1.248141\n",
      "train loss:   0.847930\n",
      "train loss:   1.082242\n",
      "train loss:   1.120427\n",
      "train loss:   0.892511\n",
      "train loss:   1.071866\n",
      "train loss:   0.965860\n",
      "train loss:   1.202152\n",
      "train loss:   1.215195\n",
      "train loss:   1.082744\n",
      "train loss:   0.655898\n",
      "train loss:   1.113990\n",
      "train loss:   1.044000\n",
      "train loss:   0.704820\n",
      "train loss:   0.802929\n",
      "train loss:   0.956127\n",
      "train loss:   0.836065\n",
      "train loss:   0.716095\n",
      "train loss:   1.300354\n",
      "train loss:   0.973584\n",
      "train loss:   1.015606\n",
      "train loss:   1.298101\n",
      "train loss:   1.056075\n",
      "train loss:   0.983630\n",
      "train loss:   0.925826\n",
      "train loss:   0.979091\n",
      "train loss:   1.119062\n",
      "train loss:   0.951626\n",
      "train loss:   0.948979\n",
      "train loss:   1.140625\n",
      "train loss:   1.047964\n",
      "train loss:   0.527469\n",
      "train loss:   1.147650\n",
      "train loss:   0.884777\n",
      "train loss:   1.229377\n",
      "train loss:   1.050477\n",
      "train loss:   0.923817\n",
      "train loss:   0.855952\n",
      "train loss:   0.916055\n",
      "train loss:   1.130860\n",
      "train loss:   0.685296\n",
      "train loss:   1.040435\n",
      "train loss:   1.345657\n",
      "########### epoch 133 ###########\n",
      "########### loop 24900 ###########\n",
      "test loss:   0.101357   test accuracy:   1.000000\n",
      "########### loop 24900 ###########\n",
      "train loss:   0.999253\n",
      "train loss:   0.943815\n",
      "train loss:   0.981970\n",
      "train loss:   0.766973\n",
      "train loss:   1.048156\n",
      "train loss:   1.093189\n",
      "train loss:   0.511164\n",
      "train loss:   0.734903\n",
      "train loss:   0.810148\n",
      "train loss:   1.089660\n",
      "train loss:   0.822939\n",
      "train loss:   0.987723\n",
      "train loss:   0.806180\n",
      "train loss:   0.893368\n",
      "train loss:   1.127184\n",
      "train loss:   0.729583\n",
      "train loss:   0.558485\n",
      "train loss:   1.088344\n",
      "train loss:   0.854133\n",
      "train loss:   0.604477\n",
      "train loss:   0.941866\n",
      "train loss:   0.658836\n",
      "train loss:   1.181363\n",
      "train loss:   0.856534\n",
      "train loss:   0.980665\n",
      "train loss:   0.998373\n",
      "train loss:   1.015271\n",
      "train loss:   0.912901\n",
      "train loss:   0.360304\n",
      "train loss:   0.628520\n",
      "train loss:   0.811933\n",
      "train loss:   1.085546\n",
      "train loss:   0.890879\n",
      "train loss:   1.048935\n",
      "train loss:   0.814181\n",
      "train loss:   0.834647\n",
      "train loss:   0.606043\n",
      "train loss:   0.983085\n",
      "train loss:   0.867277\n",
      "train loss:   0.558476\n",
      "train loss:   0.946367\n",
      "train loss:   0.723053\n",
      "train loss:   0.971618\n",
      "train loss:   1.141816\n",
      "train loss:   1.047833\n",
      "train loss:   0.783671\n",
      "train loss:   1.133913\n",
      "train loss:   1.414865\n",
      "train loss:   1.313693\n",
      "train loss:   1.236191\n",
      "########### epoch 133 ###########\n",
      "########### loop 24950 ###########\n",
      "test loss:   0.302123   test accuracy:   0.875000\n",
      "########### loop 24950 ###########\n",
      "train loss:   0.908249\n",
      "train loss:   0.781144\n",
      "train loss:   1.088726\n",
      "train loss:   0.516999\n",
      "train loss:   1.167990\n",
      "train loss:   1.053677\n",
      "train loss:   0.855275\n",
      "train loss:   0.726141\n",
      "train loss:   1.026894\n",
      "train loss:   1.019597\n",
      "train loss:   0.671601\n",
      "train loss:   0.827728\n",
      "train loss:   0.895396\n",
      "train loss:   0.924830\n",
      "train loss:   1.153096\n",
      "train loss:   0.923702\n",
      "train loss:   1.084004\n",
      "train loss:   0.984484\n",
      "train loss:   0.931651\n",
      "train loss:   0.866184\n",
      "train loss:   1.067267\n",
      "train loss:   1.483424\n",
      "train loss:   0.853701\n",
      "train loss:   1.127504\n",
      "train loss:   1.168981\n",
      "train loss:   0.968696\n",
      "train loss:   0.777658\n",
      "train loss:   1.330103\n",
      "train loss:   1.198980\n",
      "train loss:   1.120009\n",
      "train loss:   0.927183\n",
      "train loss:   0.444958\n",
      "train loss:   0.928689\n",
      "train loss:   1.023559\n",
      "train loss:   0.912876\n",
      "train loss:   0.815631\n",
      "train loss:   1.091610\n",
      "train loss:   0.884349\n",
      "train loss:   0.913276\n",
      "train loss:   1.036782\n",
      "train loss:   0.557621\n",
      "train loss:   0.867150\n",
      "train loss:   1.074220\n",
      "train loss:   0.861345\n",
      "train loss:   1.020691\n",
      "train loss:   0.970859\n",
      "train loss:   0.563215\n",
      "train loss:   0.999246\n",
      "train loss:   0.703516\n",
      "train loss:   0.929773\n",
      "########### epoch 133 ###########\n",
      "########### loop 25000 ###########\n",
      "test loss:   0.196804   test accuracy:   0.958333\n",
      "########### loop 25000 ###########\n",
      "train loss:   0.828291\n",
      "train loss:   0.890202\n",
      "train loss:   1.124253\n",
      "train loss:   0.809396\n",
      "train loss:   0.809910\n",
      "train loss:   1.484985\n",
      "train loss:   1.440993\n",
      "train loss:   0.803002\n",
      "train loss:   1.154893\n",
      "train loss:   0.968246\n",
      "train loss:   1.095045\n",
      "train loss:   0.847353\n",
      "train loss:   1.028912\n",
      "train loss:   1.333526\n",
      "train loss:   0.867844\n",
      "train loss:   0.727762\n",
      "train loss:   0.845462\n",
      "train loss:   1.280235\n",
      "train loss:   1.289585\n",
      "train loss:   1.106494\n",
      "train loss:   0.715941\n",
      "train loss:   0.849396\n",
      "train loss:   0.787955\n",
      "train loss:   0.753324\n",
      "train loss:   0.631821\n",
      "train loss:   0.981629\n",
      "train loss:   1.252908\n",
      "train loss:   0.767804\n",
      "train loss:   0.881312\n",
      "train loss:   0.748594\n",
      "train loss:   0.874468\n",
      "train loss:   0.831049\n",
      "train loss:   0.688826\n",
      "train loss:   0.890625\n",
      "train loss:   1.010333\n",
      "train loss:   0.864273\n",
      "train loss:   0.733080\n",
      "train loss:   1.248978\n",
      "train loss:   1.098997\n",
      "train loss:   0.748998\n",
      "train loss:   0.659532\n",
      "train loss:   0.935480\n",
      "train loss:   0.824756\n",
      "train loss:   0.882269\n",
      "train loss:   1.181302\n",
      "train loss:   0.615820\n",
      "train loss:   1.056943\n",
      "train loss:   1.062002\n",
      "train loss:   0.932406\n",
      "train loss:   0.941555\n",
      "########### epoch 134 ###########\n",
      "########### loop 25050 ###########\n",
      "test loss:   0.130154   test accuracy:   0.958333\n",
      "########### loop 25050 ###########\n",
      "train loss:   0.788582\n",
      "train loss:   0.738833\n",
      "train loss:   0.802151\n",
      "train loss:   1.202573\n",
      "train loss:   0.683815\n",
      "train loss:   1.098027\n",
      "train loss:   0.514793\n",
      "train loss:   1.098095\n",
      "train loss:   1.040807\n",
      "train loss:   1.183258\n",
      "train loss:   1.086344\n",
      "train loss:   0.983756\n",
      "train loss:   1.236319\n",
      "train loss:   0.757528\n",
      "train loss:   0.793961\n",
      "train loss:   0.801939\n",
      "train loss:   0.692002\n",
      "train loss:   1.081404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.945994\n",
      "train loss:   0.793783\n",
      "train loss:   1.189950\n",
      "train loss:   0.649581\n",
      "train loss:   0.679430\n",
      "train loss:   0.977068\n",
      "train loss:   1.130350\n",
      "train loss:   1.308395\n",
      "train loss:   1.247365\n",
      "train loss:   1.039517\n",
      "train loss:   0.708590\n",
      "train loss:   0.993779\n",
      "train loss:   1.137789\n",
      "train loss:   0.977062\n",
      "train loss:   1.009874\n",
      "train loss:   1.161211\n",
      "train loss:   0.712101\n",
      "train loss:   1.207969\n",
      "train loss:   0.992833\n",
      "train loss:   1.111836\n",
      "train loss:   1.028218\n",
      "train loss:   0.941604\n",
      "train loss:   1.081000\n",
      "train loss:   0.898233\n",
      "train loss:   0.933735\n",
      "train loss:   0.782485\n",
      "train loss:   1.307239\n",
      "train loss:   1.012672\n",
      "train loss:   1.054471\n",
      "train loss:   1.333019\n",
      "train loss:   0.866275\n",
      "train loss:   0.817041\n",
      "########### epoch 134 ###########\n",
      "########### loop 25100 ###########\n",
      "test loss:   0.188373   test accuracy:   0.958333\n",
      "########### loop 25100 ###########\n",
      "train loss:   1.022135\n",
      "train loss:   1.279410\n",
      "train loss:   0.783479\n",
      "train loss:   1.059470\n",
      "train loss:   0.756565\n",
      "train loss:   0.900031\n",
      "train loss:   0.984183\n",
      "train loss:   1.261522\n",
      "train loss:   0.663421\n",
      "train loss:   0.894637\n",
      "train loss:   1.013898\n",
      "train loss:   1.139499\n",
      "train loss:   0.902436\n",
      "train loss:   1.125084\n",
      "train loss:   0.730095\n",
      "train loss:   0.750129\n",
      "train loss:   1.020024\n",
      "train loss:   0.605395\n",
      "train loss:   0.680467\n",
      "train loss:   0.871106\n",
      "train loss:   0.917535\n",
      "train loss:   0.972603\n",
      "train loss:   0.878690\n",
      "train loss:   0.628949\n",
      "train loss:   0.944391\n",
      "train loss:   1.308490\n",
      "train loss:   1.092952\n",
      "train loss:   1.064030\n",
      "train loss:   0.635768\n",
      "train loss:   0.776369\n",
      "train loss:   1.045642\n",
      "train loss:   1.071325\n",
      "train loss:   0.835686\n",
      "train loss:   1.236074\n",
      "train loss:   0.946429\n",
      "train loss:   0.712981\n",
      "train loss:   0.930556\n",
      "train loss:   0.942662\n",
      "train loss:   1.043529\n",
      "train loss:   0.734827\n",
      "train loss:   0.801121\n",
      "train loss:   0.839989\n",
      "train loss:   0.727033\n",
      "train loss:   0.896676\n",
      "train loss:   1.261498\n",
      "train loss:   0.747114\n",
      "train loss:   0.930468\n",
      "train loss:   0.891617\n",
      "train loss:   0.737890\n",
      "train loss:   0.716611\n",
      "########### epoch 134 ###########\n",
      "########### loop 25150 ###########\n",
      "test loss:   0.155227   test accuracy:   0.958333\n",
      "########### loop 25150 ###########\n",
      "train loss:   1.047244\n",
      "train loss:   0.770564\n",
      "train loss:   1.146131\n",
      "train loss:   0.833951\n",
      "train loss:   0.736870\n",
      "train loss:   1.080882\n",
      "train loss:   1.125587\n",
      "train loss:   0.841687\n",
      "train loss:   0.761075\n",
      "train loss:   1.199552\n",
      "train loss:   0.683862\n",
      "train loss:   1.073979\n",
      "train loss:   0.641908\n",
      "train loss:   0.884916\n",
      "train loss:   0.966711\n",
      "train loss:   0.651840\n",
      "train loss:   1.135143\n",
      "train loss:   0.883371\n",
      "train loss:   0.710836\n",
      "train loss:   0.953012\n",
      "train loss:   0.979008\n",
      "train loss:   0.661024\n",
      "train loss:   1.126712\n",
      "train loss:   1.188763\n",
      "train loss:   1.007499\n",
      "train loss:   1.042226\n",
      "train loss:   0.932923\n",
      "train loss:   0.988228\n",
      "train loss:   0.927006\n",
      "train loss:   1.059410\n",
      "train loss:   1.214634\n",
      "train loss:   0.851966\n",
      "train loss:   1.114645\n",
      "train loss:   0.833855\n",
      "train loss:   0.860203\n",
      "train loss:   1.263613\n",
      "train loss:   1.000455\n",
      "train loss:   0.824811\n",
      "train loss:   0.655622\n",
      "train loss:   0.958362\n",
      "train loss:   1.144249\n",
      "train loss:   0.810408\n",
      "train loss:   1.017833\n",
      "train loss:   0.788816\n",
      "train loss:   0.870534\n",
      "train loss:   0.964344\n",
      "train loss:   1.280269\n",
      "train loss:   0.659410\n",
      "train loss:   1.028841\n",
      "train loss:   1.047893\n",
      "########### epoch 135 ###########\n",
      "########### loop 25200 ###########\n",
      "test loss:   0.235022   test accuracy:   0.916667\n",
      "########### loop 25200 ###########\n",
      "train loss:   1.105775\n",
      "train loss:   0.869804\n",
      "train loss:   1.002575\n",
      "train loss:   1.023694\n",
      "train loss:   1.426617\n",
      "train loss:   0.741615\n",
      "train loss:   1.169359\n",
      "train loss:   0.826972\n",
      "train loss:   0.654776\n",
      "train loss:   0.641633\n",
      "train loss:   0.954592\n",
      "train loss:   0.906160\n",
      "train loss:   0.829615\n",
      "train loss:   0.862808\n",
      "train loss:   1.135545\n",
      "train loss:   0.987761\n",
      "train loss:   1.196798\n",
      "train loss:   1.005073\n",
      "train loss:   1.396678\n",
      "train loss:   0.675753\n",
      "train loss:   0.874297\n",
      "train loss:   0.840161\n",
      "train loss:   0.894176\n",
      "train loss:   0.877441\n",
      "train loss:   0.832444\n",
      "train loss:   1.215666\n",
      "train loss:   0.654983\n",
      "train loss:   0.996610\n",
      "train loss:   1.046468\n",
      "train loss:   1.387188\n",
      "train loss:   0.254718\n",
      "train loss:   0.914706\n",
      "train loss:   1.155999\n",
      "train loss:   0.494297\n",
      "train loss:   0.784852\n",
      "train loss:   0.835365\n",
      "train loss:   1.098078\n",
      "train loss:   1.030226\n",
      "train loss:   0.900659\n",
      "train loss:   1.147949\n",
      "train loss:   0.903596\n",
      "train loss:   0.758037\n",
      "train loss:   0.952428\n",
      "train loss:   0.921149\n",
      "train loss:   0.600555\n",
      "train loss:   1.099596\n",
      "train loss:   1.011498\n",
      "train loss:   0.646186\n",
      "train loss:   0.911297\n",
      "train loss:   0.776561\n",
      "########### epoch 135 ###########\n",
      "########### loop 25250 ###########\n",
      "test loss:   0.236278   test accuracy:   0.958333\n",
      "########### loop 25250 ###########\n",
      "train loss:   1.027281\n",
      "train loss:   1.115262\n",
      "train loss:   1.159334\n",
      "train loss:   1.268773\n",
      "train loss:   1.135744\n",
      "train loss:   1.007941\n",
      "train loss:   1.005738\n",
      "train loss:   0.856632\n",
      "train loss:   0.959262\n",
      "train loss:   0.858444\n",
      "train loss:   0.994024\n",
      "train loss:   0.857923\n",
      "train loss:   0.810990\n",
      "train loss:   0.755117\n",
      "train loss:   0.860737\n",
      "train loss:   0.826837\n",
      "train loss:   0.864593\n",
      "train loss:   0.888084\n",
      "train loss:   1.063367\n",
      "train loss:   1.082435\n",
      "train loss:   0.675852\n",
      "train loss:   0.818858\n",
      "train loss:   0.721503\n",
      "train loss:   1.049444\n",
      "train loss:   1.122270\n",
      "train loss:   1.324510\n",
      "train loss:   1.072121\n",
      "train loss:   0.875868\n",
      "train loss:   0.640278\n",
      "train loss:   0.692814\n",
      "train loss:   0.928860\n",
      "train loss:   1.448915\n",
      "train loss:   0.805355\n",
      "train loss:   1.427236\n",
      "train loss:   0.780778\n",
      "train loss:   0.783088\n",
      "train loss:   0.885854\n",
      "train loss:   1.092499\n",
      "train loss:   0.575271\n",
      "train loss:   0.756248\n",
      "train loss:   0.806704\n",
      "train loss:   0.850678\n",
      "train loss:   0.816458\n",
      "train loss:   1.033992\n",
      "train loss:   0.915534\n",
      "train loss:   1.083042\n",
      "train loss:   0.924299\n",
      "train loss:   1.168338\n",
      "train loss:   1.096075\n",
      "train loss:   0.651618\n",
      "########### epoch 135 ###########\n",
      "########### loop 25300 ###########\n",
      "test loss:   0.205799   test accuracy:   0.916667\n",
      "########### loop 25300 ###########\n",
      "train loss:   1.147332\n",
      "train loss:   1.144377\n",
      "train loss:   1.029430\n",
      "train loss:   0.804743\n",
      "train loss:   0.788462\n",
      "train loss:   1.002310\n",
      "train loss:   0.727888\n",
      "train loss:   0.996797\n",
      "train loss:   0.900059\n",
      "train loss:   1.018414\n",
      "train loss:   1.231745\n",
      "train loss:   1.109850\n",
      "train loss:   0.655973\n",
      "train loss:   0.658537\n",
      "train loss:   0.689621\n",
      "train loss:   0.854753\n",
      "train loss:   1.087424\n",
      "train loss:   1.206625\n",
      "train loss:   1.256948\n",
      "train loss:   0.999367\n",
      "train loss:   0.878899\n",
      "train loss:   1.213154\n",
      "train loss:   1.251057\n",
      "train loss:   1.072526\n",
      "train loss:   0.746048\n",
      "train loss:   0.809759\n",
      "train loss:   0.961137\n",
      "train loss:   1.042224\n",
      "train loss:   0.718361\n",
      "train loss:   1.347765\n",
      "train loss:   0.983345\n",
      "train loss:   0.849996\n",
      "train loss:   0.978552\n",
      "train loss:   1.336805\n",
      "train loss:   0.833360\n",
      "train loss:   1.357476\n",
      "train loss:   0.982657\n",
      "train loss:   0.758909\n",
      "train loss:   0.774554\n",
      "train loss:   1.061346\n",
      "train loss:   0.626744\n",
      "train loss:   0.476863\n",
      "train loss:   1.021892\n",
      "train loss:   0.867358\n",
      "train loss:   0.871016\n",
      "train loss:   0.858630\n",
      "train loss:   0.566432\n",
      "train loss:   0.955915\n",
      "train loss:   1.133423\n",
      "train loss:   1.058459\n",
      "########### epoch 135 ###########\n",
      "########### loop 25350 ###########\n",
      "test loss:   0.182820   test accuracy:   0.958333\n",
      "########### loop 25350 ###########\n",
      "train loss:   0.750123\n",
      "train loss:   1.289031\n",
      "train loss:   0.988446\n",
      "train loss:   0.777976\n",
      "train loss:   0.533532\n",
      "train loss:   1.017789\n",
      "train loss:   1.398060\n",
      "train loss:   0.756089\n",
      "train loss:   0.919288\n",
      "train loss:   0.801423\n",
      "train loss:   1.279146\n",
      "train loss:   1.384700\n",
      "train loss:   0.841539\n",
      "train loss:   1.050905\n",
      "train loss:   1.031795\n",
      "train loss:   0.765587\n",
      "train loss:   0.596060\n",
      "train loss:   0.927538\n",
      "train loss:   0.877650\n",
      "train loss:   1.033527\n",
      "train loss:   1.137750\n",
      "train loss:   0.949624\n",
      "train loss:   0.929465\n",
      "train loss:   1.015675\n",
      "train loss:   0.789357\n",
      "train loss:   0.710191\n",
      "train loss:   1.057940\n",
      "train loss:   0.943953\n",
      "train loss:   0.981119\n",
      "train loss:   0.975213\n",
      "train loss:   0.727005\n",
      "train loss:   0.888188\n",
      "train loss:   0.908188\n",
      "train loss:   0.853879\n",
      "train loss:   0.960107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.145943\n",
      "train loss:   0.851214\n",
      "train loss:   0.841423\n",
      "train loss:   1.024623\n",
      "train loss:   1.276721\n",
      "train loss:   0.788841\n",
      "train loss:   0.921847\n",
      "train loss:   1.145867\n",
      "train loss:   0.727378\n",
      "train loss:   0.710961\n",
      "train loss:   1.236553\n",
      "train loss:   0.945786\n",
      "train loss:   0.874283\n",
      "train loss:   0.836220\n",
      "train loss:   0.830784\n",
      "########### epoch 136 ###########\n",
      "########### loop 25400 ###########\n",
      "test loss:   0.460301   test accuracy:   0.833333\n",
      "########### loop 25400 ###########\n",
      "train loss:   1.182997\n",
      "train loss:   1.371040\n",
      "train loss:   1.423624\n",
      "train loss:   0.764494\n",
      "train loss:   0.971114\n",
      "train loss:   1.047742\n",
      "train loss:   1.010262\n",
      "train loss:   1.176846\n",
      "train loss:   0.903973\n",
      "train loss:   1.051672\n",
      "train loss:   1.284561\n",
      "train loss:   0.863469\n",
      "train loss:   0.847673\n",
      "train loss:   0.789709\n",
      "train loss:   0.863712\n",
      "train loss:   0.864504\n",
      "train loss:   0.879894\n",
      "train loss:   0.741246\n",
      "train loss:   0.813652\n",
      "train loss:   1.395688\n",
      "train loss:   1.056093\n",
      "train loss:   0.800115\n",
      "train loss:   1.257751\n",
      "train loss:   0.943367\n",
      "train loss:   1.091842\n",
      "train loss:   0.777654\n",
      "train loss:   0.796731\n",
      "train loss:   0.915297\n",
      "train loss:   0.942153\n",
      "train loss:   0.969968\n",
      "train loss:   0.986222\n",
      "train loss:   0.894376\n",
      "train loss:   0.810786\n",
      "train loss:   0.712093\n",
      "train loss:   0.709828\n",
      "train loss:   0.936377\n",
      "train loss:   0.733729\n",
      "train loss:   0.995142\n",
      "train loss:   1.045839\n",
      "train loss:   0.979327\n",
      "train loss:   0.960119\n",
      "train loss:   0.913200\n",
      "train loss:   0.847514\n",
      "train loss:   0.811084\n",
      "train loss:   0.944082\n",
      "train loss:   0.734582\n",
      "train loss:   0.966204\n",
      "train loss:   0.778387\n",
      "train loss:   1.000325\n",
      "train loss:   1.031423\n",
      "########### epoch 136 ###########\n",
      "########### loop 25450 ###########\n",
      "test loss:   0.292237   test accuracy:   0.916667\n",
      "########### loop 25450 ###########\n",
      "train loss:   1.184741\n",
      "train loss:   0.634795\n",
      "train loss:   1.040743\n",
      "train loss:   1.169111\n",
      "train loss:   1.021781\n",
      "train loss:   1.062371\n",
      "train loss:   0.900580\n",
      "train loss:   0.921163\n",
      "train loss:   0.688641\n",
      "train loss:   1.171089\n",
      "train loss:   1.032688\n",
      "train loss:   1.054722\n",
      "train loss:   1.254035\n",
      "train loss:   0.985666\n",
      "train loss:   1.526008\n",
      "train loss:   1.346200\n",
      "train loss:   1.013074\n",
      "train loss:   1.332405\n",
      "train loss:   1.055193\n",
      "train loss:   1.205508\n",
      "train loss:   0.650118\n",
      "train loss:   1.045145\n",
      "train loss:   0.732839\n",
      "train loss:   0.888634\n",
      "train loss:   0.841087\n",
      "train loss:   0.995448\n",
      "train loss:   0.699567\n",
      "train loss:   0.956349\n",
      "train loss:   0.816530\n",
      "train loss:   0.979440\n",
      "train loss:   0.874792\n",
      "train loss:   0.916143\n",
      "train loss:   1.130439\n",
      "train loss:   1.163927\n",
      "train loss:   0.672694\n",
      "train loss:   1.025892\n",
      "train loss:   1.328810\n",
      "train loss:   1.020184\n",
      "train loss:   0.866395\n",
      "train loss:   1.128347\n",
      "train loss:   0.791900\n",
      "train loss:   0.826293\n",
      "train loss:   1.023375\n",
      "train loss:   1.237575\n",
      "train loss:   0.929143\n",
      "train loss:   1.130552\n",
      "train loss:   0.974249\n",
      "train loss:   0.899499\n",
      "train loss:   1.317329\n",
      "train loss:   0.911069\n",
      "########### epoch 136 ###########\n",
      "########### loop 25500 ###########\n",
      "test loss:   0.140576   test accuracy:   0.958333\n",
      "########### loop 25500 ###########\n",
      "train loss:   1.070291\n",
      "train loss:   1.058808\n",
      "train loss:   1.248868\n",
      "train loss:   1.212528\n",
      "train loss:   1.206547\n",
      "train loss:   0.887707\n",
      "train loss:   0.874414\n",
      "train loss:   0.659388\n",
      "train loss:   1.093138\n",
      "train loss:   1.074357\n",
      "train loss:   1.108590\n",
      "train loss:   1.628558\n",
      "train loss:   1.070202\n",
      "train loss:   1.107982\n",
      "train loss:   0.835333\n",
      "train loss:   0.878081\n",
      "train loss:   0.786809\n",
      "train loss:   0.739555\n",
      "train loss:   0.964665\n",
      "train loss:   0.646626\n",
      "train loss:   0.983990\n",
      "train loss:   0.731312\n",
      "train loss:   0.948090\n",
      "train loss:   1.163547\n",
      "train loss:   0.800607\n",
      "train loss:   0.780540\n",
      "train loss:   1.149378\n",
      "train loss:   0.915841\n",
      "train loss:   0.957623\n",
      "train loss:   1.013387\n",
      "train loss:   1.005270\n",
      "train loss:   0.578080\n",
      "train loss:   1.205691\n",
      "train loss:   0.899219\n",
      "train loss:   0.721352\n",
      "train loss:   0.773077\n",
      "train loss:   0.819706\n",
      "train loss:   1.208670\n",
      "train loss:   0.758263\n",
      "train loss:   0.970285\n",
      "train loss:   1.086215\n",
      "train loss:   1.013245\n",
      "train loss:   1.176582\n",
      "train loss:   1.088469\n",
      "train loss:   0.798824\n",
      "train loss:   0.823813\n",
      "train loss:   0.781872\n",
      "train loss:   0.959001\n",
      "train loss:   0.957729\n",
      "train loss:   0.943725\n",
      "########### epoch 136 ###########\n",
      "########### loop 25550 ###########\n",
      "test loss:   0.162640   test accuracy:   0.958333\n",
      "########### loop 25550 ###########\n",
      "train loss:   1.143215\n",
      "train loss:   1.470894\n",
      "train loss:   1.173412\n",
      "train loss:   1.104203\n",
      "train loss:   1.225037\n",
      "train loss:   0.652650\n",
      "train loss:   1.155589\n",
      "train loss:   0.660088\n",
      "train loss:   0.928047\n",
      "train loss:   0.899050\n",
      "train loss:   0.893515\n",
      "train loss:   0.837352\n",
      "train loss:   0.833742\n",
      "train loss:   0.887834\n",
      "train loss:   0.842965\n",
      "train loss:   1.301863\n",
      "train loss:   0.911874\n",
      "train loss:   1.032501\n",
      "train loss:   0.742608\n",
      "train loss:   1.189653\n",
      "train loss:   0.980972\n",
      "train loss:   0.957863\n",
      "train loss:   1.119976\n",
      "train loss:   0.942416\n",
      "train loss:   0.898073\n",
      "train loss:   0.841404\n",
      "train loss:   0.595461\n",
      "train loss:   1.023096\n",
      "train loss:   0.725285\n",
      "train loss:   1.195541\n",
      "train loss:   1.116531\n",
      "train loss:   1.293005\n",
      "train loss:   0.863123\n",
      "train loss:   1.058320\n",
      "train loss:   1.395661\n",
      "train loss:   1.104957\n",
      "train loss:   0.891596\n",
      "train loss:   0.647878\n",
      "train loss:   1.123307\n",
      "train loss:   0.911096\n",
      "train loss:   1.092610\n",
      "train loss:   0.708978\n",
      "train loss:   0.868942\n",
      "train loss:   0.651744\n",
      "train loss:   1.211898\n",
      "train loss:   1.195486\n",
      "train loss:   0.978296\n",
      "train loss:   0.831501\n",
      "train loss:   1.250483\n",
      "train loss:   0.749888\n",
      "########### epoch 137 ###########\n",
      "########### loop 25600 ###########\n",
      "test loss:   0.238852   test accuracy:   0.916667\n",
      "########### loop 25600 ###########\n",
      "train loss:   0.808419\n",
      "train loss:   1.047005\n",
      "train loss:   0.751387\n",
      "train loss:   0.895733\n",
      "train loss:   1.099957\n",
      "train loss:   1.495571\n",
      "train loss:   1.081019\n",
      "train loss:   0.723124\n",
      "train loss:   0.980802\n",
      "train loss:   0.712231\n",
      "train loss:   1.123590\n",
      "train loss:   1.013611\n",
      "train loss:   0.994460\n",
      "train loss:   0.996166\n",
      "train loss:   0.659634\n",
      "train loss:   1.255628\n",
      "train loss:   1.061994\n",
      "train loss:   0.842560\n",
      "train loss:   0.881188\n",
      "train loss:   1.027682\n",
      "train loss:   1.006645\n",
      "train loss:   0.990307\n",
      "train loss:   0.977390\n",
      "train loss:   0.794858\n",
      "train loss:   0.988941\n",
      "train loss:   0.912689\n",
      "train loss:   0.810829\n",
      "train loss:   0.902046\n",
      "train loss:   1.157157\n",
      "train loss:   0.939923\n",
      "train loss:   0.981871\n",
      "train loss:   1.441404\n",
      "train loss:   0.848195\n",
      "train loss:   0.823177\n",
      "train loss:   0.907408\n",
      "train loss:   1.213806\n",
      "train loss:   1.100713\n",
      "train loss:   1.023894\n",
      "train loss:   0.929504\n",
      "train loss:   1.044011\n",
      "train loss:   0.799418\n",
      "train loss:   0.875371\n",
      "train loss:   0.704270\n",
      "train loss:   1.127344\n",
      "train loss:   0.717828\n",
      "train loss:   0.821871\n",
      "train loss:   1.175504\n",
      "train loss:   0.913789\n",
      "train loss:   1.165568\n",
      "train loss:   0.979860\n",
      "########### epoch 137 ###########\n",
      "########### loop 25650 ###########\n",
      "test loss:   0.724104   test accuracy:   0.750000\n",
      "########### loop 25650 ###########\n",
      "train loss:   0.757899\n",
      "train loss:   0.609410\n",
      "train loss:   0.923281\n",
      "train loss:   0.943210\n",
      "train loss:   0.583656\n",
      "train loss:   0.818411\n",
      "train loss:   1.103096\n",
      "train loss:   0.858806\n",
      "train loss:   1.159853\n",
      "train loss:   0.914251\n",
      "train loss:   0.796729\n",
      "train loss:   0.995203\n",
      "train loss:   1.006923\n",
      "train loss:   0.733137\n",
      "train loss:   0.988997\n",
      "train loss:   0.960398\n",
      "train loss:   1.211244\n",
      "train loss:   1.059572\n",
      "train loss:   0.933720\n",
      "train loss:   1.220890\n",
      "train loss:   1.126605\n",
      "train loss:   0.998829\n",
      "train loss:   1.054122\n",
      "train loss:   0.933441\n",
      "train loss:   1.503493\n",
      "train loss:   0.983189\n",
      "train loss:   0.851461\n",
      "train loss:   1.330993\n",
      "train loss:   1.169575\n",
      "train loss:   0.911570\n",
      "train loss:   1.126970\n",
      "train loss:   1.104379\n",
      "train loss:   0.862712\n",
      "train loss:   1.029382\n",
      "train loss:   1.361462\n",
      "train loss:   0.938584\n",
      "train loss:   1.052202\n",
      "train loss:   0.675358\n",
      "train loss:   1.027651\n",
      "train loss:   0.491689\n",
      "train loss:   1.346008\n",
      "train loss:   0.888505\n",
      "train loss:   0.876531\n",
      "train loss:   1.101159\n",
      "train loss:   0.894126\n",
      "train loss:   0.803445\n",
      "train loss:   1.382414\n",
      "train loss:   1.530479\n",
      "train loss:   1.279958\n",
      "train loss:   0.865463\n",
      "########### epoch 137 ###########\n",
      "########### loop 25700 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.100078   test accuracy:   1.000000\n",
      "########### loop 25700 ###########\n",
      "train loss:   1.170717\n",
      "train loss:   0.688812\n",
      "train loss:   1.095629\n",
      "train loss:   1.129733\n",
      "train loss:   1.025616\n",
      "train loss:   1.115947\n",
      "train loss:   0.874685\n",
      "train loss:   0.877124\n",
      "train loss:   1.090933\n",
      "train loss:   1.000811\n",
      "train loss:   1.072788\n",
      "train loss:   0.997137\n",
      "train loss:   1.250813\n",
      "train loss:   0.651768\n",
      "train loss:   1.058943\n",
      "train loss:   0.983780\n",
      "train loss:   1.102599\n",
      "train loss:   0.788035\n",
      "train loss:   1.126819\n",
      "train loss:   0.788544\n",
      "train loss:   1.059260\n",
      "train loss:   0.973420\n",
      "train loss:   0.911628\n",
      "train loss:   1.289632\n",
      "train loss:   0.641320\n",
      "train loss:   1.053902\n",
      "train loss:   1.309505\n",
      "train loss:   0.759630\n",
      "train loss:   0.804753\n",
      "train loss:   0.795555\n",
      "train loss:   1.208468\n",
      "train loss:   0.976682\n",
      "train loss:   0.817240\n",
      "train loss:   1.176428\n",
      "train loss:   0.794682\n",
      "train loss:   1.069723\n",
      "train loss:   0.650133\n",
      "train loss:   0.834055\n",
      "train loss:   1.258698\n",
      "train loss:   1.125025\n",
      "train loss:   0.966354\n",
      "train loss:   0.712591\n",
      "train loss:   0.759127\n",
      "train loss:   1.177480\n",
      "train loss:   1.150495\n",
      "train loss:   0.880591\n",
      "train loss:   0.950660\n",
      "train loss:   1.152131\n",
      "train loss:   1.047694\n",
      "train loss:   1.061608\n",
      "########### epoch 137 ###########\n",
      "########### loop 25750 ###########\n",
      "test loss:   0.235490   test accuracy:   0.916667\n",
      "########### loop 25750 ###########\n",
      "train loss:   0.965141\n",
      "train loss:   0.814084\n",
      "train loss:   0.833856\n",
      "train loss:   0.763120\n",
      "train loss:   0.592747\n",
      "train loss:   0.565625\n",
      "train loss:   1.130605\n",
      "train loss:   1.057506\n",
      "train loss:   1.294861\n",
      "train loss:   0.748531\n",
      "train loss:   0.756039\n",
      "train loss:   1.230638\n",
      "train loss:   0.826663\n",
      "train loss:   0.943941\n",
      "train loss:   1.025317\n",
      "train loss:   0.948697\n",
      "train loss:   0.832288\n",
      "train loss:   0.915716\n",
      "train loss:   0.955618\n",
      "train loss:   1.290730\n",
      "train loss:   1.034273\n",
      "train loss:   0.951462\n",
      "train loss:   1.154531\n",
      "train loss:   1.074272\n",
      "train loss:   0.756409\n",
      "train loss:   0.810567\n",
      "train loss:   0.617328\n",
      "train loss:   0.915329\n",
      "train loss:   0.866106\n",
      "train loss:   0.420164\n",
      "train loss:   0.834502\n",
      "train loss:   1.023946\n",
      "train loss:   0.445509\n",
      "train loss:   0.994337\n",
      "train loss:   1.021392\n",
      "train loss:   1.314310\n",
      "train loss:   1.127536\n",
      "train loss:   0.837712\n",
      "train loss:   0.746102\n",
      "train loss:   1.033776\n",
      "train loss:   1.329126\n",
      "train loss:   0.607388\n",
      "train loss:   1.039409\n",
      "train loss:   1.208323\n",
      "train loss:   0.975824\n",
      "train loss:   1.168440\n",
      "train loss:   0.934223\n",
      "train loss:   0.736913\n",
      "train loss:   1.065603\n",
      "train loss:   1.063471\n",
      "########### epoch 138 ###########\n",
      "########### loop 25800 ###########\n",
      "test loss:   0.325936   test accuracy:   0.958333\n",
      "########### loop 25800 ###########\n",
      "train loss:   0.957494\n",
      "train loss:   0.639096\n",
      "train loss:   1.011878\n",
      "train loss:   0.922113\n",
      "train loss:   0.805119\n",
      "train loss:   0.882193\n",
      "train loss:   0.963446\n",
      "train loss:   0.865943\n",
      "train loss:   1.113365\n",
      "train loss:   0.918093\n",
      "train loss:   0.892743\n",
      "train loss:   0.853003\n",
      "train loss:   0.946655\n",
      "train loss:   0.925015\n",
      "train loss:   0.931755\n",
      "train loss:   0.630210\n",
      "train loss:   0.978788\n",
      "train loss:   1.004510\n",
      "train loss:   0.664728\n",
      "train loss:   0.654088\n",
      "train loss:   0.916466\n",
      "train loss:   1.040568\n",
      "train loss:   1.233809\n",
      "train loss:   0.746192\n",
      "train loss:   1.132027\n",
      "train loss:   1.040667\n",
      "train loss:   0.877571\n",
      "train loss:   1.132976\n",
      "train loss:   0.930437\n",
      "train loss:   1.016751\n",
      "train loss:   1.260308\n",
      "train loss:   1.051921\n",
      "train loss:   0.834456\n",
      "train loss:   0.486869\n",
      "train loss:   0.684522\n",
      "train loss:   0.706666\n",
      "train loss:   1.229634\n",
      "train loss:   0.750285\n",
      "train loss:   1.017967\n",
      "train loss:   0.833866\n",
      "train loss:   0.926723\n",
      "train loss:   0.754872\n",
      "train loss:   0.912180\n",
      "train loss:   1.063496\n",
      "train loss:   1.016537\n",
      "train loss:   0.791342\n",
      "train loss:   1.139663\n",
      "train loss:   0.940668\n",
      "train loss:   0.852247\n",
      "train loss:   0.824733\n",
      "########### epoch 138 ###########\n",
      "########### loop 25850 ###########\n",
      "test loss:   0.317418   test accuracy:   0.916667\n",
      "########### loop 25850 ###########\n",
      "train loss:   0.941719\n",
      "train loss:   1.280104\n",
      "train loss:   0.669732\n",
      "train loss:   0.997803\n",
      "train loss:   1.401261\n",
      "train loss:   0.921349\n",
      "train loss:   1.213138\n",
      "train loss:   0.893560\n",
      "train loss:   0.849606\n",
      "train loss:   0.900267\n",
      "train loss:   0.876332\n",
      "train loss:   1.261240\n",
      "train loss:   0.959369\n",
      "train loss:   0.948433\n",
      "train loss:   0.691582\n",
      "train loss:   0.881536\n",
      "train loss:   1.133817\n",
      "train loss:   1.062712\n",
      "train loss:   1.245770\n",
      "train loss:   1.057360\n",
      "train loss:   0.898727\n",
      "train loss:   1.189643\n",
      "train loss:   1.190731\n",
      "train loss:   0.763150\n",
      "train loss:   1.390438\n",
      "train loss:   1.079061\n",
      "train loss:   1.069115\n",
      "train loss:   1.330674\n",
      "train loss:   0.863835\n",
      "train loss:   0.839859\n",
      "train loss:   0.758012\n",
      "train loss:   0.861976\n",
      "train loss:   0.541764\n",
      "train loss:   0.951504\n",
      "train loss:   0.527618\n",
      "train loss:   1.147509\n",
      "train loss:   0.887527\n",
      "train loss:   0.754794\n",
      "train loss:   1.232624\n",
      "train loss:   1.167348\n",
      "train loss:   0.813600\n",
      "train loss:   1.295115\n",
      "train loss:   0.929109\n",
      "train loss:   0.936385\n",
      "train loss:   1.382849\n",
      "train loss:   0.970688\n",
      "train loss:   0.618743\n",
      "train loss:   1.101639\n",
      "train loss:   1.217811\n",
      "train loss:   0.960962\n",
      "########### epoch 138 ###########\n",
      "########### loop 25900 ###########\n",
      "test loss:   0.258745   test accuracy:   0.916667\n",
      "########### loop 25900 ###########\n",
      "train loss:   1.190003\n",
      "train loss:   1.167613\n",
      "train loss:   1.049197\n",
      "train loss:   1.079825\n",
      "train loss:   0.833496\n",
      "train loss:   0.910111\n",
      "train loss:   0.742201\n",
      "train loss:   0.258992\n",
      "train loss:   1.096197\n",
      "train loss:   1.124505\n",
      "train loss:   0.917842\n",
      "train loss:   1.043437\n",
      "train loss:   0.694422\n",
      "train loss:   0.661570\n",
      "train loss:   1.173229\n",
      "train loss:   1.067507\n",
      "train loss:   0.908481\n",
      "train loss:   0.890370\n",
      "train loss:   0.976821\n",
      "train loss:   0.664213\n",
      "train loss:   1.003860\n",
      "train loss:   1.056241\n",
      "train loss:   1.485396\n",
      "train loss:   0.929297\n",
      "train loss:   0.528742\n",
      "train loss:   0.758623\n",
      "train loss:   0.909577\n",
      "train loss:   1.105480\n",
      "train loss:   1.009115\n",
      "train loss:   0.948707\n",
      "train loss:   1.014788\n",
      "train loss:   0.487225\n",
      "train loss:   0.832092\n",
      "train loss:   0.789250\n",
      "train loss:   1.120556\n",
      "train loss:   1.129857\n",
      "train loss:   0.892463\n",
      "train loss:   0.776714\n",
      "train loss:   1.122307\n",
      "train loss:   1.021501\n",
      "train loss:   0.814250\n",
      "train loss:   0.920033\n",
      "train loss:   0.655771\n",
      "train loss:   0.573475\n",
      "train loss:   0.968105\n",
      "train loss:   1.239423\n",
      "train loss:   0.891963\n",
      "train loss:   0.751455\n",
      "train loss:   1.068541\n",
      "train loss:   0.796662\n",
      "########### epoch 139 ###########\n",
      "########### loop 25950 ###########\n",
      "test loss:   0.390349   test accuracy:   0.833333\n",
      "########### loop 25950 ###########\n",
      "train loss:   1.086143\n",
      "train loss:   0.908822\n",
      "train loss:   0.985099\n",
      "train loss:   1.273555\n",
      "train loss:   0.963435\n",
      "train loss:   1.248080\n",
      "train loss:   0.970979\n",
      "train loss:   1.246981\n",
      "train loss:   0.811922\n",
      "train loss:   0.982819\n",
      "train loss:   0.900639\n",
      "train loss:   0.883146\n",
      "train loss:   1.140755\n",
      "train loss:   0.797067\n",
      "train loss:   0.372643\n",
      "train loss:   0.601285\n",
      "train loss:   1.097583\n",
      "train loss:   0.946503\n",
      "train loss:   0.786064\n",
      "train loss:   0.805341\n",
      "train loss:   1.311882\n",
      "train loss:   0.809697\n",
      "train loss:   1.020232\n",
      "train loss:   0.911562\n",
      "train loss:   0.951620\n",
      "train loss:   1.120812\n",
      "train loss:   0.942581\n",
      "train loss:   1.005377\n",
      "train loss:   1.041190\n",
      "train loss:   0.822717\n",
      "train loss:   0.732601\n",
      "train loss:   1.283351\n",
      "train loss:   1.322268\n",
      "train loss:   0.906556\n",
      "train loss:   1.006691\n",
      "train loss:   1.466522\n",
      "train loss:   0.702028\n",
      "train loss:   0.860953\n",
      "train loss:   1.157460\n",
      "train loss:   1.214535\n",
      "train loss:   1.010122\n",
      "train loss:   1.175349\n",
      "train loss:   0.737579\n",
      "train loss:   1.249519\n",
      "train loss:   1.298130\n",
      "train loss:   1.241581\n",
      "train loss:   0.961046\n",
      "train loss:   1.186438\n",
      "train loss:   0.972769\n",
      "train loss:   0.592604\n",
      "########### epoch 139 ###########\n",
      "########### loop 26000 ###########\n",
      "test loss:   0.198608   test accuracy:   0.958333\n",
      "########### loop 26000 ###########\n",
      "train loss:   0.874763\n",
      "train loss:   1.052061\n",
      "train loss:   0.713662\n",
      "train loss:   0.975616\n",
      "train loss:   0.897930\n",
      "train loss:   1.100147\n",
      "train loss:   0.981472\n",
      "train loss:   0.886852\n",
      "train loss:   0.836817\n",
      "train loss:   1.014828\n",
      "train loss:   0.880260\n",
      "train loss:   1.095238\n",
      "train loss:   0.907027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.896057\n",
      "train loss:   1.186479\n",
      "train loss:   0.990722\n",
      "train loss:   0.392969\n",
      "train loss:   1.075869\n",
      "train loss:   0.948616\n",
      "train loss:   1.150100\n",
      "train loss:   1.250020\n",
      "train loss:   1.055624\n",
      "train loss:   1.000824\n",
      "train loss:   0.903561\n",
      "train loss:   0.936519\n",
      "train loss:   1.103935\n",
      "train loss:   1.239050\n",
      "train loss:   0.895839\n",
      "train loss:   0.790249\n",
      "train loss:   1.251173\n",
      "train loss:   0.801663\n",
      "train loss:   0.937193\n",
      "train loss:   0.963924\n",
      "train loss:   0.848848\n",
      "train loss:   1.013595\n",
      "train loss:   1.201138\n",
      "train loss:   0.955476\n",
      "train loss:   1.290701\n",
      "train loss:   0.603422\n",
      "train loss:   0.681250\n",
      "train loss:   1.011196\n",
      "train loss:   0.940605\n",
      "train loss:   0.529603\n",
      "train loss:   0.529438\n",
      "train loss:   0.873897\n",
      "train loss:   0.943731\n",
      "train loss:   1.047316\n",
      "train loss:   0.499825\n",
      "train loss:   0.686406\n",
      "train loss:   1.008261\n",
      "########### epoch 139 ###########\n",
      "########### loop 26050 ###########\n",
      "test loss:   0.310012   test accuracy:   0.916667\n",
      "########### loop 26050 ###########\n",
      "train loss:   0.946209\n",
      "train loss:   1.048084\n",
      "train loss:   0.861030\n",
      "train loss:   0.524121\n",
      "train loss:   0.960834\n",
      "train loss:   1.105149\n",
      "train loss:   0.597331\n",
      "train loss:   1.186197\n",
      "train loss:   1.034401\n",
      "train loss:   0.877563\n",
      "train loss:   0.825147\n",
      "train loss:   0.940557\n",
      "train loss:   1.537697\n",
      "train loss:   1.159469\n",
      "train loss:   1.000176\n",
      "train loss:   0.681234\n",
      "train loss:   0.978729\n",
      "train loss:   0.777170\n",
      "train loss:   1.119146\n",
      "train loss:   1.353263\n",
      "train loss:   1.228561\n",
      "train loss:   0.788302\n",
      "train loss:   0.889648\n",
      "train loss:   0.752854\n",
      "train loss:   1.479413\n",
      "train loss:   0.991156\n",
      "train loss:   1.090205\n",
      "train loss:   1.279095\n",
      "train loss:   0.751082\n",
      "train loss:   1.265769\n",
      "train loss:   0.904805\n",
      "train loss:   0.778102\n",
      "train loss:   0.614040\n",
      "train loss:   1.134361\n",
      "train loss:   1.410876\n",
      "train loss:   0.878338\n",
      "train loss:   0.779603\n",
      "train loss:   0.772045\n",
      "train loss:   1.303178\n",
      "train loss:   0.663392\n",
      "train loss:   1.176618\n",
      "train loss:   0.998547\n",
      "train loss:   1.048976\n",
      "train loss:   0.777778\n",
      "train loss:   0.973846\n",
      "train loss:   1.034427\n",
      "train loss:   0.790795\n",
      "train loss:   0.792924\n",
      "train loss:   0.932366\n",
      "train loss:   1.028310\n",
      "########### epoch 139 ###########\n",
      "########### loop 26100 ###########\n",
      "test loss:   0.270742   test accuracy:   0.875000\n",
      "########### loop 26100 ###########\n",
      "train loss:   0.780360\n",
      "train loss:   0.939981\n",
      "train loss:   0.806332\n",
      "train loss:   0.843064\n",
      "train loss:   1.110321\n",
      "train loss:   0.737246\n",
      "train loss:   1.120944\n",
      "train loss:   0.916636\n",
      "train loss:   1.070475\n",
      "train loss:   1.027650\n",
      "train loss:   1.517929\n",
      "train loss:   1.193811\n",
      "train loss:   0.917263\n",
      "train loss:   1.062065\n",
      "train loss:   0.663873\n",
      "train loss:   1.275273\n",
      "train loss:   0.915617\n",
      "train loss:   0.806108\n",
      "train loss:   0.921308\n",
      "train loss:   0.819912\n",
      "train loss:   0.898360\n",
      "train loss:   1.053862\n",
      "train loss:   0.901128\n",
      "train loss:   0.825120\n",
      "train loss:   0.792363\n",
      "train loss:   0.806838\n",
      "train loss:   0.983208\n",
      "train loss:   1.029857\n",
      "train loss:   0.801047\n",
      "train loss:   1.073567\n",
      "train loss:   0.927618\n",
      "train loss:   0.940724\n",
      "train loss:   0.817527\n",
      "train loss:   1.136016\n",
      "train loss:   0.768575\n",
      "train loss:   1.054724\n",
      "train loss:   1.134843\n",
      "train loss:   1.225110\n",
      "train loss:   1.001770\n",
      "train loss:   1.025582\n",
      "train loss:   0.960614\n",
      "train loss:   0.864073\n",
      "train loss:   1.049383\n",
      "train loss:   1.339424\n",
      "train loss:   1.217414\n",
      "train loss:   0.780366\n",
      "train loss:   0.914290\n",
      "train loss:   0.811330\n",
      "train loss:   0.897071\n",
      "train loss:   0.973811\n",
      "########### epoch 140 ###########\n",
      "########### loop 26150 ###########\n",
      "test loss:   0.274294   test accuracy:   0.875000\n",
      "########### loop 26150 ###########\n",
      "train loss:   0.863497\n",
      "train loss:   1.286201\n",
      "train loss:   0.958677\n",
      "train loss:   0.804944\n",
      "train loss:   0.971525\n",
      "train loss:   1.320475\n",
      "train loss:   1.060916\n",
      "train loss:   0.662825\n",
      "train loss:   1.100556\n",
      "train loss:   0.941028\n",
      "train loss:   1.103152\n",
      "train loss:   0.988003\n",
      "train loss:   0.801494\n",
      "train loss:   0.714625\n",
      "train loss:   0.917474\n",
      "train loss:   0.993140\n",
      "train loss:   1.224101\n",
      "train loss:   0.948418\n",
      "train loss:   1.092874\n",
      "train loss:   0.896136\n",
      "train loss:   0.798642\n",
      "train loss:   1.013870\n",
      "train loss:   1.110901\n",
      "train loss:   1.132124\n",
      "train loss:   1.089309\n",
      "train loss:   0.756377\n",
      "train loss:   1.034135\n",
      "train loss:   0.597417\n",
      "train loss:   0.973013\n",
      "train loss:   0.749789\n",
      "train loss:   0.675808\n",
      "train loss:   0.797308\n",
      "train loss:   1.110806\n",
      "train loss:   1.329664\n",
      "train loss:   0.712147\n",
      "train loss:   0.961406\n",
      "train loss:   1.312833\n",
      "train loss:   1.063987\n",
      "train loss:   1.430152\n",
      "train loss:   0.762568\n",
      "train loss:   0.848555\n",
      "train loss:   1.115563\n",
      "train loss:   0.704685\n",
      "train loss:   1.110826\n",
      "train loss:   0.749900\n",
      "train loss:   0.842722\n",
      "train loss:   1.143095\n",
      "train loss:   1.135036\n",
      "train loss:   0.803713\n",
      "train loss:   1.233735\n",
      "########### epoch 140 ###########\n",
      "########### loop 26200 ###########\n",
      "test loss:   0.211442   test accuracy:   0.958333\n",
      "########### loop 26200 ###########\n",
      "train loss:   0.965667\n",
      "train loss:   0.861778\n",
      "train loss:   0.873430\n",
      "train loss:   0.947034\n",
      "train loss:   0.853936\n",
      "train loss:   1.141558\n",
      "train loss:   0.788030\n",
      "train loss:   0.750032\n",
      "train loss:   1.215567\n",
      "train loss:   1.167680\n",
      "train loss:   0.664795\n",
      "train loss:   0.954607\n",
      "train loss:   0.630188\n",
      "train loss:   0.896752\n",
      "train loss:   0.827730\n",
      "train loss:   0.783864\n",
      "train loss:   0.884187\n",
      "train loss:   0.776896\n",
      "train loss:   0.711371\n",
      "train loss:   1.179150\n",
      "train loss:   0.954776\n",
      "train loss:   0.880123\n",
      "train loss:   0.763434\n",
      "train loss:   1.091853\n",
      "train loss:   1.322878\n",
      "train loss:   0.978555\n",
      "train loss:   1.063945\n",
      "train loss:   1.228770\n",
      "train loss:   1.177167\n",
      "train loss:   0.873755\n",
      "train loss:   1.118229\n",
      "train loss:   0.855936\n",
      "train loss:   0.652673\n",
      "train loss:   1.162994\n",
      "train loss:   0.574015\n",
      "train loss:   1.123106\n",
      "train loss:   0.801596\n",
      "train loss:   0.782948\n",
      "train loss:   1.127619\n",
      "train loss:   1.018697\n",
      "train loss:   0.994120\n",
      "train loss:   0.932895\n",
      "train loss:   1.034436\n",
      "train loss:   0.974873\n",
      "train loss:   1.164535\n",
      "train loss:   1.022969\n",
      "train loss:   0.888250\n",
      "train loss:   0.913805\n",
      "train loss:   0.968826\n",
      "train loss:   0.903006\n",
      "########### epoch 140 ###########\n",
      "########### loop 26250 ###########\n",
      "test loss:   0.181404   test accuracy:   0.958333\n",
      "########### loop 26250 ###########\n",
      "train loss:   0.621797\n",
      "train loss:   0.822371\n",
      "train loss:   1.144876\n",
      "train loss:   1.374374\n",
      "train loss:   1.214549\n",
      "train loss:   1.023361\n",
      "train loss:   0.743748\n",
      "train loss:   1.163973\n",
      "train loss:   0.764611\n",
      "train loss:   1.338935\n",
      "train loss:   1.107276\n",
      "train loss:   0.985417\n",
      "train loss:   1.114047\n",
      "train loss:   1.188676\n",
      "train loss:   0.790344\n",
      "train loss:   0.897283\n",
      "train loss:   1.139034\n",
      "train loss:   0.873398\n",
      "train loss:   1.181355\n",
      "train loss:   0.990236\n",
      "train loss:   1.041446\n",
      "train loss:   0.868822\n",
      "train loss:   0.809339\n",
      "train loss:   0.654251\n",
      "train loss:   1.046936\n",
      "train loss:   0.744911\n",
      "train loss:   0.943064\n",
      "train loss:   0.770142\n",
      "train loss:   0.929993\n",
      "train loss:   1.033991\n",
      "train loss:   1.186888\n",
      "train loss:   0.789583\n",
      "train loss:   0.808708\n",
      "train loss:   0.920695\n",
      "train loss:   1.099838\n",
      "train loss:   0.866016\n",
      "train loss:   1.134155\n",
      "train loss:   1.035387\n",
      "train loss:   1.007009\n",
      "train loss:   0.845064\n",
      "train loss:   0.475678\n",
      "train loss:   1.003480\n",
      "train loss:   1.034157\n",
      "train loss:   1.312182\n",
      "train loss:   0.929221\n",
      "train loss:   0.943066\n",
      "train loss:   1.036229\n",
      "train loss:   1.326957\n",
      "train loss:   1.039786\n",
      "train loss:   1.152384\n",
      "########### epoch 140 ###########\n",
      "########### loop 26300 ###########\n",
      "test loss:   0.141601   test accuracy:   0.958333\n",
      "########### loop 26300 ###########\n",
      "train loss:   0.769897\n",
      "train loss:   0.807822\n",
      "train loss:   0.776973\n",
      "train loss:   0.735693\n",
      "train loss:   0.795358\n",
      "train loss:   0.743575\n",
      "train loss:   0.832938\n",
      "train loss:   1.375418\n",
      "train loss:   1.166157\n",
      "train loss:   1.191068\n",
      "train loss:   0.778885\n",
      "train loss:   1.063264\n",
      "train loss:   1.100544\n",
      "train loss:   1.266724\n",
      "train loss:   0.932752\n",
      "train loss:   0.805207\n",
      "train loss:   1.064221\n",
      "train loss:   0.924358\n",
      "train loss:   0.888628\n",
      "train loss:   1.074436\n",
      "train loss:   1.183266\n",
      "train loss:   0.969721\n",
      "train loss:   1.154141\n",
      "train loss:   1.027947\n",
      "train loss:   1.054454\n",
      "train loss:   0.733455\n",
      "train loss:   1.103407\n",
      "train loss:   0.941344\n",
      "train loss:   0.875748\n",
      "train loss:   0.802623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.110275\n",
      "train loss:   1.250432\n",
      "train loss:   0.873437\n",
      "train loss:   0.899518\n",
      "train loss:   1.028333\n",
      "train loss:   1.119962\n",
      "train loss:   0.977570\n",
      "train loss:   0.868537\n",
      "train loss:   0.851839\n",
      "train loss:   0.866291\n",
      "train loss:   1.263016\n",
      "train loss:   0.600016\n",
      "train loss:   1.112235\n",
      "train loss:   1.108577\n",
      "train loss:   1.059405\n",
      "train loss:   0.737865\n",
      "train loss:   1.083280\n",
      "train loss:   0.744985\n",
      "train loss:   0.747238\n",
      "train loss:   0.636335\n",
      "########### epoch 141 ###########\n",
      "########### loop 26350 ###########\n",
      "test loss:   0.366291   test accuracy:   0.916667\n",
      "########### loop 26350 ###########\n",
      "train loss:   1.050510\n",
      "train loss:   0.808655\n",
      "train loss:   0.958574\n",
      "train loss:   1.146107\n",
      "train loss:   0.834603\n",
      "train loss:   0.850518\n",
      "train loss:   0.964145\n",
      "train loss:   1.113587\n",
      "train loss:   0.731084\n",
      "train loss:   0.744149\n",
      "train loss:   1.376199\n",
      "train loss:   0.894782\n",
      "train loss:   1.112947\n",
      "train loss:   1.104855\n",
      "train loss:   0.835318\n",
      "train loss:   1.051727\n",
      "train loss:   0.619352\n",
      "train loss:   0.899061\n",
      "train loss:   1.171865\n",
      "train loss:   0.908414\n",
      "train loss:   1.016340\n",
      "train loss:   0.979381\n",
      "train loss:   0.963240\n",
      "train loss:   0.974530\n",
      "train loss:   1.056630\n",
      "train loss:   1.006619\n",
      "train loss:   0.955407\n",
      "train loss:   0.878969\n",
      "train loss:   1.200943\n",
      "train loss:   0.538005\n",
      "train loss:   0.893702\n",
      "train loss:   0.479571\n",
      "train loss:   1.079805\n",
      "train loss:   1.122443\n",
      "train loss:   0.842191\n",
      "train loss:   0.689346\n",
      "train loss:   1.351886\n",
      "train loss:   1.231490\n",
      "train loss:   1.150714\n",
      "train loss:   1.021996\n",
      "train loss:   1.336790\n",
      "train loss:   1.150900\n",
      "train loss:   0.991983\n",
      "train loss:   1.285200\n",
      "train loss:   1.255101\n",
      "train loss:   1.118931\n",
      "train loss:   1.134519\n",
      "train loss:   1.062126\n",
      "train loss:   0.731177\n",
      "train loss:   1.171710\n",
      "########### epoch 141 ###########\n",
      "########### loop 26400 ###########\n",
      "test loss:   0.339078   test accuracy:   0.875000\n",
      "########### loop 26400 ###########\n",
      "train loss:   0.953058\n",
      "train loss:   1.287980\n",
      "train loss:   0.685525\n",
      "train loss:   0.770016\n",
      "train loss:   1.224300\n",
      "train loss:   1.029755\n",
      "train loss:   0.668927\n",
      "train loss:   0.837327\n",
      "train loss:   1.169070\n",
      "train loss:   0.931770\n",
      "train loss:   1.009572\n",
      "train loss:   0.900587\n",
      "train loss:   1.309378\n",
      "train loss:   1.122685\n",
      "train loss:   1.170817\n",
      "train loss:   0.965707\n",
      "train loss:   0.827208\n",
      "train loss:   0.665056\n",
      "train loss:   0.892841\n",
      "train loss:   0.874898\n",
      "train loss:   0.832299\n",
      "train loss:   1.118310\n",
      "train loss:   1.236216\n",
      "train loss:   0.880860\n",
      "train loss:   0.885857\n",
      "train loss:   0.900617\n",
      "train loss:   0.761202\n",
      "train loss:   0.709442\n",
      "train loss:   1.007921\n",
      "train loss:   1.062738\n",
      "train loss:   1.220157\n",
      "train loss:   0.798223\n",
      "train loss:   1.131533\n",
      "train loss:   1.079619\n",
      "train loss:   0.809682\n",
      "train loss:   1.091976\n",
      "train loss:   0.764627\n",
      "train loss:   1.039523\n",
      "train loss:   0.990534\n",
      "train loss:   1.291794\n",
      "train loss:   0.753494\n",
      "train loss:   1.295005\n",
      "train loss:   1.146554\n",
      "train loss:   1.016182\n",
      "train loss:   0.717764\n",
      "train loss:   0.857326\n",
      "train loss:   0.976452\n",
      "train loss:   0.861690\n",
      "train loss:   0.827298\n",
      "train loss:   0.798849\n",
      "########### epoch 141 ###########\n",
      "########### loop 26450 ###########\n",
      "test loss:   0.135272   test accuracy:   1.000000\n",
      "########### loop 26450 ###########\n",
      "train loss:   0.703378\n",
      "train loss:   1.188561\n",
      "train loss:   0.884634\n",
      "train loss:   0.912112\n",
      "train loss:   1.285906\n",
      "train loss:   0.442836\n",
      "train loss:   1.272965\n",
      "train loss:   0.821453\n",
      "train loss:   0.891964\n",
      "train loss:   0.905717\n",
      "train loss:   1.031618\n",
      "train loss:   0.693709\n",
      "train loss:   0.691895\n",
      "train loss:   0.870102\n",
      "train loss:   1.057167\n",
      "train loss:   0.836841\n",
      "train loss:   1.193805\n",
      "train loss:   0.867611\n",
      "train loss:   0.718333\n",
      "train loss:   0.738607\n",
      "train loss:   0.908071\n",
      "train loss:   1.146772\n",
      "train loss:   0.709706\n",
      "train loss:   0.767066\n",
      "train loss:   0.798850\n",
      "train loss:   0.891560\n",
      "train loss:   0.986961\n",
      "train loss:   1.114711\n",
      "train loss:   0.860188\n",
      "train loss:   1.159901\n",
      "train loss:   0.854115\n",
      "train loss:   0.850359\n",
      "train loss:   1.331247\n",
      "train loss:   0.765800\n",
      "train loss:   0.653799\n",
      "train loss:   1.154774\n",
      "train loss:   0.703264\n",
      "train loss:   0.713785\n",
      "train loss:   0.889665\n",
      "train loss:   0.797039\n",
      "train loss:   0.896552\n",
      "train loss:   1.082721\n",
      "train loss:   1.180335\n",
      "train loss:   1.118764\n",
      "train loss:   1.172639\n",
      "train loss:   1.113561\n",
      "train loss:   0.867885\n",
      "train loss:   0.772924\n",
      "train loss:   1.123732\n",
      "train loss:   0.929774\n",
      "########### epoch 141 ###########\n",
      "########### loop 26500 ###########\n",
      "test loss:   0.228720   test accuracy:   0.916667\n",
      "########### loop 26500 ###########\n",
      "train loss:   1.346797\n",
      "train loss:   1.013285\n",
      "train loss:   0.712552\n",
      "train loss:   0.566070\n",
      "train loss:   0.964748\n",
      "train loss:   1.204015\n",
      "train loss:   1.026474\n",
      "train loss:   1.234302\n",
      "train loss:   1.261630\n",
      "train loss:   1.112380\n",
      "train loss:   1.270315\n",
      "train loss:   0.979399\n",
      "train loss:   0.642191\n",
      "train loss:   0.746215\n",
      "train loss:   1.003564\n",
      "train loss:   0.831735\n",
      "train loss:   1.063059\n",
      "train loss:   0.463976\n",
      "train loss:   1.097525\n",
      "train loss:   0.959140\n",
      "train loss:   0.964031\n",
      "train loss:   1.335567\n",
      "train loss:   0.992045\n",
      "train loss:   1.099547\n",
      "train loss:   0.754683\n",
      "train loss:   1.092496\n",
      "train loss:   0.754452\n",
      "train loss:   0.829755\n",
      "train loss:   0.975906\n",
      "train loss:   0.949257\n",
      "train loss:   0.916823\n",
      "train loss:   1.381452\n",
      "train loss:   1.080641\n",
      "train loss:   0.935916\n",
      "train loss:   0.775348\n",
      "train loss:   0.869552\n",
      "train loss:   1.174215\n",
      "train loss:   1.214103\n",
      "train loss:   1.016101\n",
      "train loss:   1.282225\n",
      "train loss:   0.835063\n",
      "train loss:   0.981711\n",
      "train loss:   1.273766\n",
      "train loss:   0.907740\n",
      "train loss:   1.050229\n",
      "train loss:   1.038347\n",
      "train loss:   1.087922\n",
      "train loss:   1.079022\n",
      "train loss:   0.959914\n",
      "train loss:   1.113304\n",
      "########### epoch 142 ###########\n",
      "########### loop 26550 ###########\n",
      "test loss:   0.244612   test accuracy:   0.916667\n",
      "########### loop 26550 ###########\n",
      "train loss:   0.925652\n",
      "train loss:   0.910982\n",
      "train loss:   0.834503\n",
      "train loss:   0.640550\n",
      "train loss:   0.984329\n",
      "train loss:   0.940896\n",
      "train loss:   0.980456\n",
      "train loss:   0.897095\n",
      "train loss:   1.164596\n",
      "train loss:   1.024382\n",
      "train loss:   1.291521\n",
      "train loss:   0.850319\n",
      "train loss:   1.269098\n",
      "train loss:   1.048182\n",
      "train loss:   0.840379\n",
      "train loss:   0.582573\n",
      "train loss:   0.903476\n",
      "train loss:   1.274830\n",
      "train loss:   0.773336\n",
      "train loss:   1.156649\n",
      "train loss:   1.057515\n",
      "train loss:   0.893430\n",
      "train loss:   0.979296\n",
      "train loss:   1.176280\n",
      "train loss:   1.163846\n",
      "train loss:   1.276915\n",
      "train loss:   0.876825\n",
      "train loss:   0.839902\n",
      "train loss:   0.653420\n",
      "train loss:   1.163023\n",
      "train loss:   0.737862\n",
      "train loss:   1.250216\n",
      "train loss:   0.959813\n",
      "train loss:   0.816109\n",
      "train loss:   0.857613\n",
      "train loss:   0.626847\n",
      "train loss:   1.010450\n",
      "train loss:   0.811064\n",
      "train loss:   0.781933\n",
      "train loss:   0.930924\n",
      "train loss:   1.077105\n",
      "train loss:   0.870501\n",
      "train loss:   0.956435\n",
      "train loss:   0.967804\n",
      "train loss:   0.708266\n",
      "train loss:   1.143784\n",
      "train loss:   0.962895\n",
      "train loss:   0.565774\n",
      "train loss:   1.065058\n",
      "train loss:   1.010284\n",
      "########### epoch 142 ###########\n",
      "########### loop 26600 ###########\n",
      "test loss:   0.249814   test accuracy:   0.958333\n",
      "########### loop 26600 ###########\n",
      "train loss:   0.989789\n",
      "train loss:   0.773248\n",
      "train loss:   0.840238\n",
      "train loss:   1.081639\n",
      "train loss:   0.947621\n",
      "train loss:   1.203757\n",
      "train loss:   0.859904\n",
      "train loss:   1.015684\n",
      "train loss:   0.903189\n",
      "train loss:   1.148774\n",
      "train loss:   0.747850\n",
      "train loss:   1.063588\n",
      "train loss:   0.607614\n",
      "train loss:   1.166518\n",
      "train loss:   0.734569\n",
      "train loss:   0.724407\n",
      "train loss:   1.111611\n",
      "train loss:   1.048688\n",
      "train loss:   0.981290\n",
      "train loss:   0.869324\n",
      "train loss:   0.923148\n",
      "train loss:   0.968043\n",
      "train loss:   0.981479\n",
      "train loss:   1.254288\n",
      "train loss:   0.453467\n",
      "train loss:   0.971987\n",
      "train loss:   0.952496\n",
      "train loss:   1.260587\n",
      "train loss:   0.796945\n",
      "train loss:   0.828455\n",
      "train loss:   1.035666\n",
      "train loss:   0.772434\n",
      "train loss:   1.281670\n",
      "train loss:   0.993536\n",
      "train loss:   0.456209\n",
      "train loss:   1.096866\n",
      "train loss:   1.130389\n",
      "train loss:   0.666668\n",
      "train loss:   0.667526\n",
      "train loss:   0.998556\n",
      "train loss:   1.144905\n",
      "train loss:   0.468495\n",
      "train loss:   0.957675\n",
      "train loss:   1.136178\n",
      "train loss:   0.993126\n",
      "train loss:   1.273826\n",
      "train loss:   0.918674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.793604\n",
      "train loss:   0.937366\n",
      "train loss:   0.824686\n",
      "########### epoch 142 ###########\n",
      "########### loop 26650 ###########\n",
      "test loss:   0.177068   test accuracy:   0.958333\n",
      "########### loop 26650 ###########\n",
      "train loss:   1.077119\n",
      "train loss:   1.067787\n",
      "train loss:   0.741245\n",
      "train loss:   1.044110\n",
      "train loss:   0.931038\n",
      "train loss:   0.774590\n",
      "train loss:   1.026997\n",
      "train loss:   0.991783\n",
      "train loss:   0.936426\n",
      "train loss:   0.638935\n",
      "train loss:   1.426442\n",
      "train loss:   1.299303\n",
      "train loss:   0.863633\n",
      "train loss:   0.838186\n",
      "train loss:   1.325324\n",
      "train loss:   0.879062\n",
      "train loss:   0.580855\n",
      "train loss:   1.010950\n",
      "train loss:   1.449602\n",
      "train loss:   0.991207\n",
      "train loss:   1.085423\n",
      "train loss:   1.045017\n",
      "train loss:   1.215384\n",
      "train loss:   1.019223\n",
      "train loss:   1.072332\n",
      "train loss:   1.079441\n",
      "train loss:   1.055361\n",
      "train loss:   0.734818\n",
      "train loss:   0.704583\n",
      "train loss:   0.928778\n",
      "train loss:   1.362150\n",
      "train loss:   0.787212\n",
      "train loss:   0.947733\n",
      "train loss:   0.939751\n",
      "train loss:   0.643529\n",
      "train loss:   0.704660\n",
      "train loss:   0.902328\n",
      "train loss:   1.024824\n",
      "train loss:   0.962757\n",
      "train loss:   0.950846\n",
      "train loss:   1.094806\n",
      "train loss:   0.967232\n",
      "train loss:   1.085864\n",
      "train loss:   0.974416\n",
      "train loss:   0.899023\n",
      "train loss:   1.139872\n",
      "train loss:   0.953137\n",
      "train loss:   0.841608\n",
      "train loss:   1.173767\n",
      "train loss:   1.095971\n",
      "########### epoch 143 ###########\n",
      "########### loop 26700 ###########\n",
      "test loss:   0.201716   test accuracy:   0.958333\n",
      "########### loop 26700 ###########\n",
      "train loss:   0.900355\n",
      "train loss:   1.012902\n",
      "train loss:   0.823234\n",
      "train loss:   1.143326\n",
      "train loss:   0.879360\n",
      "train loss:   1.390487\n",
      "train loss:   0.663723\n",
      "train loss:   0.711325\n",
      "train loss:   0.997309\n",
      "train loss:   0.806077\n",
      "train loss:   1.467075\n",
      "train loss:   1.004621\n",
      "train loss:   1.104264\n",
      "train loss:   1.012028\n",
      "train loss:   0.822074\n",
      "train loss:   1.109291\n",
      "train loss:   1.028493\n",
      "train loss:   1.144753\n",
      "train loss:   0.955468\n",
      "train loss:   0.936369\n",
      "train loss:   1.210808\n",
      "train loss:   0.924161\n",
      "train loss:   0.917605\n",
      "train loss:   1.109424\n",
      "train loss:   0.635688\n",
      "train loss:   1.253514\n",
      "train loss:   0.850832\n",
      "train loss:   1.175583\n",
      "train loss:   0.806568\n",
      "train loss:   0.792915\n",
      "train loss:   0.766719\n",
      "train loss:   0.919109\n",
      "train loss:   0.775437\n",
      "train loss:   0.862974\n",
      "train loss:   1.106120\n",
      "train loss:   0.926888\n",
      "train loss:   0.830653\n",
      "train loss:   0.743229\n",
      "train loss:   0.988357\n",
      "train loss:   0.900587\n",
      "train loss:   1.026930\n",
      "train loss:   0.843599\n",
      "train loss:   1.174382\n",
      "train loss:   1.236449\n",
      "train loss:   1.153305\n",
      "train loss:   0.983053\n",
      "train loss:   0.956923\n",
      "train loss:   1.029660\n",
      "train loss:   1.135562\n",
      "train loss:   0.806518\n",
      "########### epoch 143 ###########\n",
      "########### loop 26750 ###########\n",
      "test loss:   0.226946   test accuracy:   0.958333\n",
      "########### loop 26750 ###########\n",
      "train loss:   0.963324\n",
      "train loss:   1.245667\n",
      "train loss:   1.226417\n",
      "train loss:   1.084365\n",
      "train loss:   0.927019\n",
      "train loss:   0.970700\n",
      "train loss:   0.571919\n",
      "train loss:   0.986412\n",
      "train loss:   1.111385\n",
      "train loss:   1.201858\n",
      "train loss:   1.136720\n",
      "train loss:   1.312103\n",
      "train loss:   0.861461\n",
      "train loss:   0.717318\n",
      "train loss:   0.872610\n",
      "train loss:   0.588210\n",
      "train loss:   1.012501\n",
      "train loss:   0.837929\n",
      "train loss:   0.623169\n",
      "train loss:   1.221443\n",
      "train loss:   1.087146\n",
      "train loss:   0.855344\n",
      "train loss:   0.644104\n",
      "train loss:   1.083224\n",
      "train loss:   0.730295\n",
      "train loss:   0.933445\n",
      "train loss:   0.976792\n",
      "train loss:   1.234130\n",
      "train loss:   1.153641\n",
      "train loss:   0.973319\n",
      "train loss:   0.923128\n",
      "train loss:   0.978153\n",
      "train loss:   1.081104\n",
      "train loss:   0.820851\n",
      "train loss:   1.102699\n",
      "train loss:   0.771988\n",
      "train loss:   0.733379\n",
      "train loss:   0.926948\n",
      "train loss:   1.114464\n",
      "train loss:   0.961209\n",
      "train loss:   0.797730\n",
      "train loss:   1.146047\n",
      "train loss:   0.847911\n",
      "train loss:   1.054109\n",
      "train loss:   1.180801\n",
      "train loss:   0.991174\n",
      "train loss:   1.154304\n",
      "train loss:   1.149588\n",
      "train loss:   0.980937\n",
      "train loss:   0.829693\n",
      "########### epoch 143 ###########\n",
      "########### loop 26800 ###########\n",
      "test loss:   0.209204   test accuracy:   0.875000\n",
      "########### loop 26800 ###########\n",
      "train loss:   1.178843\n",
      "train loss:   0.689342\n",
      "train loss:   0.926904\n",
      "train loss:   0.989908\n",
      "train loss:   0.790717\n",
      "train loss:   1.058949\n",
      "train loss:   1.065287\n",
      "train loss:   0.991127\n",
      "train loss:   1.170328\n",
      "train loss:   0.988246\n",
      "train loss:   0.739655\n",
      "train loss:   0.996894\n",
      "train loss:   0.980480\n",
      "train loss:   0.926338\n",
      "train loss:   0.599044\n",
      "train loss:   0.793743\n",
      "train loss:   1.484934\n",
      "train loss:   0.736163\n",
      "train loss:   0.543734\n",
      "train loss:   0.807542\n",
      "train loss:   0.902848\n",
      "train loss:   0.823734\n",
      "train loss:   0.786576\n",
      "train loss:   0.876721\n",
      "train loss:   1.079940\n",
      "train loss:   0.747763\n",
      "train loss:   1.622141\n",
      "train loss:   0.878113\n",
      "train loss:   0.836313\n",
      "train loss:   1.199094\n",
      "train loss:   1.086894\n",
      "train loss:   0.991657\n",
      "train loss:   1.252025\n",
      "train loss:   0.837287\n",
      "train loss:   0.850453\n",
      "train loss:   1.075025\n",
      "train loss:   0.911886\n",
      "train loss:   0.930274\n",
      "train loss:   0.835168\n",
      "train loss:   0.872935\n",
      "train loss:   1.036403\n",
      "train loss:   0.803291\n",
      "train loss:   0.772720\n",
      "train loss:   0.716723\n",
      "train loss:   1.560499\n",
      "train loss:   1.090372\n",
      "train loss:   0.778473\n",
      "train loss:   0.897551\n",
      "train loss:   0.971231\n",
      "train loss:   0.733112\n",
      "########### epoch 143 ###########\n",
      "########### loop 26850 ###########\n",
      "test loss:   0.135924   test accuracy:   0.958333\n",
      "########### loop 26850 ###########\n",
      "train loss:   0.776046\n",
      "train loss:   1.108307\n",
      "train loss:   1.105727\n",
      "train loss:   1.045417\n",
      "train loss:   1.166868\n",
      "train loss:   1.148209\n",
      "train loss:   0.701322\n",
      "train loss:   0.857951\n",
      "train loss:   0.798737\n",
      "train loss:   0.706975\n",
      "train loss:   1.084550\n",
      "train loss:   0.758112\n",
      "train loss:   1.014009\n",
      "train loss:   1.275400\n",
      "train loss:   0.915350\n",
      "train loss:   1.054994\n",
      "train loss:   0.504049\n",
      "train loss:   1.045705\n",
      "train loss:   0.913870\n",
      "train loss:   1.089816\n",
      "train loss:   1.212394\n",
      "train loss:   0.846986\n",
      "train loss:   1.044638\n",
      "train loss:   0.585932\n",
      "train loss:   0.877375\n",
      "train loss:   1.256840\n",
      "train loss:   0.991036\n",
      "train loss:   0.892217\n",
      "train loss:   0.762385\n",
      "train loss:   0.939121\n",
      "train loss:   0.813054\n",
      "train loss:   1.151212\n",
      "train loss:   0.868360\n",
      "train loss:   0.936118\n",
      "train loss:   0.865202\n",
      "train loss:   1.007938\n",
      "train loss:   1.279474\n",
      "train loss:   0.837358\n",
      "train loss:   0.906292\n",
      "train loss:   0.975027\n",
      "train loss:   0.775014\n",
      "train loss:   0.954726\n",
      "train loss:   0.868387\n",
      "train loss:   0.909794\n",
      "train loss:   0.845794\n",
      "train loss:   0.744310\n",
      "train loss:   0.671417\n",
      "train loss:   0.587035\n",
      "train loss:   0.891354\n",
      "train loss:   0.839356\n",
      "########### epoch 144 ###########\n",
      "########### loop 26900 ###########\n",
      "test loss:   0.099087   test accuracy:   1.000000\n",
      "########### loop 26900 ###########\n",
      "train loss:   0.716725\n",
      "train loss:   1.047339\n",
      "train loss:   0.950749\n",
      "train loss:   1.043153\n",
      "train loss:   0.685276\n",
      "train loss:   1.285827\n",
      "train loss:   0.806772\n",
      "train loss:   0.553668\n",
      "train loss:   1.001764\n",
      "train loss:   0.614786\n",
      "train loss:   1.389612\n",
      "train loss:   0.740962\n",
      "train loss:   1.000145\n",
      "train loss:   0.847630\n",
      "train loss:   0.782538\n",
      "train loss:   0.965359\n",
      "train loss:   0.895707\n",
      "train loss:   0.926197\n",
      "train loss:   0.998907\n",
      "train loss:   0.936545\n",
      "train loss:   1.162930\n",
      "train loss:   1.240965\n",
      "train loss:   0.831243\n",
      "train loss:   1.120404\n",
      "train loss:   0.784865\n",
      "train loss:   0.751842\n",
      "train loss:   0.697281\n",
      "train loss:   1.054782\n",
      "train loss:   0.943443\n",
      "train loss:   1.080549\n",
      "train loss:   0.956993\n",
      "train loss:   0.929560\n",
      "train loss:   1.029557\n",
      "train loss:   0.734371\n",
      "train loss:   0.957426\n",
      "train loss:   0.906074\n",
      "train loss:   1.257856\n",
      "train loss:   0.682354\n",
      "train loss:   1.080040\n",
      "train loss:   1.068582\n",
      "train loss:   0.850605\n",
      "train loss:   0.914843\n",
      "train loss:   1.132877\n",
      "train loss:   0.976513\n",
      "train loss:   0.901160\n",
      "train loss:   1.352243\n",
      "train loss:   1.070006\n",
      "train loss:   1.173144\n",
      "train loss:   1.066957\n",
      "train loss:   0.603526\n",
      "########### epoch 144 ###########\n",
      "########### loop 26950 ###########\n",
      "test loss:   0.215785   test accuracy:   0.958333\n",
      "########### loop 26950 ###########\n",
      "train loss:   0.870046\n",
      "train loss:   0.934618\n",
      "train loss:   0.993566\n",
      "train loss:   0.760918\n",
      "train loss:   1.137165\n",
      "train loss:   1.138933\n",
      "train loss:   0.898006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.970205\n",
      "train loss:   1.305124\n",
      "train loss:   0.692330\n",
      "train loss:   1.075235\n",
      "train loss:   1.088347\n",
      "train loss:   0.826968\n",
      "train loss:   1.107666\n",
      "train loss:   0.698642\n",
      "train loss:   1.081666\n",
      "train loss:   1.249030\n",
      "train loss:   0.991426\n",
      "train loss:   0.842804\n",
      "train loss:   0.822134\n",
      "train loss:   0.554558\n",
      "train loss:   1.078473\n",
      "train loss:   1.192149\n",
      "train loss:   0.892862\n",
      "train loss:   1.018220\n",
      "train loss:   1.000695\n",
      "train loss:   1.060915\n",
      "train loss:   1.160641\n",
      "train loss:   1.101744\n",
      "train loss:   0.832331\n",
      "train loss:   0.750657\n",
      "train loss:   0.796142\n",
      "train loss:   0.938412\n",
      "train loss:   0.680062\n",
      "train loss:   0.971810\n",
      "train loss:   0.611444\n",
      "train loss:   1.047295\n",
      "train loss:   1.137652\n",
      "train loss:   0.905459\n",
      "train loss:   1.375072\n",
      "train loss:   0.737241\n",
      "train loss:   1.520295\n",
      "train loss:   1.293273\n",
      "train loss:   0.882068\n",
      "train loss:   1.316035\n",
      "train loss:   0.914070\n",
      "train loss:   1.236034\n",
      "train loss:   0.923742\n",
      "train loss:   0.988362\n",
      "train loss:   0.807693\n",
      "########### epoch 144 ###########\n",
      "########### loop 27000 ###########\n",
      "test loss:   0.287699   test accuracy:   0.958333\n",
      "########### loop 27000 ###########\n",
      "train loss:   0.864672\n",
      "train loss:   1.204726\n",
      "train loss:   1.158338\n",
      "train loss:   1.000535\n",
      "train loss:   1.106338\n",
      "train loss:   0.613889\n",
      "train loss:   0.958432\n",
      "train loss:   0.813925\n",
      "train loss:   0.768779\n",
      "train loss:   1.411222\n",
      "train loss:   1.032366\n",
      "train loss:   0.704090\n",
      "train loss:   0.877406\n",
      "train loss:   0.883515\n",
      "train loss:   0.735761\n",
      "train loss:   0.574099\n",
      "train loss:   1.131422\n",
      "train loss:   1.035541\n",
      "train loss:   0.902547\n",
      "train loss:   0.767581\n",
      "train loss:   1.043608\n",
      "train loss:   1.066439\n",
      "train loss:   0.713115\n",
      "train loss:   0.887519\n",
      "train loss:   0.891352\n",
      "train loss:   0.763255\n",
      "train loss:   1.362216\n",
      "train loss:   0.789013\n",
      "train loss:   1.318134\n",
      "train loss:   1.238240\n",
      "train loss:   0.925669\n",
      "train loss:   1.082923\n",
      "train loss:   0.704681\n",
      "train loss:   0.723913\n",
      "train loss:   1.205439\n",
      "train loss:   0.759164\n",
      "train loss:   0.887755\n",
      "train loss:   1.058667\n",
      "train loss:   0.924441\n",
      "train loss:   1.053216\n",
      "train loss:   1.047483\n",
      "train loss:   0.896477\n",
      "train loss:   0.830819\n",
      "train loss:   0.930666\n",
      "train loss:   0.583539\n",
      "train loss:   0.977934\n",
      "train loss:   0.838673\n",
      "train loss:   0.997528\n",
      "train loss:   1.077575\n",
      "train loss:   0.867415\n",
      "########### epoch 144 ###########\n",
      "########### loop 27050 ###########\n",
      "test loss:   0.304939   test accuracy:   0.875000\n",
      "########### loop 27050 ###########\n",
      "train loss:   0.773222\n",
      "train loss:   1.066793\n",
      "train loss:   0.802133\n",
      "train loss:   0.954779\n",
      "train loss:   1.010926\n",
      "train loss:   0.800113\n",
      "train loss:   0.687968\n",
      "train loss:   1.029010\n",
      "train loss:   1.065488\n",
      "train loss:   0.847947\n",
      "train loss:   0.733269\n",
      "train loss:   0.733218\n",
      "train loss:   0.838155\n",
      "train loss:   0.661234\n",
      "train loss:   1.307294\n",
      "train loss:   1.008525\n",
      "train loss:   0.854896\n",
      "train loss:   0.776041\n",
      "train loss:   0.984210\n",
      "train loss:   0.642824\n",
      "train loss:   0.420740\n",
      "train loss:   1.217082\n",
      "train loss:   1.127853\n",
      "train loss:   0.820750\n",
      "train loss:   1.125667\n",
      "train loss:   0.989203\n",
      "train loss:   0.746098\n",
      "train loss:   0.724471\n",
      "train loss:   0.952175\n",
      "train loss:   1.079121\n",
      "train loss:   0.601876\n",
      "train loss:   0.762599\n",
      "train loss:   1.377702\n",
      "train loss:   0.950372\n",
      "train loss:   1.072051\n",
      "train loss:   1.035195\n",
      "train loss:   1.122333\n",
      "train loss:   0.825611\n",
      "train loss:   1.096728\n",
      "train loss:   0.707810\n",
      "train loss:   0.902436\n",
      "train loss:   0.781896\n",
      "train loss:   0.993819\n",
      "train loss:   0.794950\n",
      "train loss:   1.088621\n",
      "train loss:   0.718567\n",
      "train loss:   0.642125\n",
      "train loss:   1.156552\n",
      "train loss:   0.840536\n",
      "train loss:   1.189756\n",
      "########### epoch 145 ###########\n",
      "########### loop 27100 ###########\n",
      "test loss:   0.432555   test accuracy:   0.833333\n",
      "########### loop 27100 ###########\n",
      "train loss:   0.966514\n",
      "train loss:   1.009438\n",
      "train loss:   1.456685\n",
      "train loss:   1.203887\n",
      "train loss:   1.002922\n",
      "train loss:   1.213468\n",
      "train loss:   0.858435\n",
      "train loss:   1.026175\n",
      "train loss:   0.859340\n",
      "train loss:   1.098203\n",
      "train loss:   0.840978\n",
      "train loss:   1.473460\n",
      "train loss:   0.744612\n",
      "train loss:   1.290351\n",
      "train loss:   0.869051\n",
      "train loss:   0.792733\n",
      "train loss:   0.728488\n",
      "train loss:   0.770608\n",
      "train loss:   0.815950\n",
      "train loss:   1.194535\n",
      "train loss:   0.970953\n",
      "train loss:   1.046387\n",
      "train loss:   1.091125\n",
      "train loss:   0.974414\n",
      "train loss:   0.825629\n",
      "train loss:   0.984461\n",
      "train loss:   1.004322\n",
      "train loss:   0.703880\n",
      "train loss:   1.268077\n",
      "train loss:   1.094178\n",
      "train loss:   1.056156\n",
      "train loss:   0.811808\n",
      "train loss:   1.120850\n",
      "train loss:   0.670176\n",
      "train loss:   0.891688\n",
      "train loss:   0.937645\n",
      "train loss:   0.874676\n",
      "train loss:   1.028313\n",
      "train loss:   0.859262\n",
      "train loss:   1.194864\n",
      "train loss:   1.069201\n",
      "train loss:   0.814152\n",
      "train loss:   1.061039\n",
      "train loss:   1.122878\n",
      "train loss:   1.112625\n",
      "train loss:   1.007307\n",
      "train loss:   1.241647\n",
      "train loss:   0.966281\n",
      "train loss:   1.275049\n",
      "train loss:   1.248983\n",
      "########### epoch 145 ###########\n",
      "########### loop 27150 ###########\n",
      "test loss:   0.179022   test accuracy:   0.916667\n",
      "########### loop 27150 ###########\n",
      "train loss:   0.808869\n",
      "train loss:   0.843244\n",
      "train loss:   1.078458\n",
      "train loss:   0.818849\n",
      "train loss:   0.830233\n",
      "train loss:   1.401224\n",
      "train loss:   0.951132\n",
      "train loss:   0.917271\n",
      "train loss:   0.869127\n",
      "train loss:   0.426139\n",
      "train loss:   0.719262\n",
      "train loss:   0.986005\n",
      "train loss:   1.038364\n",
      "train loss:   0.779553\n",
      "train loss:   1.025893\n",
      "train loss:   1.263863\n",
      "train loss:   0.740032\n",
      "train loss:   1.114453\n",
      "train loss:   1.069672\n",
      "train loss:   1.361086\n",
      "train loss:   1.008587\n",
      "train loss:   1.057682\n",
      "train loss:   0.819604\n",
      "train loss:   0.856907\n",
      "train loss:   0.865013\n",
      "train loss:   1.020663\n",
      "train loss:   1.114403\n",
      "train loss:   1.052493\n",
      "train loss:   0.794427\n",
      "train loss:   0.867540\n",
      "train loss:   0.725911\n",
      "train loss:   0.756180\n",
      "train loss:   1.210742\n",
      "train loss:   1.040899\n",
      "train loss:   0.750525\n",
      "train loss:   1.106630\n",
      "train loss:   0.912420\n",
      "train loss:   0.797560\n",
      "train loss:   0.774825\n",
      "train loss:   0.931171\n",
      "train loss:   0.780989\n",
      "train loss:   1.134417\n",
      "train loss:   0.741934\n",
      "train loss:   1.152650\n",
      "train loss:   0.804758\n",
      "train loss:   1.209825\n",
      "train loss:   1.109298\n",
      "train loss:   0.881702\n",
      "train loss:   1.123543\n",
      "train loss:   0.733653\n",
      "########### epoch 145 ###########\n",
      "########### loop 27200 ###########\n",
      "test loss:   0.390343   test accuracy:   0.833333\n",
      "########### loop 27200 ###########\n",
      "train loss:   1.072697\n",
      "train loss:   1.398241\n",
      "train loss:   1.416426\n",
      "train loss:   1.030825\n",
      "train loss:   0.705189\n",
      "train loss:   1.162357\n",
      "train loss:   1.171917\n",
      "train loss:   0.852423\n",
      "train loss:   1.069718\n",
      "train loss:   0.846167\n",
      "train loss:   1.113246\n",
      "train loss:   1.020297\n",
      "train loss:   1.032625\n",
      "train loss:   1.101260\n",
      "train loss:   1.072242\n",
      "train loss:   1.044418\n",
      "train loss:   0.576742\n",
      "train loss:   0.870537\n",
      "train loss:   1.177650\n",
      "train loss:   0.803502\n",
      "train loss:   0.949456\n",
      "train loss:   0.826231\n",
      "train loss:   0.609203\n",
      "train loss:   0.873026\n",
      "train loss:   1.058335\n",
      "train loss:   0.927925\n",
      "train loss:   0.877380\n",
      "train loss:   1.145560\n",
      "train loss:   0.832042\n",
      "train loss:   0.968549\n",
      "train loss:   1.030084\n",
      "train loss:   1.084982\n",
      "train loss:   1.134398\n",
      "train loss:   1.036970\n",
      "train loss:   1.047538\n",
      "train loss:   1.171950\n",
      "train loss:   1.092464\n",
      "train loss:   1.012339\n",
      "train loss:   1.023793\n",
      "train loss:   0.743460\n",
      "train loss:   0.830876\n",
      "train loss:   0.905114\n",
      "train loss:   0.928542\n",
      "train loss:   0.888415\n",
      "train loss:   0.941502\n",
      "train loss:   1.011974\n",
      "train loss:   1.211543\n",
      "train loss:   0.859201\n",
      "train loss:   1.016655\n",
      "train loss:   0.997086\n",
      "########### epoch 145 ###########\n",
      "########### loop 27250 ###########\n",
      "test loss:   0.247044   test accuracy:   0.875000\n",
      "########### loop 27250 ###########\n",
      "train loss:   0.864694\n",
      "train loss:   0.669809\n",
      "train loss:   0.723046\n",
      "train loss:   0.983678\n",
      "train loss:   1.174154\n",
      "train loss:   1.026723\n",
      "train loss:   0.635425\n",
      "train loss:   0.814761\n",
      "train loss:   0.894946\n",
      "train loss:   1.091231\n",
      "train loss:   1.039735\n",
      "train loss:   0.738046\n",
      "train loss:   0.814976\n",
      "train loss:   1.070989\n",
      "train loss:   0.992384\n",
      "train loss:   1.205704\n",
      "train loss:   1.215080\n",
      "train loss:   0.922043\n",
      "train loss:   1.228662\n",
      "train loss:   0.952099\n",
      "train loss:   0.893374\n",
      "train loss:   0.777032\n",
      "train loss:   1.026055\n",
      "train loss:   0.751551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.086192\n",
      "train loss:   0.602493\n",
      "train loss:   0.840581\n",
      "train loss:   0.579572\n",
      "train loss:   1.077826\n",
      "train loss:   0.935955\n",
      "train loss:   0.700694\n",
      "train loss:   0.744448\n",
      "train loss:   1.119214\n",
      "train loss:   0.932671\n",
      "train loss:   1.050376\n",
      "train loss:   0.975599\n",
      "train loss:   0.722843\n",
      "train loss:   0.889211\n",
      "train loss:   1.272120\n",
      "train loss:   0.943887\n",
      "train loss:   0.889596\n",
      "train loss:   1.039702\n",
      "train loss:   1.415131\n",
      "train loss:   0.859868\n",
      "train loss:   1.197189\n",
      "train loss:   0.963501\n",
      "train loss:   0.776487\n",
      "train loss:   0.888686\n",
      "train loss:   0.854442\n",
      "train loss:   1.121154\n",
      "########### epoch 146 ###########\n",
      "########### loop 27300 ###########\n",
      "test loss:   0.347680   test accuracy:   0.875000\n",
      "########### loop 27300 ###########\n",
      "train loss:   0.770996\n",
      "train loss:   1.035760\n",
      "train loss:   1.024264\n",
      "train loss:   1.113976\n",
      "train loss:   0.897469\n",
      "train loss:   1.279634\n",
      "train loss:   0.691151\n",
      "train loss:   1.031043\n",
      "train loss:   0.405019\n",
      "train loss:   0.879506\n",
      "train loss:   0.922876\n",
      "train loss:   0.780309\n",
      "train loss:   1.045997\n",
      "train loss:   0.662635\n",
      "train loss:   1.005595\n",
      "train loss:   0.891842\n",
      "train loss:   0.796992\n",
      "train loss:   1.159937\n",
      "train loss:   1.061054\n",
      "train loss:   0.752079\n",
      "train loss:   0.685459\n",
      "train loss:   1.208678\n",
      "train loss:   1.136411\n",
      "train loss:   1.114480\n",
      "train loss:   1.116754\n",
      "train loss:   0.692190\n",
      "train loss:   0.871490\n",
      "train loss:   1.182135\n",
      "train loss:   0.929454\n",
      "train loss:   0.686864\n",
      "train loss:   1.181895\n",
      "train loss:   0.764764\n",
      "train loss:   0.655457\n",
      "train loss:   0.950337\n",
      "train loss:   0.865905\n",
      "train loss:   0.824541\n",
      "train loss:   0.965554\n",
      "train loss:   0.746886\n",
      "train loss:   1.182114\n",
      "train loss:   1.100578\n",
      "train loss:   1.078642\n",
      "train loss:   1.290496\n",
      "train loss:   0.689104\n",
      "train loss:   0.907395\n",
      "train loss:   0.845157\n",
      "train loss:   0.954214\n",
      "train loss:   0.909016\n",
      "train loss:   0.848712\n",
      "train loss:   1.001990\n",
      "train loss:   0.986159\n",
      "########### epoch 146 ###########\n",
      "########### loop 27350 ###########\n",
      "test loss:   0.154896   test accuracy:   1.000000\n",
      "########### loop 27350 ###########\n",
      "train loss:   0.846597\n",
      "train loss:   0.926231\n",
      "train loss:   0.724684\n",
      "train loss:   0.500423\n",
      "train loss:   1.063539\n",
      "train loss:   1.254646\n",
      "train loss:   0.784183\n",
      "train loss:   0.819207\n",
      "train loss:   1.249356\n",
      "train loss:   0.766989\n",
      "train loss:   0.870746\n",
      "train loss:   1.073610\n",
      "train loss:   0.735065\n",
      "train loss:   0.858700\n",
      "train loss:   0.784266\n",
      "train loss:   0.832986\n",
      "train loss:   0.747288\n",
      "train loss:   0.943426\n",
      "train loss:   0.900193\n",
      "train loss:   1.110513\n",
      "train loss:   1.148561\n",
      "train loss:   0.516569\n",
      "train loss:   0.935343\n",
      "train loss:   1.095043\n",
      "train loss:   1.150181\n",
      "train loss:   0.908913\n",
      "train loss:   0.887714\n",
      "train loss:   0.857023\n",
      "train loss:   0.966723\n",
      "train loss:   0.979985\n",
      "train loss:   0.869919\n",
      "train loss:   0.848579\n",
      "train loss:   1.075709\n",
      "train loss:   0.854309\n",
      "train loss:   1.106827\n",
      "train loss:   1.092597\n",
      "train loss:   1.145590\n",
      "train loss:   0.821050\n",
      "train loss:   0.885580\n",
      "train loss:   0.943087\n",
      "train loss:   1.231807\n",
      "train loss:   1.028328\n",
      "train loss:   1.156347\n",
      "train loss:   0.644295\n",
      "train loss:   0.896738\n",
      "train loss:   0.904421\n",
      "train loss:   0.799551\n",
      "train loss:   1.191389\n",
      "train loss:   0.844086\n",
      "train loss:   0.857968\n",
      "########### epoch 146 ###########\n",
      "########### loop 27400 ###########\n",
      "test loss:   0.203879   test accuracy:   0.958333\n",
      "########### loop 27400 ###########\n",
      "train loss:   0.494648\n",
      "train loss:   0.962950\n",
      "train loss:   1.242731\n",
      "train loss:   0.966817\n",
      "train loss:   0.966762\n",
      "train loss:   0.893667\n",
      "train loss:   1.082834\n",
      "train loss:   0.826529\n",
      "train loss:   1.295880\n",
      "train loss:   0.830090\n",
      "train loss:   1.119982\n",
      "train loss:   0.886514\n",
      "train loss:   0.961568\n",
      "train loss:   0.822062\n",
      "train loss:   0.853310\n",
      "train loss:   1.124454\n",
      "train loss:   1.183905\n",
      "train loss:   1.031545\n",
      "train loss:   1.176290\n",
      "train loss:   1.057353\n",
      "train loss:   1.299974\n",
      "train loss:   0.867137\n",
      "train loss:   0.760853\n",
      "train loss:   0.408279\n",
      "train loss:   0.916262\n",
      "train loss:   1.060645\n",
      "train loss:   1.226518\n",
      "train loss:   1.229327\n",
      "train loss:   0.786307\n",
      "train loss:   1.123974\n",
      "train loss:   0.828209\n",
      "train loss:   1.056707\n",
      "train loss:   0.984420\n",
      "train loss:   0.898606\n",
      "train loss:   0.846213\n",
      "train loss:   0.863917\n",
      "train loss:   1.169521\n",
      "train loss:   0.934337\n",
      "train loss:   1.002603\n",
      "train loss:   0.869921\n",
      "train loss:   0.988619\n",
      "train loss:   0.958483\n",
      "train loss:   0.752483\n",
      "train loss:   0.937780\n",
      "train loss:   0.997568\n",
      "train loss:   1.227031\n",
      "train loss:   0.945278\n",
      "train loss:   0.914595\n",
      "train loss:   1.222190\n",
      "train loss:   1.211835\n",
      "########### epoch 147 ###########\n",
      "########### loop 27450 ###########\n",
      "test loss:   0.368050   test accuracy:   0.875000\n",
      "########### loop 27450 ###########\n",
      "train loss:   0.635079\n",
      "train loss:   1.233434\n",
      "train loss:   0.691727\n",
      "train loss:   1.024795\n",
      "train loss:   1.036586\n",
      "train loss:   0.879990\n",
      "train loss:   0.944776\n",
      "train loss:   0.859067\n",
      "train loss:   0.929850\n",
      "train loss:   1.112034\n",
      "train loss:   0.986178\n",
      "train loss:   0.907539\n",
      "train loss:   0.828180\n",
      "train loss:   1.343021\n",
      "train loss:   1.177018\n",
      "train loss:   1.184825\n",
      "train loss:   1.002268\n",
      "train loss:   1.120317\n",
      "train loss:   1.204993\n",
      "train loss:   0.902556\n",
      "train loss:   1.184178\n",
      "train loss:   1.143331\n",
      "train loss:   0.921089\n",
      "train loss:   1.013439\n",
      "train loss:   1.120918\n",
      "train loss:   1.224806\n",
      "train loss:   0.784666\n",
      "train loss:   0.953679\n",
      "train loss:   1.128141\n",
      "train loss:   1.086584\n",
      "train loss:   1.197986\n",
      "train loss:   0.792175\n",
      "train loss:   0.640594\n",
      "train loss:   0.824925\n",
      "train loss:   0.978456\n",
      "train loss:   0.914548\n",
      "train loss:   1.051825\n",
      "train loss:   0.990032\n",
      "train loss:   1.117669\n",
      "train loss:   1.193126\n",
      "train loss:   1.301912\n",
      "train loss:   1.067251\n",
      "train loss:   1.024934\n",
      "train loss:   0.915431\n",
      "train loss:   0.569356\n",
      "train loss:   0.944194\n",
      "train loss:   0.644500\n",
      "train loss:   1.059929\n",
      "train loss:   0.977108\n",
      "train loss:   1.171021\n",
      "########### epoch 147 ###########\n",
      "########### loop 27500 ###########\n",
      "test loss:   0.238270   test accuracy:   0.916667\n",
      "########### loop 27500 ###########\n",
      "train loss:   1.046288\n",
      "train loss:   0.742699\n",
      "train loss:   1.194709\n",
      "train loss:   0.885633\n",
      "train loss:   1.099216\n",
      "train loss:   0.831926\n",
      "train loss:   1.049099\n",
      "train loss:   0.958423\n",
      "train loss:   0.904785\n",
      "train loss:   1.419636\n",
      "train loss:   0.574825\n",
      "train loss:   1.029742\n",
      "train loss:   1.026319\n",
      "train loss:   0.963216\n",
      "train loss:   0.788074\n",
      "train loss:   0.852494\n",
      "train loss:   1.201710\n",
      "train loss:   1.370787\n",
      "train loss:   0.683848\n",
      "train loss:   1.081407\n",
      "train loss:   0.979054\n",
      "train loss:   1.329333\n",
      "train loss:   1.151625\n",
      "train loss:   1.175918\n",
      "train loss:   0.925795\n",
      "train loss:   0.975461\n",
      "train loss:   1.034451\n",
      "train loss:   0.660975\n",
      "train loss:   1.042171\n",
      "train loss:   1.130471\n",
      "train loss:   0.880435\n",
      "train loss:   0.978860\n",
      "train loss:   0.874414\n",
      "train loss:   1.002089\n",
      "train loss:   0.737969\n",
      "train loss:   1.061471\n",
      "train loss:   1.168238\n",
      "train loss:   1.378711\n",
      "train loss:   1.351402\n",
      "train loss:   0.884132\n",
      "train loss:   0.901619\n",
      "train loss:   0.584914\n",
      "train loss:   1.445899\n",
      "train loss:   1.163669\n",
      "train loss:   0.778154\n",
      "train loss:   1.006941\n",
      "train loss:   1.369227\n",
      "train loss:   0.461549\n",
      "train loss:   0.898721\n",
      "train loss:   1.124143\n",
      "########### epoch 147 ###########\n",
      "########### loop 27550 ###########\n",
      "test loss:   0.458953   test accuracy:   0.916667\n",
      "########### loop 27550 ###########\n",
      "train loss:   1.104858\n",
      "train loss:   0.749574\n",
      "train loss:   0.967720\n",
      "train loss:   1.120338\n",
      "train loss:   0.913390\n",
      "train loss:   0.707704\n",
      "train loss:   0.969025\n",
      "train loss:   1.127071\n",
      "train loss:   0.888961\n",
      "train loss:   0.881019\n",
      "train loss:   0.908328\n",
      "train loss:   0.858530\n",
      "train loss:   0.934657\n",
      "train loss:   1.145985\n",
      "train loss:   1.041171\n",
      "train loss:   0.699071\n",
      "train loss:   1.093822\n",
      "train loss:   0.866039\n",
      "train loss:   1.039865\n",
      "train loss:   0.723354\n",
      "train loss:   1.232310\n",
      "train loss:   1.238048\n",
      "train loss:   0.887492\n",
      "train loss:   1.040698\n",
      "train loss:   0.888972\n",
      "train loss:   0.882822\n",
      "train loss:   1.123036\n",
      "train loss:   1.338757\n",
      "train loss:   1.317351\n",
      "train loss:   0.840818\n",
      "train loss:   1.249210\n",
      "train loss:   1.037188\n",
      "train loss:   1.187838\n",
      "train loss:   0.595734\n",
      "train loss:   0.830258\n",
      "train loss:   0.759027\n",
      "train loss:   1.454054\n",
      "train loss:   1.137713\n",
      "train loss:   0.640148\n",
      "train loss:   0.980992\n",
      "train loss:   1.034804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.974092\n",
      "train loss:   1.015134\n",
      "train loss:   1.447610\n",
      "train loss:   0.597250\n",
      "train loss:   0.841787\n",
      "train loss:   1.089614\n",
      "train loss:   1.229933\n",
      "train loss:   0.867375\n",
      "train loss:   0.720312\n",
      "########### epoch 147 ###########\n",
      "########### loop 27600 ###########\n",
      "test loss:   0.157373   test accuracy:   0.958333\n",
      "########### loop 27600 ###########\n",
      "train loss:   0.766412\n",
      "train loss:   1.217368\n",
      "train loss:   0.992260\n",
      "train loss:   0.877581\n",
      "train loss:   0.837108\n",
      "train loss:   0.933201\n",
      "train loss:   0.744443\n",
      "train loss:   1.088078\n",
      "train loss:   0.845262\n",
      "train loss:   0.632992\n",
      "train loss:   0.858348\n",
      "train loss:   0.939491\n",
      "train loss:   0.904721\n",
      "train loss:   0.849551\n",
      "train loss:   1.229355\n",
      "train loss:   1.143520\n",
      "train loss:   0.952856\n",
      "train loss:   0.940902\n",
      "train loss:   1.014119\n",
      "train loss:   1.030027\n",
      "train loss:   0.685642\n",
      "train loss:   0.881748\n",
      "train loss:   1.176291\n",
      "train loss:   1.137308\n",
      "train loss:   0.844304\n",
      "train loss:   1.094825\n",
      "train loss:   1.062947\n",
      "train loss:   1.130795\n",
      "train loss:   1.109557\n",
      "train loss:   0.871203\n",
      "train loss:   1.057316\n",
      "train loss:   0.585924\n",
      "train loss:   0.999228\n",
      "train loss:   1.341192\n",
      "train loss:   1.027458\n",
      "train loss:   1.131552\n",
      "train loss:   0.919003\n",
      "train loss:   1.094895\n",
      "train loss:   1.004235\n",
      "train loss:   0.981906\n",
      "train loss:   1.148336\n",
      "train loss:   0.614992\n",
      "train loss:   1.125935\n",
      "train loss:   1.323453\n",
      "train loss:   1.034377\n",
      "train loss:   0.920284\n",
      "train loss:   1.032454\n",
      "train loss:   0.743794\n",
      "train loss:   0.888084\n",
      "train loss:   0.881221\n",
      "########### epoch 148 ###########\n",
      "########### loop 27650 ###########\n",
      "test loss:   0.248855   test accuracy:   0.916667\n",
      "########### loop 27650 ###########\n",
      "train loss:   0.778077\n",
      "train loss:   0.989834\n",
      "train loss:   1.077910\n",
      "train loss:   0.815065\n",
      "train loss:   0.775271\n",
      "train loss:   1.084099\n",
      "train loss:   1.216376\n",
      "train loss:   1.594106\n",
      "train loss:   0.746708\n",
      "train loss:   0.880999\n",
      "train loss:   0.587422\n",
      "train loss:   1.137881\n",
      "train loss:   1.129951\n",
      "train loss:   0.602574\n",
      "train loss:   0.828505\n",
      "train loss:   0.909044\n",
      "train loss:   0.922552\n",
      "train loss:   0.958938\n",
      "train loss:   1.043328\n",
      "train loss:   1.209927\n",
      "train loss:   0.955371\n",
      "train loss:   0.898863\n",
      "train loss:   0.958328\n",
      "train loss:   0.924905\n",
      "train loss:   0.825790\n",
      "train loss:   0.895219\n",
      "train loss:   1.312635\n",
      "train loss:   0.969841\n",
      "train loss:   0.794491\n",
      "train loss:   0.795152\n",
      "train loss:   0.924112\n",
      "train loss:   0.846373\n",
      "train loss:   1.156273\n",
      "train loss:   1.309418\n",
      "train loss:   0.940440\n",
      "train loss:   1.030110\n",
      "train loss:   0.540826\n",
      "train loss:   1.255657\n",
      "train loss:   1.064856\n",
      "train loss:   0.917488\n",
      "train loss:   1.026309\n",
      "train loss:   0.986374\n",
      "train loss:   0.980165\n",
      "train loss:   1.099927\n",
      "train loss:   1.077271\n",
      "train loss:   0.949169\n",
      "train loss:   0.999580\n",
      "train loss:   1.210681\n",
      "train loss:   0.865737\n",
      "train loss:   1.074071\n",
      "########### epoch 148 ###########\n",
      "########### loop 27700 ###########\n",
      "test loss:   0.253504   test accuracy:   0.958333\n",
      "########### loop 27700 ###########\n",
      "train loss:   1.510028\n",
      "train loss:   1.164787\n",
      "train loss:   0.916547\n",
      "train loss:   0.888588\n",
      "train loss:   0.957858\n",
      "train loss:   0.955047\n",
      "train loss:   0.838710\n",
      "train loss:   0.771732\n",
      "train loss:   0.767378\n",
      "train loss:   0.655044\n",
      "train loss:   0.811429\n",
      "train loss:   0.723373\n",
      "train loss:   0.970778\n",
      "train loss:   1.009937\n",
      "train loss:   1.116423\n",
      "train loss:   0.954880\n",
      "train loss:   1.135716\n",
      "train loss:   0.802038\n",
      "train loss:   0.990107\n",
      "train loss:   0.553602\n",
      "train loss:   1.143920\n",
      "train loss:   0.832049\n",
      "train loss:   0.966009\n",
      "train loss:   0.789601\n",
      "train loss:   1.157093\n",
      "train loss:   1.077238\n",
      "train loss:   1.362257\n",
      "train loss:   0.727472\n",
      "train loss:   0.534802\n",
      "train loss:   0.674580\n",
      "train loss:   0.950940\n",
      "train loss:   1.097690\n",
      "train loss:   0.969926\n",
      "train loss:   1.136886\n",
      "train loss:   1.313742\n",
      "train loss:   0.621095\n",
      "train loss:   0.793887\n",
      "train loss:   1.143892\n",
      "train loss:   0.857392\n",
      "train loss:   0.972473\n",
      "train loss:   0.792141\n",
      "train loss:   1.240441\n",
      "train loss:   0.588880\n",
      "train loss:   1.105713\n",
      "train loss:   1.114073\n",
      "train loss:   0.927925\n",
      "train loss:   1.059485\n",
      "train loss:   0.691031\n",
      "train loss:   0.730657\n",
      "train loss:   0.946943\n",
      "########### epoch 148 ###########\n",
      "########### loop 27750 ###########\n",
      "test loss:   0.267408   test accuracy:   0.916667\n",
      "########### loop 27750 ###########\n",
      "train loss:   1.017884\n",
      "train loss:   1.096689\n",
      "train loss:   1.028068\n",
      "train loss:   0.618738\n",
      "train loss:   1.109407\n",
      "train loss:   0.935346\n",
      "train loss:   1.213134\n",
      "train loss:   1.101600\n",
      "train loss:   0.817630\n",
      "train loss:   0.625775\n",
      "train loss:   0.655486\n",
      "train loss:   0.853045\n",
      "train loss:   0.899741\n",
      "train loss:   1.193337\n",
      "train loss:   0.824123\n",
      "train loss:   0.855737\n",
      "train loss:   0.919098\n",
      "train loss:   0.910858\n",
      "train loss:   0.815488\n",
      "train loss:   1.345702\n",
      "train loss:   0.660918\n",
      "train loss:   1.067675\n",
      "train loss:   0.867789\n",
      "train loss:   1.144460\n",
      "train loss:   0.745293\n",
      "train loss:   0.904046\n",
      "train loss:   0.755270\n",
      "train loss:   0.804867\n",
      "train loss:   1.042906\n",
      "train loss:   1.148750\n",
      "train loss:   0.878611\n",
      "train loss:   0.621119\n",
      "train loss:   0.989713\n",
      "train loss:   1.028412\n",
      "train loss:   0.783568\n",
      "train loss:   1.048819\n",
      "train loss:   0.986465\n",
      "train loss:   1.231554\n",
      "train loss:   1.107545\n",
      "train loss:   0.826364\n",
      "train loss:   0.790621\n",
      "train loss:   0.865244\n",
      "train loss:   1.138806\n",
      "train loss:   0.954909\n",
      "train loss:   1.469059\n",
      "train loss:   1.110461\n",
      "train loss:   0.906144\n",
      "train loss:   0.957802\n",
      "train loss:   1.152772\n",
      "train loss:   0.958132\n",
      "########### epoch 148 ###########\n",
      "########### loop 27800 ###########\n",
      "test loss:   0.211783   test accuracy:   0.958333\n",
      "########### loop 27800 ###########\n",
      "train loss:   0.942383\n",
      "train loss:   1.156990\n",
      "train loss:   0.997417\n",
      "train loss:   1.040160\n",
      "train loss:   1.121626\n",
      "train loss:   1.004514\n",
      "train loss:   1.063384\n",
      "train loss:   0.817114\n",
      "train loss:   1.090972\n",
      "train loss:   1.075591\n",
      "train loss:   0.736359\n",
      "train loss:   0.932565\n",
      "train loss:   1.120938\n",
      "train loss:   0.966886\n",
      "train loss:   0.841259\n",
      "train loss:   1.159418\n",
      "train loss:   0.941058\n",
      "train loss:   1.100975\n",
      "train loss:   0.869848\n",
      "train loss:   1.423047\n",
      "train loss:   1.180539\n",
      "train loss:   0.854026\n",
      "train loss:   0.747428\n",
      "train loss:   0.892171\n",
      "train loss:   1.258025\n",
      "train loss:   0.716928\n",
      "train loss:   0.914515\n",
      "train loss:   0.554561\n",
      "train loss:   0.870745\n",
      "train loss:   0.985149\n",
      "train loss:   0.898268\n",
      "train loss:   1.082835\n",
      "train loss:   0.835428\n",
      "train loss:   0.919995\n",
      "train loss:   0.784687\n",
      "train loss:   0.877742\n",
      "train loss:   1.076066\n",
      "train loss:   1.203910\n",
      "train loss:   0.890484\n",
      "train loss:   1.035880\n",
      "train loss:   0.767714\n",
      "train loss:   0.911975\n",
      "train loss:   1.022035\n",
      "train loss:   1.109132\n",
      "train loss:   0.781652\n",
      "train loss:   0.791907\n",
      "train loss:   0.556360\n",
      "train loss:   1.121674\n",
      "train loss:   0.939533\n",
      "train loss:   0.875885\n",
      "########### epoch 149 ###########\n",
      "########### loop 27850 ###########\n",
      "test loss:   0.163545   test accuracy:   0.958333\n",
      "########### loop 27850 ###########\n",
      "train loss:   0.914786\n",
      "train loss:   0.790267\n",
      "train loss:   0.744364\n",
      "train loss:   1.107674\n",
      "train loss:   1.033956\n",
      "train loss:   1.067805\n",
      "train loss:   0.676056\n",
      "train loss:   1.163531\n",
      "train loss:   0.983029\n",
      "train loss:   0.802824\n",
      "train loss:   1.171265\n",
      "train loss:   0.893524\n",
      "train loss:   0.857753\n",
      "train loss:   1.061890\n",
      "train loss:   0.635326\n",
      "train loss:   1.265097\n",
      "train loss:   0.864936\n",
      "train loss:   0.974239\n",
      "train loss:   1.170359\n",
      "train loss:   1.298567\n",
      "train loss:   1.159519\n",
      "train loss:   0.898406\n",
      "train loss:   0.863811\n",
      "train loss:   0.967962\n",
      "train loss:   1.054806\n",
      "train loss:   0.963876\n",
      "train loss:   1.206638\n",
      "train loss:   0.832177\n",
      "train loss:   0.857688\n",
      "train loss:   1.122829\n",
      "train loss:   0.980077\n",
      "train loss:   0.803308\n",
      "train loss:   0.974051\n",
      "train loss:   1.096147\n",
      "train loss:   0.976476\n",
      "train loss:   1.099803\n",
      "train loss:   0.997247\n",
      "train loss:   0.775616\n",
      "train loss:   1.452852\n",
      "train loss:   1.256646\n",
      "train loss:   1.058986\n",
      "train loss:   0.877317\n",
      "train loss:   0.777071\n",
      "train loss:   1.001151\n",
      "train loss:   0.894991\n",
      "train loss:   0.752670\n",
      "train loss:   1.087669\n",
      "train loss:   1.261282\n",
      "train loss:   0.858715\n",
      "train loss:   1.196733\n",
      "########### epoch 149 ###########\n",
      "########### loop 27900 ###########\n",
      "test loss:   0.148459   test accuracy:   0.958333\n",
      "########### loop 27900 ###########\n",
      "train loss:   1.048020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.084466\n",
      "train loss:   1.210073\n",
      "train loss:   1.081341\n",
      "train loss:   1.080976\n",
      "train loss:   0.740986\n",
      "train loss:   0.878945\n",
      "train loss:   1.293420\n",
      "train loss:   0.967555\n",
      "train loss:   0.766261\n",
      "train loss:   1.016336\n",
      "train loss:   1.178625\n",
      "train loss:   1.181872\n",
      "train loss:   1.016488\n",
      "train loss:   1.005207\n",
      "train loss:   0.804051\n",
      "train loss:   0.879541\n",
      "train loss:   1.021293\n",
      "train loss:   0.956266\n",
      "train loss:   0.928244\n",
      "train loss:   0.763461\n",
      "train loss:   0.812587\n",
      "train loss:   1.317006\n",
      "train loss:   1.007312\n",
      "train loss:   0.985191\n",
      "train loss:   1.272680\n",
      "train loss:   0.753153\n",
      "train loss:   0.961014\n",
      "train loss:   0.845506\n",
      "train loss:   0.899402\n",
      "train loss:   0.696126\n",
      "train loss:   0.886094\n",
      "train loss:   1.185191\n",
      "train loss:   1.205065\n",
      "train loss:   1.371232\n",
      "train loss:   1.111883\n",
      "train loss:   1.080261\n",
      "train loss:   1.052657\n",
      "train loss:   0.804933\n",
      "train loss:   1.247095\n",
      "train loss:   1.195358\n",
      "train loss:   0.894403\n",
      "train loss:   1.018292\n",
      "train loss:   0.948935\n",
      "train loss:   1.062259\n",
      "train loss:   1.339539\n",
      "train loss:   1.316872\n",
      "train loss:   0.865802\n",
      "train loss:   0.897880\n",
      "train loss:   0.869990\n",
      "########### epoch 149 ###########\n",
      "########### loop 27950 ###########\n",
      "test loss:   0.339901   test accuracy:   0.916667\n",
      "########### loop 27950 ###########\n",
      "train loss:   0.930607\n",
      "train loss:   0.947606\n",
      "train loss:   0.950453\n",
      "train loss:   0.880552\n",
      "train loss:   1.122241\n",
      "train loss:   1.151230\n",
      "train loss:   1.109954\n",
      "train loss:   0.567051\n",
      "train loss:   0.913761\n",
      "train loss:   0.951156\n",
      "train loss:   0.676612\n",
      "train loss:   1.123491\n",
      "train loss:   0.961657\n",
      "train loss:   1.175254\n",
      "train loss:   1.170496\n",
      "train loss:   0.950817\n",
      "train loss:   0.853010\n",
      "train loss:   1.219550\n",
      "train loss:   1.053429\n",
      "train loss:   0.702765\n",
      "train loss:   1.159749\n",
      "train loss:   1.104705\n",
      "train loss:   0.821022\n",
      "train loss:   0.958931\n",
      "train loss:   0.862343\n",
      "train loss:   1.011660\n",
      "train loss:   1.084057\n",
      "train loss:   1.117823\n",
      "train loss:   1.002195\n",
      "train loss:   0.929102\n",
      "train loss:   1.015714\n",
      "train loss:   0.594107\n",
      "train loss:   1.043605\n",
      "train loss:   1.117414\n",
      "train loss:   1.459075\n",
      "train loss:   1.126804\n",
      "train loss:   0.974374\n",
      "train loss:   0.734416\n",
      "train loss:   0.814705\n",
      "train loss:   0.704172\n",
      "train loss:   1.086640\n",
      "train loss:   0.827096\n",
      "train loss:   0.726192\n",
      "train loss:   1.109502\n",
      "train loss:   1.251707\n",
      "train loss:   0.517438\n",
      "train loss:   0.868597\n",
      "train loss:   0.662108\n",
      "train loss:   0.802275\n",
      "train loss:   0.661662\n",
      "########### epoch 149 ###########\n",
      "########### loop 28000 ###########\n",
      "test loss:   0.088194   test accuracy:   1.000000\n",
      "########### loop 28000 ###########\n",
      "train loss:   0.914056\n",
      "train loss:   1.080851\n",
      "train loss:   0.921877\n",
      "train loss:   1.072060\n",
      "train loss:   0.723332\n",
      "train loss:   1.316437\n",
      "train loss:   0.900944\n",
      "train loss:   1.434219\n",
      "train loss:   1.487377\n",
      "train loss:   0.960982\n",
      "train loss:   1.071579\n",
      "train loss:   0.934519\n",
      "train loss:   0.869589\n",
      "train loss:   1.074413\n",
      "train loss:   0.983736\n",
      "train loss:   0.633655\n",
      "train loss:   0.721107\n",
      "train loss:   0.818970\n",
      "train loss:   0.987610\n",
      "train loss:   0.909929\n",
      "train loss:   0.961389\n",
      "train loss:   0.824383\n",
      "train loss:   1.447900\n",
      "train loss:   0.911652\n",
      "train loss:   0.938901\n",
      "train loss:   1.004822\n",
      "train loss:   1.053270\n",
      "train loss:   0.967106\n",
      "train loss:   0.999339\n",
      "train loss:   1.207433\n",
      "train loss:   0.884769\n",
      "train loss:   1.012716\n",
      "train loss:   1.043092\n",
      "train loss:   0.957121\n",
      "train loss:   1.143591\n",
      "train loss:   0.894905\n",
      "train loss:   0.717010\n",
      "train loss:   0.938169\n",
      "train loss:   0.983217\n",
      "train loss:   0.902734\n",
      "train loss:   1.064144\n",
      "train loss:   1.119385\n",
      "train loss:   1.070531\n",
      "train loss:   1.459363\n",
      "train loss:   0.888727\n",
      "train loss:   0.579504\n",
      "train loss:   0.740546\n",
      "train loss:   0.535421\n",
      "train loss:   0.755892\n",
      "train loss:   1.081940\n",
      "########### epoch 150 ###########\n",
      "########### loop 28050 ###########\n",
      "test loss:   0.303085   test accuracy:   0.916667\n",
      "########### loop 28050 ###########\n",
      "train loss:   0.909840\n",
      "train loss:   0.939023\n",
      "train loss:   1.165332\n",
      "train loss:   0.975723\n",
      "train loss:   0.915689\n",
      "train loss:   0.977957\n",
      "train loss:   1.024703\n",
      "train loss:   0.914717\n",
      "train loss:   0.807958\n",
      "train loss:   0.981233\n",
      "train loss:   0.848468\n",
      "train loss:   0.943130\n",
      "train loss:   0.685695\n",
      "train loss:   1.021684\n",
      "train loss:   1.098189\n",
      "train loss:   1.184662\n",
      "train loss:   0.959752\n",
      "train loss:   0.873004\n",
      "train loss:   0.886447\n",
      "train loss:   0.918796\n",
      "train loss:   0.933585\n",
      "train loss:   0.856890\n",
      "train loss:   1.145117\n",
      "train loss:   0.789034\n",
      "train loss:   0.832749\n",
      "train loss:   0.934173\n",
      "train loss:   0.723723\n",
      "train loss:   0.765239\n",
      "train loss:   1.451043\n",
      "train loss:   1.214829\n",
      "train loss:   0.848035\n",
      "train loss:   0.562373\n",
      "train loss:   1.044827\n",
      "train loss:   0.895399\n",
      "train loss:   0.751678\n",
      "train loss:   0.823247\n",
      "train loss:   0.822634\n",
      "train loss:   0.930364\n",
      "train loss:   0.750311\n",
      "train loss:   0.789817\n",
      "train loss:   0.746608\n",
      "train loss:   0.984828\n",
      "train loss:   0.730029\n",
      "train loss:   0.941538\n",
      "train loss:   1.339603\n",
      "train loss:   0.829891\n",
      "train loss:   0.860225\n",
      "train loss:   0.665071\n",
      "train loss:   0.993905\n",
      "train loss:   1.118058\n",
      "########### epoch 150 ###########\n",
      "########### loop 28100 ###########\n",
      "test loss:   0.242663   test accuracy:   0.916667\n",
      "########### loop 28100 ###########\n",
      "train loss:   0.816442\n",
      "train loss:   1.119728\n",
      "train loss:   0.720190\n",
      "train loss:   1.226273\n",
      "train loss:   0.957424\n",
      "train loss:   1.140251\n",
      "train loss:   1.009213\n",
      "train loss:   0.931544\n",
      "train loss:   0.932510\n",
      "train loss:   0.732445\n",
      "train loss:   0.958295\n",
      "train loss:   0.642044\n",
      "train loss:   1.150349\n",
      "train loss:   1.164709\n",
      "train loss:   1.078333\n",
      "train loss:   0.929268\n",
      "train loss:   0.960535\n",
      "train loss:   1.044360\n",
      "train loss:   0.979542\n",
      "train loss:   0.809313\n",
      "train loss:   1.044172\n",
      "train loss:   0.743086\n",
      "train loss:   1.079597\n",
      "train loss:   0.749034\n",
      "train loss:   1.050962\n",
      "train loss:   0.957110\n",
      "train loss:   1.169975\n",
      "train loss:   1.099625\n",
      "train loss:   0.892679\n",
      "train loss:   1.091897\n",
      "train loss:   1.099950\n",
      "train loss:   1.003359\n",
      "train loss:   1.172557\n",
      "train loss:   0.857528\n",
      "train loss:   0.819277\n",
      "train loss:   1.279172\n",
      "train loss:   1.127269\n",
      "train loss:   1.000398\n",
      "train loss:   1.036548\n",
      "train loss:   0.793882\n",
      "train loss:   1.026557\n",
      "train loss:   1.163665\n",
      "train loss:   1.003839\n",
      "train loss:   1.034616\n",
      "train loss:   0.558944\n",
      "train loss:   1.079996\n",
      "train loss:   0.824428\n",
      "train loss:   1.231815\n",
      "train loss:   0.952236\n",
      "train loss:   0.980307\n",
      "########### epoch 150 ###########\n",
      "########### loop 28150 ###########\n",
      "test loss:   0.297829   test accuracy:   0.916667\n",
      "########### loop 28150 ###########\n",
      "train loss:   1.016911\n",
      "train loss:   0.529423\n",
      "train loss:   0.764036\n",
      "train loss:   1.140850\n",
      "train loss:   0.574872\n",
      "train loss:   0.937451\n",
      "train loss:   0.808854\n",
      "train loss:   1.069503\n",
      "train loss:   0.938685\n",
      "train loss:   0.759009\n",
      "train loss:   0.797994\n",
      "train loss:   0.946857\n",
      "train loss:   0.950363\n",
      "train loss:   1.391897\n",
      "train loss:   1.318611\n",
      "train loss:   0.961376\n",
      "train loss:   0.687143\n",
      "train loss:   0.811585\n",
      "train loss:   1.233564\n",
      "train loss:   0.766284\n",
      "train loss:   0.994941\n",
      "train loss:   0.568667\n",
      "train loss:   0.789941\n",
      "train loss:   0.716460\n",
      "train loss:   1.027125\n",
      "train loss:   0.730220\n",
      "train loss:   0.823538\n",
      "train loss:   0.855719\n",
      "train loss:   0.583784\n",
      "train loss:   1.462901\n",
      "train loss:   0.666640\n",
      "train loss:   1.142416\n",
      "train loss:   0.511786\n",
      "train loss:   0.829984\n",
      "train loss:   0.857841\n",
      "train loss:   0.988543\n",
      "train loss:   1.165208\n",
      "train loss:   1.075018\n",
      "train loss:   0.874569\n",
      "train loss:   0.830591\n",
      "train loss:   0.994431\n",
      "train loss:   1.057119\n",
      "train loss:   0.929332\n",
      "train loss:   0.965445\n",
      "train loss:   1.189965\n",
      "train loss:   1.024358\n",
      "train loss:   1.085645\n",
      "train loss:   1.133329\n",
      "train loss:   0.917827\n",
      "train loss:   0.578965\n",
      "########### epoch 151 ###########\n",
      "########### loop 28200 ###########\n",
      "test loss:   0.437483   test accuracy:   0.875000\n",
      "########### loop 28200 ###########\n",
      "train loss:   1.136524\n",
      "train loss:   0.770608\n",
      "train loss:   0.808208\n",
      "train loss:   0.748829\n",
      "train loss:   0.871636\n",
      "train loss:   0.858340\n",
      "train loss:   0.523532\n",
      "train loss:   1.170587\n",
      "train loss:   0.879898\n",
      "train loss:   0.709484\n",
      "train loss:   0.948071\n",
      "train loss:   1.060594\n",
      "train loss:   1.037047\n",
      "train loss:   0.891501\n",
      "train loss:   1.284636\n",
      "train loss:   1.081754\n",
      "train loss:   1.062517\n",
      "train loss:   0.959437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.744805\n",
      "train loss:   0.931544\n",
      "train loss:   1.217137\n",
      "train loss:   1.069219\n",
      "train loss:   0.924815\n",
      "train loss:   1.288961\n",
      "train loss:   0.902348\n",
      "train loss:   1.215500\n",
      "train loss:   1.243704\n",
      "train loss:   1.059575\n",
      "train loss:   1.151611\n",
      "train loss:   1.192692\n",
      "train loss:   0.970002\n",
      "train loss:   0.739175\n",
      "train loss:   1.216217\n",
      "train loss:   1.026658\n",
      "train loss:   1.100286\n",
      "train loss:   1.179236\n",
      "train loss:   0.939809\n",
      "train loss:   0.742226\n",
      "train loss:   0.957618\n",
      "train loss:   1.147566\n",
      "train loss:   0.911171\n",
      "train loss:   0.630083\n",
      "train loss:   0.748143\n",
      "train loss:   1.106861\n",
      "train loss:   0.886285\n",
      "train loss:   0.745727\n",
      "train loss:   0.988517\n",
      "train loss:   0.797962\n",
      "train loss:   0.717214\n",
      "train loss:   1.189037\n",
      "########### epoch 151 ###########\n",
      "########### loop 28250 ###########\n",
      "test loss:   0.142579   test accuracy:   0.958333\n",
      "########### loop 28250 ###########\n",
      "train loss:   1.431203\n",
      "train loss:   0.901450\n",
      "train loss:   0.493794\n",
      "train loss:   1.067136\n",
      "train loss:   1.095104\n",
      "train loss:   0.668944\n",
      "train loss:   0.838734\n",
      "train loss:   0.952112\n",
      "train loss:   1.001831\n",
      "train loss:   1.034496\n",
      "train loss:   1.036330\n",
      "train loss:   0.467858\n",
      "train loss:   0.726924\n",
      "train loss:   0.871106\n",
      "train loss:   0.792604\n",
      "train loss:   0.641369\n",
      "train loss:   1.177788\n",
      "train loss:   0.955601\n",
      "train loss:   1.290143\n",
      "train loss:   0.905390\n",
      "train loss:   1.057638\n",
      "train loss:   0.830441\n",
      "train loss:   1.295346\n",
      "train loss:   1.091382\n",
      "train loss:   0.699076\n",
      "train loss:   0.990865\n",
      "train loss:   1.183212\n",
      "train loss:   0.967329\n",
      "train loss:   1.095531\n",
      "train loss:   0.462300\n",
      "train loss:   0.854437\n",
      "train loss:   1.029994\n",
      "train loss:   0.812467\n",
      "train loss:   0.927826\n",
      "train loss:   0.820465\n",
      "train loss:   0.991921\n",
      "train loss:   1.032161\n",
      "train loss:   1.132345\n",
      "train loss:   1.429516\n",
      "train loss:   0.816276\n",
      "train loss:   1.208245\n",
      "train loss:   0.919421\n",
      "train loss:   1.235722\n",
      "train loss:   1.001867\n",
      "train loss:   0.837596\n",
      "train loss:   1.097750\n",
      "train loss:   0.773203\n",
      "train loss:   1.003412\n",
      "train loss:   0.918045\n",
      "train loss:   0.762137\n",
      "########### epoch 151 ###########\n",
      "########### loop 28300 ###########\n",
      "test loss:   0.115110   test accuracy:   0.958333\n",
      "########### loop 28300 ###########\n",
      "train loss:   1.076174\n",
      "train loss:   1.177712\n",
      "train loss:   1.077114\n",
      "train loss:   0.797100\n",
      "train loss:   0.881806\n",
      "train loss:   0.875160\n",
      "train loss:   0.814091\n",
      "train loss:   0.757265\n",
      "train loss:   1.005873\n",
      "train loss:   0.784527\n",
      "train loss:   0.724756\n",
      "train loss:   1.142348\n",
      "train loss:   1.063619\n",
      "train loss:   0.587374\n",
      "train loss:   1.013426\n",
      "train loss:   0.803857\n",
      "train loss:   0.891124\n",
      "train loss:   0.797665\n",
      "train loss:   0.764320\n",
      "train loss:   1.176339\n",
      "train loss:   0.870537\n",
      "train loss:   1.219682\n",
      "train loss:   1.020183\n",
      "train loss:   1.178076\n",
      "train loss:   0.732677\n",
      "train loss:   0.811212\n",
      "train loss:   1.218165\n",
      "train loss:   1.214415\n",
      "train loss:   0.566636\n",
      "train loss:   1.176185\n",
      "train loss:   0.906385\n",
      "train loss:   1.058814\n",
      "train loss:   0.592694\n",
      "train loss:   1.160471\n",
      "train loss:   0.946886\n",
      "train loss:   1.061500\n",
      "train loss:   0.773120\n",
      "train loss:   0.700081\n",
      "train loss:   0.993562\n",
      "train loss:   0.851401\n",
      "train loss:   1.054839\n",
      "train loss:   1.326228\n",
      "train loss:   1.074176\n",
      "train loss:   0.918341\n",
      "train loss:   0.996816\n",
      "train loss:   0.861220\n",
      "train loss:   0.837207\n",
      "train loss:   0.894490\n",
      "train loss:   1.108127\n",
      "train loss:   0.995827\n",
      "########### epoch 151 ###########\n",
      "########### loop 28350 ###########\n",
      "test loss:   0.191315   test accuracy:   0.916667\n",
      "########### loop 28350 ###########\n",
      "train loss:   0.662382\n",
      "train loss:   1.079436\n",
      "train loss:   0.804155\n",
      "train loss:   1.140664\n",
      "train loss:   0.971510\n",
      "train loss:   0.935946\n",
      "train loss:   1.362060\n",
      "train loss:   0.997592\n",
      "train loss:   0.773495\n",
      "train loss:   1.137451\n",
      "train loss:   0.694958\n",
      "train loss:   1.060802\n",
      "train loss:   0.634365\n",
      "train loss:   0.865737\n",
      "train loss:   0.769205\n",
      "train loss:   1.341974\n",
      "train loss:   1.401827\n",
      "train loss:   0.613080\n",
      "train loss:   0.806980\n",
      "train loss:   0.892759\n",
      "train loss:   0.817972\n",
      "train loss:   0.649713\n",
      "train loss:   0.929027\n",
      "train loss:   0.832658\n",
      "train loss:   1.222090\n",
      "train loss:   0.940472\n",
      "train loss:   0.570165\n",
      "train loss:   0.719158\n",
      "train loss:   1.074459\n",
      "train loss:   0.752370\n",
      "train loss:   1.055374\n",
      "train loss:   0.908422\n",
      "train loss:   1.034913\n",
      "train loss:   1.028819\n",
      "train loss:   1.026980\n",
      "train loss:   0.705678\n",
      "train loss:   0.717592\n",
      "train loss:   1.007576\n",
      "train loss:   0.855605\n",
      "train loss:   0.761259\n",
      "train loss:   1.189146\n",
      "train loss:   0.616411\n",
      "train loss:   0.962891\n",
      "train loss:   1.059483\n",
      "train loss:   0.768898\n",
      "train loss:   1.207403\n",
      "train loss:   0.667572\n",
      "train loss:   0.815801\n",
      "train loss:   0.680311\n",
      "train loss:   0.995247\n",
      "########### epoch 152 ###########\n",
      "########### loop 28400 ###########\n",
      "test loss:   0.147828   test accuracy:   0.958333\n",
      "########### loop 28400 ###########\n",
      "train loss:   0.905048\n",
      "train loss:   0.945594\n",
      "train loss:   1.082049\n",
      "train loss:   1.235772\n",
      "train loss:   1.123028\n",
      "train loss:   0.795514\n",
      "train loss:   0.905509\n",
      "train loss:   0.807832\n",
      "train loss:   1.106770\n",
      "train loss:   0.964090\n",
      "train loss:   0.571578\n",
      "train loss:   0.718035\n",
      "train loss:   0.582386\n",
      "train loss:   1.007334\n",
      "train loss:   0.641496\n",
      "train loss:   1.404788\n",
      "train loss:   0.785973\n",
      "train loss:   0.838497\n",
      "train loss:   1.186588\n",
      "train loss:   0.808588\n",
      "train loss:   0.773015\n",
      "train loss:   1.191815\n",
      "train loss:   1.071970\n",
      "train loss:   1.078851\n",
      "train loss:   0.659337\n",
      "train loss:   0.920054\n",
      "train loss:   0.871248\n",
      "train loss:   1.017995\n",
      "train loss:   1.201451\n",
      "train loss:   1.024253\n",
      "train loss:   1.006971\n",
      "train loss:   0.772749\n",
      "train loss:   1.360611\n",
      "train loss:   0.794466\n",
      "train loss:   0.730090\n",
      "train loss:   1.253264\n",
      "train loss:   0.936254\n",
      "train loss:   0.916227\n",
      "train loss:   0.921150\n",
      "train loss:   0.782294\n",
      "train loss:   1.065062\n",
      "train loss:   0.972640\n",
      "train loss:   1.258493\n",
      "train loss:   1.357054\n",
      "train loss:   1.402851\n",
      "train loss:   0.687294\n",
      "train loss:   0.933541\n",
      "train loss:   0.810359\n",
      "train loss:   0.976078\n",
      "train loss:   1.188271\n",
      "########### epoch 152 ###########\n",
      "########### loop 28450 ###########\n",
      "test loss:   0.129351   test accuracy:   1.000000\n",
      "########### loop 28450 ###########\n",
      "train loss:   1.290880\n",
      "train loss:   0.720594\n",
      "train loss:   0.524415\n",
      "train loss:   1.060997\n",
      "train loss:   1.043395\n",
      "train loss:   0.947662\n",
      "train loss:   0.936735\n",
      "train loss:   1.016885\n",
      "train loss:   1.018974\n",
      "train loss:   1.183568\n",
      "train loss:   0.784310\n",
      "train loss:   0.851583\n",
      "train loss:   1.173245\n",
      "train loss:   1.002310\n",
      "train loss:   1.054711\n",
      "train loss:   1.102547\n",
      "train loss:   1.193591\n",
      "train loss:   1.312300\n",
      "train loss:   1.280967\n",
      "train loss:   1.084456\n",
      "train loss:   1.137081\n",
      "train loss:   1.157642\n",
      "train loss:   0.836275\n",
      "train loss:   0.892221\n",
      "train loss:   0.971812\n",
      "train loss:   0.937484\n",
      "train loss:   1.162946\n",
      "train loss:   1.027641\n",
      "train loss:   0.958597\n",
      "train loss:   0.874594\n",
      "train loss:   1.012823\n",
      "train loss:   1.027943\n",
      "train loss:   0.886979\n",
      "train loss:   0.722416\n",
      "train loss:   0.954884\n",
      "train loss:   0.917411\n",
      "train loss:   0.748436\n",
      "train loss:   0.844423\n",
      "train loss:   0.928714\n",
      "train loss:   0.895516\n",
      "train loss:   1.145776\n",
      "train loss:   0.899498\n",
      "train loss:   1.127595\n",
      "train loss:   0.742624\n",
      "train loss:   1.048495\n",
      "train loss:   0.809339\n",
      "train loss:   0.889248\n",
      "train loss:   0.707291\n",
      "train loss:   0.787759\n",
      "train loss:   0.961250\n",
      "########### epoch 152 ###########\n",
      "########### loop 28500 ###########\n",
      "test loss:   0.355709   test accuracy:   0.875000\n",
      "########### loop 28500 ###########\n",
      "train loss:   0.988780\n",
      "train loss:   1.086120\n",
      "train loss:   1.139987\n",
      "train loss:   0.628850\n",
      "train loss:   1.021343\n",
      "train loss:   1.078127\n",
      "train loss:   0.883718\n",
      "train loss:   0.938956\n",
      "train loss:   0.844869\n",
      "train loss:   0.949233\n",
      "train loss:   0.966445\n",
      "train loss:   0.914574\n",
      "train loss:   1.066211\n",
      "train loss:   1.069823\n",
      "train loss:   0.959557\n",
      "train loss:   1.242893\n",
      "train loss:   0.974113\n",
      "train loss:   1.068534\n",
      "train loss:   0.920777\n",
      "train loss:   0.998045\n",
      "train loss:   0.649927\n",
      "train loss:   0.736883\n",
      "train loss:   0.909479\n",
      "train loss:   1.160895\n",
      "train loss:   1.056541\n",
      "train loss:   1.196216\n",
      "train loss:   0.746522\n",
      "train loss:   0.756413\n",
      "train loss:   0.834824\n",
      "train loss:   0.851032\n",
      "train loss:   1.132294\n",
      "train loss:   1.115917\n",
      "train loss:   0.904633\n",
      "train loss:   1.344627\n",
      "train loss:   0.949354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.987315\n",
      "train loss:   0.792314\n",
      "train loss:   0.869299\n",
      "train loss:   1.471361\n",
      "train loss:   0.949509\n",
      "train loss:   0.937528\n",
      "train loss:   1.030593\n",
      "train loss:   1.349313\n",
      "train loss:   0.822446\n",
      "train loss:   0.902967\n",
      "train loss:   0.974406\n",
      "train loss:   1.112359\n",
      "train loss:   1.087913\n",
      "train loss:   1.039073\n",
      "train loss:   0.827502\n",
      "########### epoch 152 ###########\n",
      "########### loop 28550 ###########\n",
      "test loss:   0.883385   test accuracy:   0.750000\n",
      "########### loop 28550 ###########\n",
      "train loss:   0.926432\n",
      "train loss:   0.745855\n",
      "train loss:   1.225482\n",
      "train loss:   1.175551\n",
      "train loss:   0.838138\n",
      "train loss:   0.725118\n",
      "train loss:   0.808438\n",
      "train loss:   0.905321\n",
      "train loss:   0.856927\n",
      "train loss:   0.587768\n",
      "train loss:   1.259323\n",
      "train loss:   0.868262\n",
      "train loss:   0.938510\n",
      "train loss:   1.002928\n",
      "train loss:   1.003027\n",
      "train loss:   0.846345\n",
      "train loss:   0.942836\n",
      "train loss:   0.972442\n",
      "train loss:   0.933078\n",
      "train loss:   0.861099\n",
      "train loss:   1.001706\n",
      "train loss:   0.699788\n",
      "train loss:   0.883977\n",
      "train loss:   0.915131\n",
      "train loss:   0.954457\n",
      "train loss:   0.675815\n",
      "train loss:   0.889522\n",
      "train loss:   0.908308\n",
      "train loss:   0.813301\n",
      "train loss:   0.631524\n",
      "train loss:   0.790913\n",
      "train loss:   0.946098\n",
      "train loss:   0.966382\n",
      "train loss:   0.758767\n",
      "train loss:   0.828000\n",
      "train loss:   0.784948\n",
      "train loss:   1.045137\n",
      "train loss:   0.927751\n",
      "train loss:   0.999942\n",
      "train loss:   0.703054\n",
      "train loss:   1.042060\n",
      "train loss:   0.822451\n",
      "train loss:   1.002482\n",
      "train loss:   0.840580\n",
      "train loss:   0.860661\n",
      "train loss:   0.866829\n",
      "train loss:   0.872400\n",
      "train loss:   1.058295\n",
      "train loss:   1.124742\n",
      "train loss:   0.828705\n",
      "########### epoch 153 ###########\n",
      "########### loop 28600 ###########\n",
      "test loss:   0.166675   test accuracy:   0.958333\n",
      "########### loop 28600 ###########\n",
      "train loss:   1.023094\n",
      "train loss:   0.914569\n",
      "train loss:   0.987685\n",
      "train loss:   1.194460\n",
      "train loss:   1.201383\n",
      "train loss:   0.636442\n",
      "train loss:   0.878169\n",
      "train loss:   0.894545\n",
      "train loss:   0.894149\n",
      "train loss:   0.798917\n",
      "train loss:   1.187312\n",
      "train loss:   0.931933\n",
      "train loss:   0.943795\n",
      "train loss:   0.924792\n",
      "train loss:   0.852227\n",
      "train loss:   0.834942\n",
      "train loss:   0.551392\n",
      "train loss:   0.839296\n",
      "train loss:   0.878998\n",
      "train loss:   1.150973\n",
      "train loss:   1.012023\n",
      "train loss:   0.925196\n",
      "train loss:   0.888755\n",
      "train loss:   1.307900\n",
      "train loss:   0.974728\n",
      "train loss:   1.104064\n",
      "train loss:   0.947100\n",
      "train loss:   1.058453\n",
      "train loss:   0.746155\n",
      "train loss:   0.992592\n",
      "train loss:   0.644530\n",
      "train loss:   1.047359\n",
      "train loss:   1.027491\n",
      "train loss:   0.880428\n",
      "train loss:   0.718373\n",
      "train loss:   0.741858\n",
      "train loss:   0.723784\n",
      "train loss:   0.918396\n",
      "train loss:   1.031252\n",
      "train loss:   0.816861\n",
      "train loss:   0.598758\n",
      "train loss:   0.486307\n",
      "train loss:   0.973566\n",
      "train loss:   1.045945\n",
      "train loss:   1.073533\n",
      "train loss:   0.710829\n",
      "train loss:   0.991519\n",
      "train loss:   0.755278\n",
      "train loss:   1.243362\n",
      "train loss:   1.061355\n",
      "########### epoch 153 ###########\n",
      "########### loop 28650 ###########\n",
      "test loss:   0.164904   test accuracy:   0.958333\n",
      "########### loop 28650 ###########\n",
      "train loss:   1.023845\n",
      "train loss:   0.806910\n",
      "train loss:   1.178115\n",
      "train loss:   0.815386\n",
      "train loss:   1.066540\n",
      "train loss:   0.901588\n",
      "train loss:   0.591875\n",
      "train loss:   1.155139\n",
      "train loss:   1.098755\n",
      "train loss:   0.817299\n",
      "train loss:   0.752701\n",
      "train loss:   0.861213\n",
      "train loss:   0.932386\n",
      "train loss:   1.124439\n",
      "train loss:   0.586373\n",
      "train loss:   1.187473\n",
      "train loss:   1.196489\n",
      "train loss:   0.520048\n",
      "train loss:   0.983419\n",
      "train loss:   1.181896\n",
      "train loss:   0.736268\n",
      "train loss:   0.781036\n",
      "train loss:   1.213253\n",
      "train loss:   0.984122\n",
      "train loss:   0.864922\n",
      "train loss:   1.084439\n",
      "train loss:   0.942087\n",
      "train loss:   0.741801\n",
      "train loss:   0.920487\n",
      "train loss:   0.889419\n",
      "train loss:   0.878174\n",
      "train loss:   0.794119\n",
      "train loss:   1.310178\n",
      "train loss:   0.859486\n",
      "train loss:   1.301913\n",
      "train loss:   0.991867\n",
      "train loss:   0.848033\n",
      "train loss:   0.884658\n",
      "train loss:   1.001167\n",
      "train loss:   0.962826\n",
      "train loss:   0.855953\n",
      "train loss:   0.984131\n",
      "train loss:   0.766917\n",
      "train loss:   0.568059\n",
      "train loss:   1.204800\n",
      "train loss:   0.849687\n",
      "train loss:   0.565676\n",
      "train loss:   1.113146\n",
      "train loss:   0.518713\n",
      "train loss:   0.996531\n",
      "########### epoch 153 ###########\n",
      "########### loop 28700 ###########\n",
      "test loss:   0.121009   test accuracy:   1.000000\n",
      "########### loop 28700 ###########\n",
      "train loss:   0.834301\n",
      "train loss:   0.587250\n",
      "train loss:   0.545221\n",
      "train loss:   0.653056\n",
      "train loss:   0.929865\n",
      "train loss:   1.292124\n",
      "train loss:   0.848553\n",
      "train loss:   1.125111\n",
      "train loss:   1.289059\n",
      "train loss:   0.721523\n",
      "train loss:   1.298666\n",
      "train loss:   0.708814\n",
      "train loss:   1.045491\n",
      "train loss:   0.922533\n",
      "train loss:   1.350505\n",
      "train loss:   0.757736\n",
      "train loss:   0.660038\n",
      "train loss:   0.997696\n",
      "train loss:   1.064913\n",
      "train loss:   0.875830\n",
      "train loss:   0.976480\n",
      "train loss:   0.982970\n",
      "train loss:   1.025993\n",
      "train loss:   1.021376\n",
      "train loss:   0.793787\n",
      "train loss:   1.134147\n",
      "train loss:   1.185913\n",
      "train loss:   1.142552\n",
      "train loss:   0.879311\n",
      "train loss:   0.637002\n",
      "train loss:   0.848230\n",
      "train loss:   0.955355\n",
      "train loss:   0.703199\n",
      "train loss:   0.793058\n",
      "train loss:   1.192275\n",
      "train loss:   0.603747\n",
      "train loss:   1.137145\n",
      "train loss:   0.636751\n",
      "train loss:   0.907458\n",
      "train loss:   0.891398\n",
      "train loss:   0.890769\n",
      "train loss:   1.027100\n",
      "train loss:   1.039290\n",
      "train loss:   0.861872\n",
      "train loss:   1.008157\n",
      "train loss:   0.936633\n",
      "train loss:   1.124851\n",
      "train loss:   0.901523\n",
      "train loss:   1.261676\n",
      "train loss:   1.032610\n",
      "########### epoch 153 ###########\n",
      "########### loop 28750 ###########\n",
      "test loss:   0.170827   test accuracy:   0.958333\n",
      "########### loop 28750 ###########\n",
      "train loss:   1.070463\n",
      "train loss:   0.884471\n",
      "train loss:   1.147744\n",
      "train loss:   1.090029\n",
      "train loss:   1.274369\n",
      "train loss:   1.219103\n",
      "train loss:   0.776480\n",
      "train loss:   1.001805\n",
      "train loss:   1.033881\n",
      "train loss:   0.721867\n",
      "train loss:   0.885007\n",
      "train loss:   0.767933\n",
      "train loss:   1.047379\n",
      "train loss:   1.001827\n",
      "train loss:   1.072102\n",
      "train loss:   1.157862\n",
      "train loss:   0.732429\n",
      "train loss:   0.879352\n",
      "train loss:   1.015401\n",
      "train loss:   0.978720\n",
      "train loss:   1.060410\n",
      "train loss:   1.121418\n",
      "train loss:   0.800841\n",
      "train loss:   0.953453\n",
      "train loss:   0.925775\n",
      "train loss:   0.537343\n",
      "train loss:   1.189650\n",
      "train loss:   0.904036\n",
      "train loss:   0.875671\n",
      "train loss:   1.076093\n",
      "train loss:   0.783904\n",
      "train loss:   0.864399\n",
      "train loss:   0.911527\n",
      "train loss:   0.938172\n",
      "train loss:   0.683734\n",
      "train loss:   0.644291\n",
      "train loss:   1.098652\n",
      "train loss:   0.824043\n",
      "train loss:   1.084562\n",
      "train loss:   0.917669\n",
      "train loss:   0.731832\n",
      "train loss:   0.988045\n",
      "train loss:   0.784708\n",
      "train loss:   0.925962\n",
      "train loss:   0.986875\n",
      "train loss:   1.054240\n",
      "train loss:   0.750605\n",
      "train loss:   0.926489\n",
      "train loss:   1.114843\n",
      "train loss:   0.752708\n",
      "########### epoch 154 ###########\n",
      "########### loop 28800 ###########\n",
      "test loss:   0.229040   test accuracy:   0.916667\n",
      "########### loop 28800 ###########\n",
      "train loss:   1.051627\n",
      "train loss:   1.130291\n",
      "train loss:   0.999811\n",
      "train loss:   1.049187\n",
      "train loss:   1.168734\n",
      "train loss:   0.778357\n",
      "train loss:   1.323325\n",
      "train loss:   0.985410\n",
      "train loss:   1.006690\n",
      "train loss:   0.751406\n",
      "train loss:   0.731925\n",
      "train loss:   1.062359\n",
      "train loss:   1.079409\n",
      "train loss:   1.232350\n",
      "train loss:   1.019652\n",
      "train loss:   0.587586\n",
      "train loss:   0.801193\n",
      "train loss:   0.978180\n",
      "train loss:   1.543659\n",
      "train loss:   1.051646\n",
      "train loss:   0.971862\n",
      "train loss:   0.608965\n",
      "train loss:   1.022201\n",
      "train loss:   0.832004\n",
      "train loss:   0.594861\n",
      "train loss:   1.150928\n",
      "train loss:   0.892087\n",
      "train loss:   0.648859\n",
      "train loss:   1.003340\n",
      "train loss:   1.167622\n",
      "train loss:   0.679868\n",
      "train loss:   0.962159\n",
      "train loss:   1.318903\n",
      "train loss:   0.647965\n",
      "train loss:   0.985922\n",
      "train loss:   0.850446\n",
      "train loss:   1.076263\n",
      "train loss:   0.866892\n",
      "train loss:   0.937213\n",
      "train loss:   1.577633\n",
      "train loss:   1.012500\n",
      "train loss:   0.827046\n",
      "train loss:   1.212665\n",
      "train loss:   0.789496\n",
      "train loss:   1.156973\n",
      "train loss:   0.734380\n",
      "train loss:   0.608720\n",
      "train loss:   0.982936\n",
      "train loss:   0.975595\n",
      "train loss:   1.053039\n",
      "########### epoch 154 ###########\n",
      "########### loop 28850 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.450594   test accuracy:   0.791667\n",
      "########### loop 28850 ###########\n",
      "train loss:   0.817414\n",
      "train loss:   0.762117\n",
      "train loss:   0.543861\n",
      "train loss:   0.675892\n",
      "train loss:   1.210428\n",
      "train loss:   1.264340\n",
      "train loss:   0.757600\n",
      "train loss:   0.966379\n",
      "train loss:   1.163571\n",
      "train loss:   1.238905\n",
      "train loss:   0.828657\n",
      "train loss:   1.021087\n",
      "train loss:   1.008672\n",
      "train loss:   0.995804\n",
      "train loss:   0.753735\n",
      "train loss:   1.123347\n",
      "train loss:   0.690580\n",
      "train loss:   1.084252\n",
      "train loss:   1.064826\n",
      "train loss:   0.758790\n",
      "train loss:   0.826684\n",
      "train loss:   1.220469\n",
      "train loss:   1.346025\n",
      "train loss:   0.924946\n",
      "train loss:   0.869378\n",
      "train loss:   0.955245\n",
      "train loss:   0.870400\n",
      "train loss:   0.963652\n",
      "train loss:   0.963027\n",
      "train loss:   0.838716\n",
      "train loss:   0.972844\n",
      "train loss:   1.144258\n",
      "train loss:   0.753747\n",
      "train loss:   0.938362\n",
      "train loss:   0.832075\n",
      "train loss:   1.049456\n",
      "train loss:   0.728938\n",
      "train loss:   0.930246\n",
      "train loss:   0.828642\n",
      "train loss:   1.310396\n",
      "train loss:   1.059290\n",
      "train loss:   0.820566\n",
      "train loss:   0.962659\n",
      "train loss:   1.350417\n",
      "train loss:   0.788687\n",
      "train loss:   1.239445\n",
      "train loss:   0.949604\n",
      "train loss:   1.300504\n",
      "train loss:   1.265463\n",
      "train loss:   1.101095\n",
      "########### epoch 154 ###########\n",
      "########### loop 28900 ###########\n",
      "test loss:   0.095445   test accuracy:   1.000000\n",
      "########### loop 28900 ###########\n",
      "train loss:   0.896584\n",
      "train loss:   1.150015\n",
      "train loss:   0.716522\n",
      "train loss:   1.146635\n",
      "train loss:   0.829305\n",
      "train loss:   0.741238\n",
      "train loss:   1.022300\n",
      "train loss:   0.721959\n",
      "train loss:   1.144501\n",
      "train loss:   0.985770\n",
      "train loss:   1.094822\n",
      "train loss:   1.003453\n",
      "train loss:   1.128464\n",
      "train loss:   0.946224\n",
      "train loss:   0.889038\n",
      "train loss:   1.054793\n",
      "train loss:   1.068610\n",
      "train loss:   1.031344\n",
      "train loss:   0.852337\n",
      "train loss:   1.084313\n",
      "train loss:   1.205141\n",
      "train loss:   0.941504\n",
      "train loss:   0.926553\n",
      "train loss:   1.078802\n",
      "train loss:   1.012897\n",
      "train loss:   0.710575\n",
      "train loss:   0.993911\n",
      "train loss:   0.876654\n",
      "train loss:   0.923615\n",
      "train loss:   1.214029\n",
      "train loss:   0.845196\n",
      "train loss:   0.954471\n",
      "train loss:   1.101189\n",
      "train loss:   0.726128\n",
      "train loss:   0.789539\n",
      "train loss:   1.124619\n",
      "train loss:   0.779523\n",
      "train loss:   0.914525\n",
      "train loss:   0.788043\n",
      "train loss:   0.967103\n",
      "train loss:   0.984100\n",
      "train loss:   1.041463\n",
      "train loss:   0.942682\n",
      "train loss:   0.921562\n",
      "train loss:   0.797499\n",
      "train loss:   0.969478\n",
      "train loss:   0.752434\n",
      "train loss:   0.976284\n",
      "train loss:   0.885079\n",
      "train loss:   0.905250\n",
      "########### epoch 154 ###########\n",
      "########### loop 28950 ###########\n",
      "test loss:   0.110880   test accuracy:   1.000000\n",
      "########### loop 28950 ###########\n",
      "train loss:   1.011127\n",
      "train loss:   0.847529\n",
      "train loss:   0.840557\n",
      "train loss:   1.186235\n",
      "train loss:   0.799135\n",
      "train loss:   1.100407\n",
      "train loss:   1.152189\n",
      "train loss:   0.642038\n",
      "train loss:   1.017466\n",
      "train loss:   0.626613\n",
      "train loss:   0.964159\n",
      "train loss:   0.973462\n",
      "train loss:   0.971397\n",
      "train loss:   0.961182\n",
      "train loss:   0.650116\n",
      "train loss:   1.434330\n",
      "train loss:   0.643747\n",
      "train loss:   0.828503\n",
      "train loss:   1.431090\n",
      "train loss:   0.639087\n",
      "train loss:   0.908454\n",
      "train loss:   0.846014\n",
      "train loss:   0.958998\n",
      "train loss:   0.890934\n",
      "train loss:   0.991766\n",
      "train loss:   0.957748\n",
      "train loss:   0.901544\n",
      "train loss:   1.033056\n",
      "train loss:   0.883209\n",
      "train loss:   1.049513\n",
      "train loss:   0.745250\n",
      "train loss:   1.011671\n",
      "train loss:   0.841965\n",
      "train loss:   0.816290\n",
      "train loss:   0.905552\n",
      "train loss:   1.147144\n",
      "train loss:   1.260637\n",
      "train loss:   1.312700\n",
      "train loss:   1.056568\n",
      "train loss:   0.876537\n",
      "train loss:   1.119773\n",
      "train loss:   0.747890\n",
      "train loss:   1.295888\n",
      "train loss:   1.071718\n",
      "train loss:   0.994161\n",
      "train loss:   0.926871\n",
      "train loss:   1.004891\n",
      "train loss:   1.309860\n",
      "train loss:   1.037771\n",
      "train loss:   1.234263\n",
      "########### epoch 155 ###########\n",
      "########### loop 29000 ###########\n",
      "test loss:   0.107062   test accuracy:   1.000000\n",
      "########### loop 29000 ###########\n",
      "train loss:   0.927532\n",
      "train loss:   1.064112\n",
      "train loss:   1.088603\n",
      "train loss:   1.066380\n",
      "train loss:   0.784940\n",
      "train loss:   0.984977\n",
      "train loss:   0.835223\n",
      "train loss:   1.042074\n",
      "train loss:   0.932311\n",
      "train loss:   0.900711\n",
      "train loss:   0.975197\n",
      "train loss:   0.982493\n",
      "train loss:   0.663702\n",
      "train loss:   0.782897\n",
      "train loss:   0.515101\n",
      "train loss:   1.428082\n",
      "train loss:   0.770895\n",
      "train loss:   0.832607\n",
      "train loss:   0.890743\n",
      "train loss:   0.847459\n",
      "train loss:   1.097364\n",
      "train loss:   1.019840\n",
      "train loss:   1.130781\n",
      "train loss:   0.875958\n",
      "train loss:   1.079219\n",
      "train loss:   0.761548\n",
      "train loss:   1.116300\n",
      "train loss:   0.945757\n",
      "train loss:   1.060361\n",
      "train loss:   0.866136\n",
      "train loss:   0.946945\n",
      "train loss:   1.130778\n",
      "train loss:   1.082030\n",
      "train loss:   0.661648\n",
      "train loss:   0.742512\n",
      "train loss:   0.752960\n",
      "train loss:   0.988426\n",
      "train loss:   1.115788\n",
      "train loss:   1.021906\n",
      "train loss:   0.862730\n",
      "train loss:   1.030775\n",
      "train loss:   0.872630\n",
      "train loss:   0.782383\n",
      "train loss:   0.875553\n",
      "train loss:   0.851377\n",
      "train loss:   0.867713\n",
      "train loss:   0.974662\n",
      "train loss:   0.940109\n",
      "train loss:   1.066598\n",
      "train loss:   0.814699\n",
      "########### epoch 155 ###########\n",
      "########### loop 29050 ###########\n",
      "test loss:   0.275376   test accuracy:   0.916667\n",
      "########### loop 29050 ###########\n",
      "train loss:   0.772021\n",
      "train loss:   1.523496\n",
      "train loss:   0.935995\n",
      "train loss:   0.816666\n",
      "train loss:   1.001571\n",
      "train loss:   0.863148\n",
      "train loss:   0.899556\n",
      "train loss:   1.207287\n",
      "train loss:   1.182313\n",
      "train loss:   0.875121\n",
      "train loss:   0.920007\n",
      "train loss:   0.782184\n",
      "train loss:   0.960773\n",
      "train loss:   0.877200\n",
      "train loss:   0.976558\n",
      "train loss:   1.212665\n",
      "train loss:   0.956991\n",
      "train loss:   0.914401\n",
      "train loss:   0.834299\n",
      "train loss:   0.863670\n",
      "train loss:   0.976976\n",
      "train loss:   1.213391\n",
      "train loss:   0.874629\n",
      "train loss:   0.998527\n",
      "train loss:   0.511262\n",
      "train loss:   0.993836\n",
      "train loss:   0.765533\n",
      "train loss:   1.224336\n",
      "train loss:   0.764835\n",
      "train loss:   0.934756\n",
      "train loss:   0.949722\n",
      "train loss:   1.080961\n",
      "train loss:   0.807951\n",
      "train loss:   0.809515\n",
      "train loss:   0.966645\n",
      "train loss:   1.239139\n",
      "train loss:   1.169116\n",
      "train loss:   0.855850\n",
      "train loss:   0.758846\n",
      "train loss:   0.841466\n",
      "train loss:   1.019572\n",
      "train loss:   0.797155\n",
      "train loss:   0.879429\n",
      "train loss:   1.219150\n",
      "train loss:   0.480270\n",
      "train loss:   0.905749\n",
      "train loss:   1.004325\n",
      "train loss:   1.051918\n",
      "train loss:   0.878518\n",
      "train loss:   0.604336\n",
      "########### epoch 155 ###########\n",
      "########### loop 29100 ###########\n",
      "test loss:   0.181411   test accuracy:   0.958333\n",
      "########### loop 29100 ###########\n",
      "train loss:   0.946219\n",
      "train loss:   0.471352\n",
      "train loss:   1.166841\n",
      "train loss:   1.126845\n",
      "train loss:   0.798272\n",
      "train loss:   0.824085\n",
      "train loss:   1.050844\n",
      "train loss:   0.879453\n",
      "train loss:   0.900581\n",
      "train loss:   0.962014\n",
      "train loss:   0.754789\n",
      "train loss:   0.803317\n",
      "train loss:   0.849783\n",
      "train loss:   1.061213\n",
      "train loss:   0.850505\n",
      "train loss:   0.487161\n",
      "train loss:   1.222516\n",
      "train loss:   1.193821\n",
      "train loss:   0.719552\n",
      "train loss:   0.871377\n",
      "train loss:   0.996670\n",
      "train loss:   1.199362\n",
      "train loss:   0.986183\n",
      "train loss:   0.796250\n",
      "train loss:   0.867532\n",
      "train loss:   0.992679\n",
      "train loss:   0.939683\n",
      "train loss:   0.885436\n",
      "train loss:   0.970606\n",
      "train loss:   0.953566\n",
      "train loss:   0.883473\n",
      "train loss:   0.923683\n",
      "train loss:   1.359123\n",
      "train loss:   1.140213\n",
      "train loss:   0.886603\n",
      "train loss:   1.097785\n",
      "train loss:   0.780687\n",
      "train loss:   0.653847\n",
      "train loss:   1.019094\n",
      "train loss:   0.800373\n",
      "train loss:   1.033196\n",
      "train loss:   0.751832\n",
      "train loss:   0.858461\n",
      "train loss:   0.821211\n",
      "train loss:   1.233037\n",
      "train loss:   1.161607\n",
      "train loss:   0.685269\n",
      "train loss:   1.065353\n",
      "train loss:   0.763585\n",
      "train loss:   1.407609\n",
      "########### epoch 156 ###########\n",
      "########### loop 29150 ###########\n",
      "test loss:   0.246418   test accuracy:   0.916667\n",
      "########### loop 29150 ###########\n",
      "train loss:   0.816340\n",
      "train loss:   0.973533\n",
      "train loss:   1.141544\n",
      "train loss:   0.627675\n",
      "train loss:   0.674214\n",
      "train loss:   0.859509\n",
      "train loss:   0.875770\n",
      "train loss:   0.833220\n",
      "train loss:   0.939239\n",
      "train loss:   0.687787\n",
      "train loss:   0.780855\n",
      "train loss:   0.922177\n",
      "train loss:   1.229090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.929499\n",
      "train loss:   0.904811\n",
      "train loss:   1.202724\n",
      "train loss:   0.686056\n",
      "train loss:   1.121683\n",
      "train loss:   0.839292\n",
      "train loss:   1.051750\n",
      "train loss:   1.032300\n",
      "train loss:   0.895887\n",
      "train loss:   0.577494\n",
      "train loss:   1.153284\n",
      "train loss:   0.911235\n",
      "train loss:   1.026609\n",
      "train loss:   0.686143\n",
      "train loss:   1.134006\n",
      "train loss:   0.995867\n",
      "train loss:   1.131741\n",
      "train loss:   1.363591\n",
      "train loss:   0.913107\n",
      "train loss:   1.174475\n",
      "train loss:   1.044410\n",
      "train loss:   1.149146\n",
      "train loss:   0.948033\n",
      "train loss:   0.826687\n",
      "train loss:   0.887940\n",
      "train loss:   0.916056\n",
      "train loss:   1.077055\n",
      "train loss:   1.178407\n",
      "train loss:   0.953813\n",
      "train loss:   0.951426\n",
      "train loss:   0.559138\n",
      "train loss:   1.106373\n",
      "train loss:   0.496618\n",
      "train loss:   0.906579\n",
      "train loss:   1.062626\n",
      "train loss:   0.681651\n",
      "train loss:   1.139217\n",
      "########### epoch 156 ###########\n",
      "########### loop 29200 ###########\n",
      "test loss:   0.207187   test accuracy:   0.916667\n",
      "########### loop 29200 ###########\n",
      "train loss:   1.204586\n",
      "train loss:   1.027148\n",
      "train loss:   1.148070\n",
      "train loss:   1.112361\n",
      "train loss:   1.118823\n",
      "train loss:   0.625487\n",
      "train loss:   0.508564\n",
      "train loss:   1.047955\n",
      "train loss:   1.036313\n",
      "train loss:   1.162564\n",
      "train loss:   0.801136\n",
      "train loss:   1.038871\n",
      "train loss:   1.125755\n",
      "train loss:   0.617309\n",
      "train loss:   0.966659\n",
      "train loss:   0.665315\n",
      "train loss:   0.684900\n",
      "train loss:   1.119598\n",
      "train loss:   1.141992\n",
      "train loss:   1.179935\n",
      "train loss:   1.105381\n",
      "train loss:   0.812851\n",
      "train loss:   1.183175\n",
      "train loss:   0.966403\n",
      "train loss:   0.854316\n",
      "train loss:   0.927227\n",
      "train loss:   0.904004\n",
      "train loss:   1.034246\n",
      "train loss:   0.957732\n",
      "train loss:   0.792542\n",
      "train loss:   0.799354\n",
      "train loss:   1.042873\n",
      "train loss:   1.070215\n",
      "train loss:   0.970418\n",
      "train loss:   0.631581\n",
      "train loss:   0.500067\n",
      "train loss:   1.151338\n",
      "train loss:   0.882922\n",
      "train loss:   0.676171\n",
      "train loss:   0.872640\n",
      "train loss:   0.984653\n",
      "train loss:   1.001235\n",
      "train loss:   0.801837\n",
      "train loss:   0.788966\n",
      "train loss:   0.927810\n",
      "train loss:   1.035947\n",
      "train loss:   0.446951\n",
      "train loss:   1.173725\n",
      "train loss:   0.768253\n",
      "train loss:   0.866712\n",
      "########### epoch 156 ###########\n",
      "########### loop 29250 ###########\n",
      "test loss:   0.344682   test accuracy:   0.875000\n",
      "########### loop 29250 ###########\n",
      "train loss:   0.853654\n",
      "train loss:   0.871312\n",
      "train loss:   1.156399\n",
      "train loss:   1.140342\n",
      "train loss:   0.564079\n",
      "train loss:   1.006166\n",
      "train loss:   1.014328\n",
      "train loss:   0.787257\n",
      "train loss:   1.041184\n",
      "train loss:   0.971821\n",
      "train loss:   1.245735\n",
      "train loss:   0.940617\n",
      "train loss:   0.991938\n",
      "train loss:   1.077967\n",
      "train loss:   1.051527\n",
      "train loss:   0.816957\n",
      "train loss:   0.862054\n",
      "train loss:   0.968389\n",
      "train loss:   1.018273\n",
      "train loss:   0.772944\n",
      "train loss:   0.985604\n",
      "train loss:   0.749806\n",
      "train loss:   0.918744\n",
      "train loss:   1.098945\n",
      "train loss:   1.028115\n",
      "train loss:   0.899416\n",
      "train loss:   1.074594\n",
      "train loss:   0.853860\n",
      "train loss:   1.012258\n",
      "train loss:   1.056524\n",
      "train loss:   0.808708\n",
      "train loss:   0.924310\n",
      "train loss:   0.842069\n",
      "train loss:   1.227630\n",
      "train loss:   0.854545\n",
      "train loss:   0.515791\n",
      "train loss:   0.936532\n",
      "train loss:   0.911254\n",
      "train loss:   1.091401\n",
      "train loss:   1.024742\n",
      "train loss:   0.990066\n",
      "train loss:   0.940470\n",
      "train loss:   1.256457\n",
      "train loss:   1.349265\n",
      "train loss:   1.173160\n",
      "train loss:   0.882029\n",
      "train loss:   0.993057\n",
      "train loss:   0.595292\n",
      "train loss:   0.767792\n",
      "train loss:   1.082858\n",
      "########### epoch 156 ###########\n",
      "########### loop 29300 ###########\n",
      "test loss:   0.266015   test accuracy:   0.916667\n",
      "########### loop 29300 ###########\n",
      "train loss:   0.787241\n",
      "train loss:   1.084723\n",
      "train loss:   0.882396\n",
      "train loss:   1.083634\n",
      "train loss:   1.108858\n",
      "train loss:   0.774390\n",
      "train loss:   1.446112\n",
      "train loss:   1.213179\n",
      "train loss:   0.650885\n",
      "train loss:   0.770854\n",
      "train loss:   0.806384\n",
      "train loss:   1.159708\n",
      "train loss:   1.146888\n",
      "train loss:   0.839102\n",
      "train loss:   0.855875\n",
      "train loss:   0.907475\n",
      "train loss:   1.179218\n",
      "train loss:   0.881149\n",
      "train loss:   0.799952\n",
      "train loss:   1.363954\n",
      "train loss:   0.902109\n",
      "train loss:   1.042561\n",
      "train loss:   1.158908\n",
      "train loss:   0.983426\n",
      "train loss:   1.298665\n",
      "train loss:   1.060543\n",
      "train loss:   0.922737\n",
      "train loss:   0.881138\n",
      "train loss:   0.869814\n",
      "train loss:   0.993560\n",
      "train loss:   1.082899\n",
      "train loss:   0.943802\n",
      "train loss:   0.884237\n",
      "train loss:   1.127221\n",
      "train loss:   0.776063\n",
      "train loss:   0.713908\n",
      "train loss:   1.015315\n",
      "train loss:   1.103099\n",
      "train loss:   0.692436\n",
      "train loss:   0.775390\n",
      "train loss:   0.688432\n",
      "train loss:   0.818230\n",
      "train loss:   0.758483\n",
      "train loss:   0.911567\n",
      "train loss:   1.067524\n",
      "train loss:   0.707455\n",
      "train loss:   1.352910\n",
      "train loss:   0.723018\n",
      "train loss:   1.042828\n",
      "train loss:   0.715378\n",
      "########### epoch 157 ###########\n",
      "########### loop 29350 ###########\n",
      "test loss:   0.268086   test accuracy:   0.958333\n",
      "########### loop 29350 ###########\n",
      "train loss:   1.179756\n",
      "train loss:   0.964851\n",
      "train loss:   1.152864\n",
      "train loss:   0.960587\n",
      "train loss:   1.113790\n",
      "train loss:   1.193111\n",
      "train loss:   0.686629\n",
      "train loss:   0.815365\n",
      "train loss:   1.195104\n",
      "train loss:   0.879702\n",
      "train loss:   0.908532\n",
      "train loss:   0.748148\n",
      "train loss:   1.038347\n",
      "train loss:   0.575272\n",
      "train loss:   0.970581\n",
      "train loss:   0.923151\n",
      "train loss:   0.860425\n",
      "train loss:   0.914955\n",
      "train loss:   0.931878\n",
      "train loss:   1.143138\n",
      "train loss:   0.960367\n",
      "train loss:   0.782733\n",
      "train loss:   0.574368\n",
      "train loss:   1.285109\n",
      "train loss:   1.021406\n",
      "train loss:   0.714120\n",
      "train loss:   1.184598\n",
      "train loss:   0.692779\n",
      "train loss:   1.050290\n",
      "train loss:   0.811580\n",
      "train loss:   0.661981\n",
      "train loss:   0.915692\n",
      "train loss:   1.292801\n",
      "train loss:   0.703802\n",
      "train loss:   1.005314\n",
      "train loss:   1.026567\n",
      "train loss:   0.805157\n",
      "train loss:   1.007915\n",
      "train loss:   0.964748\n",
      "train loss:   0.777040\n",
      "train loss:   0.873862\n",
      "train loss:   0.797435\n",
      "train loss:   1.091828\n",
      "train loss:   0.643296\n",
      "train loss:   1.144295\n",
      "train loss:   1.056412\n",
      "train loss:   0.879404\n",
      "train loss:   0.623501\n",
      "train loss:   1.071271\n",
      "train loss:   0.850524\n",
      "########### epoch 157 ###########\n",
      "########### loop 29400 ###########\n",
      "test loss:   0.215151   test accuracy:   0.958333\n",
      "########### loop 29400 ###########\n",
      "train loss:   1.041893\n",
      "train loss:   1.303500\n",
      "train loss:   1.503894\n",
      "train loss:   0.645969\n",
      "train loss:   1.187651\n",
      "train loss:   1.099589\n",
      "train loss:   0.674319\n",
      "train loss:   1.422659\n",
      "train loss:   1.061594\n",
      "train loss:   1.242370\n",
      "train loss:   1.175484\n",
      "train loss:   1.191903\n",
      "train loss:   0.631538\n",
      "train loss:   1.051523\n",
      "train loss:   0.859493\n",
      "train loss:   1.061064\n",
      "train loss:   1.283082\n",
      "train loss:   1.017231\n",
      "train loss:   0.735713\n",
      "train loss:   0.954528\n",
      "train loss:   1.251063\n",
      "train loss:   0.613557\n",
      "train loss:   0.813707\n",
      "train loss:   1.311769\n",
      "train loss:   1.059154\n",
      "train loss:   1.093589\n",
      "train loss:   0.863300\n",
      "train loss:   0.655441\n",
      "train loss:   1.016654\n",
      "train loss:   0.956725\n",
      "train loss:   1.343126\n",
      "train loss:   0.731476\n",
      "train loss:   1.006368\n",
      "train loss:   1.240400\n",
      "train loss:   0.959097\n",
      "train loss:   0.892122\n",
      "train loss:   0.789186\n",
      "train loss:   1.127614\n",
      "train loss:   0.898965\n",
      "train loss:   1.075304\n",
      "train loss:   1.151597\n",
      "train loss:   0.789453\n",
      "train loss:   0.963582\n",
      "train loss:   0.737070\n",
      "train loss:   1.117108\n",
      "train loss:   0.987635\n",
      "train loss:   0.807760\n",
      "train loss:   1.070254\n",
      "train loss:   1.088167\n",
      "train loss:   1.091332\n",
      "########### epoch 157 ###########\n",
      "########### loop 29450 ###########\n",
      "test loss:   0.171327   test accuracy:   0.958333\n",
      "########### loop 29450 ###########\n",
      "train loss:   1.141554\n",
      "train loss:   1.053595\n",
      "train loss:   0.777070\n",
      "train loss:   1.257092\n",
      "train loss:   1.252685\n",
      "train loss:   0.682921\n",
      "train loss:   1.021093\n",
      "train loss:   0.705559\n",
      "train loss:   0.972696\n",
      "train loss:   0.668428\n",
      "train loss:   1.004358\n",
      "train loss:   1.245451\n",
      "train loss:   1.093344\n",
      "train loss:   0.967176\n",
      "train loss:   0.860620\n",
      "train loss:   0.749687\n",
      "train loss:   1.028437\n",
      "train loss:   0.899216\n",
      "train loss:   1.103518\n",
      "train loss:   0.760118\n",
      "train loss:   1.014691\n",
      "train loss:   0.900594\n",
      "train loss:   1.220451\n",
      "train loss:   1.167591\n",
      "train loss:   0.843975\n",
      "train loss:   1.019337\n",
      "train loss:   0.802027\n",
      "train loss:   0.801074\n",
      "train loss:   0.901053\n",
      "train loss:   0.834457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.833534\n",
      "train loss:   1.092431\n",
      "train loss:   0.642403\n",
      "train loss:   0.822423\n",
      "train loss:   0.779037\n",
      "train loss:   1.264901\n",
      "train loss:   0.471732\n",
      "train loss:   0.720337\n",
      "train loss:   0.509620\n",
      "train loss:   0.744332\n",
      "train loss:   1.213013\n",
      "train loss:   0.653964\n",
      "train loss:   1.145882\n",
      "train loss:   0.459550\n",
      "train loss:   0.571776\n",
      "train loss:   1.218662\n",
      "train loss:   0.832638\n",
      "train loss:   1.061770\n",
      "train loss:   1.012684\n",
      "train loss:   0.800566\n",
      "########### epoch 157 ###########\n",
      "########### loop 29500 ###########\n",
      "test loss:   0.164329   test accuracy:   0.958333\n",
      "########### loop 29500 ###########\n",
      "train loss:   1.118394\n",
      "train loss:   0.466386\n",
      "train loss:   1.004703\n",
      "train loss:   0.923034\n",
      "train loss:   1.214272\n",
      "train loss:   1.096797\n",
      "train loss:   0.876210\n",
      "train loss:   0.761171\n",
      "train loss:   0.916009\n",
      "train loss:   1.019509\n",
      "train loss:   0.756190\n",
      "train loss:   0.983745\n",
      "train loss:   0.992019\n",
      "train loss:   1.203256\n",
      "train loss:   1.083672\n",
      "train loss:   1.209133\n",
      "train loss:   0.868361\n",
      "train loss:   1.027830\n",
      "train loss:   1.685449\n",
      "train loss:   0.712880\n",
      "train loss:   1.119234\n",
      "train loss:   1.136879\n",
      "train loss:   1.138713\n",
      "train loss:   0.941224\n",
      "train loss:   1.101464\n",
      "train loss:   1.332447\n",
      "train loss:   0.936349\n",
      "train loss:   1.002107\n",
      "train loss:   1.032873\n",
      "train loss:   0.843855\n",
      "train loss:   0.624234\n",
      "train loss:   0.886563\n",
      "train loss:   1.008685\n",
      "train loss:   0.975038\n",
      "train loss:   0.998721\n",
      "train loss:   0.643511\n",
      "train loss:   0.853124\n",
      "train loss:   0.941929\n",
      "train loss:   1.013904\n",
      "train loss:   0.981217\n",
      "train loss:   0.941795\n",
      "train loss:   0.950323\n",
      "train loss:   0.822753\n",
      "train loss:   1.098293\n",
      "train loss:   1.204259\n",
      "train loss:   1.002535\n",
      "train loss:   0.629804\n",
      "train loss:   0.791551\n",
      "train loss:   0.736198\n",
      "train loss:   0.913879\n",
      "########### epoch 158 ###########\n",
      "########### loop 29550 ###########\n",
      "test loss:   0.159169   test accuracy:   0.958333\n",
      "########### loop 29550 ###########\n",
      "train loss:   0.409983\n",
      "train loss:   0.604699\n",
      "train loss:   0.970263\n",
      "train loss:   1.018188\n",
      "train loss:   0.789564\n",
      "train loss:   1.244854\n",
      "train loss:   0.884568\n",
      "train loss:   0.965165\n",
      "train loss:   0.703288\n",
      "train loss:   0.879356\n",
      "train loss:   1.167946\n",
      "train loss:   1.418067\n",
      "train loss:   1.396254\n",
      "train loss:   0.974362\n",
      "train loss:   0.852153\n",
      "train loss:   0.886325\n",
      "train loss:   1.398635\n",
      "train loss:   0.747977\n",
      "train loss:   0.811377\n",
      "train loss:   1.004011\n",
      "train loss:   1.050360\n",
      "train loss:   0.903795\n",
      "train loss:   1.404538\n",
      "train loss:   0.908333\n",
      "train loss:   0.895465\n",
      "train loss:   0.928571\n",
      "train loss:   0.812967\n",
      "train loss:   1.014791\n",
      "train loss:   0.907324\n",
      "train loss:   0.949463\n",
      "train loss:   0.630434\n",
      "train loss:   0.892433\n",
      "train loss:   1.011862\n",
      "train loss:   0.858638\n",
      "train loss:   1.221278\n",
      "train loss:   0.586545\n",
      "train loss:   0.858871\n",
      "train loss:   0.506648\n",
      "train loss:   0.816134\n",
      "train loss:   1.108955\n",
      "train loss:   0.652190\n",
      "train loss:   0.827313\n",
      "train loss:   1.095170\n",
      "train loss:   1.301025\n",
      "train loss:   0.772772\n",
      "train loss:   0.772800\n",
      "train loss:   0.869645\n",
      "train loss:   1.200502\n",
      "train loss:   1.378007\n",
      "train loss:   1.032001\n",
      "########### epoch 158 ###########\n",
      "########### loop 29600 ###########\n",
      "test loss:   0.123494   test accuracy:   1.000000\n",
      "########### loop 29600 ###########\n",
      "train loss:   0.795984\n",
      "train loss:   1.028557\n",
      "train loss:   0.800705\n",
      "train loss:   0.810854\n",
      "train loss:   1.180260\n",
      "train loss:   0.647114\n",
      "train loss:   1.093198\n",
      "train loss:   0.867733\n",
      "train loss:   0.890169\n",
      "train loss:   0.890671\n",
      "train loss:   0.938744\n",
      "train loss:   0.962694\n",
      "train loss:   0.825990\n",
      "train loss:   0.786419\n",
      "train loss:   0.930978\n",
      "train loss:   1.149503\n",
      "train loss:   0.652593\n",
      "train loss:   1.144028\n",
      "train loss:   1.054959\n",
      "train loss:   1.140005\n",
      "train loss:   1.011687\n",
      "train loss:   1.115212\n",
      "train loss:   1.152927\n",
      "train loss:   0.839340\n",
      "train loss:   0.652617\n",
      "train loss:   1.041126\n",
      "train loss:   1.137108\n",
      "train loss:   1.157761\n",
      "train loss:   1.261002\n",
      "train loss:   0.696686\n",
      "train loss:   0.750624\n",
      "train loss:   1.155443\n",
      "train loss:   0.910148\n",
      "train loss:   0.948876\n",
      "train loss:   1.049954\n",
      "train loss:   0.872290\n",
      "train loss:   0.898235\n",
      "train loss:   0.791557\n",
      "train loss:   1.087316\n",
      "train loss:   0.646501\n",
      "train loss:   0.794658\n",
      "train loss:   1.025993\n",
      "train loss:   1.367197\n",
      "train loss:   1.290249\n",
      "train loss:   0.674352\n",
      "train loss:   0.885080\n",
      "train loss:   0.689222\n",
      "train loss:   1.215660\n",
      "train loss:   0.763358\n",
      "train loss:   0.968493\n",
      "########### epoch 158 ###########\n",
      "########### loop 29650 ###########\n",
      "test loss:   0.333353   test accuracy:   0.916667\n",
      "########### loop 29650 ###########\n",
      "train loss:   1.155448\n",
      "train loss:   1.036163\n",
      "train loss:   1.058049\n",
      "train loss:   1.226019\n",
      "train loss:   0.829917\n",
      "train loss:   1.166350\n",
      "train loss:   1.262200\n",
      "train loss:   0.678737\n",
      "train loss:   0.766156\n",
      "train loss:   0.926338\n",
      "train loss:   1.011260\n",
      "train loss:   1.275083\n",
      "train loss:   1.092269\n",
      "train loss:   0.821948\n",
      "train loss:   0.973556\n",
      "train loss:   0.906191\n",
      "train loss:   0.984940\n",
      "train loss:   0.898950\n",
      "train loss:   0.900621\n",
      "train loss:   1.169597\n",
      "train loss:   0.785341\n",
      "train loss:   1.015001\n",
      "train loss:   0.657335\n",
      "train loss:   0.995363\n",
      "train loss:   1.006357\n",
      "train loss:   0.965350\n",
      "train loss:   0.806352\n",
      "train loss:   0.748819\n",
      "train loss:   0.672354\n",
      "train loss:   0.800255\n",
      "train loss:   0.871245\n",
      "train loss:   0.974070\n",
      "train loss:   1.159184\n",
      "train loss:   0.494848\n",
      "train loss:   0.765070\n",
      "train loss:   0.913904\n",
      "train loss:   0.842281\n",
      "train loss:   0.755005\n",
      "train loss:   0.799008\n",
      "train loss:   0.838321\n",
      "train loss:   0.976910\n",
      "train loss:   1.178632\n",
      "train loss:   0.778302\n",
      "train loss:   0.851281\n",
      "train loss:   0.872256\n",
      "train loss:   1.114340\n",
      "train loss:   0.986365\n",
      "train loss:   0.722905\n",
      "train loss:   1.026803\n",
      "train loss:   0.963476\n",
      "########### epoch 158 ###########\n",
      "########### loop 29700 ###########\n",
      "test loss:   0.269636   test accuracy:   0.916667\n",
      "########### loop 29700 ###########\n",
      "train loss:   0.928061\n",
      "train loss:   0.843435\n",
      "train loss:   1.048326\n",
      "train loss:   0.946631\n",
      "train loss:   1.022056\n",
      "train loss:   0.536217\n",
      "train loss:   1.018275\n",
      "train loss:   1.243766\n",
      "train loss:   1.066731\n",
      "train loss:   0.856158\n",
      "train loss:   0.893118\n",
      "train loss:   1.075715\n",
      "train loss:   0.874152\n",
      "train loss:   0.907939\n",
      "train loss:   0.855690\n",
      "train loss:   0.936814\n",
      "train loss:   0.698186\n",
      "train loss:   0.708182\n",
      "train loss:   1.248564\n",
      "train loss:   0.506006\n",
      "train loss:   1.122129\n",
      "train loss:   0.770502\n",
      "train loss:   0.794119\n",
      "train loss:   0.718841\n",
      "train loss:   0.847713\n",
      "train loss:   1.244799\n",
      "train loss:   1.128269\n",
      "train loss:   1.082815\n",
      "train loss:   0.894845\n",
      "train loss:   1.056422\n",
      "train loss:   1.077778\n",
      "train loss:   1.063468\n",
      "train loss:   1.077062\n",
      "train loss:   0.981689\n",
      "train loss:   0.702489\n",
      "train loss:   1.099327\n",
      "train loss:   0.923637\n",
      "train loss:   0.789037\n",
      "train loss:   0.981693\n",
      "train loss:   1.157440\n",
      "train loss:   0.741044\n",
      "train loss:   1.025989\n",
      "train loss:   0.687622\n",
      "train loss:   1.031525\n",
      "train loss:   1.161121\n",
      "train loss:   1.011173\n",
      "train loss:   0.913674\n",
      "train loss:   0.742394\n",
      "train loss:   0.934153\n",
      "train loss:   0.484649\n",
      "########### epoch 159 ###########\n",
      "########### loop 29750 ###########\n",
      "test loss:   0.435912   test accuracy:   0.750000\n",
      "########### loop 29750 ###########\n",
      "train loss:   0.959379\n",
      "train loss:   0.901368\n",
      "train loss:   0.809039\n",
      "train loss:   0.865679\n",
      "train loss:   0.806389\n",
      "train loss:   1.242823\n",
      "train loss:   0.706271\n",
      "train loss:   0.914577\n",
      "train loss:   0.725021\n",
      "train loss:   0.558955\n",
      "train loss:   1.200310\n",
      "train loss:   1.320371\n",
      "train loss:   0.848948\n",
      "train loss:   0.948264\n",
      "train loss:   0.963421\n",
      "train loss:   0.903308\n",
      "train loss:   1.008859\n",
      "train loss:   1.038673\n",
      "train loss:   0.609315\n",
      "train loss:   1.368203\n",
      "train loss:   0.862097\n",
      "train loss:   1.118187\n",
      "train loss:   0.800679\n",
      "train loss:   1.094214\n",
      "train loss:   0.990357\n",
      "train loss:   1.034025\n",
      "train loss:   1.150852\n",
      "train loss:   0.782644\n",
      "train loss:   0.832090\n",
      "train loss:   1.208440\n",
      "train loss:   0.901606\n",
      "train loss:   0.987296\n",
      "train loss:   0.920426\n",
      "train loss:   0.571502\n",
      "train loss:   1.068116\n",
      "train loss:   1.138110\n",
      "train loss:   0.639983\n",
      "train loss:   0.743966\n",
      "train loss:   1.066123\n",
      "train loss:   1.036740\n",
      "train loss:   1.071494\n",
      "train loss:   0.736007\n",
      "train loss:   0.706076\n",
      "train loss:   1.221601\n",
      "train loss:   1.063125\n",
      "train loss:   1.197307\n",
      "train loss:   1.032549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.929778\n",
      "train loss:   0.735792\n",
      "train loss:   0.915588\n",
      "########### epoch 159 ###########\n",
      "########### loop 29800 ###########\n",
      "test loss:   0.337860   test accuracy:   0.875000\n",
      "########### loop 29800 ###########\n",
      "train loss:   0.937204\n",
      "train loss:   0.881842\n",
      "train loss:   0.678876\n",
      "train loss:   0.905985\n",
      "train loss:   1.121938\n",
      "train loss:   0.959187\n",
      "train loss:   0.756444\n",
      "train loss:   1.185637\n",
      "train loss:   0.797781\n",
      "train loss:   0.725711\n",
      "train loss:   1.097507\n",
      "train loss:   0.865861\n",
      "train loss:   0.598087\n",
      "train loss:   0.974029\n",
      "train loss:   1.025128\n",
      "train loss:   1.175777\n",
      "train loss:   0.924360\n",
      "train loss:   0.704642\n",
      "train loss:   1.069166\n",
      "train loss:   0.793629\n",
      "train loss:   1.106494\n",
      "train loss:   1.142768\n",
      "train loss:   1.122121\n",
      "train loss:   1.012975\n",
      "train loss:   1.020060\n",
      "train loss:   0.808307\n",
      "train loss:   1.004698\n",
      "train loss:   0.797669\n",
      "train loss:   1.271896\n",
      "train loss:   1.043533\n",
      "train loss:   0.920063\n",
      "train loss:   0.801056\n",
      "train loss:   0.682231\n",
      "train loss:   0.857279\n",
      "train loss:   1.289935\n",
      "train loss:   0.649481\n",
      "train loss:   0.870375\n",
      "train loss:   1.135495\n",
      "train loss:   0.840064\n",
      "train loss:   0.556498\n",
      "train loss:   0.987568\n",
      "train loss:   0.918159\n",
      "train loss:   1.061313\n",
      "train loss:   0.653049\n",
      "train loss:   0.810442\n",
      "train loss:   0.619425\n",
      "train loss:   0.560621\n",
      "train loss:   1.143260\n",
      "train loss:   1.232744\n",
      "train loss:   1.094649\n",
      "########### epoch 159 ###########\n",
      "########### loop 29850 ###########\n",
      "test loss:   0.170044   test accuracy:   0.958333\n",
      "########### loop 29850 ###########\n",
      "train loss:   1.103041\n",
      "train loss:   1.275980\n",
      "train loss:   0.967799\n",
      "train loss:   1.234999\n",
      "train loss:   1.049200\n",
      "train loss:   1.554123\n",
      "train loss:   1.026061\n",
      "train loss:   0.782787\n",
      "train loss:   0.877428\n",
      "train loss:   0.894793\n",
      "train loss:   0.798285\n",
      "train loss:   0.798901\n",
      "train loss:   0.827145\n",
      "train loss:   0.736683\n",
      "train loss:   0.900701\n",
      "train loss:   0.943858\n",
      "train loss:   0.957630\n",
      "train loss:   0.659916\n",
      "train loss:   0.580890\n",
      "train loss:   1.110622\n",
      "train loss:   0.555518\n",
      "train loss:   0.939443\n",
      "train loss:   0.556955\n",
      "train loss:   1.206686\n",
      "train loss:   1.050669\n",
      "train loss:   1.127804\n",
      "train loss:   0.881170\n",
      "train loss:   1.239589\n",
      "train loss:   0.966173\n",
      "train loss:   1.178041\n",
      "train loss:   0.815575\n",
      "train loss:   0.604841\n",
      "train loss:   0.816447\n",
      "train loss:   0.896246\n",
      "train loss:   1.035379\n",
      "train loss:   0.981882\n",
      "train loss:   1.196502\n",
      "train loss:   1.200608\n",
      "train loss:   0.625060\n",
      "train loss:   0.675297\n",
      "train loss:   1.204048\n",
      "train loss:   0.896712\n",
      "train loss:   0.889917\n",
      "train loss:   1.149701\n",
      "train loss:   0.992736\n",
      "train loss:   1.185730\n",
      "train loss:   1.056198\n",
      "train loss:   0.747322\n",
      "train loss:   0.951393\n",
      "train loss:   1.021089\n",
      "########### epoch 160 ###########\n",
      "########### loop 29900 ###########\n",
      "test loss:   0.363234   test accuracy:   0.916667\n",
      "########### loop 29900 ###########\n",
      "train loss:   0.964354\n",
      "train loss:   0.938532\n",
      "train loss:   0.911237\n",
      "train loss:   0.825957\n",
      "train loss:   1.374558\n",
      "train loss:   0.413467\n",
      "train loss:   1.184547\n",
      "train loss:   0.770299\n",
      "train loss:   0.986970\n",
      "train loss:   0.945119\n",
      "train loss:   0.917832\n",
      "train loss:   1.239402\n",
      "train loss:   0.958108\n",
      "train loss:   0.985843\n",
      "train loss:   0.752975\n",
      "train loss:   0.622376\n",
      "train loss:   1.112439\n",
      "train loss:   1.275345\n",
      "train loss:   0.935031\n",
      "train loss:   0.708515\n",
      "train loss:   0.940673\n",
      "train loss:   1.069305\n",
      "train loss:   1.163045\n",
      "train loss:   0.974870\n",
      "train loss:   0.928693\n",
      "train loss:   0.739725\n",
      "train loss:   0.778822\n",
      "train loss:   1.214405\n",
      "train loss:   0.713293\n",
      "train loss:   0.794739\n",
      "train loss:   0.927833\n",
      "train loss:   1.387906\n",
      "train loss:   0.793008\n",
      "train loss:   1.098947\n",
      "train loss:   0.666522\n",
      "train loss:   0.600459\n",
      "train loss:   0.813477\n",
      "train loss:   0.983186\n",
      "train loss:   0.731407\n",
      "train loss:   1.054760\n",
      "train loss:   1.107375\n",
      "train loss:   0.959976\n",
      "train loss:   1.218649\n",
      "train loss:   1.173132\n",
      "train loss:   1.034191\n",
      "train loss:   0.807484\n",
      "train loss:   1.199623\n",
      "train loss:   0.915991\n",
      "train loss:   0.749946\n",
      "train loss:   1.081447\n",
      "########### epoch 160 ###########\n",
      "########### loop 29950 ###########\n",
      "test loss:   0.292477   test accuracy:   0.916667\n",
      "########### loop 29950 ###########\n",
      "train loss:   0.630063\n",
      "train loss:   1.083296\n",
      "train loss:   0.948280\n",
      "train loss:   1.017817\n",
      "train loss:   0.932066\n",
      "train loss:   0.702961\n",
      "train loss:   1.116977\n",
      "train loss:   0.834548\n",
      "train loss:   1.025106\n",
      "train loss:   0.921516\n",
      "train loss:   0.773463\n",
      "train loss:   1.018247\n",
      "train loss:   0.966199\n",
      "train loss:   0.887184\n",
      "train loss:   1.007942\n",
      "train loss:   0.962858\n",
      "train loss:   0.741378\n",
      "train loss:   0.926863\n",
      "train loss:   1.009181\n",
      "train loss:   0.918826\n",
      "train loss:   0.867173\n",
      "train loss:   0.850258\n",
      "train loss:   1.302231\n",
      "train loss:   1.072499\n",
      "train loss:   1.288246\n",
      "train loss:   0.903812\n",
      "train loss:   0.982063\n",
      "train loss:   0.653656\n",
      "train loss:   1.280743\n",
      "train loss:   1.097487\n",
      "train loss:   0.936375\n",
      "train loss:   0.961331\n",
      "train loss:   0.744811\n",
      "train loss:   1.073379\n",
      "train loss:   0.992027\n",
      "train loss:   0.933781\n",
      "train loss:   1.295577\n",
      "train loss:   0.716388\n",
      "train loss:   1.013870\n",
      "train loss:   1.080556\n",
      "train loss:   0.890580\n",
      "train loss:   1.075035\n",
      "train loss:   1.084482\n",
      "train loss:   1.203976\n",
      "train loss:   0.771584\n",
      "train loss:   1.001020\n",
      "train loss:   0.859953\n",
      "train loss:   1.098412\n",
      "train loss:   0.923282\n",
      "train loss:   1.413223\n",
      "########### epoch 160 ###########\n",
      "########### loop 30000 ###########\n",
      "test loss:   0.137093   test accuracy:   0.916667\n",
      "########### loop 30000 ###########\n",
      "train loss:   0.992704\n",
      "train loss:   0.774974\n",
      "train loss:   0.915960\n",
      "train loss:   0.694290\n",
      "train loss:   0.967453\n",
      "train loss:   1.098001\n",
      "train loss:   0.783462\n",
      "train loss:   1.326751\n",
      "train loss:   1.088294\n",
      "train loss:   1.092234\n",
      "train loss:   0.880125\n",
      "train loss:   0.851657\n",
      "train loss:   0.923467\n",
      "train loss:   1.103781\n",
      "train loss:   0.902618\n",
      "train loss:   1.213537\n",
      "train loss:   1.206149\n",
      "train loss:   0.848154\n",
      "train loss:   1.232338\n",
      "train loss:   0.799289\n",
      "train loss:   0.866307\n",
      "train loss:   0.998313\n",
      "train loss:   0.875463\n",
      "train loss:   0.700672\n",
      "train loss:   0.722690\n",
      "train loss:   0.616947\n",
      "train loss:   1.018355\n",
      "train loss:   0.763308\n",
      "train loss:   0.752408\n",
      "train loss:   0.647978\n",
      "train loss:   0.729246\n",
      "train loss:   0.919359\n",
      "train loss:   0.822744\n",
      "train loss:   0.925098\n",
      "train loss:   0.960306\n",
      "train loss:   0.885390\n",
      "train loss:   0.905700\n",
      "train loss:   0.843086\n",
      "train loss:   1.061052\n",
      "train loss:   0.949554\n",
      "train loss:   0.786062\n",
      "train loss:   1.060572\n",
      "train loss:   0.712019\n",
      "train loss:   0.809281\n",
      "train loss:   1.064386\n",
      "train loss:   0.764484\n",
      "train loss:   0.927653\n",
      "train loss:   0.731528\n",
      "train loss:   1.065671\n",
      "train loss:   1.017941\n",
      "########### epoch 160 ###########\n",
      "########### loop 30050 ###########\n",
      "test loss:   0.301980   test accuracy:   0.875000\n",
      "########### loop 30050 ###########\n",
      "train loss:   1.474721\n",
      "train loss:   0.854278\n",
      "train loss:   0.793685\n",
      "train loss:   0.928609\n",
      "train loss:   1.391622\n",
      "train loss:   1.333227\n",
      "train loss:   1.187825\n",
      "train loss:   0.945634\n",
      "train loss:   0.745004\n",
      "train loss:   1.047868\n",
      "train loss:   1.000886\n",
      "train loss:   0.810563\n",
      "train loss:   0.702273\n",
      "train loss:   0.906538\n",
      "train loss:   0.935347\n",
      "train loss:   0.627510\n",
      "train loss:   0.470229\n",
      "train loss:   0.989761\n",
      "train loss:   0.887444\n",
      "train loss:   0.834227\n",
      "train loss:   1.030593\n",
      "train loss:   1.155604\n",
      "train loss:   0.964168\n",
      "train loss:   1.012667\n",
      "train loss:   1.109933\n",
      "train loss:   0.951682\n",
      "train loss:   0.976799\n",
      "train loss:   1.209496\n",
      "train loss:   1.077348\n",
      "train loss:   0.874742\n",
      "train loss:   1.079449\n",
      "train loss:   1.163708\n",
      "train loss:   0.644022\n",
      "train loss:   1.212685\n",
      "train loss:   0.892002\n",
      "train loss:   1.058537\n",
      "train loss:   0.905010\n",
      "train loss:   1.256611\n",
      "train loss:   0.726650\n",
      "train loss:   0.760890\n",
      "train loss:   1.184452\n",
      "train loss:   0.948927\n",
      "train loss:   1.114301\n",
      "train loss:   0.869746\n",
      "train loss:   0.822646\n",
      "train loss:   0.934159\n",
      "train loss:   1.015694\n",
      "train loss:   0.902162\n",
      "train loss:   0.941181\n",
      "train loss:   1.088594\n",
      "########### epoch 161 ###########\n",
      "########### loop 30100 ###########\n",
      "test loss:   0.130691   test accuracy:   0.958333\n",
      "########### loop 30100 ###########\n",
      "train loss:   0.743855\n",
      "train loss:   0.858005\n",
      "train loss:   1.029265\n",
      "train loss:   0.933960\n",
      "train loss:   1.147961\n",
      "train loss:   0.867578\n",
      "train loss:   0.730802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.651918\n",
      "train loss:   0.783926\n",
      "train loss:   1.153605\n",
      "train loss:   0.847830\n",
      "train loss:   0.995879\n",
      "train loss:   1.306591\n",
      "train loss:   1.124270\n",
      "train loss:   1.160390\n",
      "train loss:   0.799705\n",
      "train loss:   0.911243\n",
      "train loss:   1.050366\n",
      "train loss:   0.861181\n",
      "train loss:   1.096220\n",
      "train loss:   1.181837\n",
      "train loss:   0.767943\n",
      "train loss:   0.925070\n",
      "train loss:   0.802709\n",
      "train loss:   1.213025\n",
      "train loss:   1.039602\n",
      "train loss:   1.190236\n",
      "train loss:   0.816357\n",
      "train loss:   1.001218\n",
      "train loss:   1.120391\n",
      "train loss:   1.012695\n",
      "train loss:   0.853608\n",
      "train loss:   0.872100\n",
      "train loss:   0.850707\n",
      "train loss:   1.220699\n",
      "train loss:   0.986867\n",
      "train loss:   1.132897\n",
      "train loss:   0.893467\n",
      "train loss:   1.022817\n",
      "train loss:   1.334007\n",
      "train loss:   1.036889\n",
      "train loss:   0.792591\n",
      "train loss:   0.936972\n",
      "train loss:   1.284894\n",
      "train loss:   0.879320\n",
      "train loss:   0.789548\n",
      "train loss:   0.799355\n",
      "train loss:   1.066147\n",
      "train loss:   1.089042\n",
      "train loss:   0.895688\n",
      "########### epoch 161 ###########\n",
      "########### loop 30150 ###########\n",
      "test loss:   0.383478   test accuracy:   0.833333\n",
      "########### loop 30150 ###########\n",
      "train loss:   1.302127\n",
      "train loss:   1.046716\n",
      "train loss:   0.984965\n",
      "train loss:   1.164237\n",
      "train loss:   1.211904\n",
      "train loss:   0.842348\n",
      "train loss:   0.852553\n",
      "train loss:   1.041015\n",
      "train loss:   0.764684\n",
      "train loss:   0.804403\n",
      "train loss:   0.570332\n",
      "train loss:   1.156843\n",
      "train loss:   0.901904\n",
      "train loss:   0.832991\n",
      "train loss:   0.971476\n",
      "train loss:   0.903443\n",
      "train loss:   0.742578\n",
      "train loss:   1.223125\n",
      "train loss:   1.139125\n",
      "train loss:   1.097080\n",
      "train loss:   0.985394\n",
      "train loss:   0.596237\n",
      "train loss:   0.861052\n",
      "train loss:   0.915231\n",
      "train loss:   1.139207\n",
      "train loss:   0.945367\n",
      "train loss:   0.851451\n",
      "train loss:   0.905571\n",
      "train loss:   1.000310\n",
      "train loss:   0.514018\n",
      "train loss:   0.763471\n",
      "train loss:   1.002651\n",
      "train loss:   1.089668\n",
      "train loss:   0.613271\n",
      "train loss:   0.697298\n",
      "train loss:   1.024821\n",
      "train loss:   0.961943\n",
      "train loss:   1.137425\n",
      "train loss:   0.885673\n",
      "train loss:   1.138545\n",
      "train loss:   0.921755\n",
      "train loss:   1.094115\n",
      "train loss:   0.980089\n",
      "train loss:   1.011114\n",
      "train loss:   0.978472\n",
      "train loss:   0.948167\n",
      "train loss:   0.714814\n",
      "train loss:   0.498596\n",
      "train loss:   0.939143\n",
      "train loss:   0.760801\n",
      "########### epoch 161 ###########\n",
      "########### loop 30200 ###########\n",
      "test loss:   0.316083   test accuracy:   0.875000\n",
      "########### loop 30200 ###########\n",
      "train loss:   0.961570\n",
      "train loss:   0.763129\n",
      "train loss:   1.234775\n",
      "train loss:   0.956833\n",
      "train loss:   0.864789\n",
      "train loss:   1.110693\n",
      "train loss:   1.170244\n",
      "train loss:   0.927052\n",
      "train loss:   0.894809\n",
      "train loss:   1.030164\n",
      "train loss:   0.751387\n",
      "train loss:   0.878939\n",
      "train loss:   1.031520\n",
      "train loss:   1.150655\n",
      "train loss:   0.952926\n",
      "train loss:   0.929197\n",
      "train loss:   0.712103\n",
      "train loss:   0.886347\n",
      "train loss:   0.816061\n",
      "train loss:   0.633233\n",
      "train loss:   0.818809\n",
      "train loss:   1.021197\n",
      "train loss:   1.273837\n",
      "train loss:   1.022494\n",
      "train loss:   0.670716\n",
      "train loss:   0.661511\n",
      "train loss:   1.021095\n",
      "train loss:   0.721793\n",
      "train loss:   1.310869\n",
      "train loss:   1.001940\n",
      "train loss:   1.181339\n",
      "train loss:   1.023126\n",
      "train loss:   0.899285\n",
      "train loss:   0.733283\n",
      "train loss:   1.086895\n",
      "train loss:   1.276140\n",
      "train loss:   0.993930\n",
      "train loss:   1.189535\n",
      "train loss:   1.005365\n",
      "train loss:   1.129068\n",
      "train loss:   1.204360\n",
      "train loss:   1.370541\n",
      "train loss:   0.838500\n",
      "train loss:   0.884041\n",
      "train loss:   1.156197\n",
      "train loss:   0.595841\n",
      "train loss:   0.827521\n",
      "train loss:   1.058414\n",
      "train loss:   0.895436\n",
      "train loss:   1.133240\n",
      "########### epoch 161 ###########\n",
      "########### loop 30250 ###########\n",
      "test loss:   0.261851   test accuracy:   0.916667\n",
      "########### loop 30250 ###########\n",
      "train loss:   1.130098\n",
      "train loss:   0.997206\n",
      "train loss:   0.889791\n",
      "train loss:   0.663484\n",
      "train loss:   0.990170\n",
      "train loss:   0.791968\n",
      "train loss:   1.042458\n",
      "train loss:   0.888034\n",
      "train loss:   1.118559\n",
      "train loss:   1.027607\n",
      "train loss:   1.033785\n",
      "train loss:   1.152949\n",
      "train loss:   0.395513\n",
      "train loss:   1.181481\n",
      "train loss:   1.058436\n",
      "train loss:   1.186101\n",
      "train loss:   0.702000\n",
      "train loss:   0.772746\n",
      "train loss:   1.014925\n",
      "train loss:   1.218782\n",
      "train loss:   1.021812\n",
      "train loss:   1.224924\n",
      "train loss:   1.284048\n",
      "train loss:   0.836990\n",
      "train loss:   1.099540\n",
      "train loss:   0.965832\n",
      "train loss:   0.767450\n",
      "train loss:   0.935138\n",
      "train loss:   0.971155\n",
      "train loss:   0.695397\n",
      "train loss:   0.631976\n",
      "train loss:   0.707318\n",
      "train loss:   1.445264\n",
      "train loss:   0.908238\n",
      "train loss:   0.869270\n",
      "train loss:   0.533771\n",
      "train loss:   1.406382\n",
      "train loss:   1.020489\n",
      "train loss:   0.792728\n",
      "train loss:   0.849646\n",
      "train loss:   1.003843\n",
      "train loss:   0.804832\n",
      "train loss:   1.275260\n",
      "train loss:   0.894839\n",
      "train loss:   0.750858\n",
      "train loss:   0.953948\n",
      "train loss:   0.770443\n",
      "train loss:   0.783249\n",
      "train loss:   0.915332\n",
      "train loss:   1.185105\n",
      "########### epoch 162 ###########\n",
      "########### loop 30300 ###########\n",
      "test loss:   0.059195   test accuracy:   1.000000\n",
      "########### loop 30300 ###########\n",
      "train loss:   0.872550\n",
      "train loss:   0.793247\n",
      "train loss:   1.050174\n",
      "train loss:   1.147093\n",
      "train loss:   0.720165\n",
      "train loss:   1.043149\n",
      "train loss:   0.992186\n",
      "train loss:   1.145304\n",
      "train loss:   0.671466\n",
      "train loss:   1.055513\n",
      "train loss:   0.892283\n",
      "train loss:   0.756968\n",
      "train loss:   1.023810\n",
      "train loss:   1.085953\n",
      "train loss:   1.300383\n",
      "train loss:   1.039766\n",
      "train loss:   1.247155\n",
      "train loss:   0.396779\n",
      "train loss:   0.461122\n",
      "train loss:   0.860703\n",
      "train loss:   0.963122\n",
      "train loss:   0.964133\n",
      "train loss:   1.020811\n",
      "train loss:   0.939272\n",
      "train loss:   1.233985\n",
      "train loss:   1.297354\n",
      "train loss:   1.089387\n",
      "train loss:   0.733370\n",
      "train loss:   1.227590\n",
      "train loss:   0.951701\n",
      "train loss:   1.081346\n",
      "train loss:   0.892124\n",
      "train loss:   0.735987\n",
      "train loss:   1.135538\n",
      "train loss:   0.820838\n",
      "train loss:   1.028079\n",
      "train loss:   0.831943\n",
      "train loss:   0.842667\n",
      "train loss:   0.927958\n",
      "train loss:   1.110630\n",
      "train loss:   0.816733\n",
      "train loss:   1.027272\n",
      "train loss:   0.701525\n",
      "train loss:   1.135743\n",
      "train loss:   1.134807\n",
      "train loss:   0.703685\n",
      "train loss:   1.093251\n",
      "train loss:   1.015155\n",
      "train loss:   0.844923\n",
      "train loss:   0.861015\n",
      "########### epoch 162 ###########\n",
      "########### loop 30350 ###########\n",
      "test loss:   0.382329   test accuracy:   0.875000\n",
      "########### loop 30350 ###########\n",
      "train loss:   0.451129\n",
      "train loss:   0.799216\n",
      "train loss:   1.120621\n",
      "train loss:   1.187006\n",
      "train loss:   1.154210\n",
      "train loss:   0.648024\n",
      "train loss:   1.206205\n",
      "train loss:   1.279071\n",
      "train loss:   1.152767\n",
      "train loss:   0.561571\n",
      "train loss:   0.767279\n",
      "train loss:   0.761448\n",
      "train loss:   1.097978\n",
      "train loss:   1.058795\n",
      "train loss:   0.821764\n",
      "train loss:   0.622717\n",
      "train loss:   0.872513\n",
      "train loss:   0.816549\n",
      "train loss:   1.167840\n",
      "train loss:   1.169587\n",
      "train loss:   0.997781\n",
      "train loss:   0.984399\n",
      "train loss:   1.141449\n",
      "train loss:   0.936903\n",
      "train loss:   1.064459\n",
      "train loss:   1.060367\n",
      "train loss:   1.034689\n",
      "train loss:   1.116924\n",
      "train loss:   1.147088\n",
      "train loss:   0.954128\n",
      "train loss:   0.971161\n",
      "train loss:   1.190979\n",
      "train loss:   0.955021\n",
      "train loss:   1.023653\n",
      "train loss:   0.915352\n",
      "train loss:   0.895857\n",
      "train loss:   1.323847\n",
      "train loss:   0.892726\n",
      "train loss:   1.053234\n",
      "train loss:   0.660192\n",
      "train loss:   0.942513\n",
      "train loss:   1.055962\n",
      "train loss:   0.956428\n",
      "train loss:   0.632144\n",
      "train loss:   0.954177\n",
      "train loss:   0.876736\n",
      "train loss:   0.896823\n",
      "train loss:   0.842877\n",
      "train loss:   1.022041\n",
      "train loss:   0.905312\n",
      "########### epoch 162 ###########\n",
      "########### loop 30400 ###########\n",
      "test loss:   0.129181   test accuracy:   0.958333\n",
      "########### loop 30400 ###########\n",
      "train loss:   0.744009\n",
      "train loss:   1.158609\n",
      "train loss:   0.787267\n",
      "train loss:   0.920738\n",
      "train loss:   0.696888\n",
      "train loss:   0.674476\n",
      "train loss:   1.218911\n",
      "train loss:   0.926553\n",
      "train loss:   0.999215\n",
      "train loss:   1.177266\n",
      "train loss:   1.101571\n",
      "train loss:   1.467601\n",
      "train loss:   1.029412\n",
      "train loss:   0.963080\n",
      "train loss:   0.963501\n",
      "train loss:   0.640011\n",
      "train loss:   1.047722\n",
      "train loss:   0.922970\n",
      "train loss:   0.823664\n",
      "train loss:   0.817493\n",
      "train loss:   0.562676\n",
      "train loss:   1.007174\n",
      "train loss:   1.028841\n",
      "train loss:   1.164346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.972303\n",
      "train loss:   1.043377\n",
      "train loss:   0.673051\n",
      "train loss:   1.081559\n",
      "train loss:   1.048813\n",
      "train loss:   0.734476\n",
      "train loss:   0.722562\n",
      "train loss:   0.876937\n",
      "train loss:   0.714059\n",
      "train loss:   0.896253\n",
      "train loss:   1.176947\n",
      "train loss:   0.786010\n",
      "train loss:   1.090868\n",
      "train loss:   1.177497\n",
      "train loss:   0.890482\n",
      "train loss:   1.131948\n",
      "train loss:   0.836899\n",
      "train loss:   0.769322\n",
      "train loss:   0.669590\n",
      "train loss:   1.156670\n",
      "train loss:   0.716714\n",
      "train loss:   0.997521\n",
      "train loss:   0.899116\n",
      "train loss:   1.050920\n",
      "train loss:   0.690775\n",
      "train loss:   1.081211\n",
      "########### epoch 162 ###########\n",
      "########### loop 30450 ###########\n",
      "test loss:   0.168283   test accuracy:   0.916667\n",
      "########### loop 30450 ###########\n",
      "train loss:   1.177471\n",
      "train loss:   0.815572\n",
      "train loss:   1.070537\n",
      "train loss:   0.905770\n",
      "train loss:   0.999539\n",
      "train loss:   0.710501\n",
      "train loss:   0.769068\n",
      "train loss:   0.896096\n",
      "train loss:   1.158149\n",
      "train loss:   0.699227\n",
      "train loss:   0.941408\n",
      "train loss:   0.982185\n",
      "train loss:   1.143162\n",
      "train loss:   1.112606\n",
      "train loss:   0.818548\n",
      "train loss:   0.795842\n",
      "train loss:   0.743791\n",
      "train loss:   0.791424\n",
      "train loss:   1.219759\n",
      "train loss:   0.863087\n",
      "train loss:   0.898404\n",
      "train loss:   1.151102\n",
      "train loss:   0.826857\n",
      "train loss:   0.980324\n",
      "train loss:   0.704257\n",
      "train loss:   0.461158\n",
      "train loss:   1.125890\n",
      "train loss:   1.144865\n",
      "train loss:   1.016475\n",
      "train loss:   1.041840\n",
      "train loss:   0.841443\n",
      "train loss:   0.955612\n",
      "train loss:   0.463852\n",
      "train loss:   0.673263\n",
      "train loss:   1.164616\n",
      "train loss:   0.965985\n",
      "train loss:   0.779931\n",
      "train loss:   0.753093\n",
      "train loss:   0.712558\n",
      "train loss:   0.744550\n",
      "train loss:   1.011490\n",
      "train loss:   1.047667\n",
      "train loss:   0.753089\n",
      "train loss:   0.758535\n",
      "train loss:   0.649810\n",
      "train loss:   0.912735\n",
      "train loss:   1.103857\n",
      "train loss:   1.269264\n",
      "train loss:   0.729938\n",
      "train loss:   0.655916\n",
      "########### epoch 163 ###########\n",
      "########### loop 30500 ###########\n",
      "test loss:   0.286482   test accuracy:   0.916667\n",
      "########### loop 30500 ###########\n",
      "train loss:   0.925199\n",
      "train loss:   0.874532\n",
      "train loss:   1.111542\n",
      "train loss:   0.788852\n",
      "train loss:   1.002213\n",
      "train loss:   0.971969\n",
      "train loss:   0.823816\n",
      "train loss:   1.152329\n",
      "train loss:   0.762818\n",
      "train loss:   1.083679\n",
      "train loss:   0.781071\n",
      "train loss:   0.837743\n",
      "train loss:   1.238697\n",
      "train loss:   1.113526\n",
      "train loss:   0.794347\n",
      "train loss:   0.622317\n",
      "train loss:   1.056993\n",
      "train loss:   0.887049\n",
      "train loss:   1.229927\n",
      "train loss:   1.015870\n",
      "train loss:   1.089721\n",
      "train loss:   0.979596\n",
      "train loss:   0.939599\n",
      "train loss:   0.790693\n",
      "train loss:   0.988703\n",
      "train loss:   0.796855\n",
      "train loss:   0.999467\n",
      "train loss:   1.476139\n",
      "train loss:   0.823851\n",
      "train loss:   0.964719\n",
      "train loss:   0.740478\n",
      "train loss:   0.832496\n",
      "train loss:   0.963441\n",
      "train loss:   0.707525\n",
      "train loss:   1.106054\n",
      "train loss:   1.109679\n",
      "train loss:   0.814140\n",
      "train loss:   1.133738\n",
      "train loss:   1.281226\n",
      "train loss:   0.937452\n",
      "train loss:   1.129666\n",
      "train loss:   0.790954\n",
      "train loss:   1.074465\n",
      "train loss:   0.737814\n",
      "train loss:   0.912292\n",
      "train loss:   1.029783\n",
      "train loss:   1.045587\n",
      "train loss:   1.242983\n",
      "train loss:   0.906720\n",
      "train loss:   0.721009\n",
      "########### epoch 163 ###########\n",
      "########### loop 30550 ###########\n",
      "test loss:   0.161760   test accuracy:   0.958333\n",
      "########### loop 30550 ###########\n",
      "train loss:   1.035066\n",
      "train loss:   1.000572\n",
      "train loss:   1.140533\n",
      "train loss:   0.810500\n",
      "train loss:   0.819763\n",
      "train loss:   1.289913\n",
      "train loss:   1.125242\n",
      "train loss:   0.879863\n",
      "train loss:   0.824979\n",
      "train loss:   1.133056\n",
      "train loss:   1.219940\n",
      "train loss:   1.050451\n",
      "train loss:   0.872366\n",
      "train loss:   0.797590\n",
      "train loss:   1.036242\n",
      "train loss:   0.723743\n",
      "train loss:   0.996960\n",
      "train loss:   1.151288\n",
      "train loss:   1.090844\n",
      "train loss:   1.244326\n",
      "train loss:   1.015360\n",
      "train loss:   0.772375\n",
      "train loss:   1.095564\n",
      "train loss:   0.943401\n",
      "train loss:   1.119483\n",
      "train loss:   0.660148\n",
      "train loss:   1.153175\n",
      "train loss:   0.863935\n",
      "train loss:   0.728674\n",
      "train loss:   0.790729\n",
      "train loss:   0.749512\n",
      "train loss:   0.881899\n",
      "train loss:   1.052597\n",
      "train loss:   1.222467\n",
      "train loss:   0.737824\n",
      "train loss:   1.079821\n",
      "train loss:   1.011698\n",
      "train loss:   0.640641\n",
      "train loss:   0.647193\n",
      "train loss:   0.818061\n",
      "train loss:   1.064614\n",
      "train loss:   0.968298\n",
      "train loss:   0.962492\n",
      "train loss:   0.800544\n",
      "train loss:   1.016277\n",
      "train loss:   0.864665\n",
      "train loss:   1.049222\n",
      "train loss:   0.782247\n",
      "train loss:   0.563324\n",
      "train loss:   0.898133\n",
      "########### epoch 163 ###########\n",
      "########### loop 30600 ###########\n",
      "test loss:   0.186690   test accuracy:   0.958333\n",
      "########### loop 30600 ###########\n",
      "train loss:   0.891665\n",
      "train loss:   1.062577\n",
      "train loss:   1.167622\n",
      "train loss:   0.553436\n",
      "train loss:   0.520389\n",
      "train loss:   0.568204\n",
      "train loss:   0.861612\n",
      "train loss:   0.875152\n",
      "train loss:   0.809857\n",
      "train loss:   0.764221\n",
      "train loss:   0.787883\n",
      "train loss:   0.847137\n",
      "train loss:   1.036121\n",
      "train loss:   1.132934\n",
      "train loss:   0.759311\n",
      "train loss:   0.822218\n",
      "train loss:   0.861174\n",
      "train loss:   1.240840\n",
      "train loss:   0.921930\n",
      "train loss:   1.219122\n",
      "train loss:   0.807800\n",
      "train loss:   0.722225\n",
      "train loss:   0.881323\n",
      "train loss:   0.826642\n",
      "train loss:   0.883452\n",
      "train loss:   1.028386\n",
      "train loss:   0.878493\n",
      "train loss:   0.754760\n",
      "train loss:   0.746296\n",
      "train loss:   0.977117\n",
      "train loss:   1.125926\n",
      "train loss:   1.184447\n",
      "train loss:   0.763123\n",
      "train loss:   0.867771\n",
      "train loss:   0.821444\n",
      "train loss:   0.979509\n",
      "train loss:   1.036507\n",
      "train loss:   0.497709\n",
      "train loss:   0.767834\n",
      "train loss:   1.064346\n",
      "train loss:   0.932393\n",
      "train loss:   1.072488\n",
      "train loss:   0.857709\n",
      "train loss:   1.158516\n",
      "train loss:   0.890618\n",
      "train loss:   0.973810\n",
      "train loss:   0.819687\n",
      "train loss:   0.944662\n",
      "train loss:   0.898620\n",
      "train loss:   1.018274\n",
      "########### epoch 164 ###########\n",
      "########### loop 30650 ###########\n",
      "test loss:   0.114745   test accuracy:   1.000000\n",
      "########### loop 30650 ###########\n",
      "train loss:   1.256752\n",
      "train loss:   1.003438\n",
      "train loss:   1.207556\n",
      "train loss:   1.200515\n",
      "train loss:   0.968288\n",
      "train loss:   1.046297\n",
      "train loss:   0.759574\n",
      "train loss:   0.750014\n",
      "train loss:   1.035076\n",
      "train loss:   0.944630\n",
      "train loss:   0.889616\n",
      "train loss:   0.991555\n",
      "train loss:   1.277650\n",
      "train loss:   0.910589\n",
      "train loss:   0.750459\n",
      "train loss:   1.200801\n",
      "train loss:   0.448115\n",
      "train loss:   0.842432\n",
      "train loss:   1.207105\n",
      "train loss:   0.981174\n",
      "train loss:   0.886599\n",
      "train loss:   0.567234\n",
      "train loss:   1.078921\n",
      "train loss:   0.951656\n",
      "train loss:   1.192466\n",
      "train loss:   1.491844\n",
      "train loss:   0.810767\n",
      "train loss:   1.085709\n",
      "train loss:   0.864905\n",
      "train loss:   0.827694\n",
      "train loss:   0.805676\n",
      "train loss:   1.045507\n",
      "train loss:   0.508972\n",
      "train loss:   1.134919\n",
      "train loss:   0.907166\n",
      "train loss:   1.083129\n",
      "train loss:   0.661248\n",
      "train loss:   0.852444\n",
      "train loss:   1.067915\n",
      "train loss:   0.756292\n",
      "train loss:   0.872550\n",
      "train loss:   1.096913\n",
      "train loss:   0.779000\n",
      "train loss:   1.011366\n",
      "train loss:   1.364339\n",
      "train loss:   0.807575\n",
      "train loss:   1.181707\n",
      "train loss:   1.455851\n",
      "train loss:   1.011044\n",
      "train loss:   1.220442\n",
      "########### epoch 164 ###########\n",
      "########### loop 30700 ###########\n",
      "test loss:   0.180917   test accuracy:   0.958333\n",
      "########### loop 30700 ###########\n",
      "train loss:   1.032847\n",
      "train loss:   1.067230\n",
      "train loss:   0.853349\n",
      "train loss:   1.041503\n",
      "train loss:   0.902755\n",
      "train loss:   1.052285\n",
      "train loss:   0.908358\n",
      "train loss:   1.103196\n",
      "train loss:   0.764782\n",
      "train loss:   0.859284\n",
      "train loss:   1.048377\n",
      "train loss:   0.611086\n",
      "train loss:   1.110728\n",
      "train loss:   1.176039\n",
      "train loss:   1.068890\n",
      "train loss:   1.145804\n",
      "train loss:   1.026459\n",
      "train loss:   1.270128\n",
      "train loss:   1.305016\n",
      "train loss:   0.974096\n",
      "train loss:   0.643995\n",
      "train loss:   1.190220\n",
      "train loss:   0.912262\n",
      "train loss:   1.128750\n",
      "train loss:   1.277254\n",
      "train loss:   1.083548\n",
      "train loss:   1.251963\n",
      "train loss:   1.131205\n",
      "train loss:   1.035465\n",
      "train loss:   0.861350\n",
      "train loss:   0.878207\n",
      "train loss:   0.949865\n",
      "train loss:   1.138396\n",
      "train loss:   1.064849\n",
      "train loss:   1.271277\n",
      "train loss:   0.734240\n",
      "train loss:   0.902916\n",
      "train loss:   0.789941\n",
      "train loss:   0.981278\n",
      "train loss:   1.040153\n",
      "train loss:   0.799838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.165167\n",
      "train loss:   0.787822\n",
      "train loss:   0.709742\n",
      "train loss:   0.733521\n",
      "train loss:   0.930027\n",
      "train loss:   0.842039\n",
      "train loss:   1.010097\n",
      "train loss:   1.054898\n",
      "train loss:   0.953006\n",
      "########### epoch 164 ###########\n",
      "########### loop 30750 ###########\n",
      "test loss:   0.129302   test accuracy:   0.958333\n",
      "########### loop 30750 ###########\n",
      "train loss:   1.035526\n",
      "train loss:   0.895803\n",
      "train loss:   0.613578\n",
      "train loss:   0.874015\n",
      "train loss:   0.400183\n",
      "train loss:   0.783959\n",
      "train loss:   0.830412\n",
      "train loss:   0.883242\n",
      "train loss:   0.737961\n",
      "train loss:   0.830876\n",
      "train loss:   0.995850\n",
      "train loss:   0.867312\n",
      "train loss:   0.931451\n",
      "train loss:   0.746108\n",
      "train loss:   1.030260\n",
      "train loss:   0.916175\n",
      "train loss:   0.898170\n",
      "train loss:   1.030136\n",
      "train loss:   1.313225\n",
      "train loss:   1.209749\n",
      "train loss:   0.957923\n",
      "train loss:   0.903751\n",
      "train loss:   1.040704\n",
      "train loss:   1.262830\n",
      "train loss:   1.172658\n",
      "train loss:   1.059071\n",
      "train loss:   0.592309\n",
      "train loss:   0.584444\n",
      "train loss:   0.678675\n",
      "train loss:   0.891757\n",
      "train loss:   0.967526\n",
      "train loss:   1.207408\n",
      "train loss:   0.947146\n",
      "train loss:   0.474224\n",
      "train loss:   0.664192\n",
      "train loss:   0.877836\n",
      "train loss:   0.840939\n",
      "train loss:   0.911032\n",
      "train loss:   0.892358\n",
      "train loss:   1.296520\n",
      "train loss:   0.909124\n",
      "train loss:   0.805499\n",
      "train loss:   0.957733\n",
      "train loss:   0.948339\n",
      "train loss:   1.196762\n",
      "train loss:   1.094324\n",
      "train loss:   0.897742\n",
      "train loss:   1.085467\n",
      "train loss:   1.028235\n",
      "train loss:   1.089866\n",
      "########### epoch 164 ###########\n",
      "########### loop 30800 ###########\n",
      "test loss:   0.212413   test accuracy:   0.958333\n",
      "########### loop 30800 ###########\n",
      "train loss:   0.812620\n",
      "train loss:   1.003848\n",
      "train loss:   0.762264\n",
      "train loss:   1.046508\n",
      "train loss:   0.791481\n",
      "train loss:   0.846457\n",
      "train loss:   0.791195\n",
      "train loss:   0.867470\n",
      "train loss:   1.162915\n",
      "train loss:   1.275048\n",
      "train loss:   1.027663\n",
      "train loss:   0.827264\n",
      "train loss:   0.918328\n",
      "train loss:   0.709594\n",
      "train loss:   1.105949\n",
      "train loss:   1.176210\n",
      "train loss:   0.651519\n",
      "train loss:   0.736398\n",
      "train loss:   1.120752\n",
      "train loss:   0.955980\n",
      "train loss:   1.007128\n",
      "train loss:   1.048337\n",
      "train loss:   0.880252\n",
      "train loss:   0.791113\n",
      "train loss:   0.826508\n",
      "train loss:   1.061797\n",
      "train loss:   0.789358\n",
      "train loss:   1.193017\n",
      "train loss:   1.046147\n",
      "train loss:   1.024293\n",
      "train loss:   1.029486\n",
      "train loss:   0.562678\n",
      "train loss:   1.323920\n",
      "train loss:   1.032562\n",
      "train loss:   0.824057\n",
      "train loss:   1.401876\n",
      "train loss:   0.809147\n",
      "train loss:   0.819244\n",
      "train loss:   1.274300\n",
      "train loss:   0.934459\n",
      "train loss:   1.160714\n",
      "train loss:   0.657836\n",
      "train loss:   0.566061\n",
      "train loss:   0.982922\n",
      "train loss:   0.686833\n",
      "train loss:   0.956469\n",
      "train loss:   0.954593\n",
      "train loss:   0.754468\n",
      "train loss:   1.505107\n",
      "train loss:   0.735081\n",
      "########### epoch 165 ###########\n",
      "########### loop 30850 ###########\n",
      "test loss:   0.118804   test accuracy:   1.000000\n",
      "########### loop 30850 ###########\n",
      "train loss:   0.965368\n",
      "train loss:   0.996989\n",
      "train loss:   0.814946\n",
      "train loss:   0.903695\n",
      "train loss:   0.680906\n",
      "train loss:   0.895606\n",
      "train loss:   0.919630\n",
      "train loss:   0.844388\n",
      "train loss:   0.913644\n",
      "train loss:   1.006851\n",
      "train loss:   1.021648\n",
      "train loss:   1.289078\n",
      "train loss:   0.675195\n",
      "train loss:   1.340718\n",
      "train loss:   1.049186\n",
      "train loss:   0.653636\n",
      "train loss:   0.864321\n",
      "train loss:   0.928076\n",
      "train loss:   0.975131\n",
      "train loss:   0.955341\n",
      "train loss:   0.912175\n",
      "train loss:   0.779670\n",
      "train loss:   0.496023\n",
      "train loss:   1.199645\n",
      "train loss:   0.881313\n",
      "train loss:   1.262411\n",
      "train loss:   0.735590\n",
      "train loss:   0.744302\n",
      "train loss:   1.052714\n",
      "train loss:   0.725684\n",
      "train loss:   0.958092\n",
      "train loss:   1.205335\n",
      "train loss:   0.689935\n",
      "train loss:   1.046210\n",
      "train loss:   1.009112\n",
      "train loss:   1.152013\n",
      "train loss:   1.268542\n",
      "train loss:   0.769877\n",
      "train loss:   0.794585\n",
      "train loss:   0.632072\n",
      "train loss:   1.015233\n",
      "train loss:   0.705974\n",
      "train loss:   0.934417\n",
      "train loss:   0.563680\n",
      "train loss:   0.878460\n",
      "train loss:   1.096130\n",
      "train loss:   0.795715\n",
      "train loss:   0.852273\n",
      "train loss:   0.883105\n",
      "train loss:   1.027739\n",
      "########### epoch 165 ###########\n",
      "########### loop 30900 ###########\n",
      "test loss:   0.273599   test accuracy:   0.875000\n",
      "########### loop 30900 ###########\n",
      "train loss:   1.008223\n",
      "train loss:   0.978218\n",
      "train loss:   0.740161\n",
      "train loss:   1.067881\n",
      "train loss:   1.017498\n",
      "train loss:   0.644813\n",
      "train loss:   1.035706\n",
      "train loss:   0.672726\n",
      "train loss:   0.834512\n",
      "train loss:   0.945023\n",
      "train loss:   0.886938\n",
      "train loss:   0.914021\n",
      "train loss:   1.395797\n",
      "train loss:   1.339373\n",
      "train loss:   0.467246\n",
      "train loss:   1.093897\n",
      "train loss:   0.886821\n",
      "train loss:   0.889843\n",
      "train loss:   0.908980\n",
      "train loss:   0.942080\n",
      "train loss:   0.754155\n",
      "train loss:   0.759836\n",
      "train loss:   1.193899\n",
      "train loss:   0.859263\n",
      "train loss:   0.746873\n",
      "train loss:   0.895590\n",
      "train loss:   0.986604\n",
      "train loss:   0.861637\n",
      "train loss:   1.188123\n",
      "train loss:   0.813995\n",
      "train loss:   0.750836\n",
      "train loss:   1.073315\n",
      "train loss:   0.837911\n",
      "train loss:   0.935247\n",
      "train loss:   1.044896\n",
      "train loss:   0.748624\n",
      "train loss:   1.278498\n",
      "train loss:   0.757277\n",
      "train loss:   1.108115\n",
      "train loss:   1.166262\n",
      "train loss:   0.465200\n",
      "train loss:   1.179544\n",
      "train loss:   1.304088\n",
      "train loss:   0.823425\n",
      "train loss:   0.837784\n",
      "train loss:   1.256279\n",
      "train loss:   1.077390\n",
      "train loss:   0.754621\n",
      "train loss:   1.023629\n",
      "train loss:   0.881798\n",
      "########### epoch 165 ###########\n",
      "########### loop 30950 ###########\n",
      "test loss:   0.140195   test accuracy:   1.000000\n",
      "########### loop 30950 ###########\n",
      "train loss:   0.786138\n",
      "train loss:   1.105419\n",
      "train loss:   0.855961\n",
      "train loss:   1.019079\n",
      "train loss:   0.427746\n",
      "train loss:   0.933302\n",
      "train loss:   1.128907\n",
      "train loss:   1.131514\n",
      "train loss:   0.614894\n",
      "train loss:   1.095500\n",
      "train loss:   0.714949\n",
      "train loss:   1.197570\n",
      "train loss:   0.740560\n",
      "train loss:   0.993158\n",
      "train loss:   1.143972\n",
      "train loss:   1.339460\n",
      "train loss:   0.977233\n",
      "train loss:   0.811799\n",
      "train loss:   1.044485\n",
      "train loss:   0.590479\n",
      "train loss:   0.891056\n",
      "train loss:   1.064227\n",
      "train loss:   0.893567\n",
      "train loss:   1.119920\n",
      "train loss:   0.995089\n",
      "train loss:   1.116081\n",
      "train loss:   1.016451\n",
      "train loss:   0.963612\n",
      "train loss:   0.861027\n",
      "train loss:   0.357745\n",
      "train loss:   0.873292\n",
      "train loss:   0.824846\n",
      "train loss:   0.885606\n",
      "train loss:   0.877560\n",
      "train loss:   1.060110\n",
      "train loss:   1.046061\n",
      "train loss:   0.809295\n",
      "train loss:   0.792019\n",
      "train loss:   0.762349\n",
      "train loss:   0.640243\n",
      "train loss:   0.973042\n",
      "train loss:   0.698878\n",
      "train loss:   1.145581\n",
      "train loss:   1.237292\n",
      "train loss:   0.948667\n",
      "train loss:   0.967900\n",
      "train loss:   0.820123\n",
      "train loss:   0.892787\n",
      "train loss:   1.236277\n",
      "train loss:   0.789023\n",
      "########### epoch 165 ###########\n",
      "########### loop 31000 ###########\n",
      "test loss:   0.361877   test accuracy:   0.916667\n",
      "########### loop 31000 ###########\n",
      "train loss:   1.166980\n",
      "train loss:   1.246837\n",
      "train loss:   0.741406\n",
      "train loss:   1.036218\n",
      "train loss:   1.025130\n",
      "train loss:   0.811760\n",
      "train loss:   1.121883\n",
      "train loss:   0.932884\n",
      "train loss:   0.545117\n",
      "train loss:   0.883637\n",
      "train loss:   1.065537\n",
      "train loss:   0.848213\n",
      "train loss:   0.906156\n",
      "train loss:   0.555189\n",
      "train loss:   1.321676\n",
      "train loss:   0.953958\n",
      "train loss:   1.123796\n",
      "train loss:   0.926958\n",
      "train loss:   0.912708\n",
      "train loss:   0.652837\n",
      "train loss:   0.975172\n",
      "train loss:   1.208848\n",
      "train loss:   1.015886\n",
      "train loss:   0.825688\n",
      "train loss:   1.223032\n",
      "train loss:   1.002649\n",
      "train loss:   1.106832\n",
      "train loss:   0.924576\n",
      "train loss:   0.409481\n",
      "train loss:   1.136329\n",
      "train loss:   0.744657\n",
      "train loss:   0.788739\n",
      "train loss:   1.150858\n",
      "train loss:   1.317213\n",
      "train loss:   0.908060\n",
      "train loss:   1.002221\n",
      "train loss:   0.622451\n",
      "train loss:   1.011508\n",
      "train loss:   1.209227\n",
      "train loss:   1.012899\n",
      "train loss:   0.948897\n",
      "train loss:   0.969435\n",
      "train loss:   0.705863\n",
      "train loss:   0.795956\n",
      "train loss:   0.870970\n",
      "train loss:   0.942903\n",
      "train loss:   0.955036\n",
      "train loss:   0.859446\n",
      "train loss:   0.948246\n",
      "train loss:   1.242266\n",
      "########### epoch 166 ###########\n",
      "########### loop 31050 ###########\n",
      "test loss:   0.308011   test accuracy:   0.958333\n",
      "########### loop 31050 ###########\n",
      "train loss:   0.902505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.891623\n",
      "train loss:   1.040711\n",
      "train loss:   0.814409\n",
      "train loss:   1.169461\n",
      "train loss:   0.892698\n",
      "train loss:   1.212908\n",
      "train loss:   1.026677\n",
      "train loss:   1.042977\n",
      "train loss:   1.072330\n",
      "train loss:   1.303578\n",
      "train loss:   0.725278\n",
      "train loss:   1.136977\n",
      "train loss:   0.883419\n",
      "train loss:   0.960990\n",
      "train loss:   1.207127\n",
      "train loss:   1.253283\n",
      "train loss:   0.692238\n",
      "train loss:   0.967226\n",
      "train loss:   0.879422\n",
      "train loss:   0.910178\n",
      "train loss:   0.979734\n",
      "train loss:   1.445258\n",
      "train loss:   1.005553\n",
      "train loss:   0.962204\n",
      "train loss:   1.088367\n",
      "train loss:   0.827158\n",
      "train loss:   0.948412\n",
      "train loss:   0.916333\n",
      "train loss:   0.882544\n",
      "train loss:   0.879458\n",
      "train loss:   1.124064\n",
      "train loss:   0.804942\n",
      "train loss:   1.078443\n",
      "train loss:   0.764467\n",
      "train loss:   1.178786\n",
      "train loss:   1.013268\n",
      "train loss:   1.201377\n",
      "train loss:   0.866291\n",
      "train loss:   0.948035\n",
      "train loss:   0.929937\n",
      "train loss:   1.359504\n",
      "train loss:   0.631816\n",
      "train loss:   0.806864\n",
      "train loss:   0.740486\n",
      "train loss:   0.888844\n",
      "train loss:   1.093683\n",
      "train loss:   0.951783\n",
      "train loss:   0.746076\n",
      "train loss:   1.230448\n",
      "########### epoch 166 ###########\n",
      "########### loop 31100 ###########\n",
      "test loss:   0.254370   test accuracy:   0.958333\n",
      "########### loop 31100 ###########\n",
      "train loss:   0.762327\n",
      "train loss:   1.110142\n",
      "train loss:   1.219114\n",
      "train loss:   0.994061\n",
      "train loss:   1.213593\n",
      "train loss:   1.137652\n",
      "train loss:   0.899571\n",
      "train loss:   1.264737\n",
      "train loss:   0.538229\n",
      "train loss:   1.025138\n",
      "train loss:   1.136691\n",
      "train loss:   1.160669\n",
      "train loss:   1.161940\n",
      "train loss:   0.972591\n",
      "train loss:   0.760173\n",
      "train loss:   0.996599\n",
      "train loss:   1.100807\n",
      "train loss:   0.858340\n",
      "train loss:   0.707021\n",
      "train loss:   0.849796\n",
      "train loss:   0.801371\n",
      "train loss:   0.881817\n",
      "train loss:   0.902740\n",
      "train loss:   1.010195\n",
      "train loss:   0.749632\n",
      "train loss:   0.769222\n",
      "train loss:   0.754353\n",
      "train loss:   0.438280\n",
      "train loss:   0.880092\n",
      "train loss:   0.982461\n",
      "train loss:   0.835411\n",
      "train loss:   0.976965\n",
      "train loss:   1.041328\n",
      "train loss:   0.720488\n",
      "train loss:   0.799279\n",
      "train loss:   1.011244\n",
      "train loss:   0.968845\n",
      "train loss:   0.813459\n",
      "train loss:   0.898241\n",
      "train loss:   0.853134\n",
      "train loss:   0.538788\n",
      "train loss:   1.085145\n",
      "train loss:   0.821657\n",
      "train loss:   0.811826\n",
      "train loss:   0.626179\n",
      "train loss:   1.032453\n",
      "train loss:   1.098466\n",
      "train loss:   1.178618\n",
      "train loss:   1.145425\n",
      "train loss:   0.887808\n",
      "########### epoch 166 ###########\n",
      "########### loop 31150 ###########\n",
      "test loss:   0.338000   test accuracy:   0.916667\n",
      "########### loop 31150 ###########\n",
      "train loss:   0.904500\n",
      "train loss:   1.135970\n",
      "train loss:   0.839253\n",
      "train loss:   0.782619\n",
      "train loss:   0.984729\n",
      "train loss:   1.009918\n",
      "train loss:   0.989590\n",
      "train loss:   0.798774\n",
      "train loss:   0.734234\n",
      "train loss:   1.127066\n",
      "train loss:   0.491125\n",
      "train loss:   1.115061\n",
      "train loss:   0.806374\n",
      "train loss:   0.794195\n",
      "train loss:   0.952786\n",
      "train loss:   1.055832\n",
      "train loss:   1.104312\n",
      "train loss:   0.582457\n",
      "train loss:   0.813577\n",
      "train loss:   1.025268\n",
      "train loss:   0.771023\n",
      "train loss:   0.683144\n",
      "train loss:   0.997344\n",
      "train loss:   0.798034\n",
      "train loss:   0.563859\n",
      "train loss:   1.092310\n",
      "train loss:   0.958164\n",
      "train loss:   0.645574\n",
      "train loss:   0.966617\n",
      "train loss:   0.900528\n",
      "train loss:   1.275675\n",
      "train loss:   0.810743\n",
      "train loss:   1.083518\n",
      "train loss:   1.170777\n",
      "train loss:   0.901333\n",
      "train loss:   1.062040\n",
      "train loss:   0.922151\n",
      "train loss:   0.789762\n",
      "train loss:   0.855828\n",
      "train loss:   0.979704\n",
      "train loss:   0.875779\n",
      "train loss:   0.973047\n",
      "train loss:   1.117492\n",
      "train loss:   0.805631\n",
      "train loss:   1.049955\n",
      "train loss:   0.866763\n",
      "train loss:   1.061316\n",
      "train loss:   0.710836\n",
      "train loss:   1.188127\n",
      "train loss:   0.678925\n",
      "########### epoch 166 ###########\n",
      "########### loop 31200 ###########\n",
      "test loss:   0.242499   test accuracy:   0.916667\n",
      "########### loop 31200 ###########\n",
      "train loss:   0.835200\n",
      "train loss:   1.029432\n",
      "train loss:   0.939202\n",
      "train loss:   1.026674\n",
      "train loss:   1.198277\n",
      "train loss:   0.840586\n",
      "train loss:   0.915774\n",
      "train loss:   0.878083\n",
      "train loss:   0.691441\n",
      "train loss:   0.869872\n",
      "train loss:   0.864919\n",
      "train loss:   1.035650\n",
      "train loss:   0.925744\n",
      "train loss:   0.959037\n",
      "train loss:   0.639163\n",
      "train loss:   0.930137\n",
      "train loss:   1.041551\n",
      "train loss:   1.146218\n",
      "train loss:   0.830097\n",
      "train loss:   1.118598\n",
      "train loss:   0.926731\n",
      "train loss:   0.514448\n",
      "train loss:   0.693275\n",
      "train loss:   0.873975\n",
      "train loss:   0.927314\n",
      "train loss:   1.151927\n",
      "train loss:   0.764610\n",
      "train loss:   0.747881\n",
      "train loss:   0.683787\n",
      "train loss:   0.929719\n",
      "train loss:   0.929428\n",
      "train loss:   1.427144\n",
      "train loss:   0.820762\n",
      "train loss:   1.533221\n",
      "train loss:   1.055714\n",
      "train loss:   0.654521\n",
      "train loss:   0.984567\n",
      "train loss:   1.237603\n",
      "train loss:   1.001202\n",
      "train loss:   1.427867\n",
      "train loss:   0.726382\n",
      "train loss:   0.965330\n",
      "train loss:   1.114817\n",
      "train loss:   0.806160\n",
      "train loss:   0.483022\n",
      "train loss:   1.208093\n",
      "train loss:   0.837012\n",
      "train loss:   0.876603\n",
      "train loss:   0.899877\n",
      "train loss:   1.260415\n",
      "########### epoch 167 ###########\n",
      "########### loop 31250 ###########\n",
      "test loss:   0.391072   test accuracy:   0.833333\n",
      "########### loop 31250 ###########\n",
      "train loss:   1.019007\n",
      "train loss:   0.911938\n",
      "train loss:   1.104613\n",
      "train loss:   0.973714\n",
      "train loss:   0.773436\n",
      "train loss:   0.735420\n",
      "train loss:   0.933359\n",
      "train loss:   0.654738\n",
      "train loss:   0.973259\n",
      "train loss:   0.929269\n",
      "train loss:   0.927470\n",
      "train loss:   0.953570\n",
      "train loss:   0.784334\n",
      "train loss:   0.791950\n",
      "train loss:   0.547450\n",
      "train loss:   0.958642\n",
      "train loss:   1.274122\n",
      "train loss:   0.878191\n",
      "train loss:   0.804355\n",
      "train loss:   0.847081\n",
      "train loss:   1.154904\n",
      "train loss:   0.756062\n",
      "train loss:   1.010904\n",
      "train loss:   1.054189\n",
      "train loss:   1.271926\n",
      "train loss:   1.204887\n",
      "train loss:   1.023306\n",
      "train loss:   0.714857\n",
      "train loss:   0.979866\n",
      "train loss:   0.746313\n",
      "train loss:   1.171672\n",
      "train loss:   0.869030\n",
      "train loss:   1.368272\n",
      "train loss:   1.039908\n",
      "train loss:   1.106851\n",
      "train loss:   0.676294\n",
      "train loss:   0.602125\n",
      "train loss:   1.100718\n",
      "train loss:   1.012308\n",
      "train loss:   1.159337\n",
      "train loss:   1.487123\n",
      "train loss:   1.070880\n",
      "train loss:   0.972446\n",
      "train loss:   0.719676\n",
      "train loss:   0.919903\n",
      "train loss:   1.078542\n",
      "train loss:   0.943230\n",
      "train loss:   0.996797\n",
      "train loss:   0.852234\n",
      "train loss:   0.979849\n",
      "########### epoch 167 ###########\n",
      "########### loop 31300 ###########\n",
      "test loss:   0.373978   test accuracy:   0.875000\n",
      "########### loop 31300 ###########\n",
      "train loss:   1.083155\n",
      "train loss:   0.909505\n",
      "train loss:   0.963401\n",
      "train loss:   1.241864\n",
      "train loss:   1.361937\n",
      "train loss:   1.140816\n",
      "train loss:   0.928013\n",
      "train loss:   1.140679\n",
      "train loss:   1.040986\n",
      "train loss:   0.975853\n",
      "train loss:   1.057456\n",
      "train loss:   1.214784\n",
      "train loss:   0.954095\n",
      "train loss:   0.591601\n",
      "train loss:   1.204847\n",
      "train loss:   0.516215\n",
      "train loss:   1.057819\n",
      "train loss:   0.909398\n",
      "train loss:   1.319365\n",
      "train loss:   1.048074\n",
      "train loss:   1.190040\n",
      "train loss:   0.829506\n",
      "train loss:   0.688237\n",
      "train loss:   1.186007\n",
      "train loss:   1.243174\n",
      "train loss:   1.119828\n",
      "train loss:   0.659912\n",
      "train loss:   0.970059\n",
      "train loss:   1.019641\n",
      "train loss:   0.904548\n",
      "train loss:   0.527783\n",
      "train loss:   0.863247\n",
      "train loss:   1.350043\n",
      "train loss:   0.837443\n",
      "train loss:   0.728091\n",
      "train loss:   1.014901\n",
      "train loss:   1.161770\n",
      "train loss:   0.659405\n",
      "train loss:   1.066878\n",
      "train loss:   1.126848\n",
      "train loss:   1.033106\n",
      "train loss:   0.925819\n",
      "train loss:   0.997967\n",
      "train loss:   1.135562\n",
      "train loss:   0.949233\n",
      "train loss:   0.863403\n",
      "train loss:   1.373638\n",
      "train loss:   1.006666\n",
      "train loss:   0.853524\n",
      "train loss:   0.867218\n",
      "########### epoch 167 ###########\n",
      "########### loop 31350 ###########\n",
      "test loss:   0.202856   test accuracy:   0.958333\n",
      "########### loop 31350 ###########\n",
      "train loss:   0.788190\n",
      "train loss:   0.993021\n",
      "train loss:   0.983213\n",
      "train loss:   1.209857\n",
      "train loss:   0.835277\n",
      "train loss:   1.239100\n",
      "train loss:   0.999789\n",
      "train loss:   0.987793\n",
      "train loss:   0.926162\n",
      "train loss:   1.029372\n",
      "train loss:   0.690968\n",
      "train loss:   0.991463\n",
      "train loss:   0.977649\n",
      "train loss:   0.908869\n",
      "train loss:   0.951044\n",
      "train loss:   0.808920\n",
      "train loss:   0.774797\n",
      "train loss:   0.924379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.088172\n",
      "train loss:   0.843288\n",
      "train loss:   0.586473\n",
      "train loss:   0.907682\n",
      "train loss:   0.980407\n",
      "train loss:   1.177474\n",
      "train loss:   0.975880\n",
      "train loss:   0.803428\n",
      "train loss:   0.670676\n",
      "train loss:   1.207345\n",
      "train loss:   0.765703\n",
      "train loss:   1.187279\n",
      "train loss:   0.571934\n",
      "train loss:   0.900001\n",
      "train loss:   1.167556\n",
      "train loss:   0.936382\n",
      "train loss:   1.060035\n",
      "train loss:   1.217671\n",
      "train loss:   0.925451\n",
      "train loss:   1.204808\n",
      "train loss:   1.099641\n",
      "train loss:   1.242827\n",
      "train loss:   0.888534\n",
      "train loss:   0.735694\n",
      "train loss:   0.930367\n",
      "train loss:   0.929454\n",
      "train loss:   1.004879\n",
      "train loss:   0.856572\n",
      "train loss:   1.192965\n",
      "train loss:   0.837271\n",
      "train loss:   1.076771\n",
      "train loss:   0.787474\n",
      "########### epoch 168 ###########\n",
      "########### loop 31400 ###########\n",
      "test loss:   0.171990   test accuracy:   0.958333\n",
      "########### loop 31400 ###########\n",
      "train loss:   0.947448\n",
      "train loss:   0.768635\n",
      "train loss:   1.104435\n",
      "train loss:   0.728881\n",
      "train loss:   1.111149\n",
      "train loss:   1.120965\n",
      "train loss:   1.045153\n",
      "train loss:   0.852186\n",
      "train loss:   1.016412\n",
      "train loss:   1.151454\n",
      "train loss:   1.174470\n",
      "train loss:   0.824579\n",
      "train loss:   0.884107\n",
      "train loss:   0.977819\n",
      "train loss:   0.601724\n",
      "train loss:   0.762210\n",
      "train loss:   0.788529\n",
      "train loss:   0.855381\n",
      "train loss:   0.816366\n",
      "train loss:   0.864517\n",
      "train loss:   0.638609\n",
      "train loss:   0.429063\n",
      "train loss:   0.866525\n",
      "train loss:   1.200729\n",
      "train loss:   1.103001\n",
      "train loss:   0.939694\n",
      "train loss:   0.774153\n",
      "train loss:   0.907789\n",
      "train loss:   1.063867\n",
      "train loss:   0.937473\n",
      "train loss:   1.052454\n",
      "train loss:   0.693918\n",
      "train loss:   0.842524\n",
      "train loss:   1.156402\n",
      "train loss:   0.869629\n",
      "train loss:   0.799555\n",
      "train loss:   0.775028\n",
      "train loss:   1.429991\n",
      "train loss:   0.800458\n",
      "train loss:   1.031588\n",
      "train loss:   0.871523\n",
      "train loss:   0.976888\n",
      "train loss:   0.907258\n",
      "train loss:   0.577446\n",
      "train loss:   1.259513\n",
      "train loss:   1.148359\n",
      "train loss:   0.877047\n",
      "train loss:   0.820569\n",
      "train loss:   1.157218\n",
      "train loss:   0.752738\n",
      "########### epoch 168 ###########\n",
      "########### loop 31450 ###########\n",
      "test loss:   0.233623   test accuracy:   0.958333\n",
      "########### loop 31450 ###########\n",
      "train loss:   0.764898\n",
      "train loss:   1.023421\n",
      "train loss:   1.053704\n",
      "train loss:   1.128695\n",
      "train loss:   1.106506\n",
      "train loss:   0.877100\n",
      "train loss:   0.948923\n",
      "train loss:   0.860159\n",
      "train loss:   0.890963\n",
      "train loss:   0.665269\n",
      "train loss:   1.004652\n",
      "train loss:   0.874198\n",
      "train loss:   0.942642\n",
      "train loss:   1.357190\n",
      "train loss:   0.744635\n",
      "train loss:   0.884105\n",
      "train loss:   0.920437\n",
      "train loss:   0.827072\n",
      "train loss:   1.192998\n",
      "train loss:   0.924753\n",
      "train loss:   0.805241\n",
      "train loss:   1.041969\n",
      "train loss:   0.614691\n",
      "train loss:   0.817624\n",
      "train loss:   0.924190\n",
      "train loss:   0.669342\n",
      "train loss:   1.287663\n",
      "train loss:   1.074337\n",
      "train loss:   0.828911\n",
      "train loss:   0.948753\n",
      "train loss:   0.969927\n",
      "train loss:   0.888609\n",
      "train loss:   0.975423\n",
      "train loss:   1.204555\n",
      "train loss:   0.721615\n",
      "train loss:   1.168537\n",
      "train loss:   0.939272\n",
      "train loss:   0.751647\n",
      "train loss:   1.293309\n",
      "train loss:   0.769612\n",
      "train loss:   0.961221\n",
      "train loss:   1.162160\n",
      "train loss:   0.943463\n",
      "train loss:   1.055824\n",
      "train loss:   0.963781\n",
      "train loss:   0.703283\n",
      "train loss:   0.878743\n",
      "train loss:   0.872101\n",
      "train loss:   0.941922\n",
      "train loss:   0.795674\n",
      "########### epoch 168 ###########\n",
      "########### loop 31500 ###########\n",
      "test loss:   0.127541   test accuracy:   1.000000\n",
      "########### loop 31500 ###########\n",
      "train loss:   0.981932\n",
      "train loss:   1.061894\n",
      "train loss:   0.925434\n",
      "train loss:   0.944884\n",
      "train loss:   0.690146\n",
      "train loss:   0.834010\n",
      "train loss:   1.308352\n",
      "train loss:   1.087716\n",
      "train loss:   0.962127\n",
      "train loss:   1.193619\n",
      "train loss:   0.955428\n",
      "train loss:   0.867478\n",
      "train loss:   1.112101\n",
      "train loss:   1.061291\n",
      "train loss:   1.078638\n",
      "train loss:   1.213180\n",
      "train loss:   0.746759\n",
      "train loss:   1.450030\n",
      "train loss:   1.020866\n",
      "train loss:   1.115609\n",
      "train loss:   1.096099\n",
      "train loss:   0.940443\n",
      "train loss:   0.911093\n",
      "train loss:   1.180290\n",
      "train loss:   0.791350\n",
      "train loss:   1.001740\n",
      "train loss:   0.724609\n",
      "train loss:   0.892329\n",
      "train loss:   1.083539\n",
      "train loss:   1.070668\n",
      "train loss:   1.021433\n",
      "train loss:   1.225475\n",
      "train loss:   0.906655\n",
      "train loss:   1.062245\n",
      "train loss:   0.985861\n",
      "train loss:   0.863021\n",
      "train loss:   0.750580\n",
      "train loss:   1.240651\n",
      "train loss:   0.670998\n",
      "train loss:   1.188016\n",
      "train loss:   0.524204\n",
      "train loss:   0.694577\n",
      "train loss:   0.591746\n",
      "train loss:   0.851237\n",
      "train loss:   0.935277\n",
      "train loss:   0.882042\n",
      "train loss:   0.892973\n",
      "train loss:   0.936762\n",
      "train loss:   1.032392\n",
      "train loss:   0.869194\n",
      "########### epoch 168 ###########\n",
      "########### loop 31550 ###########\n",
      "test loss:   0.318975   test accuracy:   0.958333\n",
      "########### loop 31550 ###########\n",
      "train loss:   0.750918\n",
      "train loss:   1.152603\n",
      "train loss:   1.138304\n",
      "train loss:   1.035470\n",
      "train loss:   0.878977\n",
      "train loss:   0.972663\n",
      "train loss:   1.124282\n",
      "train loss:   0.570066\n",
      "train loss:   0.535518\n",
      "train loss:   1.038810\n",
      "train loss:   0.773442\n",
      "train loss:   0.905200\n",
      "train loss:   0.860126\n",
      "train loss:   0.913917\n",
      "train loss:   1.045622\n",
      "train loss:   0.940046\n",
      "train loss:   1.228062\n",
      "train loss:   1.062678\n",
      "train loss:   0.957497\n",
      "train loss:   1.148215\n",
      "train loss:   0.996288\n",
      "train loss:   1.149420\n",
      "train loss:   0.935366\n",
      "train loss:   1.244895\n",
      "train loss:   0.825394\n",
      "train loss:   0.999558\n",
      "train loss:   1.033363\n",
      "train loss:   0.996953\n",
      "train loss:   0.869009\n",
      "train loss:   0.927820\n",
      "train loss:   0.924406\n",
      "train loss:   0.788934\n",
      "train loss:   0.886352\n",
      "train loss:   1.210663\n",
      "train loss:   1.173130\n",
      "train loss:   1.197054\n",
      "train loss:   1.153628\n",
      "train loss:   0.979532\n",
      "train loss:   1.289017\n",
      "train loss:   0.676721\n",
      "train loss:   0.648030\n",
      "train loss:   1.084081\n",
      "train loss:   1.151187\n",
      "train loss:   0.810240\n",
      "train loss:   1.313301\n",
      "train loss:   0.804531\n",
      "train loss:   0.829047\n",
      "train loss:   1.172628\n",
      "train loss:   1.031934\n",
      "train loss:   0.991702\n",
      "########### epoch 169 ###########\n",
      "########### loop 31600 ###########\n",
      "test loss:   0.268142   test accuracy:   0.958333\n",
      "########### loop 31600 ###########\n",
      "train loss:   1.076009\n",
      "train loss:   1.211966\n",
      "train loss:   0.670303\n",
      "train loss:   0.985541\n",
      "train loss:   0.666073\n",
      "train loss:   0.922841\n",
      "train loss:   0.847127\n",
      "train loss:   1.011441\n",
      "train loss:   0.708393\n",
      "train loss:   0.750201\n",
      "train loss:   0.916212\n",
      "train loss:   0.777692\n",
      "train loss:   0.887662\n",
      "train loss:   0.819733\n",
      "train loss:   0.967663\n",
      "train loss:   0.787472\n",
      "train loss:   0.912365\n",
      "train loss:   0.969361\n",
      "train loss:   0.730229\n",
      "train loss:   0.668701\n",
      "train loss:   0.639358\n",
      "train loss:   0.843956\n",
      "train loss:   0.678597\n",
      "train loss:   0.956975\n",
      "train loss:   1.223163\n",
      "train loss:   1.041130\n",
      "train loss:   0.915994\n",
      "train loss:   1.047074\n",
      "train loss:   0.692859\n",
      "train loss:   0.828925\n",
      "train loss:   0.770863\n",
      "train loss:   0.865348\n",
      "train loss:   1.374875\n",
      "train loss:   0.789845\n",
      "train loss:   1.010102\n",
      "train loss:   0.774035\n",
      "train loss:   1.154693\n",
      "train loss:   0.918587\n",
      "train loss:   1.043844\n",
      "train loss:   1.199786\n",
      "train loss:   0.750407\n",
      "train loss:   1.134871\n",
      "train loss:   1.130861\n",
      "train loss:   1.153384\n",
      "train loss:   1.142267\n",
      "train loss:   0.875990\n",
      "train loss:   1.169372\n",
      "train loss:   0.945181\n",
      "train loss:   0.968958\n",
      "train loss:   1.158116\n",
      "########### epoch 169 ###########\n",
      "########### loop 31650 ###########\n",
      "test loss:   0.292610   test accuracy:   0.875000\n",
      "########### loop 31650 ###########\n",
      "train loss:   0.645028\n",
      "train loss:   0.900379\n",
      "train loss:   1.228939\n",
      "train loss:   0.825279\n",
      "train loss:   0.617386\n",
      "train loss:   1.064146\n",
      "train loss:   0.803977\n",
      "train loss:   0.722733\n",
      "train loss:   0.870143\n",
      "train loss:   1.255101\n",
      "train loss:   1.355164\n",
      "train loss:   1.151232\n",
      "train loss:   0.611572\n",
      "train loss:   0.696426\n",
      "train loss:   0.938618\n",
      "train loss:   0.806886\n",
      "train loss:   0.986078\n",
      "train loss:   0.938630\n",
      "train loss:   0.734406\n",
      "train loss:   0.897537\n",
      "train loss:   0.930944\n",
      "train loss:   0.912952\n",
      "train loss:   0.777438\n",
      "train loss:   1.210777\n",
      "train loss:   1.073642\n",
      "train loss:   0.979704\n",
      "train loss:   1.543214\n",
      "train loss:   0.867159\n",
      "train loss:   0.702206\n",
      "train loss:   1.049228\n",
      "train loss:   0.715713\n",
      "train loss:   0.760830\n",
      "train loss:   0.829472\n",
      "train loss:   0.783477\n",
      "train loss:   1.335894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.968130\n",
      "train loss:   0.937999\n",
      "train loss:   0.718954\n",
      "train loss:   1.166755\n",
      "train loss:   0.948147\n",
      "train loss:   0.900802\n",
      "train loss:   0.531105\n",
      "train loss:   1.098928\n",
      "train loss:   0.823637\n",
      "train loss:   1.099163\n",
      "train loss:   0.875666\n",
      "train loss:   1.160503\n",
      "train loss:   0.802026\n",
      "train loss:   0.983352\n",
      "train loss:   0.954394\n",
      "########### epoch 169 ###########\n",
      "########### loop 31700 ###########\n",
      "test loss:   0.113248   test accuracy:   0.958333\n",
      "########### loop 31700 ###########\n",
      "train loss:   0.945377\n",
      "train loss:   1.004130\n",
      "train loss:   0.774497\n",
      "train loss:   1.079545\n",
      "train loss:   0.948975\n",
      "train loss:   0.613464\n",
      "train loss:   1.201735\n",
      "train loss:   0.684423\n",
      "train loss:   1.074562\n",
      "train loss:   0.854757\n",
      "train loss:   0.946242\n",
      "train loss:   0.780873\n",
      "train loss:   1.108799\n",
      "train loss:   0.688579\n",
      "train loss:   0.642248\n",
      "train loss:   1.030818\n",
      "train loss:   0.801843\n",
      "train loss:   0.840889\n",
      "train loss:   0.653024\n",
      "train loss:   0.833004\n",
      "train loss:   0.939720\n",
      "train loss:   1.005447\n",
      "train loss:   0.959365\n",
      "train loss:   0.782496\n",
      "train loss:   0.940954\n",
      "train loss:   0.905987\n",
      "train loss:   0.734755\n",
      "train loss:   1.013114\n",
      "train loss:   1.161648\n",
      "train loss:   1.398685\n",
      "train loss:   0.971232\n",
      "train loss:   0.764150\n",
      "train loss:   0.744780\n",
      "train loss:   0.823815\n",
      "train loss:   1.061880\n",
      "train loss:   0.970342\n",
      "train loss:   0.534549\n",
      "train loss:   0.957382\n",
      "train loss:   1.050585\n",
      "train loss:   0.801221\n",
      "train loss:   1.035418\n",
      "train loss:   0.662613\n",
      "train loss:   1.161472\n",
      "train loss:   1.086017\n",
      "train loss:   1.277212\n",
      "train loss:   0.995020\n",
      "train loss:   1.121159\n",
      "train loss:   0.671356\n",
      "train loss:   0.656741\n",
      "train loss:   0.752881\n",
      "########### epoch 169 ###########\n",
      "########### loop 31750 ###########\n",
      "test loss:   0.140985   test accuracy:   0.958333\n",
      "########### loop 31750 ###########\n",
      "train loss:   1.157269\n",
      "train loss:   0.802828\n",
      "train loss:   1.001195\n",
      "train loss:   1.127531\n",
      "train loss:   1.251334\n",
      "train loss:   0.937712\n",
      "train loss:   0.970618\n",
      "train loss:   0.857283\n",
      "train loss:   0.904158\n",
      "train loss:   1.000912\n",
      "train loss:   0.734408\n",
      "train loss:   1.180716\n",
      "train loss:   0.968259\n",
      "train loss:   0.807774\n",
      "train loss:   1.446496\n",
      "train loss:   0.942935\n",
      "train loss:   0.886953\n",
      "train loss:   1.008198\n",
      "train loss:   0.885708\n",
      "train loss:   0.889744\n",
      "train loss:   0.895578\n",
      "train loss:   1.153633\n",
      "train loss:   0.859762\n",
      "train loss:   0.988336\n",
      "train loss:   0.801741\n",
      "train loss:   1.180244\n",
      "train loss:   1.090155\n",
      "train loss:   0.749544\n",
      "train loss:   0.685980\n",
      "train loss:   0.627434\n",
      "train loss:   0.671254\n",
      "train loss:   0.877454\n",
      "train loss:   0.981857\n",
      "train loss:   1.258659\n",
      "train loss:   1.097370\n",
      "train loss:   1.019973\n",
      "train loss:   0.728994\n",
      "train loss:   1.112410\n",
      "train loss:   1.138256\n",
      "train loss:   1.031390\n",
      "train loss:   0.895744\n",
      "train loss:   0.986128\n",
      "train loss:   0.955090\n",
      "train loss:   0.905641\n",
      "train loss:   0.940959\n",
      "train loss:   0.946057\n",
      "train loss:   0.789175\n",
      "train loss:   1.016071\n",
      "train loss:   1.085449\n",
      "train loss:   1.263196\n",
      "########### epoch 170 ###########\n",
      "########### loop 31800 ###########\n",
      "test loss:   0.125638   test accuracy:   1.000000\n",
      "########### loop 31800 ###########\n",
      "train loss:   0.879664\n",
      "train loss:   1.009410\n",
      "train loss:   0.874870\n",
      "train loss:   0.686736\n",
      "train loss:   0.652426\n",
      "train loss:   0.552075\n",
      "train loss:   1.216950\n",
      "train loss:   0.803087\n",
      "train loss:   0.870810\n",
      "train loss:   0.819879\n",
      "train loss:   0.856466\n",
      "train loss:   0.722691\n",
      "train loss:   0.728715\n",
      "train loss:   0.888526\n",
      "train loss:   1.042843\n",
      "train loss:   1.218161\n",
      "train loss:   1.118123\n",
      "train loss:   1.217584\n",
      "train loss:   0.961734\n",
      "train loss:   0.864021\n",
      "train loss:   1.074247\n",
      "train loss:   1.100729\n",
      "train loss:   1.104277\n",
      "train loss:   1.162230\n",
      "train loss:   0.618242\n",
      "train loss:   0.827236\n",
      "train loss:   0.889985\n",
      "train loss:   0.699867\n",
      "train loss:   1.033750\n",
      "train loss:   0.826176\n",
      "train loss:   1.047341\n",
      "train loss:   0.805962\n",
      "train loss:   1.010924\n",
      "train loss:   0.897887\n",
      "train loss:   1.398968\n",
      "train loss:   1.181956\n",
      "train loss:   0.966909\n",
      "train loss:   1.116727\n",
      "train loss:   1.162995\n",
      "train loss:   0.927473\n",
      "train loss:   1.378785\n",
      "train loss:   1.085764\n",
      "train loss:   1.350205\n",
      "train loss:   0.931616\n",
      "train loss:   0.909175\n",
      "train loss:   1.055503\n",
      "train loss:   1.217706\n",
      "train loss:   0.899533\n",
      "train loss:   0.803934\n",
      "train loss:   1.102641\n",
      "########### epoch 170 ###########\n",
      "########### loop 31850 ###########\n",
      "test loss:   0.525280   test accuracy:   0.750000\n",
      "########### loop 31850 ###########\n",
      "train loss:   1.200104\n",
      "train loss:   0.972636\n",
      "train loss:   0.887902\n",
      "train loss:   0.674305\n",
      "train loss:   1.208313\n",
      "train loss:   1.431596\n",
      "train loss:   1.014702\n",
      "train loss:   1.332848\n",
      "train loss:   0.578885\n",
      "train loss:   1.251676\n",
      "train loss:   0.809233\n",
      "train loss:   0.649739\n",
      "train loss:   1.032806\n",
      "train loss:   1.090813\n",
      "train loss:   0.892881\n",
      "train loss:   0.887205\n",
      "train loss:   0.856936\n",
      "train loss:   1.085648\n",
      "train loss:   1.009576\n",
      "train loss:   0.660236\n",
      "train loss:   0.879203\n",
      "train loss:   1.280208\n",
      "train loss:   0.767614\n",
      "train loss:   1.145964\n",
      "train loss:   0.818669\n",
      "train loss:   0.923163\n",
      "train loss:   1.031139\n",
      "train loss:   1.044540\n",
      "train loss:   1.386347\n",
      "train loss:   0.697222\n",
      "train loss:   1.003534\n",
      "train loss:   0.588639\n",
      "train loss:   0.914130\n",
      "train loss:   0.974594\n",
      "train loss:   1.144058\n",
      "train loss:   1.084652\n",
      "train loss:   1.425964\n",
      "train loss:   0.748452\n",
      "train loss:   0.740363\n",
      "train loss:   0.849040\n",
      "train loss:   0.489513\n",
      "train loss:   1.171830\n",
      "train loss:   0.851682\n",
      "train loss:   0.987503\n",
      "train loss:   0.968737\n",
      "train loss:   0.683622\n",
      "train loss:   0.886980\n",
      "train loss:   0.973623\n",
      "train loss:   1.015402\n",
      "train loss:   0.674127\n",
      "########### epoch 170 ###########\n",
      "########### loop 31900 ###########\n",
      "test loss:   0.263205   test accuracy:   0.958333\n",
      "########### loop 31900 ###########\n",
      "train loss:   1.255206\n",
      "train loss:   0.751108\n",
      "train loss:   1.243261\n",
      "train loss:   1.045718\n",
      "train loss:   0.662532\n",
      "train loss:   0.694448\n",
      "train loss:   0.668182\n",
      "train loss:   1.022120\n",
      "train loss:   1.354988\n",
      "train loss:   0.740276\n",
      "train loss:   1.020857\n",
      "train loss:   1.038236\n",
      "train loss:   1.208367\n",
      "train loss:   1.015395\n",
      "train loss:   0.950199\n",
      "train loss:   1.133872\n",
      "train loss:   0.977625\n",
      "train loss:   1.099386\n",
      "train loss:   1.064370\n",
      "train loss:   0.660173\n",
      "train loss:   0.983608\n",
      "train loss:   1.071533\n",
      "train loss:   1.092358\n",
      "train loss:   1.058332\n",
      "train loss:   0.947600\n",
      "train loss:   1.485911\n",
      "train loss:   0.979545\n",
      "train loss:   0.529212\n",
      "train loss:   1.011151\n",
      "train loss:   0.969016\n",
      "train loss:   1.072434\n",
      "train loss:   1.263201\n",
      "train loss:   1.091895\n",
      "train loss:   0.996393\n",
      "train loss:   0.772345\n",
      "train loss:   1.043876\n",
      "train loss:   1.198609\n",
      "train loss:   0.810814\n",
      "train loss:   0.826056\n",
      "train loss:   0.549989\n",
      "train loss:   1.062253\n",
      "train loss:   0.641589\n",
      "train loss:   0.791982\n",
      "train loss:   1.175731\n",
      "train loss:   1.045996\n",
      "train loss:   0.940709\n",
      "train loss:   1.160243\n",
      "train loss:   0.823763\n",
      "train loss:   0.902776\n",
      "train loss:   1.057297\n",
      "########### epoch 170 ###########\n",
      "########### loop 31950 ###########\n",
      "test loss:   0.074683   test accuracy:   1.000000\n",
      "########### loop 31950 ###########\n",
      "train loss:   0.970811\n",
      "train loss:   0.847580\n",
      "train loss:   0.927725\n",
      "train loss:   1.169722\n",
      "train loss:   1.122180\n",
      "train loss:   1.120094\n",
      "train loss:   1.120343\n",
      "train loss:   1.111571\n",
      "train loss:   0.879708\n",
      "train loss:   0.916577\n",
      "train loss:   1.314335\n",
      "train loss:   0.941500\n",
      "train loss:   0.566898\n",
      "train loss:   0.924444\n",
      "train loss:   0.843714\n",
      "train loss:   1.163274\n",
      "train loss:   0.621404\n",
      "train loss:   0.846534\n",
      "train loss:   0.937528\n",
      "train loss:   1.013430\n",
      "train loss:   1.151459\n",
      "train loss:   0.851779\n",
      "train loss:   1.098968\n",
      "train loss:   0.920726\n",
      "train loss:   0.906359\n",
      "train loss:   0.620840\n",
      "train loss:   1.078461\n",
      "train loss:   0.954241\n",
      "train loss:   0.798282\n",
      "train loss:   0.745181\n",
      "train loss:   0.702561\n",
      "train loss:   0.775911\n",
      "train loss:   0.928363\n",
      "train loss:   0.901428\n",
      "train loss:   1.200526\n",
      "train loss:   1.081234\n",
      "train loss:   1.062983\n",
      "train loss:   0.885913\n",
      "train loss:   1.039750\n",
      "train loss:   0.926598\n",
      "train loss:   1.083591\n",
      "train loss:   0.452869\n",
      "train loss:   0.959992\n",
      "train loss:   0.693695\n",
      "train loss:   1.455076\n",
      "train loss:   1.285654\n",
      "train loss:   1.235859\n",
      "train loss:   0.748052\n",
      "train loss:   1.039799\n",
      "train loss:   0.713139\n",
      "########### epoch 171 ###########\n",
      "########### loop 32000 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.085264   test accuracy:   1.000000\n",
      "########### loop 32000 ###########\n",
      "train loss:   1.248865\n",
      "train loss:   1.148532\n",
      "train loss:   0.988667\n",
      "train loss:   1.388576\n",
      "train loss:   1.220206\n",
      "train loss:   0.818404\n",
      "train loss:   1.044466\n",
      "train loss:   0.796780\n",
      "train loss:   0.669309\n",
      "train loss:   1.100917\n",
      "train loss:   1.075576\n",
      "train loss:   0.985662\n",
      "train loss:   1.231054\n",
      "train loss:   0.858804\n",
      "train loss:   0.784432\n",
      "train loss:   1.019159\n",
      "train loss:   0.668807\n",
      "train loss:   1.219748\n",
      "train loss:   1.010937\n",
      "train loss:   1.292521\n",
      "train loss:   0.967427\n",
      "train loss:   1.008099\n",
      "train loss:   0.939548\n",
      "train loss:   0.795153\n",
      "train loss:   0.768433\n",
      "train loss:   0.999467\n",
      "train loss:   1.165944\n",
      "train loss:   1.017063\n",
      "train loss:   0.829728\n",
      "train loss:   0.929855\n",
      "train loss:   1.053765\n",
      "train loss:   1.223042\n",
      "train loss:   0.966328\n",
      "train loss:   0.995161\n",
      "train loss:   0.591074\n",
      "train loss:   1.305658\n",
      "train loss:   0.853754\n",
      "train loss:   0.725680\n",
      "train loss:   1.044004\n",
      "train loss:   1.335695\n",
      "train loss:   1.107951\n",
      "train loss:   0.927539\n",
      "train loss:   0.958731\n",
      "train loss:   1.108331\n",
      "train loss:   0.738598\n",
      "train loss:   0.802331\n",
      "train loss:   1.055360\n",
      "train loss:   0.938908\n",
      "train loss:   1.239697\n",
      "train loss:   1.080183\n",
      "########### epoch 171 ###########\n",
      "########### loop 32050 ###########\n",
      "test loss:   0.101469   test accuracy:   1.000000\n",
      "########### loop 32050 ###########\n",
      "train loss:   0.870243\n",
      "train loss:   0.688367\n",
      "train loss:   0.934043\n",
      "train loss:   1.037496\n",
      "train loss:   1.133039\n",
      "train loss:   1.431336\n",
      "train loss:   0.794336\n",
      "train loss:   1.072541\n",
      "train loss:   1.172700\n",
      "train loss:   0.806850\n",
      "train loss:   0.974339\n",
      "train loss:   0.998295\n",
      "train loss:   0.896998\n",
      "train loss:   1.110751\n",
      "train loss:   0.575705\n",
      "train loss:   0.415948\n",
      "train loss:   0.950480\n",
      "train loss:   0.710608\n",
      "train loss:   0.726718\n",
      "train loss:   0.733228\n",
      "train loss:   1.218681\n",
      "train loss:   0.744564\n",
      "train loss:   1.332848\n",
      "train loss:   0.702184\n",
      "train loss:   0.998327\n",
      "train loss:   1.136632\n",
      "train loss:   0.629724\n",
      "train loss:   1.062256\n",
      "train loss:   0.774794\n",
      "train loss:   0.896173\n",
      "train loss:   1.140200\n",
      "train loss:   1.367243\n",
      "train loss:   0.812962\n",
      "train loss:   1.045801\n",
      "train loss:   0.885855\n",
      "train loss:   0.802418\n",
      "train loss:   1.107371\n",
      "train loss:   1.007541\n",
      "train loss:   0.987410\n",
      "train loss:   1.131378\n",
      "train loss:   1.026957\n",
      "train loss:   0.887510\n",
      "train loss:   1.034871\n",
      "train loss:   0.737502\n",
      "train loss:   0.789011\n",
      "train loss:   0.918590\n",
      "train loss:   0.609695\n",
      "train loss:   1.047676\n",
      "train loss:   0.908370\n",
      "train loss:   0.929402\n",
      "########### epoch 171 ###########\n",
      "########### loop 32100 ###########\n",
      "test loss:   0.074794   test accuracy:   1.000000\n",
      "########### loop 32100 ###########\n",
      "train loss:   0.751099\n",
      "train loss:   0.809873\n",
      "train loss:   0.723758\n",
      "train loss:   0.924039\n",
      "train loss:   1.278945\n",
      "train loss:   1.182918\n",
      "train loss:   0.796617\n",
      "train loss:   0.732818\n",
      "train loss:   0.924725\n",
      "train loss:   0.865490\n",
      "train loss:   1.173069\n",
      "train loss:   0.683369\n",
      "train loss:   1.172234\n",
      "train loss:   1.100577\n",
      "train loss:   0.868590\n",
      "train loss:   0.904173\n",
      "train loss:   0.843510\n",
      "train loss:   0.877004\n",
      "train loss:   1.037230\n",
      "train loss:   0.798303\n",
      "train loss:   0.972555\n",
      "train loss:   0.745262\n",
      "train loss:   0.788272\n",
      "train loss:   1.116052\n",
      "train loss:   1.310190\n",
      "train loss:   1.057789\n",
      "train loss:   0.653814\n",
      "train loss:   1.046107\n",
      "train loss:   0.587012\n",
      "train loss:   1.203162\n",
      "train loss:   0.896436\n",
      "train loss:   1.049271\n",
      "train loss:   1.205133\n",
      "train loss:   1.010585\n",
      "train loss:   1.094305\n",
      "train loss:   0.891158\n",
      "train loss:   0.750646\n",
      "train loss:   0.892084\n",
      "train loss:   1.317124\n",
      "train loss:   1.167953\n",
      "train loss:   1.130181\n",
      "train loss:   0.824208\n",
      "train loss:   0.770849\n",
      "train loss:   0.991310\n",
      "train loss:   0.822305\n",
      "train loss:   0.954410\n",
      "train loss:   0.914133\n",
      "train loss:   1.171784\n",
      "train loss:   1.266985\n",
      "train loss:   0.706239\n",
      "########### epoch 172 ###########\n",
      "########### loop 32150 ###########\n",
      "test loss:   0.287941   test accuracy:   0.958333\n",
      "########### loop 32150 ###########\n",
      "train loss:   0.841040\n",
      "train loss:   0.666136\n",
      "train loss:   1.070309\n",
      "train loss:   1.142447\n",
      "train loss:   0.963269\n",
      "train loss:   0.696536\n",
      "train loss:   1.161994\n",
      "train loss:   1.104069\n",
      "train loss:   1.046696\n",
      "train loss:   0.749040\n",
      "train loss:   1.101193\n",
      "train loss:   0.967906\n",
      "train loss:   0.815620\n",
      "train loss:   0.720735\n",
      "train loss:   1.079754\n",
      "train loss:   1.069267\n",
      "train loss:   1.030969\n",
      "train loss:   1.125368\n",
      "train loss:   1.354858\n",
      "train loss:   0.644307\n",
      "train loss:   0.678538\n",
      "train loss:   1.334568\n",
      "train loss:   0.966752\n",
      "train loss:   1.109376\n",
      "train loss:   1.350287\n",
      "train loss:   0.591200\n",
      "train loss:   0.514198\n",
      "train loss:   0.766616\n",
      "train loss:   1.306324\n",
      "train loss:   1.111566\n",
      "train loss:   0.875378\n",
      "train loss:   0.703685\n",
      "train loss:   1.027412\n",
      "train loss:   0.943960\n",
      "train loss:   0.947917\n",
      "train loss:   1.134321\n",
      "train loss:   1.097356\n",
      "train loss:   1.331045\n",
      "train loss:   0.832325\n",
      "train loss:   0.843249\n",
      "train loss:   0.914633\n",
      "train loss:   0.950347\n",
      "train loss:   1.092901\n",
      "train loss:   0.944099\n",
      "train loss:   1.202988\n",
      "train loss:   0.876227\n",
      "train loss:   0.927253\n",
      "train loss:   1.123849\n",
      "train loss:   1.182332\n",
      "train loss:   1.137836\n",
      "########### epoch 172 ###########\n",
      "########### loop 32200 ###########\n",
      "test loss:   0.283295   test accuracy:   0.916667\n",
      "########### loop 32200 ###########\n",
      "train loss:   0.776921\n",
      "train loss:   1.007464\n",
      "train loss:   0.855037\n",
      "train loss:   0.827147\n",
      "train loss:   0.492190\n",
      "train loss:   0.972867\n",
      "train loss:   0.887219\n",
      "train loss:   1.261697\n",
      "train loss:   0.859766\n",
      "train loss:   1.117852\n",
      "train loss:   1.037616\n",
      "train loss:   0.979993\n",
      "train loss:   1.433391\n",
      "train loss:   0.994138\n",
      "train loss:   0.958245\n",
      "train loss:   0.953407\n",
      "train loss:   1.071507\n",
      "train loss:   1.162043\n",
      "train loss:   0.761365\n",
      "train loss:   0.898646\n",
      "train loss:   0.920119\n",
      "train loss:   0.715304\n",
      "train loss:   0.871978\n",
      "train loss:   0.930496\n",
      "train loss:   0.880281\n",
      "train loss:   0.884538\n",
      "train loss:   0.894719\n",
      "train loss:   1.287192\n",
      "train loss:   1.311505\n",
      "train loss:   0.719980\n",
      "train loss:   0.885301\n",
      "train loss:   1.034237\n",
      "train loss:   1.248188\n",
      "train loss:   0.946854\n",
      "train loss:   1.140285\n",
      "train loss:   0.982165\n",
      "train loss:   0.696860\n",
      "train loss:   0.875697\n",
      "train loss:   1.039364\n",
      "train loss:   0.940455\n",
      "train loss:   0.768368\n",
      "train loss:   1.232232\n",
      "train loss:   0.821859\n",
      "train loss:   0.971574\n",
      "train loss:   0.820264\n",
      "train loss:   1.345516\n",
      "train loss:   1.082908\n",
      "train loss:   1.018857\n",
      "train loss:   0.908427\n",
      "train loss:   1.059275\n",
      "########### epoch 172 ###########\n",
      "########### loop 32250 ###########\n",
      "test loss:   0.162365   test accuracy:   1.000000\n",
      "########### loop 32250 ###########\n",
      "train loss:   0.887473\n",
      "train loss:   0.990522\n",
      "train loss:   1.101008\n",
      "train loss:   0.591266\n",
      "train loss:   1.277849\n",
      "train loss:   1.258402\n",
      "train loss:   0.803419\n",
      "train loss:   1.254600\n",
      "train loss:   1.073058\n",
      "train loss:   1.020605\n",
      "train loss:   1.042589\n",
      "train loss:   0.832966\n",
      "train loss:   1.269849\n",
      "train loss:   0.834906\n",
      "train loss:   0.891525\n",
      "train loss:   1.106771\n",
      "train loss:   1.074850\n",
      "train loss:   1.005165\n",
      "train loss:   1.123342\n",
      "train loss:   1.035334\n",
      "train loss:   0.875209\n",
      "train loss:   1.233965\n",
      "train loss:   1.135037\n",
      "train loss:   1.244582\n",
      "train loss:   1.278499\n",
      "train loss:   1.015465\n",
      "train loss:   0.786294\n",
      "train loss:   0.796052\n",
      "train loss:   0.906165\n",
      "train loss:   0.859062\n",
      "train loss:   0.788970\n",
      "train loss:   1.122942\n",
      "train loss:   0.979159\n",
      "train loss:   1.117002\n",
      "train loss:   0.922399\n",
      "train loss:   0.991416\n",
      "train loss:   0.724331\n",
      "train loss:   0.709171\n",
      "train loss:   1.104015\n",
      "train loss:   0.551335\n",
      "train loss:   0.926090\n",
      "train loss:   0.918046\n",
      "train loss:   1.016382\n",
      "train loss:   1.209823\n",
      "train loss:   0.724934\n",
      "train loss:   1.359576\n",
      "train loss:   0.738938\n",
      "train loss:   0.560829\n",
      "train loss:   0.533670\n",
      "train loss:   0.722986\n",
      "########### epoch 172 ###########\n",
      "########### loop 32300 ###########\n",
      "test loss:   0.222317   test accuracy:   0.916667\n",
      "########### loop 32300 ###########\n",
      "train loss:   0.719087\n",
      "train loss:   1.159058\n",
      "train loss:   0.910502\n",
      "train loss:   0.815815\n",
      "train loss:   1.405852\n",
      "train loss:   0.858823\n",
      "train loss:   0.809360\n",
      "train loss:   1.108312\n",
      "train loss:   1.025169\n",
      "train loss:   1.137132\n",
      "train loss:   1.140829\n",
      "train loss:   1.219435\n",
      "train loss:   1.082283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.957494\n",
      "train loss:   1.041370\n",
      "train loss:   0.963524\n",
      "train loss:   1.299051\n",
      "train loss:   1.101930\n",
      "train loss:   0.885758\n",
      "train loss:   1.230403\n",
      "train loss:   1.215885\n",
      "train loss:   1.260153\n",
      "train loss:   0.739898\n",
      "train loss:   1.206796\n",
      "train loss:   1.039329\n",
      "train loss:   0.866307\n",
      "train loss:   0.778911\n",
      "train loss:   0.924283\n",
      "train loss:   0.930837\n",
      "train loss:   0.767683\n",
      "train loss:   0.961654\n",
      "train loss:   0.749222\n",
      "train loss:   1.225901\n",
      "train loss:   0.914679\n",
      "train loss:   1.181352\n",
      "train loss:   0.664074\n",
      "train loss:   0.992271\n",
      "train loss:   1.219601\n",
      "train loss:   0.687643\n",
      "train loss:   1.189997\n",
      "train loss:   0.564654\n",
      "train loss:   1.150490\n",
      "train loss:   1.250312\n",
      "train loss:   1.170784\n",
      "train loss:   1.124812\n",
      "train loss:   0.880481\n",
      "train loss:   0.681124\n",
      "train loss:   1.123023\n",
      "train loss:   0.796345\n",
      "train loss:   0.700530\n",
      "########### epoch 173 ###########\n",
      "########### loop 32350 ###########\n",
      "test loss:   0.110765   test accuracy:   1.000000\n",
      "########### loop 32350 ###########\n",
      "train loss:   0.495093\n",
      "train loss:   0.939609\n",
      "train loss:   1.017550\n",
      "train loss:   0.705162\n",
      "train loss:   1.278000\n",
      "train loss:   1.123322\n",
      "train loss:   0.825182\n",
      "train loss:   0.987002\n",
      "train loss:   0.959820\n",
      "train loss:   1.125520\n",
      "train loss:   1.471997\n",
      "train loss:   0.934156\n",
      "train loss:   0.803694\n",
      "train loss:   1.170251\n",
      "train loss:   1.035864\n",
      "train loss:   0.934427\n",
      "train loss:   1.055478\n",
      "train loss:   0.797425\n",
      "train loss:   0.881547\n",
      "train loss:   0.893366\n",
      "train loss:   0.962654\n",
      "train loss:   0.816720\n",
      "train loss:   0.939026\n",
      "train loss:   1.085656\n",
      "train loss:   1.223347\n",
      "train loss:   0.851865\n",
      "train loss:   1.065855\n",
      "train loss:   0.783481\n",
      "train loss:   1.437058\n",
      "train loss:   0.911633\n",
      "train loss:   0.846613\n",
      "train loss:   1.053452\n",
      "train loss:   1.090842\n",
      "train loss:   1.277587\n",
      "train loss:   0.902309\n",
      "train loss:   1.079251\n",
      "train loss:   0.967790\n",
      "train loss:   0.730704\n",
      "train loss:   0.936372\n",
      "train loss:   1.266115\n",
      "train loss:   0.940885\n",
      "train loss:   1.144483\n",
      "train loss:   0.567575\n",
      "train loss:   0.899398\n",
      "train loss:   1.293718\n",
      "train loss:   0.678831\n",
      "train loss:   1.151760\n",
      "train loss:   0.865729\n",
      "train loss:   0.957067\n",
      "train loss:   0.957955\n",
      "########### epoch 173 ###########\n",
      "########### loop 32400 ###########\n",
      "test loss:   0.203542   test accuracy:   0.916667\n",
      "########### loop 32400 ###########\n",
      "train loss:   0.725144\n",
      "train loss:   0.721124\n",
      "train loss:   1.210581\n",
      "train loss:   0.752196\n",
      "train loss:   1.021036\n",
      "train loss:   0.803137\n",
      "train loss:   1.227994\n",
      "train loss:   0.593161\n",
      "train loss:   0.825920\n",
      "train loss:   0.960908\n",
      "train loss:   0.789178\n",
      "train loss:   0.901458\n",
      "train loss:   1.191737\n",
      "train loss:   0.806297\n",
      "train loss:   0.807809\n",
      "train loss:   1.475016\n",
      "train loss:   1.113039\n",
      "train loss:   1.069601\n",
      "train loss:   0.731012\n",
      "train loss:   0.858205\n",
      "train loss:   0.827761\n",
      "train loss:   0.801262\n",
      "train loss:   1.151909\n",
      "train loss:   0.971368\n",
      "train loss:   0.919450\n",
      "train loss:   0.621214\n",
      "train loss:   0.897471\n",
      "train loss:   0.797609\n",
      "train loss:   0.894463\n",
      "train loss:   1.254173\n",
      "train loss:   1.039983\n",
      "train loss:   1.173355\n",
      "train loss:   1.086895\n",
      "train loss:   0.756376\n",
      "train loss:   0.793622\n",
      "train loss:   0.615478\n",
      "train loss:   0.936442\n",
      "train loss:   1.193565\n",
      "train loss:   0.697987\n",
      "train loss:   1.195761\n",
      "train loss:   0.915662\n",
      "train loss:   1.004202\n",
      "train loss:   1.020525\n",
      "train loss:   0.966859\n",
      "train loss:   1.021229\n",
      "train loss:   0.917373\n",
      "train loss:   1.424771\n",
      "train loss:   0.913314\n",
      "train loss:   0.988937\n",
      "train loss:   1.063403\n",
      "########### epoch 173 ###########\n",
      "########### loop 32450 ###########\n",
      "test loss:   0.439145   test accuracy:   0.875000\n",
      "########### loop 32450 ###########\n",
      "train loss:   0.969590\n",
      "train loss:   0.822083\n",
      "train loss:   0.993547\n",
      "train loss:   1.003238\n",
      "train loss:   1.027485\n",
      "train loss:   0.818970\n",
      "train loss:   0.651267\n",
      "train loss:   0.976647\n",
      "train loss:   1.042129\n",
      "train loss:   1.203393\n",
      "train loss:   0.850037\n",
      "train loss:   0.517420\n",
      "train loss:   1.023959\n",
      "train loss:   0.993016\n",
      "train loss:   0.527610\n",
      "train loss:   0.876035\n",
      "train loss:   1.005847\n",
      "train loss:   0.867865\n",
      "train loss:   0.980770\n",
      "train loss:   0.799721\n",
      "train loss:   1.000361\n",
      "train loss:   0.720513\n",
      "train loss:   1.172905\n",
      "train loss:   0.792063\n",
      "train loss:   0.992938\n",
      "train loss:   1.291964\n",
      "train loss:   0.771569\n",
      "train loss:   1.214566\n",
      "train loss:   0.787964\n",
      "train loss:   0.860238\n",
      "train loss:   1.112088\n",
      "train loss:   0.947273\n",
      "train loss:   1.291873\n",
      "train loss:   0.928445\n",
      "train loss:   0.934016\n",
      "train loss:   0.755794\n",
      "train loss:   0.967048\n",
      "train loss:   1.044537\n",
      "train loss:   0.731752\n",
      "train loss:   1.311960\n",
      "train loss:   0.742104\n",
      "train loss:   0.700431\n",
      "train loss:   0.972598\n",
      "train loss:   1.037218\n",
      "train loss:   1.176046\n",
      "train loss:   0.886833\n",
      "train loss:   0.991026\n",
      "train loss:   0.871386\n",
      "train loss:   0.879953\n",
      "train loss:   1.226350\n",
      "########### epoch 173 ###########\n",
      "########### loop 32500 ###########\n",
      "test loss:   0.377801   test accuracy:   0.916667\n",
      "########### loop 32500 ###########\n",
      "train loss:   0.811574\n",
      "train loss:   0.682234\n",
      "train loss:   0.846845\n",
      "train loss:   1.003288\n",
      "train loss:   1.082955\n",
      "train loss:   1.079309\n",
      "train loss:   1.026764\n",
      "train loss:   0.821254\n",
      "train loss:   1.344232\n",
      "train loss:   0.984101\n",
      "train loss:   1.129222\n",
      "train loss:   0.792592\n",
      "train loss:   1.074343\n",
      "train loss:   0.616891\n",
      "train loss:   0.676354\n",
      "train loss:   0.904670\n",
      "train loss:   0.795247\n",
      "train loss:   1.410673\n",
      "train loss:   0.922802\n",
      "train loss:   1.062150\n",
      "train loss:   1.040351\n",
      "train loss:   1.242868\n",
      "train loss:   1.237398\n",
      "train loss:   0.468085\n",
      "train loss:   0.654569\n",
      "train loss:   1.112469\n",
      "train loss:   0.976753\n",
      "train loss:   0.796982\n",
      "train loss:   1.099363\n",
      "train loss:   1.295630\n",
      "train loss:   0.944001\n",
      "train loss:   1.371789\n",
      "train loss:   0.906504\n",
      "train loss:   1.359429\n",
      "train loss:   0.816269\n",
      "train loss:   0.993744\n",
      "train loss:   0.772902\n",
      "train loss:   0.645056\n",
      "train loss:   0.857663\n",
      "train loss:   0.851166\n",
      "train loss:   0.508440\n",
      "train loss:   1.063616\n",
      "train loss:   0.843049\n",
      "train loss:   0.760203\n",
      "train loss:   0.934479\n",
      "train loss:   0.613776\n",
      "train loss:   0.825661\n",
      "train loss:   1.160765\n",
      "train loss:   1.082537\n",
      "train loss:   0.929011\n",
      "########### epoch 174 ###########\n",
      "########### loop 32550 ###########\n",
      "test loss:   0.108980   test accuracy:   1.000000\n",
      "########### loop 32550 ###########\n",
      "train loss:   1.074200\n",
      "train loss:   0.987516\n",
      "train loss:   0.976792\n",
      "train loss:   1.016523\n",
      "train loss:   1.187517\n",
      "train loss:   1.195145\n",
      "train loss:   1.196020\n",
      "train loss:   0.955373\n",
      "train loss:   0.932218\n",
      "train loss:   1.065962\n",
      "train loss:   1.399356\n",
      "train loss:   1.192816\n",
      "train loss:   0.744283\n",
      "train loss:   0.628128\n",
      "train loss:   0.491762\n",
      "train loss:   0.687876\n",
      "train loss:   0.918403\n",
      "train loss:   1.204803\n",
      "train loss:   1.012496\n",
      "train loss:   1.095924\n",
      "train loss:   1.120268\n",
      "train loss:   0.886261\n",
      "train loss:   1.029654\n",
      "train loss:   1.201331\n",
      "train loss:   1.136851\n",
      "train loss:   1.003125\n",
      "train loss:   0.907426\n",
      "train loss:   0.531955\n",
      "train loss:   1.051439\n",
      "train loss:   1.096819\n",
      "train loss:   0.828435\n",
      "train loss:   0.940423\n",
      "train loss:   1.043202\n",
      "train loss:   0.802267\n",
      "train loss:   0.833659\n",
      "train loss:   0.670370\n",
      "train loss:   1.111409\n",
      "train loss:   1.323791\n",
      "train loss:   1.043329\n",
      "train loss:   0.888915\n",
      "train loss:   0.900975\n",
      "train loss:   1.092680\n",
      "train loss:   0.839415\n",
      "train loss:   1.178026\n",
      "train loss:   0.731342\n",
      "train loss:   0.928023\n",
      "train loss:   1.089409\n",
      "train loss:   0.882110\n",
      "train loss:   0.821955\n",
      "train loss:   0.765506\n",
      "########### epoch 174 ###########\n",
      "########### loop 32600 ###########\n",
      "test loss:   0.237656   test accuracy:   0.916667\n",
      "########### loop 32600 ###########\n",
      "train loss:   0.628725\n",
      "train loss:   0.781422\n",
      "train loss:   1.010379\n",
      "train loss:   0.851075\n",
      "train loss:   1.043745\n",
      "train loss:   1.051420\n",
      "train loss:   0.985884\n",
      "train loss:   0.545459\n",
      "train loss:   1.026702\n",
      "train loss:   0.693899\n",
      "train loss:   0.984649\n",
      "train loss:   1.128125\n",
      "train loss:   0.887774\n",
      "train loss:   0.861414\n",
      "train loss:   0.909677\n",
      "train loss:   1.087826\n",
      "train loss:   0.975935\n",
      "train loss:   1.264188\n",
      "train loss:   0.557277\n",
      "train loss:   0.815486\n",
      "train loss:   0.775384\n",
      "train loss:   1.209908\n",
      "train loss:   0.938893\n",
      "train loss:   1.450452\n",
      "train loss:   1.385926\n",
      "train loss:   1.224477\n",
      "train loss:   1.068048\n",
      "train loss:   1.089614\n",
      "train loss:   0.990769\n",
      "train loss:   1.349160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.183393\n",
      "train loss:   0.721409\n",
      "train loss:   0.928888\n",
      "train loss:   0.709539\n",
      "train loss:   0.744410\n",
      "train loss:   1.030204\n",
      "train loss:   1.429748\n",
      "train loss:   0.908339\n",
      "train loss:   0.596847\n",
      "train loss:   0.727649\n",
      "train loss:   1.069893\n",
      "train loss:   0.732089\n",
      "train loss:   1.007906\n",
      "train loss:   1.012961\n",
      "train loss:   0.861079\n",
      "train loss:   1.119060\n",
      "train loss:   1.102870\n",
      "train loss:   0.824308\n",
      "train loss:   1.099262\n",
      "train loss:   1.029591\n",
      "########### epoch 174 ###########\n",
      "########### loop 32650 ###########\n",
      "test loss:   0.379875   test accuracy:   0.875000\n",
      "########### loop 32650 ###########\n",
      "train loss:   0.884898\n",
      "train loss:   0.651906\n",
      "train loss:   0.801921\n",
      "train loss:   0.882532\n",
      "train loss:   0.817992\n",
      "train loss:   0.835882\n",
      "train loss:   1.326881\n",
      "train loss:   0.607954\n",
      "train loss:   0.883343\n",
      "train loss:   1.035124\n",
      "train loss:   0.608897\n",
      "train loss:   1.106130\n",
      "train loss:   1.005049\n",
      "train loss:   1.018857\n",
      "train loss:   1.247845\n",
      "train loss:   0.934536\n",
      "train loss:   1.267448\n",
      "train loss:   0.827314\n",
      "train loss:   1.359547\n",
      "train loss:   0.820491\n",
      "train loss:   1.031373\n",
      "train loss:   0.863925\n",
      "train loss:   1.132427\n",
      "train loss:   1.221318\n",
      "train loss:   0.802631\n",
      "train loss:   0.835265\n",
      "train loss:   0.820725\n",
      "train loss:   0.926297\n",
      "train loss:   1.126643\n",
      "train loss:   0.799687\n",
      "train loss:   1.166393\n",
      "train loss:   0.839895\n",
      "train loss:   1.188175\n",
      "train loss:   0.908641\n",
      "train loss:   1.242222\n",
      "train loss:   0.814075\n",
      "train loss:   0.592120\n",
      "train loss:   0.860198\n",
      "train loss:   1.144635\n",
      "train loss:   0.929313\n",
      "train loss:   1.198560\n",
      "train loss:   0.844737\n",
      "train loss:   0.985720\n",
      "train loss:   1.200896\n",
      "train loss:   0.691268\n",
      "train loss:   0.892071\n",
      "train loss:   1.135484\n",
      "train loss:   0.860528\n",
      "train loss:   1.068361\n",
      "train loss:   1.010117\n",
      "########### epoch 174 ###########\n",
      "########### loop 32700 ###########\n",
      "test loss:   0.526346   test accuracy:   0.875000\n",
      "########### loop 32700 ###########\n",
      "train loss:   1.062109\n",
      "train loss:   1.182353\n",
      "train loss:   0.833395\n",
      "train loss:   0.938059\n",
      "train loss:   0.912588\n",
      "train loss:   0.883068\n",
      "train loss:   1.048979\n",
      "train loss:   1.118532\n",
      "train loss:   0.567161\n",
      "train loss:   1.177277\n",
      "train loss:   1.363822\n",
      "train loss:   0.872870\n",
      "train loss:   1.073170\n",
      "train loss:   0.993297\n",
      "train loss:   0.965757\n",
      "train loss:   1.262939\n",
      "train loss:   0.635175\n",
      "train loss:   0.535193\n",
      "train loss:   0.932673\n",
      "train loss:   1.012353\n",
      "train loss:   1.113695\n",
      "train loss:   1.038405\n",
      "train loss:   0.855177\n",
      "train loss:   0.709289\n",
      "train loss:   0.934247\n",
      "train loss:   0.979087\n",
      "train loss:   1.197694\n",
      "train loss:   0.804719\n",
      "train loss:   1.095424\n",
      "train loss:   1.238677\n",
      "train loss:   0.865333\n",
      "train loss:   0.971757\n",
      "train loss:   1.357422\n",
      "train loss:   0.977632\n",
      "train loss:   0.995297\n",
      "train loss:   0.842750\n",
      "train loss:   1.101265\n",
      "train loss:   1.093987\n",
      "train loss:   0.930520\n",
      "train loss:   0.986090\n",
      "train loss:   0.669610\n",
      "train loss:   0.898894\n",
      "train loss:   0.608367\n",
      "train loss:   1.103343\n",
      "train loss:   0.693147\n",
      "train loss:   0.737504\n",
      "train loss:   1.015638\n",
      "train loss:   0.954455\n",
      "train loss:   1.156949\n",
      "train loss:   0.702053\n",
      "########### epoch 175 ###########\n",
      "########### loop 32750 ###########\n",
      "test loss:   0.269962   test accuracy:   0.875000\n",
      "########### loop 32750 ###########\n",
      "train loss:   0.698744\n",
      "train loss:   0.715012\n",
      "train loss:   1.123823\n",
      "train loss:   0.940583\n",
      "train loss:   0.899850\n",
      "train loss:   0.930947\n",
      "train loss:   0.904698\n",
      "train loss:   0.805973\n",
      "train loss:   1.214194\n",
      "train loss:   1.107406\n",
      "train loss:   1.067116\n",
      "train loss:   0.910055\n",
      "train loss:   1.046859\n",
      "train loss:   1.175380\n",
      "train loss:   1.093137\n",
      "train loss:   1.043562\n",
      "train loss:   0.493263\n",
      "train loss:   0.392084\n",
      "train loss:   0.927490\n",
      "train loss:   1.015582\n",
      "train loss:   0.910262\n",
      "train loss:   1.080994\n",
      "train loss:   1.110482\n",
      "train loss:   0.541796\n",
      "train loss:   1.213101\n",
      "train loss:   0.664362\n",
      "train loss:   0.852948\n",
      "train loss:   1.260524\n",
      "train loss:   1.041734\n",
      "train loss:   0.941975\n",
      "train loss:   0.985113\n",
      "train loss:   0.695096\n",
      "train loss:   0.993984\n",
      "train loss:   0.739921\n",
      "train loss:   1.168857\n",
      "train loss:   1.319788\n",
      "train loss:   0.652134\n",
      "train loss:   0.984886\n",
      "train loss:   0.953703\n",
      "train loss:   1.027609\n",
      "train loss:   0.777571\n",
      "train loss:   0.932846\n",
      "train loss:   0.665767\n",
      "train loss:   1.069263\n",
      "train loss:   0.749801\n",
      "train loss:   1.089685\n",
      "train loss:   1.213745\n",
      "train loss:   1.174462\n",
      "train loss:   0.612444\n",
      "train loss:   0.943794\n",
      "########### epoch 175 ###########\n",
      "########### loop 32800 ###########\n",
      "test loss:   0.229296   test accuracy:   0.916667\n",
      "########### loop 32800 ###########\n",
      "train loss:   0.747047\n",
      "train loss:   0.563244\n",
      "train loss:   0.898051\n",
      "train loss:   0.795188\n",
      "train loss:   0.811520\n",
      "train loss:   1.101602\n",
      "train loss:   1.128043\n",
      "train loss:   1.144364\n",
      "train loss:   1.252610\n",
      "train loss:   1.065945\n",
      "train loss:   0.804186\n",
      "train loss:   0.816611\n",
      "train loss:   0.635370\n",
      "train loss:   1.094966\n",
      "train loss:   0.643858\n",
      "train loss:   0.773750\n",
      "train loss:   0.846923\n",
      "train loss:   0.959501\n",
      "train loss:   0.680303\n",
      "train loss:   0.749823\n",
      "train loss:   0.949893\n",
      "train loss:   0.718127\n",
      "train loss:   0.733071\n",
      "train loss:   0.775065\n",
      "train loss:   0.695269\n",
      "train loss:   0.937817\n",
      "train loss:   1.180759\n",
      "train loss:   1.114928\n",
      "train loss:   0.814454\n",
      "train loss:   0.635479\n",
      "train loss:   1.207763\n",
      "train loss:   1.000010\n",
      "train loss:   1.294277\n",
      "train loss:   0.683215\n",
      "train loss:   0.997045\n",
      "train loss:   0.935556\n",
      "train loss:   0.734801\n",
      "train loss:   0.704791\n",
      "train loss:   1.127212\n",
      "train loss:   0.850581\n",
      "train loss:   0.844708\n",
      "train loss:   1.059773\n",
      "train loss:   0.625967\n",
      "train loss:   0.541176\n",
      "train loss:   0.828310\n",
      "train loss:   1.157895\n",
      "train loss:   1.146250\n",
      "train loss:   1.149733\n",
      "train loss:   1.030451\n",
      "train loss:   0.836962\n",
      "########### epoch 175 ###########\n",
      "########### loop 32850 ###########\n",
      "test loss:   0.050211   test accuracy:   1.000000\n",
      "########### loop 32850 ###########\n",
      "train loss:   0.780513\n",
      "train loss:   1.075163\n",
      "train loss:   0.961596\n",
      "train loss:   0.980014\n",
      "train loss:   1.067310\n",
      "train loss:   0.919196\n",
      "train loss:   1.170174\n",
      "train loss:   0.985865\n",
      "train loss:   0.982788\n",
      "train loss:   1.083707\n",
      "train loss:   0.901166\n",
      "train loss:   1.408761\n",
      "train loss:   0.934789\n",
      "train loss:   0.956593\n",
      "train loss:   0.653711\n",
      "train loss:   0.954601\n",
      "train loss:   1.061876\n",
      "train loss:   1.284552\n",
      "train loss:   1.183696\n",
      "train loss:   1.150267\n",
      "train loss:   1.086567\n",
      "train loss:   0.868910\n",
      "train loss:   0.996797\n",
      "train loss:   0.956050\n",
      "train loss:   0.997359\n",
      "train loss:   0.900418\n",
      "train loss:   1.004059\n",
      "train loss:   0.981560\n",
      "train loss:   1.035755\n",
      "train loss:   0.684258\n",
      "train loss:   0.889776\n",
      "train loss:   0.979867\n",
      "train loss:   1.050418\n",
      "train loss:   0.725383\n",
      "train loss:   1.318542\n",
      "train loss:   1.326803\n",
      "train loss:   1.094737\n",
      "train loss:   0.812126\n",
      "train loss:   0.815050\n",
      "train loss:   0.837443\n",
      "train loss:   1.041728\n",
      "train loss:   1.006916\n",
      "train loss:   0.641196\n",
      "train loss:   0.970423\n",
      "train loss:   0.854766\n",
      "train loss:   0.723655\n",
      "train loss:   1.116791\n",
      "train loss:   0.958744\n",
      "train loss:   0.901518\n",
      "train loss:   1.311047\n",
      "########### epoch 176 ###########\n",
      "########### loop 32900 ###########\n",
      "test loss:   0.087367   test accuracy:   1.000000\n",
      "########### loop 32900 ###########\n",
      "train loss:   1.081912\n",
      "train loss:   0.925169\n",
      "train loss:   0.700521\n",
      "train loss:   0.560441\n",
      "train loss:   0.780591\n",
      "train loss:   1.034168\n",
      "train loss:   0.786097\n",
      "train loss:   0.732226\n",
      "train loss:   0.965486\n",
      "train loss:   0.594500\n",
      "train loss:   0.980851\n",
      "train loss:   1.072586\n",
      "train loss:   1.202618\n",
      "train loss:   1.055203\n",
      "train loss:   0.700754\n",
      "train loss:   0.909630\n",
      "train loss:   0.759556\n",
      "train loss:   1.129723\n",
      "train loss:   0.711131\n",
      "train loss:   0.649520\n",
      "train loss:   1.100577\n",
      "train loss:   0.890416\n",
      "train loss:   0.571435\n",
      "train loss:   0.878655\n",
      "train loss:   0.880050\n",
      "train loss:   0.947328\n",
      "train loss:   0.908445\n",
      "train loss:   1.164782\n",
      "train loss:   1.366408\n",
      "train loss:   1.126102\n",
      "train loss:   1.063346\n",
      "train loss:   0.985376\n",
      "train loss:   1.059607\n",
      "train loss:   1.133327\n",
      "train loss:   0.860814\n",
      "train loss:   1.147542\n",
      "train loss:   1.121458\n",
      "train loss:   0.568332\n",
      "train loss:   0.551593\n",
      "train loss:   0.802692\n",
      "train loss:   0.890912\n",
      "train loss:   0.851411\n",
      "train loss:   1.053167\n",
      "train loss:   0.928904\n",
      "train loss:   0.593884\n",
      "train loss:   0.698563\n",
      "train loss:   0.740138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.054240\n",
      "train loss:   0.602975\n",
      "train loss:   0.734732\n",
      "########### epoch 176 ###########\n",
      "########### loop 32950 ###########\n",
      "test loss:   0.326677   test accuracy:   0.916667\n",
      "########### loop 32950 ###########\n",
      "train loss:   0.735392\n",
      "train loss:   1.312818\n",
      "train loss:   1.178259\n",
      "train loss:   0.881240\n",
      "train loss:   0.460573\n",
      "train loss:   0.813983\n",
      "train loss:   1.287438\n",
      "train loss:   0.635167\n",
      "train loss:   0.725966\n",
      "train loss:   0.995374\n",
      "train loss:   1.150327\n",
      "train loss:   0.992084\n",
      "train loss:   0.814293\n",
      "train loss:   0.903995\n",
      "train loss:   0.963215\n",
      "train loss:   0.905399\n",
      "train loss:   0.839447\n",
      "train loss:   1.023575\n",
      "train loss:   1.019065\n",
      "train loss:   0.761835\n",
      "train loss:   1.070557\n",
      "train loss:   1.003855\n",
      "train loss:   1.157356\n",
      "train loss:   0.863378\n",
      "train loss:   0.444851\n",
      "train loss:   1.142948\n",
      "train loss:   1.215540\n",
      "train loss:   1.037708\n",
      "train loss:   0.792986\n",
      "train loss:   0.794420\n",
      "train loss:   1.069673\n",
      "train loss:   0.796121\n",
      "train loss:   0.786280\n",
      "train loss:   0.634616\n",
      "train loss:   1.037397\n",
      "train loss:   1.043937\n",
      "train loss:   0.795920\n",
      "train loss:   0.749503\n",
      "train loss:   0.891676\n",
      "train loss:   1.102135\n",
      "train loss:   1.140669\n",
      "train loss:   0.609514\n",
      "train loss:   1.091791\n",
      "train loss:   0.883508\n",
      "train loss:   1.079228\n",
      "train loss:   0.841125\n",
      "train loss:   0.913890\n",
      "train loss:   1.007822\n",
      "train loss:   1.019682\n",
      "train loss:   0.759732\n",
      "########### epoch 176 ###########\n",
      "########### loop 33000 ###########\n",
      "test loss:   0.626263   test accuracy:   0.791667\n",
      "########### loop 33000 ###########\n",
      "train loss:   0.816485\n",
      "train loss:   0.802788\n",
      "train loss:   0.895190\n",
      "train loss:   0.671319\n",
      "train loss:   0.780872\n",
      "train loss:   0.605303\n",
      "train loss:   0.714030\n",
      "train loss:   0.943770\n",
      "train loss:   0.961987\n",
      "train loss:   1.013143\n",
      "train loss:   0.846548\n",
      "train loss:   1.059097\n",
      "train loss:   0.977054\n",
      "train loss:   0.806633\n",
      "train loss:   0.988355\n",
      "train loss:   0.744245\n",
      "train loss:   1.074568\n",
      "train loss:   1.118323\n",
      "train loss:   0.945101\n",
      "train loss:   0.789693\n",
      "train loss:   1.065759\n",
      "train loss:   1.115688\n",
      "train loss:   0.976860\n",
      "train loss:   0.971655\n",
      "train loss:   0.814281\n",
      "train loss:   0.756666\n",
      "train loss:   0.668366\n",
      "train loss:   0.933744\n",
      "train loss:   0.988909\n",
      "train loss:   0.773184\n",
      "train loss:   0.960844\n",
      "train loss:   1.398391\n",
      "train loss:   0.986184\n",
      "train loss:   1.098704\n",
      "train loss:   1.038172\n",
      "train loss:   1.167071\n",
      "train loss:   0.839918\n",
      "train loss:   1.084197\n",
      "train loss:   1.064175\n",
      "train loss:   0.971542\n",
      "train loss:   0.679132\n",
      "train loss:   0.710954\n",
      "train loss:   0.732702\n",
      "train loss:   1.004890\n",
      "train loss:   0.941079\n",
      "train loss:   0.840183\n",
      "train loss:   0.930402\n",
      "train loss:   0.633989\n",
      "train loss:   1.025573\n",
      "train loss:   1.062913\n",
      "########### epoch 176 ###########\n",
      "########### loop 33050 ###########\n",
      "test loss:   0.201770   test accuracy:   0.916667\n",
      "########### loop 33050 ###########\n",
      "train loss:   1.062754\n",
      "train loss:   0.764008\n",
      "train loss:   1.117652\n",
      "train loss:   0.721083\n",
      "train loss:   0.906344\n",
      "train loss:   1.100055\n",
      "train loss:   0.577199\n",
      "train loss:   0.853189\n",
      "train loss:   0.843305\n",
      "train loss:   0.705373\n",
      "train loss:   0.669123\n",
      "train loss:   1.012473\n",
      "train loss:   1.353146\n",
      "train loss:   0.804686\n",
      "train loss:   1.138121\n",
      "train loss:   1.494635\n",
      "train loss:   0.861156\n",
      "train loss:   0.735463\n",
      "train loss:   0.906247\n",
      "train loss:   0.792729\n",
      "train loss:   0.630400\n",
      "train loss:   0.727391\n",
      "train loss:   0.865861\n",
      "train loss:   0.711035\n",
      "train loss:   0.717569\n",
      "train loss:   1.151073\n",
      "train loss:   0.938716\n",
      "train loss:   0.913851\n",
      "train loss:   1.043384\n",
      "train loss:   0.988463\n",
      "train loss:   1.031642\n",
      "train loss:   0.955812\n",
      "train loss:   0.540430\n",
      "train loss:   1.131490\n",
      "train loss:   0.996177\n",
      "train loss:   0.869626\n",
      "train loss:   1.121222\n",
      "train loss:   1.031564\n",
      "train loss:   0.992589\n",
      "train loss:   0.967679\n",
      "train loss:   1.063849\n",
      "train loss:   1.242783\n",
      "train loss:   1.057537\n",
      "train loss:   0.891612\n",
      "train loss:   0.891405\n",
      "train loss:   1.293806\n",
      "train loss:   0.650606\n",
      "train loss:   0.902639\n",
      "train loss:   0.821764\n",
      "train loss:   0.813463\n",
      "########### epoch 177 ###########\n",
      "########### loop 33100 ###########\n",
      "test loss:   0.205731   test accuracy:   0.958333\n",
      "########### loop 33100 ###########\n",
      "train loss:   0.923682\n",
      "train loss:   1.206869\n",
      "train loss:   0.839906\n",
      "train loss:   0.833421\n",
      "train loss:   1.065490\n",
      "train loss:   0.749232\n",
      "train loss:   1.026029\n",
      "train loss:   0.816213\n",
      "train loss:   0.809465\n",
      "train loss:   0.880472\n",
      "train loss:   0.961595\n",
      "train loss:   0.987567\n",
      "train loss:   0.696467\n",
      "train loss:   0.849488\n",
      "train loss:   1.323935\n",
      "train loss:   1.310334\n",
      "train loss:   1.135656\n",
      "train loss:   0.979419\n",
      "train loss:   0.566342\n",
      "train loss:   1.101502\n",
      "train loss:   0.931939\n",
      "train loss:   0.534318\n",
      "train loss:   1.053414\n",
      "train loss:   1.154399\n",
      "train loss:   0.957549\n",
      "train loss:   0.978310\n",
      "train loss:   0.904612\n",
      "train loss:   0.891472\n",
      "train loss:   0.788812\n",
      "train loss:   0.937066\n",
      "train loss:   0.946861\n",
      "train loss:   0.817082\n",
      "train loss:   0.787625\n",
      "train loss:   1.295799\n",
      "train loss:   0.773952\n",
      "train loss:   1.032115\n",
      "train loss:   0.891830\n",
      "train loss:   1.092381\n",
      "train loss:   0.833638\n",
      "train loss:   0.916574\n",
      "train loss:   1.280560\n",
      "train loss:   0.928073\n",
      "train loss:   0.752713\n",
      "train loss:   1.111945\n",
      "train loss:   0.642539\n",
      "train loss:   0.883808\n",
      "train loss:   0.808270\n",
      "train loss:   1.163681\n",
      "train loss:   0.980009\n",
      "train loss:   0.675914\n",
      "########### epoch 177 ###########\n",
      "########### loop 33150 ###########\n",
      "test loss:   0.131128   test accuracy:   1.000000\n",
      "########### loop 33150 ###########\n",
      "train loss:   1.097709\n",
      "train loss:   0.614032\n",
      "train loss:   0.926624\n",
      "train loss:   0.990989\n",
      "train loss:   0.924951\n",
      "train loss:   1.280690\n",
      "train loss:   0.676044\n",
      "train loss:   0.870382\n",
      "train loss:   0.752285\n",
      "train loss:   1.055081\n",
      "train loss:   1.110631\n",
      "train loss:   1.101426\n",
      "train loss:   0.668436\n",
      "train loss:   0.933018\n",
      "train loss:   1.279095\n",
      "train loss:   1.095390\n",
      "train loss:   0.932863\n",
      "train loss:   1.066095\n",
      "train loss:   1.299048\n",
      "train loss:   0.828379\n",
      "train loss:   1.473073\n",
      "train loss:   1.149618\n",
      "train loss:   1.054757\n",
      "train loss:   1.199830\n",
      "train loss:   1.138093\n",
      "train loss:   0.911155\n",
      "train loss:   0.774532\n",
      "train loss:   0.888437\n",
      "train loss:   1.010561\n",
      "train loss:   1.167230\n",
      "train loss:   0.739629\n",
      "train loss:   1.111601\n",
      "train loss:   0.897540\n",
      "train loss:   0.740926\n",
      "train loss:   1.232695\n",
      "train loss:   0.933519\n",
      "train loss:   0.678741\n",
      "train loss:   0.976445\n",
      "train loss:   0.926528\n",
      "train loss:   0.994617\n",
      "train loss:   0.615548\n",
      "train loss:   0.871058\n",
      "train loss:   0.647881\n",
      "train loss:   1.063299\n",
      "train loss:   0.831641\n",
      "train loss:   0.938633\n",
      "train loss:   1.199742\n",
      "train loss:   1.218064\n",
      "train loss:   0.828954\n",
      "train loss:   1.384662\n",
      "########### epoch 177 ###########\n",
      "########### loop 33200 ###########\n",
      "test loss:   0.118025   test accuracy:   0.958333\n",
      "########### loop 33200 ###########\n",
      "train loss:   0.680010\n",
      "train loss:   1.091146\n",
      "train loss:   0.677688\n",
      "train loss:   1.114873\n",
      "train loss:   0.799718\n",
      "train loss:   0.785928\n",
      "train loss:   1.170228\n",
      "train loss:   0.900835\n",
      "train loss:   0.909164\n",
      "train loss:   1.138154\n",
      "train loss:   0.403101\n",
      "train loss:   0.922707\n",
      "train loss:   1.163010\n",
      "train loss:   1.135186\n",
      "train loss:   0.967251\n",
      "train loss:   1.290689\n",
      "train loss:   0.973423\n",
      "train loss:   1.034879\n",
      "train loss:   1.152994\n",
      "train loss:   1.125005\n",
      "train loss:   1.045135\n",
      "train loss:   1.157938\n",
      "train loss:   0.851231\n",
      "train loss:   1.073677\n",
      "train loss:   0.648097\n",
      "train loss:   1.034410\n",
      "train loss:   0.772029\n",
      "train loss:   0.943780\n",
      "train loss:   0.976317\n",
      "train loss:   0.709068\n",
      "train loss:   0.943482\n",
      "train loss:   1.069598\n",
      "train loss:   0.727621\n",
      "train loss:   0.988528\n",
      "train loss:   0.874513\n",
      "train loss:   1.154956\n",
      "train loss:   0.835028\n",
      "train loss:   0.790275\n",
      "train loss:   1.073842\n",
      "train loss:   0.936247\n",
      "train loss:   0.835320\n",
      "train loss:   1.276187\n",
      "train loss:   1.164426\n",
      "train loss:   1.268653\n",
      "train loss:   1.238434\n",
      "train loss:   1.168092\n",
      "train loss:   1.124732\n",
      "train loss:   0.875330\n",
      "train loss:   0.883613\n",
      "train loss:   0.820035\n",
      "########### epoch 177 ###########\n",
      "########### loop 33250 ###########\n",
      "test loss:   0.227742   test accuracy:   0.916667\n",
      "########### loop 33250 ###########\n",
      "train loss:   0.682829\n",
      "train loss:   0.876439\n",
      "train loss:   1.044021\n",
      "train loss:   0.728449\n",
      "train loss:   0.691992\n",
      "train loss:   1.118201\n",
      "train loss:   1.281163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.947871\n",
      "train loss:   0.943891\n",
      "train loss:   0.905172\n",
      "train loss:   0.830193\n",
      "train loss:   1.163968\n",
      "train loss:   0.849457\n",
      "train loss:   0.951223\n",
      "train loss:   0.972837\n",
      "train loss:   1.176878\n",
      "train loss:   1.139949\n",
      "train loss:   0.925071\n",
      "train loss:   0.814183\n",
      "train loss:   0.829267\n",
      "train loss:   1.317817\n",
      "train loss:   0.979784\n",
      "train loss:   0.921852\n",
      "train loss:   1.036749\n",
      "train loss:   0.931852\n",
      "train loss:   1.071004\n",
      "train loss:   1.262348\n",
      "train loss:   1.045686\n",
      "train loss:   0.819431\n",
      "train loss:   1.056662\n",
      "train loss:   0.933426\n",
      "train loss:   0.680944\n",
      "train loss:   0.830824\n",
      "train loss:   0.638522\n",
      "train loss:   1.211861\n",
      "train loss:   0.845162\n",
      "train loss:   1.358004\n",
      "train loss:   0.639077\n",
      "train loss:   0.922987\n",
      "train loss:   0.859079\n",
      "train loss:   0.853586\n",
      "train loss:   0.696517\n",
      "train loss:   1.030267\n",
      "train loss:   1.036274\n",
      "train loss:   0.703026\n",
      "train loss:   0.677323\n",
      "train loss:   0.942135\n",
      "train loss:   1.146968\n",
      "train loss:   1.112540\n",
      "train loss:   0.731157\n",
      "########### epoch 178 ###########\n",
      "########### loop 33300 ###########\n",
      "test loss:   0.203596   test accuracy:   0.958333\n",
      "########### loop 33300 ###########\n",
      "train loss:   0.979203\n",
      "train loss:   0.805727\n",
      "train loss:   1.336573\n",
      "train loss:   1.076421\n",
      "train loss:   0.750301\n",
      "train loss:   0.887647\n",
      "train loss:   0.955368\n",
      "train loss:   1.054449\n",
      "train loss:   0.783234\n",
      "train loss:   1.054498\n",
      "train loss:   0.614289\n",
      "train loss:   1.206180\n",
      "train loss:   0.942855\n",
      "train loss:   1.140661\n",
      "train loss:   1.171654\n",
      "train loss:   0.705878\n",
      "train loss:   0.998567\n",
      "train loss:   0.894465\n",
      "train loss:   0.804285\n",
      "train loss:   0.507388\n",
      "train loss:   1.107939\n",
      "train loss:   1.108052\n",
      "train loss:   1.020683\n",
      "train loss:   0.775132\n",
      "train loss:   0.842702\n",
      "train loss:   0.638583\n",
      "train loss:   1.002275\n",
      "train loss:   0.762672\n",
      "train loss:   0.990514\n",
      "train loss:   1.077949\n",
      "train loss:   0.791420\n",
      "train loss:   0.842371\n",
      "train loss:   1.434049\n",
      "train loss:   0.929899\n",
      "train loss:   0.680582\n",
      "train loss:   0.827336\n",
      "train loss:   0.874377\n",
      "train loss:   0.852253\n",
      "train loss:   0.863633\n",
      "train loss:   0.887664\n",
      "train loss:   0.715490\n",
      "train loss:   0.676541\n",
      "train loss:   1.004644\n",
      "train loss:   0.999226\n",
      "train loss:   0.995658\n",
      "train loss:   0.860269\n",
      "train loss:   1.025619\n",
      "train loss:   1.088611\n",
      "train loss:   1.195124\n",
      "train loss:   0.916756\n",
      "########### epoch 178 ###########\n",
      "########### loop 33350 ###########\n",
      "test loss:   0.147091   test accuracy:   0.958333\n",
      "########### loop 33350 ###########\n",
      "train loss:   0.902124\n",
      "train loss:   0.631107\n",
      "train loss:   1.050046\n",
      "train loss:   0.820250\n",
      "train loss:   1.014734\n",
      "train loss:   1.179436\n",
      "train loss:   0.840962\n",
      "train loss:   1.240734\n",
      "train loss:   1.185708\n",
      "train loss:   0.834588\n",
      "train loss:   0.307877\n",
      "train loss:   0.892383\n",
      "train loss:   0.748676\n",
      "train loss:   1.090713\n",
      "train loss:   0.945473\n",
      "train loss:   1.072446\n",
      "train loss:   0.980649\n",
      "train loss:   0.954569\n",
      "train loss:   0.972167\n",
      "train loss:   0.807596\n",
      "train loss:   1.032434\n",
      "train loss:   1.111685\n",
      "train loss:   0.829106\n",
      "train loss:   1.020214\n",
      "train loss:   0.874094\n",
      "train loss:   1.036597\n",
      "train loss:   1.208498\n",
      "train loss:   0.866227\n",
      "train loss:   1.279684\n",
      "train loss:   1.333896\n",
      "train loss:   1.100231\n",
      "train loss:   0.888524\n",
      "train loss:   0.925707\n",
      "train loss:   0.984064\n",
      "train loss:   1.090259\n",
      "train loss:   0.766924\n",
      "train loss:   1.080200\n",
      "train loss:   0.742310\n",
      "train loss:   0.970192\n",
      "train loss:   0.661891\n",
      "train loss:   0.592997\n",
      "train loss:   1.253764\n",
      "train loss:   1.249120\n",
      "train loss:   1.295493\n",
      "train loss:   1.081666\n",
      "train loss:   0.706049\n",
      "train loss:   1.032199\n",
      "train loss:   0.911019\n",
      "train loss:   0.869329\n",
      "train loss:   1.361389\n",
      "########### epoch 178 ###########\n",
      "########### loop 33400 ###########\n",
      "test loss:   0.188842   test accuracy:   0.958333\n",
      "########### loop 33400 ###########\n",
      "train loss:   1.192673\n",
      "train loss:   0.968720\n",
      "train loss:   1.067914\n",
      "train loss:   1.043195\n",
      "train loss:   0.927008\n",
      "train loss:   1.047111\n",
      "train loss:   1.013548\n",
      "train loss:   1.042571\n",
      "train loss:   0.549388\n",
      "train loss:   1.065583\n",
      "train loss:   1.186934\n",
      "train loss:   1.022387\n",
      "train loss:   0.791403\n",
      "train loss:   0.976690\n",
      "train loss:   1.132488\n",
      "train loss:   0.705657\n",
      "train loss:   0.937502\n",
      "train loss:   0.935927\n",
      "train loss:   1.225830\n",
      "train loss:   1.187666\n",
      "train loss:   0.934494\n",
      "train loss:   0.653346\n",
      "train loss:   1.223954\n",
      "train loss:   0.917381\n",
      "train loss:   0.867563\n",
      "train loss:   0.806357\n",
      "train loss:   0.493044\n",
      "train loss:   1.239972\n",
      "train loss:   1.030331\n",
      "train loss:   1.004719\n",
      "train loss:   0.897230\n",
      "train loss:   1.180358\n",
      "train loss:   1.288334\n",
      "train loss:   1.136468\n",
      "train loss:   1.172526\n",
      "train loss:   0.875806\n",
      "train loss:   0.930858\n",
      "train loss:   0.656365\n",
      "train loss:   0.719192\n",
      "train loss:   1.041473\n",
      "train loss:   0.886553\n",
      "train loss:   0.962170\n",
      "train loss:   0.923120\n",
      "train loss:   0.819873\n",
      "train loss:   1.025211\n",
      "train loss:   0.956265\n",
      "train loss:   0.713044\n",
      "train loss:   1.328108\n",
      "train loss:   1.075645\n",
      "train loss:   1.116197\n",
      "########### epoch 178 ###########\n",
      "########### loop 33450 ###########\n",
      "test loss:   0.202575   test accuracy:   0.916667\n",
      "########### loop 33450 ###########\n",
      "train loss:   0.949794\n",
      "train loss:   1.207649\n",
      "train loss:   0.721688\n",
      "train loss:   0.610717\n",
      "train loss:   0.878589\n",
      "train loss:   0.816418\n",
      "train loss:   1.029442\n",
      "train loss:   0.767098\n",
      "train loss:   1.331069\n",
      "train loss:   0.945266\n",
      "train loss:   1.257679\n",
      "train loss:   0.903781\n",
      "train loss:   1.019047\n",
      "train loss:   1.140932\n",
      "train loss:   1.239146\n",
      "train loss:   1.106786\n",
      "train loss:   1.025168\n",
      "train loss:   1.029739\n",
      "train loss:   0.935457\n",
      "train loss:   1.038161\n",
      "train loss:   1.339700\n",
      "train loss:   0.945533\n",
      "train loss:   1.207164\n",
      "train loss:   0.986766\n",
      "train loss:   0.997066\n",
      "train loss:   0.976184\n",
      "train loss:   1.133643\n",
      "train loss:   1.036565\n",
      "train loss:   0.978286\n",
      "train loss:   0.934315\n",
      "train loss:   1.122142\n",
      "train loss:   0.939613\n",
      "train loss:   0.651792\n",
      "train loss:   1.027467\n",
      "train loss:   0.894658\n",
      "train loss:   0.741818\n",
      "train loss:   1.171819\n",
      "train loss:   0.892352\n",
      "train loss:   0.833492\n",
      "train loss:   0.970117\n",
      "train loss:   1.279893\n",
      "train loss:   0.962436\n",
      "train loss:   0.897004\n",
      "train loss:   1.149012\n",
      "train loss:   0.890019\n",
      "train loss:   1.117840\n",
      "train loss:   1.205767\n",
      "train loss:   0.937636\n",
      "train loss:   0.975585\n",
      "train loss:   1.134084\n",
      "########### epoch 179 ###########\n",
      "########### loop 33500 ###########\n",
      "test loss:   0.263054   test accuracy:   0.875000\n",
      "########### loop 33500 ###########\n",
      "train loss:   0.895800\n",
      "train loss:   1.393144\n",
      "train loss:   1.173064\n",
      "train loss:   0.973142\n",
      "train loss:   1.038165\n",
      "train loss:   0.986113\n",
      "train loss:   1.106609\n",
      "train loss:   0.913471\n",
      "train loss:   0.540683\n",
      "train loss:   1.128075\n",
      "train loss:   1.290647\n",
      "train loss:   0.744365\n",
      "train loss:   0.796892\n",
      "train loss:   0.837356\n",
      "train loss:   1.003896\n",
      "train loss:   0.782141\n",
      "train loss:   0.844562\n",
      "train loss:   0.912968\n",
      "train loss:   1.032804\n",
      "train loss:   0.918469\n",
      "train loss:   0.938967\n",
      "train loss:   0.661200\n",
      "train loss:   1.082307\n",
      "train loss:   0.827690\n",
      "train loss:   0.954694\n",
      "train loss:   0.608664\n",
      "train loss:   1.230643\n",
      "train loss:   1.487392\n",
      "train loss:   1.399467\n",
      "train loss:   1.181039\n",
      "train loss:   0.960252\n",
      "train loss:   1.048767\n",
      "train loss:   1.152679\n",
      "train loss:   0.974955\n",
      "train loss:   1.178646\n",
      "train loss:   0.935726\n",
      "train loss:   0.854719\n",
      "train loss:   1.037378\n",
      "train loss:   1.028346\n",
      "train loss:   0.708487\n",
      "train loss:   1.032551\n",
      "train loss:   1.012471\n",
      "train loss:   0.793373\n",
      "train loss:   0.875399\n",
      "train loss:   1.092583\n",
      "train loss:   1.179031\n",
      "train loss:   0.738511\n",
      "train loss:   1.368994\n",
      "train loss:   1.248266\n",
      "train loss:   0.881750\n",
      "########### epoch 179 ###########\n",
      "########### loop 33550 ###########\n",
      "test loss:   0.241223   test accuracy:   0.916667\n",
      "########### loop 33550 ###########\n",
      "train loss:   0.626836\n",
      "train loss:   1.025301\n",
      "train loss:   0.868271\n",
      "train loss:   0.910338\n",
      "train loss:   0.865582\n",
      "train loss:   0.923449\n",
      "train loss:   1.153250\n",
      "train loss:   1.132428\n",
      "train loss:   0.977704\n",
      "train loss:   0.971852\n",
      "train loss:   0.919143\n",
      "train loss:   0.915089\n",
      "train loss:   1.109524\n",
      "train loss:   1.118642\n",
      "train loss:   0.731696\n",
      "train loss:   1.013368\n",
      "train loss:   0.674942\n",
      "train loss:   1.066048\n",
      "train loss:   1.025256\n",
      "train loss:   0.843137\n",
      "train loss:   0.937968\n",
      "train loss:   0.933035\n",
      "train loss:   1.310390\n",
      "train loss:   0.911725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.236761\n",
      "train loss:   0.556827\n",
      "train loss:   0.767382\n",
      "train loss:   0.986919\n",
      "train loss:   0.909545\n",
      "train loss:   0.924100\n",
      "train loss:   0.961254\n",
      "train loss:   0.866693\n",
      "train loss:   0.995943\n",
      "train loss:   0.927453\n",
      "train loss:   0.852706\n",
      "train loss:   1.149856\n",
      "train loss:   1.051750\n",
      "train loss:   0.814597\n",
      "train loss:   0.865359\n",
      "train loss:   1.339767\n",
      "train loss:   0.552844\n",
      "train loss:   0.935081\n",
      "train loss:   0.861166\n",
      "train loss:   0.854571\n",
      "train loss:   0.854457\n",
      "train loss:   0.845933\n",
      "train loss:   1.161885\n",
      "train loss:   1.089561\n",
      "train loss:   1.081453\n",
      "train loss:   0.500226\n",
      "########### epoch 179 ###########\n",
      "########### loop 33600 ###########\n",
      "test loss:   0.358139   test accuracy:   0.916667\n",
      "########### loop 33600 ###########\n",
      "train loss:   0.915133\n",
      "train loss:   0.975831\n",
      "train loss:   0.759187\n",
      "train loss:   1.024262\n",
      "train loss:   0.898904\n",
      "train loss:   1.032634\n",
      "train loss:   1.061662\n",
      "train loss:   1.129637\n",
      "train loss:   1.194583\n",
      "train loss:   0.793128\n",
      "train loss:   0.857751\n",
      "train loss:   0.864428\n",
      "train loss:   1.177078\n",
      "train loss:   1.150884\n",
      "train loss:   0.961945\n",
      "train loss:   0.974206\n",
      "train loss:   1.053785\n",
      "train loss:   0.880802\n",
      "train loss:   1.062998\n",
      "train loss:   1.007666\n",
      "train loss:   1.044447\n",
      "train loss:   1.339746\n",
      "train loss:   0.994887\n",
      "train loss:   1.480958\n",
      "train loss:   0.803804\n",
      "train loss:   1.286270\n",
      "train loss:   1.213443\n",
      "train loss:   0.845508\n",
      "train loss:   0.949730\n",
      "train loss:   0.720454\n",
      "train loss:   1.045409\n",
      "train loss:   0.880667\n",
      "train loss:   1.358388\n",
      "train loss:   0.978123\n",
      "train loss:   0.766463\n",
      "train loss:   1.150012\n",
      "train loss:   0.663053\n",
      "train loss:   1.093317\n",
      "train loss:   0.935791\n",
      "train loss:   1.239727\n",
      "train loss:   0.996772\n",
      "train loss:   0.942228\n",
      "train loss:   0.771503\n",
      "train loss:   0.742118\n",
      "train loss:   1.188445\n",
      "train loss:   1.057869\n",
      "train loss:   0.587857\n",
      "train loss:   0.895074\n",
      "train loss:   0.852978\n",
      "train loss:   0.697079\n",
      "########### epoch 179 ###########\n",
      "########### loop 33650 ###########\n",
      "test loss:   0.155174   test accuracy:   0.958333\n",
      "########### loop 33650 ###########\n",
      "train loss:   0.873854\n",
      "train loss:   1.013544\n",
      "train loss:   1.300690\n",
      "train loss:   1.003042\n",
      "train loss:   0.686874\n",
      "train loss:   1.267431\n",
      "train loss:   0.925456\n",
      "train loss:   0.803718\n",
      "train loss:   1.012994\n",
      "train loss:   1.204366\n",
      "train loss:   1.167722\n",
      "train loss:   0.773745\n",
      "train loss:   1.093311\n",
      "train loss:   0.733501\n",
      "train loss:   0.908726\n",
      "train loss:   0.732484\n",
      "train loss:   0.976649\n",
      "train loss:   0.994799\n",
      "train loss:   1.266908\n",
      "train loss:   1.335251\n",
      "train loss:   1.107279\n",
      "train loss:   1.109318\n",
      "train loss:   0.938110\n",
      "train loss:   0.984219\n",
      "train loss:   1.186996\n",
      "train loss:   1.026261\n",
      "train loss:   0.929478\n",
      "train loss:   0.737787\n",
      "train loss:   0.721424\n",
      "train loss:   1.173243\n",
      "train loss:   1.050582\n",
      "train loss:   1.589810\n",
      "train loss:   1.092361\n",
      "train loss:   1.046419\n",
      "train loss:   0.968436\n",
      "train loss:   0.434172\n",
      "train loss:   1.189298\n",
      "train loss:   0.835897\n",
      "train loss:   0.567847\n",
      "train loss:   0.745454\n",
      "train loss:   1.072792\n",
      "train loss:   0.768759\n",
      "train loss:   0.447950\n",
      "train loss:   0.815431\n",
      "train loss:   1.044189\n",
      "train loss:   0.892926\n",
      "train loss:   0.910344\n",
      "train loss:   0.796992\n",
      "train loss:   0.994866\n",
      "train loss:   0.938491\n",
      "########### epoch 180 ###########\n",
      "########### loop 33700 ###########\n",
      "test loss:   0.147599   test accuracy:   0.958333\n",
      "########### loop 33700 ###########\n",
      "train loss:   1.270073\n",
      "train loss:   1.248296\n",
      "train loss:   0.740603\n",
      "train loss:   0.585607\n",
      "train loss:   1.283977\n",
      "train loss:   0.802591\n",
      "train loss:   0.867136\n",
      "train loss:   0.649002\n",
      "train loss:   0.627536\n",
      "train loss:   0.733226\n",
      "train loss:   1.133529\n",
      "train loss:   0.795397\n",
      "train loss:   1.053327\n",
      "train loss:   1.077777\n",
      "train loss:   1.253455\n",
      "train loss:   0.628347\n",
      "train loss:   0.878689\n",
      "train loss:   0.968402\n",
      "train loss:   0.922404\n",
      "train loss:   0.818226\n",
      "train loss:   0.738614\n",
      "train loss:   0.943460\n",
      "train loss:   1.157938\n",
      "train loss:   0.627410\n",
      "train loss:   1.068628\n",
      "train loss:   1.114086\n",
      "train loss:   0.983285\n",
      "train loss:   0.889951\n",
      "train loss:   1.152739\n",
      "train loss:   0.549314\n",
      "train loss:   1.066531\n",
      "train loss:   0.997597\n",
      "train loss:   1.146950\n",
      "train loss:   1.083999\n",
      "train loss:   1.132188\n",
      "train loss:   1.350774\n",
      "train loss:   1.086039\n",
      "train loss:   0.616612\n",
      "train loss:   0.819726\n",
      "train loss:   0.960234\n",
      "train loss:   1.033008\n",
      "train loss:   1.118752\n",
      "train loss:   1.183963\n",
      "train loss:   1.225678\n",
      "train loss:   0.851321\n",
      "train loss:   1.260972\n",
      "train loss:   1.402458\n",
      "train loss:   1.150952\n",
      "train loss:   0.784165\n",
      "train loss:   1.231273\n",
      "########### epoch 180 ###########\n",
      "########### loop 33750 ###########\n",
      "test loss:   0.122904   test accuracy:   1.000000\n",
      "########### loop 33750 ###########\n",
      "train loss:   0.861081\n",
      "train loss:   1.034244\n",
      "train loss:   0.863658\n",
      "train loss:   0.840346\n",
      "train loss:   0.986362\n",
      "train loss:   0.913332\n",
      "train loss:   0.990999\n",
      "train loss:   0.741922\n",
      "train loss:   1.071176\n",
      "train loss:   1.127552\n",
      "train loss:   0.933636\n",
      "train loss:   0.825743\n",
      "train loss:   1.008752\n",
      "train loss:   0.764565\n",
      "train loss:   0.789873\n",
      "train loss:   0.901133\n",
      "train loss:   1.262606\n",
      "train loss:   0.791896\n",
      "train loss:   1.151197\n",
      "train loss:   0.807229\n",
      "train loss:   0.994117\n",
      "train loss:   0.762325\n",
      "train loss:   1.027315\n",
      "train loss:   1.335385\n",
      "train loss:   0.904244\n",
      "train loss:   1.061950\n",
      "train loss:   0.796783\n",
      "train loss:   0.946954\n",
      "train loss:   0.900422\n",
      "train loss:   1.027965\n",
      "train loss:   1.149070\n",
      "train loss:   1.140406\n",
      "train loss:   1.131266\n",
      "train loss:   0.656447\n",
      "train loss:   1.084796\n",
      "train loss:   0.932187\n",
      "train loss:   0.782323\n",
      "train loss:   0.937841\n",
      "train loss:   0.949811\n",
      "train loss:   1.092863\n",
      "train loss:   1.152625\n",
      "train loss:   0.864585\n",
      "train loss:   0.794689\n",
      "train loss:   1.059730\n",
      "train loss:   0.964846\n",
      "train loss:   0.844033\n",
      "train loss:   0.869318\n",
      "train loss:   0.772453\n",
      "train loss:   1.163820\n",
      "train loss:   1.292099\n",
      "########### epoch 180 ###########\n",
      "########### loop 33800 ###########\n",
      "test loss:   0.332502   test accuracy:   0.916667\n",
      "########### loop 33800 ###########\n",
      "train loss:   0.901481\n",
      "train loss:   0.717903\n",
      "train loss:   0.782548\n",
      "train loss:   1.241724\n",
      "train loss:   1.096597\n",
      "train loss:   0.765176\n",
      "train loss:   0.910547\n",
      "train loss:   1.146223\n",
      "train loss:   0.731554\n",
      "train loss:   1.075925\n",
      "train loss:   1.109111\n",
      "train loss:   0.941646\n",
      "train loss:   0.668218\n",
      "train loss:   0.853372\n",
      "train loss:   1.084965\n",
      "train loss:   0.889317\n",
      "train loss:   1.188654\n",
      "train loss:   0.806236\n",
      "train loss:   0.592154\n",
      "train loss:   0.796628\n",
      "train loss:   0.767454\n",
      "train loss:   0.807509\n",
      "train loss:   0.913374\n",
      "train loss:   0.976182\n",
      "train loss:   1.057475\n",
      "train loss:   1.181885\n",
      "train loss:   0.826089\n",
      "train loss:   0.734430\n",
      "train loss:   1.139106\n",
      "train loss:   0.435299\n",
      "train loss:   1.140694\n",
      "train loss:   1.023711\n",
      "train loss:   0.821304\n",
      "train loss:   1.136738\n",
      "train loss:   0.623439\n",
      "train loss:   0.942460\n",
      "train loss:   0.775186\n",
      "train loss:   1.393097\n",
      "train loss:   0.996119\n",
      "train loss:   0.908648\n",
      "train loss:   1.150906\n",
      "train loss:   0.999153\n",
      "train loss:   1.106868\n",
      "train loss:   0.513535\n",
      "train loss:   1.191472\n",
      "train loss:   0.985538\n",
      "train loss:   0.838501\n",
      "train loss:   0.980043\n",
      "train loss:   0.851104\n",
      "train loss:   1.034149\n",
      "########### epoch 181 ###########\n",
      "########### loop 33850 ###########\n",
      "test loss:   0.258313   test accuracy:   0.916667\n",
      "########### loop 33850 ###########\n",
      "train loss:   0.904734\n",
      "train loss:   0.826258\n",
      "train loss:   0.807801\n",
      "train loss:   1.029305\n",
      "train loss:   0.885088\n",
      "train loss:   0.948831\n",
      "train loss:   1.210320\n",
      "train loss:   0.736023\n",
      "train loss:   1.109239\n",
      "train loss:   0.933340\n",
      "train loss:   1.087516\n",
      "train loss:   0.967288\n",
      "train loss:   0.963142\n",
      "train loss:   0.674468\n",
      "train loss:   0.976455\n",
      "train loss:   0.967757\n",
      "train loss:   0.879004\n",
      "train loss:   0.877467\n",
      "train loss:   1.009934\n",
      "train loss:   1.226050\n",
      "train loss:   0.978252\n",
      "train loss:   0.555015\n",
      "train loss:   0.660217\n",
      "train loss:   0.735963\n",
      "train loss:   1.248407\n",
      "train loss:   1.163269\n",
      "train loss:   1.234236\n",
      "train loss:   1.102991\n",
      "train loss:   1.072491\n",
      "train loss:   0.647348\n",
      "train loss:   1.102614\n",
      "train loss:   0.625167\n",
      "train loss:   1.215393\n",
      "train loss:   1.049583\n",
      "train loss:   1.305786\n",
      "train loss:   0.861472\n",
      "train loss:   0.995706\n",
      "train loss:   0.885683\n",
      "train loss:   1.083679\n",
      "train loss:   0.952847\n",
      "train loss:   1.144929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.891050\n",
      "train loss:   0.673131\n",
      "train loss:   0.930079\n",
      "train loss:   0.574910\n",
      "train loss:   0.999188\n",
      "train loss:   0.895728\n",
      "train loss:   1.120524\n",
      "train loss:   0.805036\n",
      "train loss:   0.949088\n",
      "########### epoch 181 ###########\n",
      "########### loop 33900 ###########\n",
      "test loss:   0.140722   test accuracy:   0.958333\n",
      "########### loop 33900 ###########\n",
      "train loss:   0.681900\n",
      "train loss:   0.638959\n",
      "train loss:   1.193594\n",
      "train loss:   1.090382\n",
      "train loss:   0.954938\n",
      "train loss:   1.041887\n",
      "train loss:   1.104846\n",
      "train loss:   0.927000\n",
      "train loss:   0.850880\n",
      "train loss:   1.157963\n",
      "train loss:   1.235327\n",
      "train loss:   1.221977\n",
      "train loss:   0.893177\n",
      "train loss:   0.814586\n",
      "train loss:   1.393418\n",
      "train loss:   1.001075\n",
      "train loss:   1.385451\n",
      "train loss:   0.751920\n",
      "train loss:   0.705490\n",
      "train loss:   1.103966\n",
      "train loss:   0.729054\n",
      "train loss:   0.808042\n",
      "train loss:   1.085233\n",
      "train loss:   1.023375\n",
      "train loss:   1.055782\n",
      "train loss:   1.093085\n",
      "train loss:   0.900074\n",
      "train loss:   0.950376\n",
      "train loss:   0.991031\n",
      "train loss:   1.012857\n",
      "train loss:   1.153924\n",
      "train loss:   1.053256\n",
      "train loss:   1.014361\n",
      "train loss:   0.974501\n",
      "train loss:   0.724073\n",
      "train loss:   0.947617\n",
      "train loss:   0.866586\n",
      "train loss:   0.901457\n",
      "train loss:   0.999939\n",
      "train loss:   0.870822\n",
      "train loss:   1.167172\n",
      "train loss:   1.047765\n",
      "train loss:   1.409330\n",
      "train loss:   0.958952\n",
      "train loss:   0.554471\n",
      "train loss:   1.042602\n",
      "train loss:   1.170968\n",
      "train loss:   0.656453\n",
      "train loss:   0.840616\n",
      "train loss:   0.888050\n",
      "########### epoch 181 ###########\n",
      "########### loop 33950 ###########\n",
      "test loss:   0.192227   test accuracy:   0.916667\n",
      "########### loop 33950 ###########\n",
      "train loss:   0.671710\n",
      "train loss:   0.855793\n",
      "train loss:   1.192180\n",
      "train loss:   0.983389\n",
      "train loss:   1.336790\n",
      "train loss:   0.792080\n",
      "train loss:   0.466579\n",
      "train loss:   0.632331\n",
      "train loss:   0.746289\n",
      "train loss:   1.102243\n",
      "train loss:   1.148492\n",
      "train loss:   1.047877\n",
      "train loss:   1.190136\n",
      "train loss:   1.318845\n",
      "train loss:   0.962648\n",
      "train loss:   0.980271\n",
      "train loss:   1.159019\n",
      "train loss:   0.828589\n",
      "train loss:   1.229259\n",
      "train loss:   1.004593\n",
      "train loss:   0.973629\n",
      "train loss:   1.015917\n",
      "train loss:   0.743046\n",
      "train loss:   1.089488\n",
      "train loss:   1.014974\n",
      "train loss:   0.904875\n",
      "train loss:   0.766314\n",
      "train loss:   1.133637\n",
      "train loss:   0.871510\n",
      "train loss:   1.026584\n",
      "train loss:   1.039117\n",
      "train loss:   0.608899\n",
      "train loss:   0.718736\n",
      "train loss:   0.989375\n",
      "train loss:   0.955607\n",
      "train loss:   1.053718\n",
      "train loss:   1.203389\n",
      "train loss:   0.899033\n",
      "train loss:   1.013047\n",
      "train loss:   1.300625\n",
      "train loss:   1.046165\n",
      "train loss:   1.114272\n",
      "train loss:   0.637164\n",
      "train loss:   0.910980\n",
      "train loss:   1.062646\n",
      "train loss:   0.656917\n",
      "train loss:   1.052862\n",
      "train loss:   0.636428\n",
      "train loss:   0.641011\n",
      "train loss:   0.992046\n",
      "########### epoch 181 ###########\n",
      "########### loop 34000 ###########\n",
      "test loss:   0.220169   test accuracy:   0.916667\n",
      "########### loop 34000 ###########\n",
      "train loss:   0.994720\n",
      "train loss:   0.932421\n",
      "train loss:   1.072970\n",
      "train loss:   1.069500\n",
      "train loss:   1.094235\n",
      "train loss:   0.806307\n",
      "train loss:   1.242468\n",
      "train loss:   0.625975\n",
      "train loss:   0.785552\n",
      "train loss:   1.013891\n",
      "train loss:   0.944622\n",
      "train loss:   0.800779\n",
      "train loss:   0.854858\n",
      "train loss:   0.647532\n",
      "train loss:   0.946931\n",
      "train loss:   1.080256\n",
      "train loss:   0.907254\n",
      "train loss:   1.051123\n",
      "train loss:   0.759025\n",
      "train loss:   1.130139\n",
      "train loss:   0.965576\n",
      "train loss:   1.264188\n",
      "train loss:   1.006241\n",
      "train loss:   0.867985\n",
      "train loss:   0.995989\n",
      "train loss:   1.216379\n",
      "train loss:   1.234840\n",
      "train loss:   0.681773\n",
      "train loss:   0.809881\n",
      "train loss:   1.408475\n",
      "train loss:   1.079311\n",
      "train loss:   1.047120\n",
      "train loss:   0.905914\n",
      "train loss:   0.858560\n",
      "train loss:   0.983995\n",
      "train loss:   0.842082\n",
      "train loss:   1.175363\n",
      "train loss:   0.635509\n",
      "train loss:   0.892355\n",
      "train loss:   0.738140\n",
      "train loss:   0.742523\n",
      "train loss:   1.035210\n",
      "train loss:   0.799453\n",
      "train loss:   0.747057\n",
      "train loss:   0.938767\n",
      "train loss:   0.894894\n",
      "train loss:   0.820378\n",
      "train loss:   1.025239\n",
      "train loss:   0.861657\n",
      "train loss:   0.948557\n",
      "########### epoch 182 ###########\n",
      "########### loop 34050 ###########\n",
      "test loss:   0.226862   test accuracy:   0.916667\n",
      "########### loop 34050 ###########\n",
      "train loss:   0.875850\n",
      "train loss:   0.744336\n",
      "train loss:   1.145005\n",
      "train loss:   1.234057\n",
      "train loss:   1.256191\n",
      "train loss:   1.046495\n",
      "train loss:   0.663267\n",
      "train loss:   1.122072\n",
      "train loss:   1.286410\n",
      "train loss:   0.512361\n",
      "train loss:   0.571354\n",
      "train loss:   0.714331\n",
      "train loss:   1.144912\n",
      "train loss:   0.717219\n",
      "train loss:   0.912745\n",
      "train loss:   0.925048\n",
      "train loss:   0.844264\n",
      "train loss:   0.968076\n",
      "train loss:   0.908647\n",
      "train loss:   1.024477\n",
      "train loss:   0.614315\n",
      "train loss:   0.707501\n",
      "train loss:   0.827053\n",
      "train loss:   0.903806\n",
      "train loss:   0.812119\n",
      "train loss:   1.048940\n",
      "train loss:   1.044990\n",
      "train loss:   1.344087\n",
      "train loss:   1.125158\n",
      "train loss:   0.980069\n",
      "train loss:   0.826186\n",
      "train loss:   1.300189\n",
      "train loss:   1.221019\n",
      "train loss:   0.722902\n",
      "train loss:   0.646494\n",
      "train loss:   1.217880\n",
      "train loss:   1.188916\n",
      "train loss:   0.914298\n",
      "train loss:   0.867637\n",
      "train loss:   0.779752\n",
      "train loss:   1.060236\n",
      "train loss:   0.877598\n",
      "train loss:   1.105437\n",
      "train loss:   0.710094\n",
      "train loss:   0.921972\n",
      "train loss:   1.491840\n",
      "train loss:   1.165560\n",
      "train loss:   1.250255\n",
      "train loss:   1.203025\n",
      "train loss:   0.739342\n",
      "########### epoch 182 ###########\n",
      "########### loop 34100 ###########\n",
      "test loss:   0.345183   test accuracy:   0.916667\n",
      "########### loop 34100 ###########\n",
      "train loss:   0.774184\n",
      "train loss:   0.736623\n",
      "train loss:   0.932985\n",
      "train loss:   0.936473\n",
      "train loss:   0.595617\n",
      "train loss:   0.932066\n",
      "train loss:   0.989221\n",
      "train loss:   0.667674\n",
      "train loss:   0.781658\n",
      "train loss:   0.809920\n",
      "train loss:   0.783038\n",
      "train loss:   1.254796\n",
      "train loss:   0.897721\n",
      "train loss:   1.226153\n",
      "train loss:   0.957331\n",
      "train loss:   0.918285\n",
      "train loss:   0.965730\n",
      "train loss:   1.138924\n",
      "train loss:   0.849732\n",
      "train loss:   1.029553\n",
      "train loss:   1.002588\n",
      "train loss:   0.919807\n",
      "train loss:   0.790254\n",
      "train loss:   0.712782\n",
      "train loss:   0.675802\n",
      "train loss:   0.667955\n",
      "train loss:   1.230166\n",
      "train loss:   1.115345\n",
      "train loss:   0.965258\n",
      "train loss:   0.904077\n",
      "train loss:   0.773268\n",
      "train loss:   0.921049\n",
      "train loss:   0.715098\n",
      "train loss:   1.163965\n",
      "train loss:   0.576796\n",
      "train loss:   0.994773\n",
      "train loss:   1.027039\n",
      "train loss:   0.859085\n",
      "train loss:   0.566557\n",
      "train loss:   1.105585\n",
      "train loss:   0.871537\n",
      "train loss:   0.466433\n",
      "train loss:   1.118454\n",
      "train loss:   0.732697\n",
      "train loss:   1.034623\n",
      "train loss:   0.896884\n",
      "train loss:   1.111639\n",
      "train loss:   1.289219\n",
      "train loss:   0.807346\n",
      "train loss:   0.752159\n",
      "########### epoch 182 ###########\n",
      "########### loop 34150 ###########\n",
      "test loss:   0.409060   test accuracy:   0.833333\n",
      "########### loop 34150 ###########\n",
      "train loss:   0.998381\n",
      "train loss:   0.733598\n",
      "train loss:   1.000143\n",
      "train loss:   1.172687\n",
      "train loss:   0.944062\n",
      "train loss:   0.759852\n",
      "train loss:   1.142105\n",
      "train loss:   0.790428\n",
      "train loss:   1.076080\n",
      "train loss:   0.413988\n",
      "train loss:   1.046034\n",
      "train loss:   1.195990\n",
      "train loss:   1.252991\n",
      "train loss:   1.267489\n",
      "train loss:   0.828268\n",
      "train loss:   1.054428\n",
      "train loss:   1.039570\n",
      "train loss:   1.191290\n",
      "train loss:   0.746891\n",
      "train loss:   1.124534\n",
      "train loss:   0.824773\n",
      "train loss:   1.017566\n",
      "train loss:   0.571436\n",
      "train loss:   0.877049\n",
      "train loss:   0.960046\n",
      "train loss:   0.773375\n",
      "train loss:   1.119962\n",
      "train loss:   1.264870\n",
      "train loss:   0.916743\n",
      "train loss:   1.043020\n",
      "train loss:   0.982867\n",
      "train loss:   1.011829\n",
      "train loss:   1.002082\n",
      "train loss:   0.804128\n",
      "train loss:   0.875561\n",
      "train loss:   0.791036\n",
      "train loss:   0.820012\n",
      "train loss:   1.093651\n",
      "train loss:   0.774767\n",
      "train loss:   0.966376\n",
      "train loss:   0.916619\n",
      "train loss:   0.737048\n",
      "train loss:   1.064370\n",
      "train loss:   0.903704\n",
      "train loss:   0.670092\n",
      "train loss:   1.149711\n",
      "train loss:   0.982400\n",
      "train loss:   0.908254\n",
      "train loss:   0.941772\n",
      "train loss:   1.234147\n",
      "########### epoch 182 ###########\n",
      "########### loop 34200 ###########\n",
      "test loss:   0.151382   test accuracy:   1.000000\n",
      "########### loop 34200 ###########\n",
      "train loss:   0.838333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.101573\n",
      "train loss:   1.011348\n",
      "train loss:   1.139426\n",
      "train loss:   1.034851\n",
      "train loss:   1.028105\n",
      "train loss:   0.884252\n",
      "train loss:   0.823085\n",
      "train loss:   0.866731\n",
      "train loss:   0.677809\n",
      "train loss:   1.027772\n",
      "train loss:   0.705348\n",
      "train loss:   1.199598\n",
      "train loss:   0.459667\n",
      "train loss:   0.811165\n",
      "train loss:   0.896209\n",
      "train loss:   0.812744\n",
      "train loss:   1.169121\n",
      "train loss:   1.139364\n",
      "train loss:   0.603748\n",
      "train loss:   1.058374\n",
      "train loss:   1.329854\n",
      "train loss:   0.502803\n",
      "train loss:   0.870671\n",
      "train loss:   1.017125\n",
      "train loss:   0.905185\n",
      "train loss:   0.937432\n",
      "train loss:   1.201499\n",
      "train loss:   1.202908\n",
      "train loss:   0.990066\n",
      "train loss:   1.038051\n",
      "train loss:   0.423889\n",
      "train loss:   0.781134\n",
      "train loss:   1.125111\n",
      "train loss:   0.970979\n",
      "train loss:   1.024697\n",
      "train loss:   1.073654\n",
      "train loss:   0.778963\n",
      "train loss:   1.308651\n",
      "train loss:   0.921554\n",
      "train loss:   0.634512\n",
      "train loss:   0.679092\n",
      "train loss:   1.238159\n",
      "train loss:   0.626479\n",
      "train loss:   0.444020\n",
      "train loss:   1.070771\n",
      "train loss:   0.425473\n",
      "train loss:   0.958230\n",
      "train loss:   1.092412\n",
      "train loss:   0.459636\n",
      "########### epoch 183 ###########\n",
      "########### loop 34250 ###########\n",
      "test loss:   0.332602   test accuracy:   0.875000\n",
      "########### loop 34250 ###########\n",
      "train loss:   0.988255\n",
      "train loss:   0.732785\n",
      "train loss:   0.796490\n",
      "train loss:   0.674647\n",
      "train loss:   0.536816\n",
      "train loss:   0.966251\n",
      "train loss:   0.980426\n",
      "train loss:   1.106382\n",
      "train loss:   1.013990\n",
      "train loss:   1.140810\n",
      "train loss:   1.031813\n",
      "train loss:   1.358074\n",
      "train loss:   0.653875\n",
      "train loss:   0.853576\n",
      "train loss:   1.386102\n",
      "train loss:   0.666283\n",
      "train loss:   0.928999\n",
      "train loss:   0.991293\n",
      "train loss:   0.857540\n",
      "train loss:   0.652997\n",
      "train loss:   1.346732\n",
      "train loss:   0.908116\n",
      "train loss:   1.185019\n",
      "train loss:   1.324900\n",
      "train loss:   0.721892\n",
      "train loss:   0.948381\n",
      "train loss:   0.929467\n",
      "train loss:   1.011972\n",
      "train loss:   1.300438\n",
      "train loss:   1.007912\n",
      "train loss:   0.866218\n",
      "train loss:   1.109054\n",
      "train loss:   1.212227\n",
      "train loss:   1.011669\n",
      "train loss:   0.900107\n",
      "train loss:   0.944704\n",
      "train loss:   1.056003\n",
      "train loss:   0.754856\n",
      "train loss:   0.922131\n",
      "train loss:   0.812232\n",
      "train loss:   1.072322\n",
      "train loss:   0.888942\n",
      "train loss:   0.539077\n",
      "train loss:   1.096450\n",
      "train loss:   1.196199\n",
      "train loss:   0.597355\n",
      "train loss:   1.235952\n",
      "train loss:   0.865075\n",
      "train loss:   1.034491\n",
      "train loss:   0.951355\n",
      "########### epoch 183 ###########\n",
      "########### loop 34300 ###########\n",
      "test loss:   0.190165   test accuracy:   0.958333\n",
      "########### loop 34300 ###########\n",
      "train loss:   1.103925\n",
      "train loss:   1.027018\n",
      "train loss:   0.824529\n",
      "train loss:   0.656453\n",
      "train loss:   1.042593\n",
      "train loss:   0.689656\n",
      "train loss:   1.386423\n",
      "train loss:   1.094312\n",
      "train loss:   0.961410\n",
      "train loss:   1.401795\n",
      "train loss:   1.240747\n",
      "train loss:   1.220538\n",
      "train loss:   1.207495\n",
      "train loss:   0.855431\n",
      "train loss:   1.150853\n",
      "train loss:   0.867608\n",
      "train loss:   1.140224\n",
      "train loss:   0.970951\n",
      "train loss:   0.905846\n",
      "train loss:   1.003780\n",
      "train loss:   0.803446\n",
      "train loss:   1.040824\n",
      "train loss:   0.656607\n",
      "train loss:   0.949011\n",
      "train loss:   0.949569\n",
      "train loss:   0.983406\n",
      "train loss:   0.914957\n",
      "train loss:   1.080542\n",
      "train loss:   0.914171\n",
      "train loss:   1.041972\n",
      "train loss:   0.942982\n",
      "train loss:   1.075897\n",
      "train loss:   0.874698\n",
      "train loss:   0.966174\n",
      "train loss:   1.102375\n",
      "train loss:   0.966297\n",
      "train loss:   1.194427\n",
      "train loss:   0.901309\n",
      "train loss:   1.324563\n",
      "train loss:   0.821886\n",
      "train loss:   0.966702\n",
      "train loss:   0.743692\n",
      "train loss:   1.292644\n",
      "train loss:   1.143319\n",
      "train loss:   0.777764\n",
      "train loss:   1.062384\n",
      "train loss:   1.063184\n",
      "train loss:   0.801711\n",
      "train loss:   0.689525\n",
      "train loss:   1.029421\n",
      "########### epoch 183 ###########\n",
      "########### loop 34350 ###########\n",
      "test loss:   0.148311   test accuracy:   0.958333\n",
      "########### loop 34350 ###########\n",
      "train loss:   0.837577\n",
      "train loss:   0.863670\n",
      "train loss:   1.217382\n",
      "train loss:   0.652742\n",
      "train loss:   0.984278\n",
      "train loss:   0.935184\n",
      "train loss:   1.398382\n",
      "train loss:   1.136841\n",
      "train loss:   1.083749\n",
      "train loss:   1.027697\n",
      "train loss:   1.104172\n",
      "train loss:   1.104535\n",
      "train loss:   0.926120\n",
      "train loss:   0.916436\n",
      "train loss:   0.867814\n",
      "train loss:   0.572779\n",
      "train loss:   0.722190\n",
      "train loss:   1.191637\n",
      "train loss:   0.637662\n",
      "train loss:   1.021415\n",
      "train loss:   1.093871\n",
      "train loss:   0.898372\n",
      "train loss:   0.899060\n",
      "train loss:   0.780976\n",
      "train loss:   1.464281\n",
      "train loss:   0.745502\n",
      "train loss:   0.938424\n",
      "train loss:   0.808553\n",
      "train loss:   0.942333\n",
      "train loss:   1.394655\n",
      "train loss:   0.577148\n",
      "train loss:   0.981557\n",
      "train loss:   0.543548\n",
      "train loss:   0.781820\n",
      "train loss:   0.871275\n",
      "train loss:   0.675452\n",
      "train loss:   0.922685\n",
      "train loss:   0.701951\n",
      "train loss:   1.050349\n",
      "train loss:   1.008681\n",
      "train loss:   1.084334\n",
      "train loss:   0.884247\n",
      "train loss:   1.202828\n",
      "train loss:   0.983719\n",
      "train loss:   1.204816\n",
      "train loss:   1.094786\n",
      "train loss:   0.743809\n",
      "train loss:   0.936175\n",
      "train loss:   1.252458\n",
      "train loss:   1.146639\n",
      "########### epoch 183 ###########\n",
      "########### loop 34400 ###########\n",
      "test loss:   0.384474   test accuracy:   0.875000\n",
      "########### loop 34400 ###########\n",
      "train loss:   0.915052\n",
      "train loss:   1.015873\n",
      "train loss:   1.032604\n",
      "train loss:   0.746094\n",
      "train loss:   1.108194\n",
      "train loss:   0.852560\n",
      "train loss:   1.113756\n",
      "train loss:   0.958625\n",
      "train loss:   0.980858\n",
      "train loss:   0.836554\n",
      "train loss:   0.736370\n",
      "train loss:   1.027806\n",
      "train loss:   0.725925\n",
      "train loss:   1.168036\n",
      "train loss:   0.642646\n",
      "train loss:   1.014441\n",
      "train loss:   1.142966\n",
      "train loss:   0.945808\n",
      "train loss:   1.152260\n",
      "train loss:   0.563124\n",
      "train loss:   0.820699\n",
      "train loss:   1.053126\n",
      "train loss:   1.212684\n",
      "train loss:   1.024741\n",
      "train loss:   1.057127\n",
      "train loss:   0.935787\n",
      "train loss:   1.290932\n",
      "train loss:   0.876555\n",
      "train loss:   1.054424\n",
      "train loss:   0.892310\n",
      "train loss:   1.057512\n",
      "train loss:   1.109789\n",
      "train loss:   1.033685\n",
      "train loss:   0.781183\n",
      "train loss:   1.253672\n",
      "train loss:   1.052201\n",
      "train loss:   1.040485\n",
      "train loss:   0.878910\n",
      "train loss:   1.090734\n",
      "train loss:   0.974189\n",
      "train loss:   0.757033\n",
      "train loss:   0.923164\n",
      "train loss:   0.909461\n",
      "train loss:   0.817151\n",
      "train loss:   0.771802\n",
      "train loss:   1.106745\n",
      "train loss:   0.862085\n",
      "train loss:   1.061629\n",
      "train loss:   1.282853\n",
      "train loss:   1.180597\n",
      "########### epoch 184 ###########\n",
      "########### loop 34450 ###########\n",
      "test loss:   0.106706   test accuracy:   0.958333\n",
      "########### loop 34450 ###########\n",
      "train loss:   0.919488\n",
      "train loss:   0.638348\n",
      "train loss:   0.792746\n",
      "train loss:   0.976859\n",
      "train loss:   0.536140\n",
      "train loss:   0.845456\n",
      "train loss:   0.570527\n",
      "train loss:   0.876759\n",
      "train loss:   1.067536\n",
      "train loss:   0.647344\n",
      "train loss:   1.058511\n",
      "train loss:   1.158365\n",
      "train loss:   0.810534\n",
      "train loss:   1.113553\n",
      "train loss:   0.865319\n",
      "train loss:   1.085523\n",
      "train loss:   1.152493\n",
      "train loss:   1.074098\n",
      "train loss:   1.051909\n",
      "train loss:   0.950489\n",
      "train loss:   0.850349\n",
      "train loss:   0.732405\n",
      "train loss:   0.996336\n",
      "train loss:   0.823229\n",
      "train loss:   0.915088\n",
      "train loss:   1.145776\n",
      "train loss:   0.981644\n",
      "train loss:   0.928640\n",
      "train loss:   0.845639\n",
      "train loss:   1.286348\n",
      "train loss:   0.873355\n",
      "train loss:   1.049001\n",
      "train loss:   0.724593\n",
      "train loss:   1.004782\n",
      "train loss:   0.837166\n",
      "train loss:   0.848971\n",
      "train loss:   0.986733\n",
      "train loss:   1.058756\n",
      "train loss:   0.880945\n",
      "train loss:   0.994707\n",
      "train loss:   0.909613\n",
      "train loss:   1.021772\n",
      "train loss:   1.143847\n",
      "train loss:   0.884620\n",
      "train loss:   1.141220\n",
      "train loss:   0.909506\n",
      "train loss:   1.111066\n",
      "train loss:   0.946742\n",
      "train loss:   1.255554\n",
      "train loss:   1.175116\n",
      "########### epoch 184 ###########\n",
      "########### loop 34500 ###########\n",
      "test loss:   0.178666   test accuracy:   0.958333\n",
      "########### loop 34500 ###########\n",
      "train loss:   0.937256\n",
      "train loss:   0.911357\n",
      "train loss:   1.082082\n",
      "train loss:   0.969450\n",
      "train loss:   0.573868\n",
      "train loss:   0.664246\n",
      "train loss:   0.991099\n",
      "train loss:   0.810873\n",
      "train loss:   0.876185\n",
      "train loss:   1.113923\n",
      "train loss:   1.103901\n",
      "train loss:   0.930230\n",
      "train loss:   0.562328\n",
      "train loss:   1.135007\n",
      "train loss:   0.840122\n",
      "train loss:   0.746072\n",
      "train loss:   1.207053\n",
      "train loss:   0.779413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.014663\n",
      "train loss:   1.297811\n",
      "train loss:   0.788857\n",
      "train loss:   1.144312\n",
      "train loss:   1.203868\n",
      "train loss:   0.680930\n",
      "train loss:   0.988572\n",
      "train loss:   1.070786\n",
      "train loss:   0.805161\n",
      "train loss:   0.617697\n",
      "train loss:   0.927042\n",
      "train loss:   0.937071\n",
      "train loss:   1.137466\n",
      "train loss:   0.917858\n",
      "train loss:   0.910173\n",
      "train loss:   1.042585\n",
      "train loss:   0.955906\n",
      "train loss:   1.056933\n",
      "train loss:   0.969958\n",
      "train loss:   0.733611\n",
      "train loss:   0.723633\n",
      "train loss:   1.397679\n",
      "train loss:   1.036206\n",
      "train loss:   1.367266\n",
      "train loss:   1.248430\n",
      "train loss:   1.053193\n",
      "train loss:   0.985106\n",
      "train loss:   0.770715\n",
      "train loss:   1.070051\n",
      "train loss:   1.202686\n",
      "train loss:   0.973152\n",
      "train loss:   1.164652\n",
      "########### epoch 184 ###########\n",
      "########### loop 34550 ###########\n",
      "test loss:   0.158010   test accuracy:   1.000000\n",
      "########### loop 34550 ###########\n",
      "train loss:   0.676356\n",
      "train loss:   0.899419\n",
      "train loss:   1.060463\n",
      "train loss:   0.847026\n",
      "train loss:   0.943664\n",
      "train loss:   1.004315\n",
      "train loss:   0.965170\n",
      "train loss:   0.985934\n",
      "train loss:   0.769119\n",
      "train loss:   1.012855\n",
      "train loss:   0.845760\n",
      "train loss:   0.933792\n",
      "train loss:   0.924285\n",
      "train loss:   0.783897\n",
      "train loss:   0.888105\n",
      "train loss:   0.699098\n",
      "train loss:   0.725290\n",
      "train loss:   0.674291\n",
      "train loss:   0.781282\n",
      "train loss:   1.144836\n",
      "train loss:   0.999559\n",
      "train loss:   0.951149\n",
      "train loss:   1.127179\n",
      "train loss:   0.948448\n",
      "train loss:   1.110440\n",
      "train loss:   1.036500\n",
      "train loss:   0.800785\n",
      "train loss:   0.685786\n",
      "train loss:   1.138830\n",
      "train loss:   0.994435\n",
      "train loss:   0.755189\n",
      "train loss:   0.945966\n",
      "train loss:   0.948120\n",
      "train loss:   1.279085\n",
      "train loss:   0.705729\n",
      "train loss:   1.103484\n",
      "train loss:   1.237846\n",
      "train loss:   0.804711\n",
      "train loss:   1.119515\n",
      "train loss:   0.625502\n",
      "train loss:   1.054095\n",
      "train loss:   0.956480\n",
      "train loss:   0.799155\n",
      "train loss:   0.546236\n",
      "train loss:   0.711315\n",
      "train loss:   1.256703\n",
      "train loss:   1.068483\n",
      "train loss:   0.676821\n",
      "train loss:   0.754344\n",
      "train loss:   1.154249\n",
      "########### epoch 185 ###########\n",
      "########### loop 34600 ###########\n",
      "test loss:   0.163137   test accuracy:   0.958333\n",
      "########### loop 34600 ###########\n",
      "train loss:   0.810159\n",
      "train loss:   1.206015\n",
      "train loss:   0.788904\n",
      "train loss:   1.202813\n",
      "train loss:   0.999163\n",
      "train loss:   0.986403\n",
      "train loss:   1.215618\n",
      "train loss:   0.975805\n",
      "train loss:   1.146116\n",
      "train loss:   1.225839\n",
      "train loss:   0.693278\n",
      "train loss:   0.908463\n",
      "train loss:   1.028535\n",
      "train loss:   0.805781\n",
      "train loss:   0.806975\n",
      "train loss:   1.142793\n",
      "train loss:   0.791877\n",
      "train loss:   0.967170\n",
      "train loss:   0.721876\n",
      "train loss:   1.033607\n",
      "train loss:   1.016881\n",
      "train loss:   0.763429\n",
      "train loss:   1.068977\n",
      "train loss:   0.995017\n",
      "train loss:   0.827464\n",
      "train loss:   1.379932\n",
      "train loss:   1.045187\n",
      "train loss:   0.830568\n",
      "train loss:   0.653301\n",
      "train loss:   1.165661\n",
      "train loss:   1.089775\n",
      "train loss:   0.860939\n",
      "train loss:   0.722732\n",
      "train loss:   0.814251\n",
      "train loss:   1.002658\n",
      "train loss:   1.181247\n",
      "train loss:   0.637537\n",
      "train loss:   0.784511\n",
      "train loss:   1.482028\n",
      "train loss:   0.725203\n",
      "train loss:   1.073804\n",
      "train loss:   1.049410\n",
      "train loss:   1.185995\n",
      "train loss:   1.066721\n",
      "train loss:   0.397323\n",
      "train loss:   0.910539\n",
      "train loss:   0.926881\n",
      "train loss:   0.883502\n",
      "train loss:   0.731053\n",
      "train loss:   0.567726\n",
      "########### epoch 185 ###########\n",
      "########### loop 34650 ###########\n",
      "test loss:   0.313015   test accuracy:   0.916667\n",
      "########### loop 34650 ###########\n",
      "train loss:   0.962153\n",
      "train loss:   0.857007\n",
      "train loss:   1.075187\n",
      "train loss:   0.529621\n",
      "train loss:   0.797948\n",
      "train loss:   0.966036\n",
      "train loss:   1.112576\n",
      "train loss:   0.643563\n",
      "train loss:   0.901176\n",
      "train loss:   0.773704\n",
      "train loss:   0.716469\n",
      "train loss:   0.822940\n",
      "train loss:   0.837302\n",
      "train loss:   0.985144\n",
      "train loss:   0.812022\n",
      "train loss:   0.740223\n",
      "train loss:   1.021511\n",
      "train loss:   0.565981\n",
      "train loss:   1.275012\n",
      "train loss:   0.928590\n",
      "train loss:   1.054487\n",
      "train loss:   0.997996\n",
      "train loss:   0.857021\n",
      "train loss:   0.732171\n",
      "train loss:   1.040153\n",
      "train loss:   1.216732\n",
      "train loss:   0.688084\n",
      "train loss:   0.838266\n",
      "train loss:   1.216776\n",
      "train loss:   1.104558\n",
      "train loss:   1.053236\n",
      "train loss:   0.902066\n",
      "train loss:   0.859038\n",
      "train loss:   0.721770\n",
      "train loss:   0.837506\n",
      "train loss:   0.626587\n",
      "train loss:   1.072014\n",
      "train loss:   1.302583\n",
      "train loss:   0.661801\n",
      "train loss:   1.105702\n",
      "train loss:   1.126373\n",
      "train loss:   1.279692\n",
      "train loss:   1.142912\n",
      "train loss:   0.900681\n",
      "train loss:   0.804199\n",
      "train loss:   1.124558\n",
      "train loss:   1.169531\n",
      "train loss:   1.411027\n",
      "train loss:   0.725330\n",
      "train loss:   0.854399\n",
      "########### epoch 185 ###########\n",
      "########### loop 34700 ###########\n",
      "test loss:   0.256007   test accuracy:   0.916667\n",
      "########### loop 34700 ###########\n",
      "train loss:   1.046406\n",
      "train loss:   1.133764\n",
      "train loss:   0.646802\n",
      "train loss:   0.871566\n",
      "train loss:   0.994845\n",
      "train loss:   0.893878\n",
      "train loss:   1.005594\n",
      "train loss:   1.033289\n",
      "train loss:   0.920365\n",
      "train loss:   0.861139\n",
      "train loss:   1.038909\n",
      "train loss:   1.058901\n",
      "train loss:   0.994015\n",
      "train loss:   0.960163\n",
      "train loss:   0.999141\n",
      "train loss:   1.020818\n",
      "train loss:   0.930079\n",
      "train loss:   1.036286\n",
      "train loss:   0.619087\n",
      "train loss:   1.071511\n",
      "train loss:   0.846659\n",
      "train loss:   0.839123\n",
      "train loss:   0.898167\n",
      "train loss:   0.712772\n",
      "train loss:   0.834681\n",
      "train loss:   1.044812\n",
      "train loss:   0.955568\n",
      "train loss:   1.179628\n",
      "train loss:   0.980433\n",
      "train loss:   1.106348\n",
      "train loss:   1.063399\n",
      "train loss:   0.918500\n",
      "train loss:   1.025399\n",
      "train loss:   0.764321\n",
      "train loss:   0.701567\n",
      "train loss:   1.025614\n",
      "train loss:   1.139855\n",
      "train loss:   0.812934\n",
      "train loss:   0.695193\n",
      "train loss:   0.825782\n",
      "train loss:   1.332747\n",
      "train loss:   0.758087\n",
      "train loss:   0.797446\n",
      "train loss:   1.151570\n",
      "train loss:   0.739845\n",
      "train loss:   1.154348\n",
      "train loss:   0.724772\n",
      "train loss:   1.343868\n",
      "train loss:   0.491715\n",
      "train loss:   0.822978\n",
      "########### epoch 185 ###########\n",
      "########### loop 34750 ###########\n",
      "test loss:   0.190308   test accuracy:   0.958333\n",
      "########### loop 34750 ###########\n",
      "train loss:   0.973188\n",
      "train loss:   1.047456\n",
      "train loss:   0.694378\n",
      "train loss:   1.390255\n",
      "train loss:   1.127611\n",
      "train loss:   0.951800\n",
      "train loss:   1.078374\n",
      "train loss:   0.955164\n",
      "train loss:   0.815815\n",
      "train loss:   0.936523\n",
      "train loss:   1.032690\n",
      "train loss:   0.903030\n",
      "train loss:   1.054616\n",
      "train loss:   0.769281\n",
      "train loss:   0.847172\n",
      "train loss:   1.067173\n",
      "train loss:   1.121328\n",
      "train loss:   1.133982\n",
      "train loss:   1.124119\n",
      "train loss:   0.790962\n",
      "train loss:   0.810825\n",
      "train loss:   0.870174\n",
      "train loss:   0.738556\n",
      "train loss:   0.874804\n",
      "train loss:   1.105018\n",
      "train loss:   1.145638\n",
      "train loss:   0.753827\n",
      "train loss:   0.783717\n",
      "train loss:   1.169424\n",
      "train loss:   0.870478\n",
      "train loss:   0.910689\n",
      "train loss:   1.012576\n",
      "train loss:   0.917324\n",
      "train loss:   0.449472\n",
      "train loss:   0.980453\n",
      "train loss:   0.753438\n",
      "train loss:   0.836993\n",
      "train loss:   1.158374\n",
      "train loss:   1.170334\n",
      "train loss:   1.102221\n",
      "train loss:   1.341158\n",
      "train loss:   1.265756\n",
      "train loss:   0.655942\n",
      "train loss:   0.919523\n",
      "train loss:   0.713417\n",
      "train loss:   0.515151\n",
      "train loss:   0.735469\n",
      "train loss:   0.680635\n",
      "train loss:   1.217710\n",
      "train loss:   0.750056\n",
      "########### epoch 186 ###########\n",
      "########### loop 34800 ###########\n",
      "test loss:   0.296857   test accuracy:   0.833333\n",
      "########### loop 34800 ###########\n",
      "train loss:   1.041608\n",
      "train loss:   0.841524\n",
      "train loss:   0.912726\n",
      "train loss:   1.007229\n",
      "train loss:   0.979587\n",
      "train loss:   0.942279\n",
      "train loss:   0.674809\n",
      "train loss:   1.288263\n",
      "train loss:   1.054597\n",
      "train loss:   1.048273\n",
      "train loss:   0.967497\n",
      "train loss:   0.789882\n",
      "train loss:   0.732172\n",
      "train loss:   0.881572\n",
      "train loss:   0.850223\n",
      "train loss:   0.830492\n",
      "train loss:   0.981150\n",
      "train loss:   1.028077\n",
      "train loss:   1.232321\n",
      "train loss:   0.860235\n",
      "train loss:   1.109830\n",
      "train loss:   0.422550\n",
      "train loss:   0.832732\n",
      "train loss:   1.045154\n",
      "train loss:   1.358488\n",
      "train loss:   0.880244\n",
      "train loss:   1.051653\n",
      "train loss:   1.084372\n",
      "train loss:   0.900754\n",
      "train loss:   0.861832\n",
      "train loss:   1.114402\n",
      "train loss:   0.667104\n",
      "train loss:   0.705741\n",
      "train loss:   1.101950\n",
      "train loss:   1.299997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.680486\n",
      "train loss:   0.657868\n",
      "train loss:   0.836809\n",
      "train loss:   0.739757\n",
      "train loss:   1.049647\n",
      "train loss:   0.821754\n",
      "train loss:   0.561762\n",
      "train loss:   0.685578\n",
      "train loss:   1.365170\n",
      "train loss:   1.478103\n",
      "train loss:   0.887751\n",
      "train loss:   1.045019\n",
      "train loss:   1.286279\n",
      "train loss:   0.900695\n",
      "train loss:   1.126189\n",
      "########### epoch 186 ###########\n",
      "########### loop 34850 ###########\n",
      "test loss:   0.269774   test accuracy:   0.916667\n",
      "########### loop 34850 ###########\n",
      "train loss:   0.978400\n",
      "train loss:   0.902742\n",
      "train loss:   1.131511\n",
      "train loss:   1.267717\n",
      "train loss:   0.978948\n",
      "train loss:   0.636308\n",
      "train loss:   0.992228\n",
      "train loss:   1.103957\n",
      "train loss:   0.826282\n",
      "train loss:   1.259494\n",
      "train loss:   0.670141\n",
      "train loss:   1.144676\n",
      "train loss:   0.701337\n",
      "train loss:   1.195844\n",
      "train loss:   0.651745\n",
      "train loss:   0.720848\n",
      "train loss:   1.104945\n",
      "train loss:   0.641596\n",
      "train loss:   0.717864\n",
      "train loss:   0.532183\n",
      "train loss:   1.004731\n",
      "train loss:   1.246789\n",
      "train loss:   0.835367\n",
      "train loss:   1.040400\n",
      "train loss:   1.107339\n",
      "train loss:   0.756141\n",
      "train loss:   0.924976\n",
      "train loss:   0.781867\n",
      "train loss:   0.956700\n",
      "train loss:   0.726518\n",
      "train loss:   0.903816\n",
      "train loss:   0.679937\n",
      "train loss:   0.959053\n",
      "train loss:   0.875878\n",
      "train loss:   0.794255\n",
      "train loss:   0.810723\n",
      "train loss:   0.856194\n",
      "train loss:   0.883147\n",
      "train loss:   0.897956\n",
      "train loss:   0.868949\n",
      "train loss:   1.191725\n",
      "train loss:   0.911485\n",
      "train loss:   0.991001\n",
      "train loss:   0.827259\n",
      "train loss:   0.767727\n",
      "train loss:   1.013109\n",
      "train loss:   0.871205\n",
      "train loss:   0.870341\n",
      "train loss:   0.822389\n",
      "train loss:   0.988733\n",
      "########### epoch 186 ###########\n",
      "########### loop 34900 ###########\n",
      "test loss:   0.361549   test accuracy:   0.875000\n",
      "########### loop 34900 ###########\n",
      "train loss:   1.074924\n",
      "train loss:   1.370635\n",
      "train loss:   0.769389\n",
      "train loss:   0.680763\n",
      "train loss:   0.775046\n",
      "train loss:   1.118260\n",
      "train loss:   1.046365\n",
      "train loss:   0.917169\n",
      "train loss:   0.917112\n",
      "train loss:   0.739121\n",
      "train loss:   0.799730\n",
      "train loss:   0.987122\n",
      "train loss:   1.007883\n",
      "train loss:   0.975355\n",
      "train loss:   0.969560\n",
      "train loss:   1.001814\n",
      "train loss:   1.005816\n",
      "train loss:   1.069146\n",
      "train loss:   0.917748\n",
      "train loss:   1.110356\n",
      "train loss:   0.920669\n",
      "train loss:   0.836578\n",
      "train loss:   1.095021\n",
      "train loss:   0.974099\n",
      "train loss:   1.001772\n",
      "train loss:   1.253864\n",
      "train loss:   0.988941\n",
      "train loss:   0.972130\n",
      "train loss:   0.660174\n",
      "train loss:   0.836385\n",
      "train loss:   0.846339\n",
      "train loss:   1.033735\n",
      "train loss:   0.886526\n",
      "train loss:   0.939706\n",
      "train loss:   0.930268\n",
      "train loss:   1.025002\n",
      "train loss:   0.837585\n",
      "train loss:   0.846109\n",
      "train loss:   1.105446\n",
      "train loss:   1.001249\n",
      "train loss:   0.891187\n",
      "train loss:   0.629023\n",
      "train loss:   0.912116\n",
      "train loss:   0.838698\n",
      "train loss:   1.011073\n",
      "train loss:   1.018312\n",
      "train loss:   1.087331\n",
      "train loss:   1.059125\n",
      "train loss:   0.869341\n",
      "train loss:   0.896629\n",
      "########### epoch 186 ###########\n",
      "########### loop 34950 ###########\n",
      "test loss:   0.299338   test accuracy:   0.875000\n",
      "########### loop 34950 ###########\n",
      "train loss:   0.425337\n",
      "train loss:   0.858944\n",
      "train loss:   0.846592\n",
      "train loss:   0.819492\n",
      "train loss:   1.042419\n",
      "train loss:   0.823920\n",
      "train loss:   0.843990\n",
      "train loss:   0.558051\n",
      "train loss:   0.932415\n",
      "train loss:   0.973476\n",
      "train loss:   0.833056\n",
      "train loss:   1.336404\n",
      "train loss:   1.065093\n",
      "train loss:   0.808225\n",
      "train loss:   0.530950\n",
      "train loss:   0.809581\n",
      "train loss:   0.770421\n",
      "train loss:   0.520983\n",
      "train loss:   0.933180\n",
      "train loss:   0.654612\n",
      "train loss:   0.848220\n",
      "train loss:   1.088854\n",
      "train loss:   0.609058\n",
      "train loss:   0.989587\n",
      "train loss:   1.260838\n",
      "train loss:   0.817286\n",
      "train loss:   0.831002\n",
      "train loss:   0.970821\n",
      "train loss:   1.134964\n",
      "train loss:   0.888653\n",
      "train loss:   0.974234\n",
      "train loss:   0.780079\n",
      "train loss:   0.819779\n",
      "train loss:   0.604304\n",
      "train loss:   0.990014\n",
      "train loss:   0.560798\n",
      "train loss:   0.965767\n",
      "train loss:   0.957900\n",
      "train loss:   0.723379\n",
      "train loss:   1.290150\n",
      "train loss:   0.715435\n",
      "train loss:   0.898943\n",
      "train loss:   0.908865\n",
      "train loss:   0.876655\n",
      "train loss:   1.414355\n",
      "train loss:   1.262656\n",
      "train loss:   0.590705\n",
      "train loss:   0.981311\n",
      "train loss:   1.151806\n",
      "train loss:   1.017905\n",
      "########### epoch 187 ###########\n",
      "########### loop 35000 ###########\n",
      "test loss:   0.347847   test accuracy:   0.916667\n",
      "########### loop 35000 ###########\n",
      "train loss:   0.753528\n",
      "train loss:   1.041466\n",
      "train loss:   0.776874\n",
      "train loss:   0.639728\n",
      "train loss:   1.168380\n",
      "train loss:   0.732457\n",
      "train loss:   1.265116\n",
      "train loss:   0.693513\n",
      "train loss:   0.690250\n",
      "train loss:   1.036367\n",
      "train loss:   1.366899\n",
      "train loss:   0.730486\n",
      "train loss:   0.675559\n",
      "train loss:   1.014410\n",
      "train loss:   0.938977\n",
      "train loss:   0.887127\n",
      "train loss:   0.964959\n",
      "train loss:   1.326843\n",
      "train loss:   1.286696\n",
      "train loss:   0.928418\n",
      "train loss:   0.648533\n",
      "train loss:   1.058105\n",
      "train loss:   1.001096\n",
      "train loss:   0.579983\n",
      "train loss:   1.180160\n",
      "train loss:   0.952916\n",
      "train loss:   0.712910\n",
      "train loss:   1.016768\n",
      "train loss:   0.817320\n",
      "train loss:   1.091442\n",
      "train loss:   1.022398\n",
      "train loss:   0.807182\n",
      "train loss:   0.745667\n",
      "train loss:   0.911053\n",
      "train loss:   0.924302\n",
      "train loss:   1.132213\n",
      "train loss:   0.744506\n",
      "train loss:   0.997041\n",
      "train loss:   0.854627\n",
      "train loss:   0.758666\n",
      "train loss:   0.918422\n",
      "train loss:   0.834814\n",
      "train loss:   0.861123\n",
      "train loss:   1.030011\n",
      "train loss:   1.158914\n",
      "train loss:   0.977356\n",
      "train loss:   0.655311\n",
      "train loss:   0.609272\n",
      "train loss:   1.189021\n",
      "train loss:   0.970773\n",
      "########### epoch 187 ###########\n",
      "########### loop 35050 ###########\n",
      "test loss:   0.158068   test accuracy:   0.958333\n",
      "########### loop 35050 ###########\n",
      "train loss:   0.648705\n",
      "train loss:   0.696151\n",
      "train loss:   0.901023\n",
      "train loss:   0.759256\n",
      "train loss:   0.588358\n",
      "train loss:   0.849790\n",
      "train loss:   1.320354\n",
      "train loss:   0.844611\n",
      "train loss:   0.723012\n",
      "train loss:   0.895586\n",
      "train loss:   1.020069\n",
      "train loss:   1.024055\n",
      "train loss:   0.864209\n",
      "train loss:   0.879155\n",
      "train loss:   0.663614\n",
      "train loss:   0.822384\n",
      "train loss:   1.216276\n",
      "train loss:   1.089292\n",
      "train loss:   1.138029\n",
      "train loss:   0.988703\n",
      "train loss:   1.146566\n",
      "train loss:   0.806040\n",
      "train loss:   0.967331\n",
      "train loss:   0.898755\n",
      "train loss:   1.081690\n",
      "train loss:   0.884850\n",
      "train loss:   0.810068\n",
      "train loss:   0.660908\n",
      "train loss:   0.967925\n",
      "train loss:   0.752703\n",
      "train loss:   0.963583\n",
      "train loss:   1.524546\n",
      "train loss:   0.946724\n",
      "train loss:   1.131301\n",
      "train loss:   1.067732\n",
      "train loss:   1.047869\n",
      "train loss:   0.794687\n",
      "train loss:   0.800678\n",
      "train loss:   0.514349\n",
      "train loss:   1.029325\n",
      "train loss:   0.977630\n",
      "train loss:   0.764925\n",
      "train loss:   0.903301\n",
      "train loss:   1.067743\n",
      "train loss:   0.614684\n",
      "train loss:   0.924639\n",
      "train loss:   0.659587\n",
      "train loss:   1.101230\n",
      "train loss:   0.836608\n",
      "train loss:   1.115921\n",
      "########### epoch 187 ###########\n",
      "########### loop 35100 ###########\n",
      "test loss:   0.064123   test accuracy:   1.000000\n",
      "########### loop 35100 ###########\n",
      "train loss:   0.491714\n",
      "train loss:   1.071162\n",
      "train loss:   0.763132\n",
      "train loss:   1.065576\n",
      "train loss:   0.962167\n",
      "train loss:   0.927580\n",
      "train loss:   0.850175\n",
      "train loss:   0.973703\n",
      "train loss:   1.269204\n",
      "train loss:   0.986480\n",
      "train loss:   0.992824\n",
      "train loss:   0.753949\n",
      "train loss:   0.738732\n",
      "train loss:   1.018787\n",
      "train loss:   0.470100\n",
      "train loss:   0.912731\n",
      "train loss:   1.063660\n",
      "train loss:   0.967523\n",
      "train loss:   0.676059\n",
      "train loss:   1.055225\n",
      "train loss:   0.817251\n",
      "train loss:   1.013611\n",
      "train loss:   1.106863\n",
      "train loss:   0.693761\n",
      "train loss:   1.372713\n",
      "train loss:   0.845041\n",
      "train loss:   1.060207\n",
      "train loss:   1.047014\n",
      "train loss:   1.200124\n",
      "train loss:   0.541618\n",
      "train loss:   0.827661\n",
      "train loss:   0.878355\n",
      "train loss:   0.836394\n",
      "train loss:   0.723524\n",
      "train loss:   1.413144\n",
      "train loss:   0.942307\n",
      "train loss:   0.974980\n",
      "train loss:   0.865277\n",
      "train loss:   0.954652\n",
      "train loss:   1.218849\n",
      "train loss:   0.652613\n",
      "train loss:   1.406374\n",
      "train loss:   1.326991\n",
      "train loss:   0.912044\n",
      "train loss:   0.879040\n",
      "train loss:   1.239454\n",
      "train loss:   0.827517\n",
      "train loss:   1.229749\n",
      "train loss:   0.870104\n",
      "train loss:   0.888190\n",
      "########### epoch 187 ###########\n",
      "########### loop 35150 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.205995   test accuracy:   0.958333\n",
      "########### loop 35150 ###########\n",
      "train loss:   0.791369\n",
      "train loss:   0.943659\n",
      "train loss:   0.757054\n",
      "train loss:   0.323133\n",
      "train loss:   0.882003\n",
      "train loss:   0.807057\n",
      "train loss:   0.488302\n",
      "train loss:   1.124821\n",
      "train loss:   1.026496\n",
      "train loss:   0.790455\n",
      "train loss:   1.024631\n",
      "train loss:   0.889892\n",
      "train loss:   1.215731\n",
      "train loss:   0.886452\n",
      "train loss:   1.160178\n",
      "train loss:   0.753058\n",
      "train loss:   0.962338\n",
      "train loss:   0.777417\n",
      "train loss:   0.934713\n",
      "train loss:   1.153667\n",
      "train loss:   0.873468\n",
      "train loss:   0.717686\n",
      "train loss:   1.036338\n",
      "train loss:   0.972417\n",
      "train loss:   1.163374\n",
      "train loss:   0.785439\n",
      "train loss:   0.811499\n",
      "train loss:   0.739228\n",
      "train loss:   1.100102\n",
      "train loss:   0.889942\n",
      "train loss:   0.882630\n",
      "train loss:   0.870391\n",
      "train loss:   1.206746\n",
      "train loss:   0.587520\n",
      "train loss:   0.927337\n",
      "train loss:   0.784922\n",
      "train loss:   1.004933\n",
      "train loss:   1.174405\n",
      "train loss:   0.677400\n",
      "train loss:   0.988830\n",
      "train loss:   1.009053\n",
      "train loss:   0.814494\n",
      "train loss:   1.204230\n",
      "train loss:   1.030747\n",
      "train loss:   1.063655\n",
      "train loss:   0.970720\n",
      "train loss:   0.784078\n",
      "train loss:   0.786286\n",
      "train loss:   1.311700\n",
      "train loss:   0.825471\n",
      "########### epoch 188 ###########\n",
      "########### loop 35200 ###########\n",
      "test loss:   0.550406   test accuracy:   0.708333\n",
      "########### loop 35200 ###########\n",
      "train loss:   1.135538\n",
      "train loss:   0.980810\n",
      "train loss:   0.767372\n",
      "train loss:   0.752928\n",
      "train loss:   0.754420\n",
      "train loss:   0.814675\n",
      "train loss:   1.404013\n",
      "train loss:   1.030058\n",
      "train loss:   0.986753\n",
      "train loss:   0.985322\n",
      "train loss:   0.804286\n",
      "train loss:   1.077787\n",
      "train loss:   0.754057\n",
      "train loss:   0.803392\n",
      "train loss:   1.107988\n",
      "train loss:   0.797702\n",
      "train loss:   1.331336\n",
      "train loss:   1.113654\n",
      "train loss:   0.787740\n",
      "train loss:   1.086965\n",
      "train loss:   0.775413\n",
      "train loss:   0.786164\n",
      "train loss:   0.845913\n",
      "train loss:   0.873976\n",
      "train loss:   1.115980\n",
      "train loss:   0.927723\n",
      "train loss:   0.628382\n",
      "train loss:   1.453038\n",
      "train loss:   0.680153\n",
      "train loss:   0.896109\n",
      "train loss:   1.165212\n",
      "train loss:   0.929886\n",
      "train loss:   0.960227\n",
      "train loss:   0.785645\n",
      "train loss:   0.799006\n",
      "train loss:   0.920585\n",
      "train loss:   0.983662\n",
      "train loss:   1.115826\n",
      "train loss:   0.942634\n",
      "train loss:   0.734915\n",
      "train loss:   0.810958\n",
      "train loss:   1.148754\n",
      "train loss:   0.870409\n",
      "train loss:   1.325133\n",
      "train loss:   0.789836\n",
      "train loss:   0.745790\n",
      "train loss:   1.127312\n",
      "train loss:   1.170045\n",
      "train loss:   0.776944\n",
      "train loss:   1.141962\n",
      "########### epoch 188 ###########\n",
      "########### loop 35250 ###########\n",
      "test loss:   0.342853   test accuracy:   0.875000\n",
      "########### loop 35250 ###########\n",
      "train loss:   1.233129\n",
      "train loss:   0.975122\n",
      "train loss:   0.828427\n",
      "train loss:   0.943305\n",
      "train loss:   0.934927\n",
      "train loss:   0.645164\n",
      "train loss:   0.949151\n",
      "train loss:   1.195613\n",
      "train loss:   1.131454\n",
      "train loss:   1.223549\n",
      "train loss:   0.948555\n",
      "train loss:   0.841340\n",
      "train loss:   0.851562\n",
      "train loss:   0.800574\n",
      "train loss:   0.754541\n",
      "train loss:   0.774935\n",
      "train loss:   0.835943\n",
      "train loss:   0.680584\n",
      "train loss:   0.897528\n",
      "train loss:   0.962188\n",
      "train loss:   1.139943\n",
      "train loss:   0.935683\n",
      "train loss:   1.071370\n",
      "train loss:   0.840009\n",
      "train loss:   0.787553\n",
      "train loss:   1.112706\n",
      "train loss:   0.799586\n",
      "train loss:   0.731057\n",
      "train loss:   0.685135\n",
      "train loss:   0.986311\n",
      "train loss:   1.401657\n",
      "train loss:   0.641981\n",
      "train loss:   0.985390\n",
      "train loss:   0.902353\n",
      "train loss:   0.997824\n",
      "train loss:   1.082845\n",
      "train loss:   1.258532\n",
      "train loss:   1.168431\n",
      "train loss:   1.135402\n",
      "train loss:   1.159870\n",
      "train loss:   1.187634\n",
      "train loss:   1.085254\n",
      "train loss:   0.938138\n",
      "train loss:   1.253297\n",
      "train loss:   1.016633\n",
      "train loss:   1.077866\n",
      "train loss:   1.448015\n",
      "train loss:   0.894113\n",
      "train loss:   1.036199\n",
      "train loss:   1.028014\n",
      "########### epoch 188 ###########\n",
      "########### loop 35300 ###########\n",
      "test loss:   0.153590   test accuracy:   0.958333\n",
      "########### loop 35300 ###########\n",
      "train loss:   0.813387\n",
      "train loss:   0.704131\n",
      "train loss:   1.080451\n",
      "train loss:   0.769827\n",
      "train loss:   0.953171\n",
      "train loss:   0.930355\n",
      "train loss:   0.894531\n",
      "train loss:   0.825313\n",
      "train loss:   0.887639\n",
      "train loss:   0.738598\n",
      "train loss:   0.841133\n",
      "train loss:   0.931811\n",
      "train loss:   0.914234\n",
      "train loss:   0.860219\n",
      "train loss:   0.878866\n",
      "train loss:   0.873071\n",
      "train loss:   1.380024\n",
      "train loss:   0.915379\n",
      "train loss:   0.807718\n",
      "train loss:   0.856785\n",
      "train loss:   0.988204\n",
      "train loss:   1.075836\n",
      "train loss:   1.131986\n",
      "train loss:   0.928237\n",
      "train loss:   0.888863\n",
      "train loss:   0.752101\n",
      "train loss:   1.581104\n",
      "train loss:   1.169319\n",
      "train loss:   1.031129\n",
      "train loss:   0.668800\n",
      "train loss:   0.919350\n",
      "train loss:   1.193845\n",
      "train loss:   1.001648\n",
      "train loss:   1.257599\n",
      "train loss:   1.095420\n",
      "train loss:   1.357566\n",
      "train loss:   0.952819\n",
      "train loss:   0.931410\n",
      "train loss:   0.892846\n",
      "train loss:   0.820601\n",
      "train loss:   1.033906\n",
      "train loss:   0.726506\n",
      "train loss:   0.947072\n",
      "train loss:   0.811642\n",
      "train loss:   0.831522\n",
      "train loss:   1.054908\n",
      "train loss:   0.996367\n",
      "train loss:   0.941578\n",
      "train loss:   0.760264\n",
      "train loss:   0.602163\n",
      "########### epoch 189 ###########\n",
      "########### loop 35350 ###########\n",
      "test loss:   0.092062   test accuracy:   1.000000\n",
      "########### loop 35350 ###########\n",
      "train loss:   1.007701\n",
      "train loss:   1.038863\n",
      "train loss:   0.847180\n",
      "train loss:   0.863599\n",
      "train loss:   1.017672\n",
      "train loss:   1.017802\n",
      "train loss:   1.094448\n",
      "train loss:   0.603558\n",
      "train loss:   0.729434\n",
      "train loss:   0.819815\n",
      "train loss:   0.907366\n",
      "train loss:   1.031732\n",
      "train loss:   0.762537\n",
      "train loss:   1.235645\n",
      "train loss:   0.603307\n",
      "train loss:   0.869067\n",
      "train loss:   0.958552\n",
      "train loss:   0.572040\n",
      "train loss:   0.611707\n",
      "train loss:   0.794606\n",
      "train loss:   1.086217\n",
      "train loss:   0.808413\n",
      "train loss:   0.955902\n",
      "train loss:   1.278362\n",
      "train loss:   0.968039\n",
      "train loss:   0.788062\n",
      "train loss:   1.250731\n",
      "train loss:   1.134447\n",
      "train loss:   0.845096\n",
      "train loss:   0.992378\n",
      "train loss:   1.028043\n",
      "train loss:   0.927623\n",
      "train loss:   0.895841\n",
      "train loss:   0.967566\n",
      "train loss:   1.072248\n",
      "train loss:   0.886879\n",
      "train loss:   0.861911\n",
      "train loss:   0.478964\n",
      "train loss:   1.027666\n",
      "train loss:   0.807880\n",
      "train loss:   1.118382\n",
      "train loss:   0.784355\n",
      "train loss:   0.799913\n",
      "train loss:   1.185540\n",
      "train loss:   0.843862\n",
      "train loss:   0.955998\n",
      "train loss:   0.586427\n",
      "train loss:   0.997864\n",
      "train loss:   0.910205\n",
      "train loss:   1.081680\n",
      "########### epoch 189 ###########\n",
      "########### loop 35400 ###########\n",
      "test loss:   0.201358   test accuracy:   0.916667\n",
      "########### loop 35400 ###########\n",
      "train loss:   0.769312\n",
      "train loss:   0.918580\n",
      "train loss:   1.093705\n",
      "train loss:   0.958588\n",
      "train loss:   0.924316\n",
      "train loss:   0.517535\n",
      "train loss:   0.960980\n",
      "train loss:   0.864952\n",
      "train loss:   1.007339\n",
      "train loss:   0.907328\n",
      "train loss:   1.069339\n",
      "train loss:   0.956957\n",
      "train loss:   0.795160\n",
      "train loss:   0.841484\n",
      "train loss:   0.913457\n",
      "train loss:   0.484358\n",
      "train loss:   0.972100\n",
      "train loss:   0.982860\n",
      "train loss:   0.929001\n",
      "train loss:   0.565003\n",
      "train loss:   0.876184\n",
      "train loss:   1.076236\n",
      "train loss:   1.051652\n",
      "train loss:   0.989935\n",
      "train loss:   0.807779\n",
      "train loss:   1.000074\n",
      "train loss:   0.577202\n",
      "train loss:   1.137609\n",
      "train loss:   1.050797\n",
      "train loss:   1.033722\n",
      "train loss:   0.780846\n",
      "train loss:   1.473982\n",
      "train loss:   0.792986\n",
      "train loss:   1.263494\n",
      "train loss:   0.881409\n",
      "train loss:   1.087990\n",
      "train loss:   1.184999\n",
      "train loss:   1.145810\n",
      "train loss:   0.723002\n",
      "train loss:   0.700116\n",
      "train loss:   0.874226\n",
      "train loss:   1.013586\n",
      "train loss:   1.099897\n",
      "train loss:   0.895457\n",
      "train loss:   1.020322\n",
      "train loss:   0.789780\n",
      "train loss:   0.580768\n",
      "train loss:   0.710349\n",
      "train loss:   1.174809\n",
      "train loss:   0.707542\n",
      "########### epoch 189 ###########\n",
      "########### loop 35450 ###########\n",
      "test loss:   0.141442   test accuracy:   0.916667\n",
      "########### loop 35450 ###########\n",
      "train loss:   0.855973\n",
      "train loss:   0.621174\n",
      "train loss:   1.382283\n",
      "train loss:   1.109804\n",
      "train loss:   0.950621\n",
      "train loss:   1.068169\n",
      "train loss:   1.183588\n",
      "train loss:   1.188646\n",
      "train loss:   1.099407\n",
      "train loss:   0.879618\n",
      "train loss:   1.164626\n",
      "train loss:   0.677565\n",
      "train loss:   1.025748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.019461\n",
      "train loss:   0.896113\n",
      "train loss:   0.932738\n",
      "train loss:   0.918720\n",
      "train loss:   0.759034\n",
      "train loss:   1.228450\n",
      "train loss:   0.814635\n",
      "train loss:   0.837957\n",
      "train loss:   1.177940\n",
      "train loss:   0.958340\n",
      "train loss:   1.011497\n",
      "train loss:   0.918426\n",
      "train loss:   1.168963\n",
      "train loss:   0.817520\n",
      "train loss:   1.133673\n",
      "train loss:   0.974033\n",
      "train loss:   0.621063\n",
      "train loss:   1.116729\n",
      "train loss:   0.684483\n",
      "train loss:   1.027429\n",
      "train loss:   1.205261\n",
      "train loss:   1.155328\n",
      "train loss:   0.660830\n",
      "train loss:   1.058054\n",
      "train loss:   0.701572\n",
      "train loss:   0.580061\n",
      "train loss:   1.234329\n",
      "train loss:   1.190786\n",
      "train loss:   0.958077\n",
      "train loss:   0.735453\n",
      "train loss:   0.948499\n",
      "train loss:   1.025730\n",
      "train loss:   1.042103\n",
      "train loss:   1.204284\n",
      "train loss:   0.927745\n",
      "train loss:   1.125096\n",
      "train loss:   1.094337\n",
      "########### epoch 189 ###########\n",
      "########### loop 35500 ###########\n",
      "test loss:   0.122054   test accuracy:   1.000000\n",
      "########### loop 35500 ###########\n",
      "train loss:   1.130605\n",
      "train loss:   0.542343\n",
      "train loss:   1.041502\n",
      "train loss:   0.979481\n",
      "train loss:   1.306466\n",
      "train loss:   0.844668\n",
      "train loss:   1.050151\n",
      "train loss:   1.045656\n",
      "train loss:   0.855213\n",
      "train loss:   1.036052\n",
      "train loss:   0.931301\n",
      "train loss:   0.897064\n",
      "train loss:   1.039308\n",
      "train loss:   1.227722\n",
      "train loss:   1.287138\n",
      "train loss:   1.230911\n",
      "train loss:   1.048113\n",
      "train loss:   1.001445\n",
      "train loss:   0.528964\n",
      "train loss:   1.073308\n",
      "train loss:   0.743940\n",
      "train loss:   0.901771\n",
      "train loss:   0.997342\n",
      "train loss:   0.956031\n",
      "train loss:   1.046690\n",
      "train loss:   1.260705\n",
      "train loss:   1.189514\n",
      "train loss:   0.798655\n",
      "train loss:   1.285545\n",
      "train loss:   0.946754\n",
      "train loss:   0.784440\n",
      "train loss:   0.924484\n",
      "train loss:   0.948818\n",
      "train loss:   0.857707\n",
      "train loss:   1.027533\n",
      "train loss:   0.878288\n",
      "train loss:   0.965045\n",
      "train loss:   0.827170\n",
      "train loss:   0.818509\n",
      "train loss:   0.974418\n",
      "train loss:   0.864224\n",
      "train loss:   0.974045\n",
      "train loss:   0.782480\n",
      "train loss:   0.849044\n",
      "train loss:   1.282892\n",
      "train loss:   0.732623\n",
      "train loss:   1.122244\n",
      "train loss:   0.723172\n",
      "train loss:   0.998735\n",
      "train loss:   0.827241\n",
      "########### epoch 190 ###########\n",
      "########### loop 35550 ###########\n",
      "test loss:   0.270481   test accuracy:   0.958333\n",
      "########### loop 35550 ###########\n",
      "train loss:   0.888617\n",
      "train loss:   0.785789\n",
      "train loss:   0.935166\n",
      "train loss:   0.782877\n",
      "train loss:   1.147519\n",
      "train loss:   1.038123\n",
      "train loss:   0.841897\n",
      "train loss:   1.008112\n",
      "train loss:   0.467356\n",
      "train loss:   0.990705\n",
      "train loss:   0.978016\n",
      "train loss:   1.218894\n",
      "train loss:   0.574266\n",
      "train loss:   0.680428\n",
      "train loss:   1.135696\n",
      "train loss:   1.168966\n",
      "train loss:   0.991720\n",
      "train loss:   1.290624\n",
      "train loss:   0.885660\n",
      "train loss:   0.614908\n",
      "train loss:   1.050603\n",
      "train loss:   1.153775\n",
      "train loss:   0.608372\n",
      "train loss:   0.800427\n",
      "train loss:   0.665013\n",
      "train loss:   0.883416\n",
      "train loss:   1.015833\n",
      "train loss:   0.845969\n",
      "train loss:   0.994290\n",
      "train loss:   0.974836\n",
      "train loss:   0.589080\n",
      "train loss:   0.751947\n",
      "train loss:   1.182445\n",
      "train loss:   1.014411\n",
      "train loss:   0.718498\n",
      "train loss:   1.218189\n",
      "train loss:   0.997680\n",
      "train loss:   1.125559\n",
      "train loss:   0.935906\n",
      "train loss:   0.865551\n",
      "train loss:   0.417061\n",
      "train loss:   0.745128\n",
      "train loss:   0.870998\n",
      "train loss:   1.164961\n",
      "train loss:   1.045330\n",
      "train loss:   0.955087\n",
      "train loss:   0.688528\n",
      "train loss:   1.074066\n",
      "train loss:   1.041026\n",
      "train loss:   0.943855\n",
      "########### epoch 190 ###########\n",
      "########### loop 35600 ###########\n",
      "test loss:   0.110109   test accuracy:   0.958333\n",
      "########### loop 35600 ###########\n",
      "train loss:   0.483226\n",
      "train loss:   0.863465\n",
      "train loss:   0.845680\n",
      "train loss:   0.914137\n",
      "train loss:   0.664263\n",
      "train loss:   0.925012\n",
      "train loss:   0.739764\n",
      "train loss:   0.970947\n",
      "train loss:   0.718372\n",
      "train loss:   1.062819\n",
      "train loss:   1.243689\n",
      "train loss:   0.755190\n",
      "train loss:   1.202933\n",
      "train loss:   0.944854\n",
      "train loss:   1.023649\n",
      "train loss:   0.950330\n",
      "train loss:   0.990937\n",
      "train loss:   0.850535\n",
      "train loss:   0.904910\n",
      "train loss:   0.609590\n",
      "train loss:   0.933876\n",
      "train loss:   1.075579\n",
      "train loss:   1.143056\n",
      "train loss:   1.086499\n",
      "train loss:   1.059408\n",
      "train loss:   1.088226\n",
      "train loss:   0.669694\n",
      "train loss:   0.962284\n",
      "train loss:   1.192584\n",
      "train loss:   0.998525\n",
      "train loss:   1.129801\n",
      "train loss:   0.900090\n",
      "train loss:   0.953799\n",
      "train loss:   0.592924\n",
      "train loss:   0.779647\n",
      "train loss:   0.725990\n",
      "train loss:   0.536709\n",
      "train loss:   0.947586\n",
      "train loss:   0.957377\n",
      "train loss:   0.889997\n",
      "train loss:   0.597899\n",
      "train loss:   0.905398\n",
      "train loss:   0.937350\n",
      "train loss:   0.987895\n",
      "train loss:   1.170054\n",
      "train loss:   0.899427\n",
      "train loss:   1.092052\n",
      "train loss:   0.838717\n",
      "train loss:   0.745402\n",
      "train loss:   0.879112\n",
      "########### epoch 190 ###########\n",
      "########### loop 35650 ###########\n",
      "test loss:   0.254201   test accuracy:   0.916667\n",
      "########### loop 35650 ###########\n",
      "train loss:   1.043416\n",
      "train loss:   1.248531\n",
      "train loss:   0.748402\n",
      "train loss:   0.908710\n",
      "train loss:   0.576194\n",
      "train loss:   0.982489\n",
      "train loss:   1.070418\n",
      "train loss:   1.019837\n",
      "train loss:   0.965918\n",
      "train loss:   1.127061\n",
      "train loss:   0.746140\n",
      "train loss:   0.805493\n",
      "train loss:   0.740987\n",
      "train loss:   1.205308\n",
      "train loss:   0.605312\n",
      "train loss:   0.868045\n",
      "train loss:   0.854247\n",
      "train loss:   0.813082\n",
      "train loss:   0.812275\n",
      "train loss:   0.908218\n",
      "train loss:   0.700029\n",
      "train loss:   1.207257\n",
      "train loss:   0.897979\n",
      "train loss:   1.115349\n",
      "train loss:   1.222844\n",
      "train loss:   0.979416\n",
      "train loss:   0.652769\n",
      "train loss:   1.179612\n",
      "train loss:   1.283730\n",
      "train loss:   0.961879\n",
      "train loss:   1.022397\n",
      "train loss:   1.129092\n",
      "train loss:   0.964527\n",
      "train loss:   0.898614\n",
      "train loss:   1.053710\n",
      "train loss:   0.956444\n",
      "train loss:   0.436376\n",
      "train loss:   0.868528\n",
      "train loss:   0.819759\n",
      "train loss:   0.537299\n",
      "train loss:   0.931105\n",
      "train loss:   0.949863\n",
      "train loss:   1.066986\n",
      "train loss:   0.826471\n",
      "train loss:   0.831343\n",
      "train loss:   1.145244\n",
      "train loss:   0.623891\n",
      "train loss:   0.641649\n",
      "train loss:   0.718224\n",
      "train loss:   0.977907\n",
      "########### epoch 190 ###########\n",
      "########### loop 35700 ###########\n",
      "test loss:   0.092345   test accuracy:   1.000000\n",
      "########### loop 35700 ###########\n",
      "train loss:   1.166506\n",
      "train loss:   0.939981\n",
      "train loss:   0.871926\n",
      "train loss:   1.120980\n",
      "train loss:   0.705515\n",
      "train loss:   1.367062\n",
      "train loss:   1.433797\n",
      "train loss:   0.888076\n",
      "train loss:   0.944996\n",
      "train loss:   1.008425\n",
      "train loss:   1.083005\n",
      "train loss:   0.688362\n",
      "train loss:   0.669135\n",
      "train loss:   0.752450\n",
      "train loss:   1.053295\n",
      "train loss:   0.870683\n",
      "train loss:   1.295841\n",
      "train loss:   0.949478\n",
      "train loss:   0.970313\n",
      "train loss:   0.778743\n",
      "train loss:   0.677116\n",
      "train loss:   0.975430\n",
      "train loss:   0.770079\n",
      "train loss:   1.173753\n",
      "train loss:   0.933986\n",
      "train loss:   0.724212\n",
      "train loss:   0.932131\n",
      "train loss:   1.225857\n",
      "train loss:   0.974074\n",
      "train loss:   0.748404\n",
      "train loss:   0.910739\n",
      "train loss:   1.039730\n",
      "train loss:   0.831404\n",
      "train loss:   0.868774\n",
      "train loss:   1.273881\n",
      "train loss:   0.770735\n",
      "train loss:   0.926866\n",
      "train loss:   0.930578\n",
      "train loss:   0.914199\n",
      "train loss:   0.961165\n",
      "train loss:   1.087351\n",
      "train loss:   0.717196\n",
      "train loss:   0.982898\n",
      "train loss:   0.978908\n",
      "train loss:   0.487197\n",
      "train loss:   1.271483\n",
      "train loss:   1.005305\n",
      "train loss:   1.099117\n",
      "train loss:   0.938901\n",
      "train loss:   1.275806\n",
      "########### epoch 191 ###########\n",
      "########### loop 35750 ###########\n",
      "test loss:   0.245410   test accuracy:   0.916667\n",
      "########### loop 35750 ###########\n",
      "train loss:   0.713601\n",
      "train loss:   0.827159\n",
      "train loss:   0.907472\n",
      "train loss:   0.949290\n",
      "train loss:   0.891227\n",
      "train loss:   1.502936\n",
      "train loss:   0.776958\n",
      "train loss:   0.846297\n",
      "train loss:   1.347938\n",
      "train loss:   0.759023\n",
      "train loss:   0.950165\n",
      "train loss:   0.781215\n",
      "train loss:   1.037645\n",
      "train loss:   1.054026\n",
      "train loss:   1.377651\n",
      "train loss:   0.998046\n",
      "train loss:   0.690410\n",
      "train loss:   0.947404\n",
      "train loss:   1.000182\n",
      "train loss:   1.138217\n",
      "train loss:   0.875356\n",
      "train loss:   1.055707\n",
      "train loss:   1.248414\n",
      "train loss:   0.835181\n",
      "train loss:   0.925200\n",
      "train loss:   1.107129\n",
      "train loss:   1.578271\n",
      "train loss:   0.897202\n",
      "train loss:   0.975381\n",
      "train loss:   0.978222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.785631\n",
      "train loss:   1.326717\n",
      "train loss:   1.066927\n",
      "train loss:   0.672757\n",
      "train loss:   0.932018\n",
      "train loss:   0.921017\n",
      "train loss:   0.845762\n",
      "train loss:   0.891020\n",
      "train loss:   1.072245\n",
      "train loss:   0.946368\n",
      "train loss:   0.831339\n",
      "train loss:   1.018538\n",
      "train loss:   0.689418\n",
      "train loss:   0.915315\n",
      "train loss:   0.900832\n",
      "train loss:   0.986051\n",
      "train loss:   1.130863\n",
      "train loss:   1.161556\n",
      "train loss:   1.103588\n",
      "train loss:   0.850671\n",
      "########### epoch 191 ###########\n",
      "########### loop 35800 ###########\n",
      "test loss:   0.095760   test accuracy:   1.000000\n",
      "########### loop 35800 ###########\n",
      "train loss:   0.835350\n",
      "train loss:   1.148731\n",
      "train loss:   0.578083\n",
      "train loss:   1.265200\n",
      "train loss:   0.425239\n",
      "train loss:   0.918820\n",
      "train loss:   0.707819\n",
      "train loss:   0.971073\n",
      "train loss:   1.086602\n",
      "train loss:   0.671145\n",
      "train loss:   0.672273\n",
      "train loss:   0.953354\n",
      "train loss:   1.083213\n",
      "train loss:   0.740760\n",
      "train loss:   0.567465\n",
      "train loss:   0.712624\n",
      "train loss:   0.951303\n",
      "train loss:   1.073236\n",
      "train loss:   0.928723\n",
      "train loss:   1.058188\n",
      "train loss:   1.165659\n",
      "train loss:   1.042911\n",
      "train loss:   0.957219\n",
      "train loss:   0.809845\n",
      "train loss:   1.054576\n",
      "train loss:   1.019324\n",
      "train loss:   0.892544\n",
      "train loss:   0.889465\n",
      "train loss:   1.055984\n",
      "train loss:   0.973273\n",
      "train loss:   0.807679\n",
      "train loss:   1.219136\n",
      "train loss:   1.068549\n",
      "train loss:   0.787094\n",
      "train loss:   1.000432\n",
      "train loss:   0.860929\n",
      "train loss:   0.873926\n",
      "train loss:   1.249396\n",
      "train loss:   0.735694\n",
      "train loss:   0.839535\n",
      "train loss:   0.453885\n",
      "train loss:   1.161811\n",
      "train loss:   1.396211\n",
      "train loss:   0.903118\n",
      "train loss:   0.845180\n",
      "train loss:   1.126044\n",
      "train loss:   1.013643\n",
      "train loss:   0.869532\n",
      "train loss:   0.964188\n",
      "train loss:   1.179411\n",
      "########### epoch 191 ###########\n",
      "########### loop 35850 ###########\n",
      "test loss:   0.146128   test accuracy:   0.958333\n",
      "########### loop 35850 ###########\n",
      "train loss:   1.288980\n",
      "train loss:   0.819903\n",
      "train loss:   0.867009\n",
      "train loss:   0.859944\n",
      "train loss:   0.908432\n",
      "train loss:   1.105425\n",
      "train loss:   0.958410\n",
      "train loss:   0.655079\n",
      "train loss:   1.302318\n",
      "train loss:   0.723130\n",
      "train loss:   1.170847\n",
      "train loss:   0.993378\n",
      "train loss:   1.064753\n",
      "train loss:   1.022397\n",
      "train loss:   0.864340\n",
      "train loss:   0.972194\n",
      "train loss:   0.646057\n",
      "train loss:   1.001913\n",
      "train loss:   0.782902\n",
      "train loss:   0.721669\n",
      "train loss:   0.818728\n",
      "train loss:   0.476342\n",
      "train loss:   1.231052\n",
      "train loss:   1.288793\n",
      "train loss:   1.272335\n",
      "train loss:   0.634006\n",
      "train loss:   0.640079\n",
      "train loss:   0.589941\n",
      "train loss:   0.873100\n",
      "train loss:   0.866644\n",
      "train loss:   0.716832\n",
      "train loss:   0.971410\n",
      "train loss:   0.839534\n",
      "train loss:   1.377427\n",
      "train loss:   0.853738\n",
      "train loss:   0.882209\n",
      "train loss:   1.141958\n",
      "train loss:   0.719785\n",
      "train loss:   0.745762\n",
      "train loss:   1.002792\n",
      "train loss:   1.064656\n",
      "train loss:   0.829831\n",
      "train loss:   0.740871\n",
      "train loss:   1.191756\n",
      "train loss:   1.063748\n",
      "train loss:   0.891950\n",
      "train loss:   1.417938\n",
      "train loss:   1.082582\n",
      "train loss:   0.863025\n",
      "train loss:   0.921797\n",
      "########### epoch 191 ###########\n",
      "########### loop 35900 ###########\n",
      "test loss:   0.187531   test accuracy:   0.916667\n",
      "########### loop 35900 ###########\n",
      "train loss:   1.057527\n",
      "train loss:   0.801692\n",
      "train loss:   0.911657\n",
      "train loss:   1.058146\n",
      "train loss:   1.119453\n",
      "train loss:   0.992278\n",
      "train loss:   1.026172\n",
      "train loss:   1.015339\n",
      "train loss:   0.698680\n",
      "train loss:   0.805353\n",
      "train loss:   0.946757\n",
      "train loss:   1.082610\n",
      "train loss:   0.561647\n",
      "train loss:   1.221477\n",
      "train loss:   1.304346\n",
      "train loss:   0.973161\n",
      "train loss:   0.706586\n",
      "train loss:   0.866801\n",
      "train loss:   0.801187\n",
      "train loss:   1.058929\n",
      "train loss:   0.884020\n",
      "train loss:   0.737378\n",
      "train loss:   1.191144\n",
      "train loss:   1.170656\n",
      "train loss:   0.913954\n",
      "train loss:   0.866939\n",
      "train loss:   0.945318\n",
      "train loss:   1.009552\n",
      "train loss:   1.040029\n",
      "train loss:   0.845636\n",
      "train loss:   1.277617\n",
      "train loss:   0.903849\n",
      "train loss:   0.764459\n",
      "train loss:   0.687800\n",
      "train loss:   0.854614\n",
      "train loss:   1.125153\n",
      "train loss:   0.563361\n",
      "train loss:   0.857498\n",
      "train loss:   1.146716\n",
      "train loss:   1.155210\n",
      "train loss:   0.742545\n",
      "train loss:   1.086487\n",
      "train loss:   1.101737\n",
      "train loss:   1.172807\n",
      "train loss:   0.718793\n",
      "train loss:   0.836738\n",
      "train loss:   1.034096\n",
      "train loss:   1.060753\n",
      "train loss:   1.214688\n",
      "train loss:   0.633148\n",
      "########### epoch 192 ###########\n",
      "########### loop 35950 ###########\n",
      "test loss:   0.076523   test accuracy:   1.000000\n",
      "########### loop 35950 ###########\n",
      "train loss:   0.984180\n",
      "train loss:   0.864649\n",
      "train loss:   0.800751\n",
      "train loss:   0.804655\n",
      "train loss:   0.920648\n",
      "train loss:   0.923128\n",
      "train loss:   0.847575\n",
      "train loss:   0.992001\n",
      "train loss:   1.166450\n",
      "train loss:   0.759213\n",
      "train loss:   0.793563\n",
      "train loss:   0.842318\n",
      "train loss:   1.000703\n",
      "train loss:   1.057494\n",
      "train loss:   1.186784\n",
      "train loss:   0.899178\n",
      "train loss:   1.025060\n",
      "train loss:   1.073262\n",
      "train loss:   0.801717\n",
      "train loss:   1.099752\n",
      "train loss:   0.957482\n",
      "train loss:   0.901003\n",
      "train loss:   0.895093\n",
      "train loss:   1.064806\n",
      "train loss:   0.826728\n",
      "train loss:   0.905942\n",
      "train loss:   0.421525\n",
      "train loss:   1.197865\n",
      "train loss:   0.581415\n",
      "train loss:   0.850178\n",
      "train loss:   0.816874\n",
      "train loss:   0.927411\n",
      "train loss:   0.747979\n",
      "train loss:   1.173171\n",
      "train loss:   0.968282\n",
      "train loss:   1.271924\n",
      "train loss:   0.716111\n",
      "train loss:   0.809829\n",
      "train loss:   1.087146\n",
      "train loss:   0.892183\n",
      "train loss:   0.758236\n",
      "train loss:   0.786828\n",
      "train loss:   0.730389\n",
      "train loss:   0.616513\n",
      "train loss:   0.990497\n",
      "train loss:   0.861397\n",
      "train loss:   0.994639\n",
      "train loss:   0.851595\n",
      "train loss:   0.949257\n",
      "train loss:   0.913961\n",
      "########### epoch 192 ###########\n",
      "########### loop 36000 ###########\n",
      "test loss:   0.210079   test accuracy:   0.916667\n",
      "########### loop 36000 ###########\n",
      "train loss:   0.968132\n",
      "train loss:   1.091596\n",
      "train loss:   0.972140\n",
      "train loss:   1.354105\n",
      "train loss:   0.955822\n",
      "train loss:   1.053270\n",
      "train loss:   0.998702\n",
      "train loss:   0.164475\n",
      "train loss:   0.698933\n",
      "train loss:   1.029248\n",
      "train loss:   1.028996\n",
      "train loss:   0.842259\n",
      "train loss:   0.573898\n",
      "train loss:   0.919725\n",
      "train loss:   0.525326\n",
      "train loss:   0.508869\n",
      "train loss:   0.695911\n",
      "train loss:   0.922065\n",
      "train loss:   1.087980\n",
      "train loss:   0.897678\n",
      "train loss:   0.918741\n",
      "train loss:   0.966459\n",
      "train loss:   1.141639\n",
      "train loss:   0.719898\n",
      "train loss:   1.329456\n",
      "train loss:   0.750817\n",
      "train loss:   0.822308\n",
      "train loss:   0.715269\n",
      "train loss:   1.114319\n",
      "train loss:   1.141616\n",
      "train loss:   0.707679\n",
      "train loss:   0.884342\n",
      "train loss:   1.425718\n",
      "train loss:   1.058936\n",
      "train loss:   0.869868\n",
      "train loss:   0.689893\n",
      "train loss:   0.905949\n",
      "train loss:   0.753678\n",
      "train loss:   0.751106\n",
      "train loss:   0.969267\n",
      "train loss:   1.047922\n",
      "train loss:   1.120040\n",
      "train loss:   1.076230\n",
      "train loss:   1.014548\n",
      "train loss:   1.312956\n",
      "train loss:   1.121050\n",
      "train loss:   1.203091\n",
      "train loss:   1.244005\n",
      "train loss:   0.973261\n",
      "train loss:   1.222695\n",
      "########### epoch 192 ###########\n",
      "########### loop 36050 ###########\n",
      "test loss:   0.103257   test accuracy:   1.000000\n",
      "########### loop 36050 ###########\n",
      "train loss:   1.088593\n",
      "train loss:   1.109505\n",
      "train loss:   0.801463\n",
      "train loss:   0.901900\n",
      "train loss:   0.912714\n",
      "train loss:   1.046148\n",
      "train loss:   1.338258\n",
      "train loss:   0.576427\n",
      "train loss:   1.100312\n",
      "train loss:   1.165667\n",
      "train loss:   1.028867\n",
      "train loss:   0.876563\n",
      "train loss:   1.172668\n",
      "train loss:   0.804729\n",
      "train loss:   1.134109\n",
      "train loss:   0.972377\n",
      "train loss:   0.973404\n",
      "train loss:   0.884933\n",
      "train loss:   1.033185\n",
      "train loss:   0.979982\n",
      "train loss:   0.856565\n",
      "train loss:   0.520657\n",
      "train loss:   0.911063\n",
      "train loss:   1.226718\n",
      "train loss:   1.040518\n",
      "train loss:   0.939499\n",
      "train loss:   0.975234\n",
      "train loss:   1.067836\n",
      "train loss:   1.172866\n",
      "train loss:   0.982742\n",
      "train loss:   0.887664\n",
      "train loss:   0.761004\n",
      "train loss:   0.992926\n",
      "train loss:   0.864832\n",
      "train loss:   1.090327\n",
      "train loss:   1.144009\n",
      "train loss:   0.940182\n",
      "train loss:   0.832269\n",
      "train loss:   0.857413\n",
      "train loss:   0.846056\n",
      "train loss:   0.776605\n",
      "train loss:   0.900751\n",
      "train loss:   0.934361\n",
      "train loss:   0.949587\n",
      "train loss:   0.677622\n",
      "train loss:   1.497532\n",
      "train loss:   1.090644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.790713\n",
      "train loss:   1.168181\n",
      "train loss:   1.255845\n",
      "########### epoch 193 ###########\n",
      "########### loop 36100 ###########\n",
      "test loss:   0.261303   test accuracy:   0.916667\n",
      "########### loop 36100 ###########\n",
      "train loss:   1.296659\n",
      "train loss:   1.062654\n",
      "train loss:   0.957599\n",
      "train loss:   0.596737\n",
      "train loss:   0.897855\n",
      "train loss:   1.006039\n",
      "train loss:   0.902048\n",
      "train loss:   0.736212\n",
      "train loss:   1.103368\n",
      "train loss:   0.931630\n",
      "train loss:   1.239196\n",
      "train loss:   1.037085\n",
      "train loss:   0.729824\n",
      "train loss:   1.386737\n",
      "train loss:   1.140017\n",
      "train loss:   0.747387\n",
      "train loss:   0.817582\n",
      "train loss:   0.886132\n",
      "train loss:   1.068633\n",
      "train loss:   0.913519\n",
      "train loss:   0.834370\n",
      "train loss:   0.988068\n",
      "train loss:   0.859194\n",
      "train loss:   0.946131\n",
      "train loss:   1.113912\n",
      "train loss:   0.928537\n",
      "train loss:   1.352097\n",
      "train loss:   1.030365\n",
      "train loss:   0.472116\n",
      "train loss:   0.884145\n",
      "train loss:   0.708774\n",
      "train loss:   0.943090\n",
      "train loss:   0.984639\n",
      "train loss:   0.909287\n",
      "train loss:   1.053410\n",
      "train loss:   1.042576\n",
      "train loss:   0.589632\n",
      "train loss:   1.185082\n",
      "train loss:   0.843130\n",
      "train loss:   1.109031\n",
      "train loss:   0.961348\n",
      "train loss:   1.095097\n",
      "train loss:   1.123154\n",
      "train loss:   1.141912\n",
      "train loss:   0.749549\n",
      "train loss:   1.113950\n",
      "train loss:   0.593813\n",
      "train loss:   0.726107\n",
      "train loss:   1.092448\n",
      "train loss:   1.112524\n",
      "########### epoch 193 ###########\n",
      "########### loop 36150 ###########\n",
      "test loss:   0.317816   test accuracy:   0.875000\n",
      "########### loop 36150 ###########\n",
      "train loss:   0.760998\n",
      "train loss:   0.802904\n",
      "train loss:   0.783099\n",
      "train loss:   0.990393\n",
      "train loss:   0.884537\n",
      "train loss:   1.188752\n",
      "train loss:   0.859031\n",
      "train loss:   1.309054\n",
      "train loss:   1.027946\n",
      "train loss:   1.092577\n",
      "train loss:   1.175204\n",
      "train loss:   0.722951\n",
      "train loss:   1.069653\n",
      "train loss:   0.820897\n",
      "train loss:   1.301645\n",
      "train loss:   0.575572\n",
      "train loss:   0.927943\n",
      "train loss:   1.025993\n",
      "train loss:   1.204540\n",
      "train loss:   0.819405\n",
      "train loss:   0.937087\n",
      "train loss:   0.949600\n",
      "train loss:   1.165447\n",
      "train loss:   1.444982\n",
      "train loss:   0.782024\n",
      "train loss:   0.689386\n",
      "train loss:   0.987050\n",
      "train loss:   1.270755\n",
      "train loss:   1.072731\n",
      "train loss:   1.257622\n",
      "train loss:   0.679563\n",
      "train loss:   0.894667\n",
      "train loss:   1.158857\n",
      "train loss:   0.706130\n",
      "train loss:   0.774632\n",
      "train loss:   0.782986\n",
      "train loss:   0.903165\n",
      "train loss:   1.120394\n",
      "train loss:   1.015484\n",
      "train loss:   0.912178\n",
      "train loss:   0.863251\n",
      "train loss:   0.974685\n",
      "train loss:   1.425167\n",
      "train loss:   0.910496\n",
      "train loss:   1.035772\n",
      "train loss:   1.129810\n",
      "train loss:   0.819713\n",
      "train loss:   1.060129\n",
      "train loss:   0.877670\n",
      "train loss:   1.121384\n",
      "########### epoch 193 ###########\n",
      "########### loop 36200 ###########\n",
      "test loss:   0.595538   test accuracy:   0.833333\n",
      "########### loop 36200 ###########\n",
      "train loss:   1.152327\n",
      "train loss:   0.812484\n",
      "train loss:   0.790617\n",
      "train loss:   0.623603\n",
      "train loss:   0.987968\n",
      "train loss:   0.942310\n",
      "train loss:   1.069630\n",
      "train loss:   1.086903\n",
      "train loss:   1.075309\n",
      "train loss:   0.913413\n",
      "train loss:   1.054775\n",
      "train loss:   1.184067\n",
      "train loss:   1.306736\n",
      "train loss:   1.138718\n",
      "train loss:   1.479610\n",
      "train loss:   0.815266\n",
      "train loss:   0.909548\n",
      "train loss:   0.972717\n",
      "train loss:   0.885192\n",
      "train loss:   0.646981\n",
      "train loss:   0.618416\n",
      "train loss:   1.060975\n",
      "train loss:   0.858199\n",
      "train loss:   1.031840\n",
      "train loss:   0.710220\n",
      "train loss:   0.916483\n",
      "train loss:   0.802575\n",
      "train loss:   0.858602\n",
      "train loss:   0.924349\n",
      "train loss:   0.874320\n",
      "train loss:   1.305051\n",
      "train loss:   1.014873\n",
      "train loss:   0.876038\n",
      "train loss:   0.788503\n",
      "train loss:   0.830569\n",
      "train loss:   1.179126\n",
      "train loss:   0.883889\n",
      "train loss:   1.024435\n",
      "train loss:   1.137875\n",
      "train loss:   0.841290\n",
      "train loss:   0.995388\n",
      "train loss:   1.227584\n",
      "train loss:   0.786872\n",
      "train loss:   0.882694\n",
      "train loss:   1.156455\n",
      "train loss:   0.715643\n",
      "train loss:   0.990274\n",
      "train loss:   0.987761\n",
      "train loss:   1.141030\n",
      "train loss:   0.946680\n",
      "########### epoch 193 ###########\n",
      "########### loop 36250 ###########\n",
      "test loss:   0.116329   test accuracy:   0.958333\n",
      "########### loop 36250 ###########\n",
      "train loss:   0.883409\n",
      "train loss:   0.543365\n",
      "train loss:   1.127023\n",
      "train loss:   1.099802\n",
      "train loss:   0.890212\n",
      "train loss:   0.932852\n",
      "train loss:   0.794215\n",
      "train loss:   0.709676\n",
      "train loss:   0.960400\n",
      "train loss:   1.129353\n",
      "train loss:   0.818891\n",
      "train loss:   0.817976\n",
      "train loss:   0.820389\n",
      "train loss:   1.265047\n",
      "train loss:   0.818067\n",
      "train loss:   0.969170\n",
      "train loss:   1.160227\n",
      "train loss:   1.043204\n",
      "train loss:   1.168425\n",
      "train loss:   0.827549\n",
      "train loss:   1.265548\n",
      "train loss:   1.357940\n",
      "train loss:   0.928747\n",
      "train loss:   0.974116\n",
      "train loss:   1.274105\n",
      "train loss:   0.865608\n",
      "train loss:   1.136347\n",
      "train loss:   0.871672\n",
      "train loss:   1.237920\n",
      "train loss:   1.071477\n",
      "train loss:   1.244022\n",
      "train loss:   1.009219\n",
      "train loss:   0.896920\n",
      "train loss:   0.594041\n",
      "train loss:   1.094741\n",
      "train loss:   0.646712\n",
      "train loss:   1.004037\n",
      "train loss:   1.111877\n",
      "train loss:   1.108620\n",
      "train loss:   1.135762\n",
      "train loss:   0.699098\n",
      "train loss:   1.293350\n",
      "train loss:   0.962319\n",
      "train loss:   1.192436\n",
      "train loss:   0.813341\n",
      "train loss:   1.036816\n",
      "train loss:   0.973067\n",
      "train loss:   0.851273\n",
      "train loss:   1.097035\n",
      "train loss:   0.910902\n",
      "########### epoch 194 ###########\n",
      "########### loop 36300 ###########\n",
      "test loss:   0.269347   test accuracy:   0.958333\n",
      "########### loop 36300 ###########\n",
      "train loss:   0.977629\n",
      "train loss:   1.140872\n",
      "train loss:   0.903885\n",
      "train loss:   0.622328\n",
      "train loss:   0.660218\n",
      "train loss:   0.789742\n",
      "train loss:   0.802100\n",
      "train loss:   0.789862\n",
      "train loss:   1.081973\n",
      "train loss:   0.833137\n",
      "train loss:   1.238412\n",
      "train loss:   0.826986\n",
      "train loss:   0.934081\n",
      "train loss:   0.960266\n",
      "train loss:   1.247844\n",
      "train loss:   0.979550\n",
      "train loss:   1.033282\n",
      "train loss:   1.129226\n",
      "train loss:   1.093188\n",
      "train loss:   0.896357\n",
      "train loss:   0.743728\n",
      "train loss:   1.148131\n",
      "train loss:   0.975115\n",
      "train loss:   0.705260\n",
      "train loss:   0.773188\n",
      "train loss:   1.025860\n",
      "train loss:   0.946328\n",
      "train loss:   0.803031\n",
      "train loss:   0.993008\n",
      "train loss:   0.737865\n",
      "train loss:   1.355662\n",
      "train loss:   1.299671\n",
      "train loss:   0.497864\n",
      "train loss:   0.922073\n",
      "train loss:   0.879342\n",
      "train loss:   1.059428\n",
      "train loss:   0.729599\n",
      "train loss:   1.049667\n",
      "train loss:   0.874177\n",
      "train loss:   0.970550\n",
      "train loss:   1.179135\n",
      "train loss:   0.873163\n",
      "train loss:   0.985715\n",
      "train loss:   0.779231\n",
      "train loss:   0.975114\n",
      "train loss:   1.063504\n",
      "train loss:   1.030112\n",
      "train loss:   0.925320\n",
      "train loss:   1.157998\n",
      "train loss:   0.658844\n",
      "########### epoch 194 ###########\n",
      "########### loop 36350 ###########\n",
      "test loss:   0.108176   test accuracy:   1.000000\n",
      "########### loop 36350 ###########\n",
      "train loss:   1.153899\n",
      "train loss:   1.287496\n",
      "train loss:   0.660183\n",
      "train loss:   0.889905\n",
      "train loss:   1.014041\n",
      "train loss:   0.800422\n",
      "train loss:   0.878149\n",
      "train loss:   0.915989\n",
      "train loss:   0.778270\n",
      "train loss:   0.957020\n",
      "train loss:   0.968982\n",
      "train loss:   1.135422\n",
      "train loss:   0.836209\n",
      "train loss:   0.679713\n",
      "train loss:   1.072774\n",
      "train loss:   1.154159\n",
      "train loss:   0.908715\n",
      "train loss:   1.187847\n",
      "train loss:   1.047766\n",
      "train loss:   1.114674\n",
      "train loss:   0.807569\n",
      "train loss:   1.096712\n",
      "train loss:   1.329960\n",
      "train loss:   1.006172\n",
      "train loss:   0.899923\n",
      "train loss:   1.005239\n",
      "train loss:   1.201446\n",
      "train loss:   0.840218\n",
      "train loss:   1.150438\n",
      "train loss:   0.828666\n",
      "train loss:   0.748522\n",
      "train loss:   0.788230\n",
      "train loss:   1.062879\n",
      "train loss:   0.928503\n",
      "train loss:   0.971633\n",
      "train loss:   1.119776\n",
      "train loss:   0.625179\n",
      "train loss:   1.112924\n",
      "train loss:   0.895279\n",
      "train loss:   0.737215\n",
      "train loss:   0.897280\n",
      "train loss:   0.964364\n",
      "train loss:   1.059845\n",
      "train loss:   1.038576\n",
      "train loss:   0.869790\n",
      "train loss:   0.692795\n",
      "train loss:   1.147345\n",
      "train loss:   0.881173\n",
      "train loss:   0.968974\n",
      "train loss:   0.787624\n",
      "########### epoch 194 ###########\n",
      "########### loop 36400 ###########\n",
      "test loss:   0.177889   test accuracy:   0.958333\n",
      "########### loop 36400 ###########\n",
      "train loss:   0.797356\n",
      "train loss:   0.797275\n",
      "train loss:   1.043942\n",
      "train loss:   0.709197\n",
      "train loss:   1.127850\n",
      "train loss:   0.961355\n",
      "train loss:   1.271136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.928128\n",
      "train loss:   1.079687\n",
      "train loss:   0.893673\n",
      "train loss:   0.999328\n",
      "train loss:   0.683613\n",
      "train loss:   0.660005\n",
      "train loss:   0.914046\n",
      "train loss:   1.015059\n",
      "train loss:   1.058547\n",
      "train loss:   1.198835\n",
      "train loss:   0.761686\n",
      "train loss:   0.582304\n",
      "train loss:   1.455875\n",
      "train loss:   0.972362\n",
      "train loss:   1.055801\n",
      "train loss:   0.671349\n",
      "train loss:   0.767556\n",
      "train loss:   0.939021\n",
      "train loss:   0.847105\n",
      "train loss:   0.905698\n",
      "train loss:   0.979052\n",
      "train loss:   0.914037\n",
      "train loss:   0.636118\n",
      "train loss:   1.025372\n",
      "train loss:   1.124743\n",
      "train loss:   0.793755\n",
      "train loss:   0.853570\n",
      "train loss:   1.176149\n",
      "train loss:   0.771850\n",
      "train loss:   0.959582\n",
      "train loss:   1.107026\n",
      "train loss:   0.862209\n",
      "train loss:   0.886108\n",
      "train loss:   1.274233\n",
      "train loss:   1.009994\n",
      "train loss:   0.927958\n",
      "train loss:   1.087782\n",
      "train loss:   0.997223\n",
      "train loss:   0.778011\n",
      "train loss:   0.946343\n",
      "train loss:   1.050429\n",
      "train loss:   0.979993\n",
      "train loss:   1.055156\n",
      "########### epoch 194 ###########\n",
      "########### loop 36450 ###########\n",
      "test loss:   0.302109   test accuracy:   0.916667\n",
      "########### loop 36450 ###########\n",
      "train loss:   0.845399\n",
      "train loss:   1.157901\n",
      "train loss:   0.959781\n",
      "train loss:   0.979477\n",
      "train loss:   0.998946\n",
      "train loss:   0.956547\n",
      "train loss:   1.014367\n",
      "train loss:   0.869833\n",
      "train loss:   0.617652\n",
      "train loss:   0.476222\n",
      "train loss:   1.144658\n",
      "train loss:   0.956607\n",
      "train loss:   1.032619\n",
      "train loss:   0.902343\n",
      "train loss:   1.032482\n",
      "train loss:   1.079867\n",
      "train loss:   0.916495\n",
      "train loss:   1.081507\n",
      "train loss:   0.940618\n",
      "train loss:   0.876512\n",
      "train loss:   0.630478\n",
      "train loss:   0.957573\n",
      "train loss:   0.757414\n",
      "train loss:   0.969493\n",
      "train loss:   0.709265\n",
      "train loss:   1.176250\n",
      "train loss:   0.949693\n",
      "train loss:   0.968157\n",
      "train loss:   0.808396\n",
      "train loss:   0.761672\n",
      "train loss:   1.239686\n",
      "train loss:   0.888494\n",
      "train loss:   1.270494\n",
      "train loss:   0.715044\n",
      "train loss:   0.918472\n",
      "train loss:   0.809273\n",
      "train loss:   1.003417\n",
      "train loss:   1.320112\n",
      "train loss:   0.867280\n",
      "train loss:   1.205471\n",
      "train loss:   0.907050\n",
      "train loss:   1.199663\n",
      "train loss:   1.037307\n",
      "train loss:   0.729927\n",
      "train loss:   0.872939\n",
      "train loss:   0.731077\n",
      "train loss:   0.834751\n",
      "train loss:   0.619260\n",
      "train loss:   0.878624\n",
      "train loss:   1.131958\n",
      "########### epoch 195 ###########\n",
      "########### loop 36500 ###########\n",
      "test loss:   0.365768   test accuracy:   0.916667\n",
      "########### loop 36500 ###########\n",
      "train loss:   1.135667\n",
      "train loss:   0.715635\n",
      "train loss:   1.025659\n",
      "train loss:   0.581142\n",
      "train loss:   1.153330\n",
      "train loss:   0.983457\n",
      "train loss:   0.738162\n",
      "train loss:   1.372260\n",
      "train loss:   0.891327\n",
      "train loss:   1.246796\n",
      "train loss:   0.490209\n",
      "train loss:   0.915746\n",
      "train loss:   1.243808\n",
      "train loss:   0.594754\n",
      "train loss:   0.911197\n",
      "train loss:   0.806946\n",
      "train loss:   0.815071\n",
      "train loss:   1.030171\n",
      "train loss:   1.085273\n",
      "train loss:   1.396448\n",
      "train loss:   0.970170\n",
      "train loss:   0.883734\n",
      "train loss:   1.131526\n",
      "train loss:   0.865019\n",
      "train loss:   1.319916\n",
      "train loss:   0.886358\n",
      "train loss:   0.979051\n",
      "train loss:   0.811001\n",
      "train loss:   0.686620\n",
      "train loss:   1.120619\n",
      "train loss:   1.148859\n",
      "train loss:   0.721193\n",
      "train loss:   0.865749\n",
      "train loss:   0.849163\n",
      "train loss:   0.793698\n",
      "train loss:   0.860236\n",
      "train loss:   0.704497\n",
      "train loss:   1.122592\n",
      "train loss:   0.796370\n",
      "train loss:   0.986377\n",
      "train loss:   0.857266\n",
      "train loss:   0.856919\n",
      "train loss:   1.035263\n",
      "train loss:   1.147241\n",
      "train loss:   0.799961\n",
      "train loss:   0.996984\n",
      "train loss:   0.810217\n",
      "train loss:   0.962361\n",
      "train loss:   0.567809\n",
      "train loss:   1.038230\n",
      "########### epoch 195 ###########\n",
      "########### loop 36550 ###########\n",
      "test loss:   0.199058   test accuracy:   0.958333\n",
      "########### loop 36550 ###########\n",
      "train loss:   1.147738\n",
      "train loss:   0.888608\n",
      "train loss:   0.928997\n",
      "train loss:   1.214834\n",
      "train loss:   0.934171\n",
      "train loss:   0.611771\n",
      "train loss:   1.020865\n",
      "train loss:   1.062621\n",
      "train loss:   0.697469\n",
      "train loss:   0.754744\n",
      "train loss:   0.926414\n",
      "train loss:   0.958796\n",
      "train loss:   0.970150\n",
      "train loss:   0.970882\n",
      "train loss:   0.795681\n",
      "train loss:   0.995261\n",
      "train loss:   0.944844\n",
      "train loss:   1.249671\n",
      "train loss:   0.791841\n",
      "train loss:   1.004178\n",
      "train loss:   0.803308\n",
      "train loss:   0.898250\n",
      "train loss:   1.114152\n",
      "train loss:   0.584654\n",
      "train loss:   0.919005\n",
      "train loss:   1.082601\n",
      "train loss:   0.967440\n",
      "train loss:   1.302237\n",
      "train loss:   0.753900\n",
      "train loss:   1.018715\n",
      "train loss:   0.705815\n",
      "train loss:   1.250522\n",
      "train loss:   1.035505\n",
      "train loss:   0.729000\n",
      "train loss:   0.822497\n",
      "train loss:   0.756273\n",
      "train loss:   0.669045\n",
      "train loss:   0.809133\n",
      "train loss:   0.878546\n",
      "train loss:   0.921080\n",
      "train loss:   0.872448\n",
      "train loss:   1.302600\n",
      "train loss:   0.886506\n",
      "train loss:   0.796944\n",
      "train loss:   0.561587\n",
      "train loss:   0.885745\n",
      "train loss:   0.865488\n",
      "train loss:   1.132440\n",
      "train loss:   1.098096\n",
      "train loss:   0.821641\n",
      "########### epoch 195 ###########\n",
      "########### loop 36600 ###########\n",
      "test loss:   0.065133   test accuracy:   1.000000\n",
      "########### loop 36600 ###########\n",
      "train loss:   1.066455\n",
      "train loss:   0.726111\n",
      "train loss:   0.698044\n",
      "train loss:   1.401213\n",
      "train loss:   0.654231\n",
      "train loss:   0.890299\n",
      "train loss:   1.076359\n",
      "train loss:   0.959692\n",
      "train loss:   1.088196\n",
      "train loss:   1.129685\n",
      "train loss:   1.440028\n",
      "train loss:   1.038950\n",
      "train loss:   0.900057\n",
      "train loss:   0.693689\n",
      "train loss:   1.162383\n",
      "train loss:   0.730316\n",
      "train loss:   0.861295\n",
      "train loss:   0.937841\n",
      "train loss:   0.600882\n",
      "train loss:   0.644991\n",
      "train loss:   0.877988\n",
      "train loss:   1.260650\n",
      "train loss:   0.858341\n",
      "train loss:   1.042132\n",
      "train loss:   0.883632\n",
      "train loss:   1.106250\n",
      "train loss:   0.842991\n",
      "train loss:   0.816829\n",
      "train loss:   1.168837\n",
      "train loss:   0.731444\n",
      "train loss:   0.735546\n",
      "train loss:   1.218447\n",
      "train loss:   0.725159\n",
      "train loss:   1.056844\n",
      "train loss:   0.606310\n",
      "train loss:   0.797084\n",
      "train loss:   1.176192\n",
      "train loss:   1.237491\n",
      "train loss:   0.902350\n",
      "train loss:   0.946820\n",
      "train loss:   1.056753\n",
      "train loss:   1.074767\n",
      "train loss:   1.012383\n",
      "train loss:   0.938706\n",
      "train loss:   1.184565\n",
      "train loss:   0.776622\n",
      "train loss:   0.940841\n",
      "train loss:   1.068028\n",
      "train loss:   0.704118\n",
      "train loss:   0.858118\n",
      "########### epoch 195 ###########\n",
      "########### loop 36650 ###########\n",
      "test loss:   0.443272   test accuracy:   0.833333\n",
      "########### loop 36650 ###########\n",
      "train loss:   1.059658\n",
      "train loss:   0.766014\n",
      "train loss:   1.010746\n",
      "train loss:   1.005551\n",
      "train loss:   0.981967\n",
      "train loss:   1.428348\n",
      "train loss:   1.173785\n",
      "train loss:   0.878534\n",
      "train loss:   0.727621\n",
      "train loss:   0.735760\n",
      "train loss:   0.973102\n",
      "train loss:   0.734510\n",
      "train loss:   0.746604\n",
      "train loss:   0.796741\n",
      "train loss:   0.983413\n",
      "train loss:   1.059329\n",
      "train loss:   0.660454\n",
      "train loss:   1.186118\n",
      "train loss:   0.785236\n",
      "train loss:   1.078848\n",
      "train loss:   0.840138\n",
      "train loss:   0.829097\n",
      "train loss:   1.040870\n",
      "train loss:   0.589163\n",
      "train loss:   0.542973\n",
      "train loss:   0.998976\n",
      "train loss:   0.801553\n",
      "train loss:   1.000468\n",
      "train loss:   1.083221\n",
      "train loss:   0.890630\n",
      "train loss:   0.972741\n",
      "train loss:   1.157505\n",
      "train loss:   1.176542\n",
      "train loss:   1.054196\n",
      "train loss:   1.352561\n",
      "train loss:   1.067401\n",
      "train loss:   0.837375\n",
      "train loss:   0.840064\n",
      "train loss:   1.005401\n",
      "train loss:   1.259284\n",
      "train loss:   0.916552\n",
      "train loss:   0.944138\n",
      "train loss:   1.087985\n",
      "train loss:   1.064152\n",
      "train loss:   1.179925\n",
      "train loss:   1.022318\n",
      "train loss:   0.624955\n",
      "train loss:   0.840940\n",
      "train loss:   0.766279\n",
      "train loss:   1.391237\n",
      "########### epoch 196 ###########\n",
      "########### loop 36700 ###########\n",
      "test loss:   0.229389   test accuracy:   0.916667\n",
      "########### loop 36700 ###########\n",
      "train loss:   0.830962\n",
      "train loss:   0.943022\n",
      "train loss:   0.598204\n",
      "train loss:   0.993856\n",
      "train loss:   1.200383\n",
      "train loss:   0.807815\n",
      "train loss:   0.828479\n",
      "train loss:   0.872126\n",
      "train loss:   1.052230\n",
      "train loss:   1.021704\n",
      "train loss:   1.472345\n",
      "train loss:   1.011464\n",
      "train loss:   0.998218\n",
      "train loss:   0.779578\n",
      "train loss:   1.131809\n",
      "train loss:   0.920447\n",
      "train loss:   1.207936\n",
      "train loss:   1.012154\n",
      "train loss:   0.736208\n",
      "train loss:   0.784297\n",
      "train loss:   1.057160\n",
      "train loss:   0.937929\n",
      "train loss:   0.868055\n",
      "train loss:   1.020363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.829568\n",
      "train loss:   0.882400\n",
      "train loss:   0.913965\n",
      "train loss:   1.047444\n",
      "train loss:   0.971555\n",
      "train loss:   1.125588\n",
      "train loss:   1.103075\n",
      "train loss:   0.843240\n",
      "train loss:   0.901331\n",
      "train loss:   1.134526\n",
      "train loss:   1.232153\n",
      "train loss:   0.995118\n",
      "train loss:   1.039089\n",
      "train loss:   1.151705\n",
      "train loss:   1.154601\n",
      "train loss:   0.513249\n",
      "train loss:   1.062318\n",
      "train loss:   0.856046\n",
      "train loss:   0.991575\n",
      "train loss:   0.724980\n",
      "train loss:   1.073919\n",
      "train loss:   0.783942\n",
      "train loss:   0.834556\n",
      "train loss:   1.098016\n",
      "train loss:   0.880003\n",
      "train loss:   1.193455\n",
      "########### epoch 196 ###########\n",
      "########### loop 36750 ###########\n",
      "test loss:   0.373984   test accuracy:   0.875000\n",
      "########### loop 36750 ###########\n",
      "train loss:   0.986334\n",
      "train loss:   0.708840\n",
      "train loss:   0.812642\n",
      "train loss:   0.915163\n",
      "train loss:   0.686805\n",
      "train loss:   1.119224\n",
      "train loss:   1.072977\n",
      "train loss:   0.980434\n",
      "train loss:   0.606378\n",
      "train loss:   1.106594\n",
      "train loss:   0.877239\n",
      "train loss:   1.221024\n",
      "train loss:   0.787851\n",
      "train loss:   0.774736\n",
      "train loss:   1.178366\n",
      "train loss:   0.950331\n",
      "train loss:   0.811862\n",
      "train loss:   1.153774\n",
      "train loss:   1.055008\n",
      "train loss:   0.822055\n",
      "train loss:   0.951701\n",
      "train loss:   1.052507\n",
      "train loss:   1.269819\n",
      "train loss:   1.161587\n",
      "train loss:   1.071817\n",
      "train loss:   0.939550\n",
      "train loss:   1.010687\n",
      "train loss:   0.769318\n",
      "train loss:   1.052349\n",
      "train loss:   0.969321\n",
      "train loss:   0.913451\n",
      "train loss:   1.001950\n",
      "train loss:   0.885064\n",
      "train loss:   0.839483\n",
      "train loss:   1.088219\n",
      "train loss:   1.130658\n",
      "train loss:   1.086874\n",
      "train loss:   1.217248\n",
      "train loss:   0.886328\n",
      "train loss:   0.512524\n",
      "train loss:   1.304773\n",
      "train loss:   1.018851\n",
      "train loss:   1.119223\n",
      "train loss:   0.694648\n",
      "train loss:   0.673887\n",
      "train loss:   1.184511\n",
      "train loss:   0.766607\n",
      "train loss:   0.721832\n",
      "train loss:   1.427831\n",
      "train loss:   0.955306\n",
      "########### epoch 196 ###########\n",
      "########### loop 36800 ###########\n",
      "test loss:   0.186778   test accuracy:   0.916667\n",
      "########### loop 36800 ###########\n",
      "train loss:   0.960935\n",
      "train loss:   1.005267\n",
      "train loss:   0.804624\n",
      "train loss:   0.799268\n",
      "train loss:   1.218895\n",
      "train loss:   0.949140\n",
      "train loss:   1.115176\n",
      "train loss:   0.743541\n",
      "train loss:   0.878630\n",
      "train loss:   0.964887\n",
      "train loss:   0.610173\n",
      "train loss:   1.154754\n",
      "train loss:   1.260308\n",
      "train loss:   1.135055\n",
      "train loss:   1.173146\n",
      "train loss:   0.899448\n",
      "train loss:   1.366556\n",
      "train loss:   1.018089\n",
      "train loss:   1.180291\n",
      "train loss:   1.018880\n",
      "train loss:   0.945335\n",
      "train loss:   0.746123\n",
      "train loss:   0.851116\n",
      "train loss:   0.690985\n",
      "train loss:   0.902433\n",
      "train loss:   1.157200\n",
      "train loss:   0.809579\n",
      "train loss:   0.732056\n",
      "train loss:   0.698401\n",
      "train loss:   0.959545\n",
      "train loss:   0.864171\n",
      "train loss:   1.016693\n",
      "train loss:   0.923756\n",
      "train loss:   1.188849\n",
      "train loss:   1.335088\n",
      "train loss:   1.061625\n",
      "train loss:   1.008138\n",
      "train loss:   0.850614\n",
      "train loss:   0.927161\n",
      "train loss:   0.878192\n",
      "train loss:   0.993636\n",
      "train loss:   0.883734\n",
      "train loss:   0.531479\n",
      "train loss:   1.249061\n",
      "train loss:   1.317365\n",
      "train loss:   0.789913\n",
      "train loss:   0.949139\n",
      "train loss:   1.098141\n",
      "train loss:   1.151741\n",
      "train loss:   1.047845\n",
      "########### epoch 197 ###########\n",
      "########### loop 36850 ###########\n",
      "test loss:   0.426718   test accuracy:   0.833333\n",
      "########### loop 36850 ###########\n",
      "train loss:   0.793511\n",
      "train loss:   1.150778\n",
      "train loss:   1.090487\n",
      "train loss:   0.685661\n",
      "train loss:   0.648041\n",
      "train loss:   0.972390\n",
      "train loss:   0.890146\n",
      "train loss:   0.825756\n",
      "train loss:   1.025392\n",
      "train loss:   0.884731\n",
      "train loss:   1.054732\n",
      "train loss:   0.802354\n",
      "train loss:   1.250843\n",
      "train loss:   1.082068\n",
      "train loss:   0.905199\n",
      "train loss:   0.625444\n",
      "train loss:   0.655655\n",
      "train loss:   1.298233\n",
      "train loss:   1.134620\n",
      "train loss:   0.703177\n",
      "train loss:   0.822500\n",
      "train loss:   0.983811\n",
      "train loss:   0.721387\n",
      "train loss:   0.726733\n",
      "train loss:   1.145954\n",
      "train loss:   1.007512\n",
      "train loss:   0.755173\n",
      "train loss:   1.036595\n",
      "train loss:   1.366208\n",
      "train loss:   1.432489\n",
      "train loss:   1.162426\n",
      "train loss:   0.968926\n",
      "train loss:   0.744692\n",
      "train loss:   1.444031\n",
      "train loss:   0.724077\n",
      "train loss:   0.775157\n",
      "train loss:   0.875307\n",
      "train loss:   0.991940\n",
      "train loss:   0.945068\n",
      "train loss:   0.881155\n",
      "train loss:   1.030491\n",
      "train loss:   0.866247\n",
      "train loss:   1.254506\n",
      "train loss:   0.745583\n",
      "train loss:   0.809728\n",
      "train loss:   0.926906\n",
      "train loss:   1.083901\n",
      "train loss:   1.077955\n",
      "train loss:   1.021528\n",
      "train loss:   1.483508\n",
      "########### epoch 197 ###########\n",
      "########### loop 36900 ###########\n",
      "test loss:   0.137703   test accuracy:   1.000000\n",
      "########### loop 36900 ###########\n",
      "train loss:   1.020036\n",
      "train loss:   0.729249\n",
      "train loss:   1.014548\n",
      "train loss:   0.765730\n",
      "train loss:   1.021387\n",
      "train loss:   1.264870\n",
      "train loss:   1.065169\n",
      "train loss:   0.755237\n",
      "train loss:   0.904932\n",
      "train loss:   0.943165\n",
      "train loss:   1.100850\n",
      "train loss:   0.926247\n",
      "train loss:   1.392345\n",
      "train loss:   0.957626\n",
      "train loss:   1.362395\n",
      "train loss:   0.894020\n",
      "train loss:   1.177430\n",
      "train loss:   0.812128\n",
      "train loss:   0.999422\n",
      "train loss:   0.929622\n",
      "train loss:   0.938658\n",
      "train loss:   1.187293\n",
      "train loss:   1.123098\n",
      "train loss:   0.882960\n",
      "train loss:   0.893256\n",
      "train loss:   0.912910\n",
      "train loss:   0.580709\n",
      "train loss:   1.193748\n",
      "train loss:   0.896596\n",
      "train loss:   1.059385\n",
      "train loss:   0.918624\n",
      "train loss:   1.078378\n",
      "train loss:   0.791369\n",
      "train loss:   0.942853\n",
      "train loss:   1.099910\n",
      "train loss:   1.007360\n",
      "train loss:   0.932694\n",
      "train loss:   0.688482\n",
      "train loss:   1.028222\n",
      "train loss:   1.188024\n",
      "train loss:   0.925110\n",
      "train loss:   0.976898\n",
      "train loss:   1.133292\n",
      "train loss:   0.653125\n",
      "train loss:   0.913813\n",
      "train loss:   0.644506\n",
      "train loss:   0.777907\n",
      "train loss:   1.330774\n",
      "train loss:   1.346648\n",
      "train loss:   1.339316\n",
      "########### epoch 197 ###########\n",
      "########### loop 36950 ###########\n",
      "test loss:   0.269029   test accuracy:   0.916667\n",
      "########### loop 36950 ###########\n",
      "train loss:   1.157836\n",
      "train loss:   0.837481\n",
      "train loss:   0.659907\n",
      "train loss:   0.796804\n",
      "train loss:   0.904069\n",
      "train loss:   0.950938\n",
      "train loss:   0.911070\n",
      "train loss:   1.315910\n",
      "train loss:   0.865939\n",
      "train loss:   0.769447\n",
      "train loss:   0.953831\n",
      "train loss:   0.866072\n",
      "train loss:   1.032989\n",
      "train loss:   0.713836\n",
      "train loss:   0.969910\n",
      "train loss:   0.848162\n",
      "train loss:   0.708243\n",
      "train loss:   0.859359\n",
      "train loss:   1.036166\n",
      "train loss:   1.072680\n",
      "train loss:   0.951160\n",
      "train loss:   1.029505\n",
      "train loss:   0.674313\n",
      "train loss:   0.947254\n",
      "train loss:   1.140455\n",
      "train loss:   0.688025\n",
      "train loss:   0.878384\n",
      "train loss:   0.583798\n",
      "train loss:   0.870259\n",
      "train loss:   1.036857\n",
      "train loss:   1.348891\n",
      "train loss:   0.737135\n",
      "train loss:   1.027687\n",
      "train loss:   1.025812\n",
      "train loss:   0.914545\n",
      "train loss:   1.396624\n",
      "train loss:   1.199925\n",
      "train loss:   1.085844\n",
      "train loss:   1.015931\n",
      "train loss:   1.244314\n",
      "train loss:   1.063835\n",
      "train loss:   0.703616\n",
      "train loss:   0.991738\n",
      "train loss:   0.943938\n",
      "train loss:   1.117023\n",
      "train loss:   0.973885\n",
      "train loss:   0.789482\n",
      "train loss:   0.648429\n",
      "train loss:   0.699365\n",
      "train loss:   0.629172\n",
      "########### epoch 197 ###########\n",
      "########### loop 37000 ###########\n",
      "test loss:   0.364777   test accuracy:   0.875000\n",
      "########### loop 37000 ###########\n",
      "train loss:   0.745662\n",
      "train loss:   1.176813\n",
      "train loss:   0.940083\n",
      "train loss:   0.825535\n",
      "train loss:   0.866105\n",
      "train loss:   1.067283\n",
      "train loss:   1.170673\n",
      "train loss:   0.643402\n",
      "train loss:   0.845566\n",
      "train loss:   1.128257\n",
      "train loss:   1.004609\n",
      "train loss:   0.726677\n",
      "train loss:   0.392928\n",
      "train loss:   0.623306\n",
      "train loss:   1.174637\n",
      "train loss:   1.016585\n",
      "train loss:   1.027486\n",
      "train loss:   1.180065\n",
      "train loss:   0.866990\n",
      "train loss:   0.978135\n",
      "train loss:   1.281434\n",
      "train loss:   0.505696\n",
      "train loss:   1.062762\n",
      "train loss:   0.876549\n",
      "train loss:   0.936715\n",
      "train loss:   0.941866\n",
      "train loss:   0.707474\n",
      "train loss:   0.477798\n",
      "train loss:   1.018052\n",
      "train loss:   0.894976\n",
      "train loss:   1.054652\n",
      "train loss:   0.974039\n",
      "train loss:   0.917255\n",
      "train loss:   0.708434\n",
      "train loss:   0.762039\n",
      "train loss:   0.594117\n",
      "train loss:   0.746235\n",
      "train loss:   0.709629\n",
      "train loss:   0.909416\n",
      "train loss:   0.948410\n",
      "train loss:   1.067066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.396979\n",
      "train loss:   0.859125\n",
      "train loss:   1.056163\n",
      "train loss:   0.925622\n",
      "train loss:   0.767188\n",
      "train loss:   1.057406\n",
      "train loss:   0.615390\n",
      "train loss:   1.351120\n",
      "train loss:   0.788239\n",
      "########### epoch 198 ###########\n",
      "########### loop 37050 ###########\n",
      "test loss:   0.155537   test accuracy:   1.000000\n",
      "########### loop 37050 ###########\n",
      "train loss:   1.112106\n",
      "train loss:   0.861732\n",
      "train loss:   1.047012\n",
      "train loss:   1.194531\n",
      "train loss:   1.286505\n",
      "train loss:   1.001562\n",
      "train loss:   0.847456\n",
      "train loss:   1.070141\n",
      "train loss:   0.902062\n",
      "train loss:   0.699558\n",
      "train loss:   0.919065\n",
      "train loss:   0.909393\n",
      "train loss:   1.368153\n",
      "train loss:   0.939400\n",
      "train loss:   0.660377\n",
      "train loss:   0.762833\n",
      "train loss:   1.031051\n",
      "train loss:   1.109593\n",
      "train loss:   0.864013\n",
      "train loss:   1.222270\n",
      "train loss:   0.903784\n",
      "train loss:   1.409072\n",
      "train loss:   0.407273\n",
      "train loss:   0.966489\n",
      "train loss:   0.697725\n",
      "train loss:   1.062691\n",
      "train loss:   0.967436\n",
      "train loss:   0.772765\n",
      "train loss:   0.866631\n",
      "train loss:   0.802482\n",
      "train loss:   0.873448\n",
      "train loss:   1.294460\n",
      "train loss:   1.216062\n",
      "train loss:   0.968955\n",
      "train loss:   0.755473\n",
      "train loss:   1.130948\n",
      "train loss:   1.014790\n",
      "train loss:   0.841760\n",
      "train loss:   0.494608\n",
      "train loss:   0.900845\n",
      "train loss:   1.209460\n",
      "train loss:   0.710321\n",
      "train loss:   1.008346\n",
      "train loss:   1.066585\n",
      "train loss:   1.160105\n",
      "train loss:   0.956502\n",
      "train loss:   1.083201\n",
      "train loss:   1.079527\n",
      "train loss:   0.799616\n",
      "train loss:   0.848955\n",
      "########### epoch 198 ###########\n",
      "########### loop 37100 ###########\n",
      "test loss:   0.264285   test accuracy:   0.916667\n",
      "########### loop 37100 ###########\n",
      "train loss:   0.958661\n",
      "train loss:   0.552070\n",
      "train loss:   0.882411\n",
      "train loss:   0.817447\n",
      "train loss:   1.185666\n",
      "train loss:   1.307192\n",
      "train loss:   0.969030\n",
      "train loss:   0.539505\n",
      "train loss:   0.876324\n",
      "train loss:   0.777612\n",
      "train loss:   0.857431\n",
      "train loss:   0.559649\n",
      "train loss:   1.032504\n",
      "train loss:   0.654136\n",
      "train loss:   0.942223\n",
      "train loss:   0.622652\n",
      "train loss:   0.781613\n",
      "train loss:   1.040288\n",
      "train loss:   1.489230\n",
      "train loss:   1.024656\n",
      "train loss:   1.193819\n",
      "train loss:   0.679446\n",
      "train loss:   1.036484\n",
      "train loss:   0.605848\n",
      "train loss:   0.969648\n",
      "train loss:   0.844686\n",
      "train loss:   0.705747\n",
      "train loss:   0.911757\n",
      "train loss:   1.055899\n",
      "train loss:   0.524659\n",
      "train loss:   1.107535\n",
      "train loss:   0.804597\n",
      "train loss:   0.774623\n",
      "train loss:   0.619285\n",
      "train loss:   0.968563\n",
      "train loss:   1.175574\n",
      "train loss:   0.880577\n",
      "train loss:   1.432440\n",
      "train loss:   0.861055\n",
      "train loss:   0.844392\n",
      "train loss:   1.176077\n",
      "train loss:   0.789406\n",
      "train loss:   1.139336\n",
      "train loss:   0.981398\n",
      "train loss:   0.910023\n",
      "train loss:   1.337834\n",
      "train loss:   0.822396\n",
      "train loss:   0.703677\n",
      "train loss:   0.999130\n",
      "train loss:   1.027627\n",
      "########### epoch 198 ###########\n",
      "########### loop 37150 ###########\n",
      "test loss:   0.167644   test accuracy:   0.958333\n",
      "########### loop 37150 ###########\n",
      "train loss:   0.974537\n",
      "train loss:   1.135281\n",
      "train loss:   0.950436\n",
      "train loss:   0.849541\n",
      "train loss:   1.053515\n",
      "train loss:   0.531395\n",
      "train loss:   1.154316\n",
      "train loss:   0.957579\n",
      "train loss:   1.198105\n",
      "train loss:   0.817758\n",
      "train loss:   0.886738\n",
      "train loss:   0.761374\n",
      "train loss:   0.648663\n",
      "train loss:   0.845241\n",
      "train loss:   0.936447\n",
      "train loss:   1.148397\n",
      "train loss:   0.789123\n",
      "train loss:   1.074815\n",
      "train loss:   0.686852\n",
      "train loss:   0.598796\n",
      "train loss:   0.893281\n",
      "train loss:   0.930916\n",
      "train loss:   0.911255\n",
      "train loss:   0.722537\n",
      "train loss:   0.973524\n",
      "train loss:   1.430421\n",
      "train loss:   0.997481\n",
      "train loss:   0.891873\n",
      "train loss:   0.868650\n",
      "train loss:   0.776610\n",
      "train loss:   1.094029\n",
      "train loss:   0.892571\n",
      "train loss:   0.841991\n",
      "train loss:   1.061031\n",
      "train loss:   1.147642\n",
      "train loss:   1.056160\n",
      "train loss:   1.153755\n",
      "train loss:   1.219488\n",
      "train loss:   0.677975\n",
      "train loss:   0.504679\n",
      "train loss:   0.963382\n",
      "train loss:   1.280319\n",
      "train loss:   1.176742\n",
      "train loss:   0.899706\n",
      "train loss:   0.900028\n",
      "train loss:   1.302802\n",
      "train loss:   0.730344\n",
      "train loss:   1.125667\n",
      "train loss:   0.928953\n",
      "train loss:   1.022941\n",
      "########### epoch 198 ###########\n",
      "########### loop 37200 ###########\n",
      "test loss:   0.427746   test accuracy:   0.875000\n",
      "########### loop 37200 ###########\n",
      "train loss:   0.997337\n",
      "train loss:   0.888852\n",
      "train loss:   0.723489\n",
      "train loss:   0.910603\n",
      "train loss:   1.134656\n",
      "train loss:   0.757337\n",
      "train loss:   1.077213\n",
      "train loss:   0.983711\n",
      "train loss:   0.958555\n",
      "train loss:   1.113439\n",
      "train loss:   1.091239\n",
      "train loss:   1.105100\n",
      "train loss:   0.894765\n",
      "train loss:   1.226864\n",
      "train loss:   1.000713\n",
      "train loss:   1.048120\n",
      "train loss:   0.895552\n",
      "train loss:   0.925591\n",
      "train loss:   1.170421\n",
      "train loss:   1.010281\n",
      "train loss:   0.707132\n",
      "train loss:   0.958933\n",
      "train loss:   1.013552\n",
      "train loss:   1.057722\n",
      "train loss:   1.036966\n",
      "train loss:   0.905552\n",
      "train loss:   1.037712\n",
      "train loss:   1.026170\n",
      "train loss:   1.045596\n",
      "train loss:   1.148769\n",
      "train loss:   1.065566\n",
      "train loss:   1.086436\n",
      "train loss:   1.043733\n",
      "train loss:   1.208507\n",
      "train loss:   1.360232\n",
      "train loss:   1.143796\n",
      "train loss:   1.175262\n",
      "train loss:   1.241147\n",
      "train loss:   1.012462\n",
      "train loss:   1.049510\n",
      "train loss:   1.208806\n",
      "train loss:   0.769780\n",
      "train loss:   0.993398\n",
      "train loss:   1.267575\n",
      "train loss:   0.770915\n",
      "train loss:   1.224705\n",
      "train loss:   1.013969\n",
      "train loss:   0.896641\n",
      "train loss:   0.826236\n",
      "train loss:   1.120127\n",
      "########### epoch 199 ###########\n",
      "########### loop 37250 ###########\n",
      "test loss:   0.130398   test accuracy:   0.958333\n",
      "########### loop 37250 ###########\n",
      "train loss:   0.554983\n",
      "train loss:   0.891390\n",
      "train loss:   1.173713\n",
      "train loss:   1.142018\n",
      "train loss:   1.107160\n",
      "train loss:   1.168465\n",
      "train loss:   1.019133\n",
      "train loss:   1.135355\n",
      "train loss:   1.482479\n",
      "train loss:   0.631991\n",
      "train loss:   1.182379\n",
      "train loss:   0.969807\n",
      "train loss:   0.882846\n",
      "train loss:   1.028413\n",
      "train loss:   1.127909\n",
      "train loss:   0.884883\n",
      "train loss:   1.092970\n",
      "train loss:   0.737458\n",
      "train loss:   0.771582\n",
      "train loss:   1.296993\n",
      "train loss:   0.972317\n",
      "train loss:   0.986199\n",
      "train loss:   1.005490\n",
      "train loss:   0.893174\n",
      "train loss:   1.056881\n",
      "train loss:   0.992476\n",
      "train loss:   0.954591\n",
      "train loss:   0.805672\n",
      "train loss:   1.207842\n",
      "train loss:   0.918844\n",
      "train loss:   0.768943\n",
      "train loss:   1.202183\n",
      "train loss:   1.266109\n",
      "train loss:   1.158490\n",
      "train loss:   1.043930\n",
      "train loss:   0.996696\n",
      "train loss:   1.003734\n",
      "train loss:   0.813193\n",
      "train loss:   0.889972\n",
      "train loss:   1.066648\n",
      "train loss:   1.159830\n",
      "train loss:   0.711244\n",
      "train loss:   1.239903\n",
      "train loss:   0.976044\n",
      "train loss:   0.883833\n",
      "train loss:   0.934928\n",
      "train loss:   0.780506\n",
      "train loss:   0.744721\n",
      "train loss:   1.309226\n",
      "train loss:   0.691651\n",
      "########### epoch 199 ###########\n",
      "########### loop 37300 ###########\n",
      "test loss:   0.309225   test accuracy:   0.875000\n",
      "########### loop 37300 ###########\n",
      "train loss:   1.096522\n",
      "train loss:   1.133254\n",
      "train loss:   0.969759\n",
      "train loss:   1.293606\n",
      "train loss:   1.054064\n",
      "train loss:   1.004126\n",
      "train loss:   1.227780\n",
      "train loss:   0.872983\n",
      "train loss:   0.862650\n",
      "train loss:   0.963936\n",
      "train loss:   1.240633\n",
      "train loss:   1.001129\n",
      "train loss:   0.932597\n",
      "train loss:   0.970080\n",
      "train loss:   0.849286\n",
      "train loss:   1.163864\n",
      "train loss:   1.053260\n",
      "train loss:   0.766056\n",
      "train loss:   1.073328\n",
      "train loss:   0.951605\n",
      "train loss:   0.802477\n",
      "train loss:   0.965893\n",
      "train loss:   0.977542\n",
      "train loss:   0.799942\n",
      "train loss:   0.878271\n",
      "train loss:   0.832990\n",
      "train loss:   1.049884\n",
      "train loss:   0.590268\n",
      "train loss:   1.170104\n",
      "train loss:   1.025464\n",
      "train loss:   1.074143\n",
      "train loss:   0.641717\n",
      "train loss:   0.830838\n",
      "train loss:   0.944628\n",
      "train loss:   1.000808\n",
      "train loss:   1.394255\n",
      "train loss:   0.686880\n",
      "train loss:   0.880344\n",
      "train loss:   1.009840\n",
      "train loss:   1.444362\n",
      "train loss:   1.037668\n",
      "train loss:   0.744189\n",
      "train loss:   0.987023\n",
      "train loss:   0.887543\n",
      "train loss:   0.959399\n",
      "train loss:   0.585760\n",
      "train loss:   0.808287\n",
      "train loss:   1.033140\n",
      "train loss:   0.700956\n",
      "train loss:   0.885926\n",
      "########### epoch 199 ###########\n",
      "########### loop 37350 ###########\n",
      "test loss:   0.150599   test accuracy:   0.958333\n",
      "########### loop 37350 ###########\n",
      "train loss:   1.001474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   0.960229\n",
      "train loss:   1.169029\n",
      "train loss:   0.735311\n",
      "train loss:   0.775417\n",
      "train loss:   0.955036\n",
      "train loss:   0.893158\n",
      "train loss:   0.916462\n",
      "train loss:   1.045383\n",
      "train loss:   0.876334\n",
      "train loss:   0.802858\n",
      "train loss:   0.944323\n",
      "train loss:   1.180802\n",
      "train loss:   0.837481\n",
      "train loss:   1.216418\n",
      "train loss:   0.876014\n",
      "train loss:   0.889124\n",
      "train loss:   0.993264\n",
      "train loss:   0.459879\n",
      "train loss:   0.794501\n",
      "train loss:   0.716717\n",
      "train loss:   0.810923\n",
      "train loss:   1.009451\n",
      "train loss:   0.860826\n",
      "train loss:   0.586444\n",
      "train loss:   0.941597\n",
      "train loss:   1.101914\n",
      "train loss:   0.855387\n",
      "train loss:   0.595361\n",
      "train loss:   1.130717\n",
      "train loss:   0.984826\n",
      "train loss:   1.068027\n",
      "train loss:   1.068730\n",
      "train loss:   0.761801\n",
      "train loss:   0.737065\n",
      "train loss:   0.806993\n",
      "train loss:   1.068618\n",
      "train loss:   1.015931\n",
      "train loss:   1.241793\n",
      "train loss:   0.935030\n",
      "train loss:   1.234833\n",
      "train loss:   0.880537\n",
      "train loss:   1.145246\n",
      "train loss:   0.613566\n",
      "train loss:   1.039027\n",
      "train loss:   0.908248\n",
      "train loss:   0.799482\n",
      "train loss:   1.147563\n",
      "train loss:   0.948154\n",
      "train loss:   1.155380\n",
      "########### epoch 199 ###########\n",
      "########### loop 37400 ###########\n",
      "test loss:   0.253663   test accuracy:   0.875000\n",
      "########### loop 37400 ###########\n",
      "train loss:   0.798066\n",
      "train loss:   1.004895\n",
      "train loss:   0.890001\n",
      "train loss:   0.923121\n",
      "train loss:   0.949339\n",
      "train loss:   0.955439\n",
      "train loss:   1.272254\n",
      "train loss:   1.299444\n",
      "train loss:   1.034755\n",
      "train loss:   0.716579\n",
      "train loss:   1.016721\n",
      "train loss:   0.795714\n",
      "train loss:   0.831817\n",
      "train loss:   0.936994\n",
      "train loss:   0.773406\n",
      "train loss:   0.887681\n",
      "train loss:   1.055826\n",
      "train loss:   1.097361\n",
      "train loss:   0.787340\n",
      "train loss:   1.083356\n",
      "train loss:   1.135912\n",
      "train loss:   0.928598\n",
      "train loss:   0.981023\n",
      "train loss:   0.653293\n",
      "train loss:   0.770193\n",
      "train loss:   0.910221\n",
      "train loss:   0.852899\n",
      "train loss:   0.692031\n",
      "train loss:   1.315951\n",
      "train loss:   0.982795\n",
      "train loss:   0.992671\n",
      "train loss:   0.821015\n",
      "train loss:   1.019801\n",
      "train loss:   0.905612\n",
      "train loss:   1.072465\n",
      "train loss:   1.126916\n",
      "train loss:   1.118545\n",
      "train loss:   0.717568\n",
      "train loss:   1.273603\n",
      "train loss:   0.655325\n",
      "train loss:   0.882134\n",
      "train loss:   1.225226\n",
      "train loss:   0.983624\n",
      "train loss:   1.165518\n",
      "train loss:   0.791338\n",
      "train loss:   0.874109\n",
      "train loss:   0.834516\n",
      "train loss:   0.893979\n",
      "train loss:   0.667132\n",
      "train loss:   1.005332\n",
      "########### epoch 200 ###########\n",
      "########### loop 37450 ###########\n",
      "test loss:   0.172280   test accuracy:   0.916667\n",
      "########### loop 37450 ###########\n",
      "train loss:   0.727738\n",
      "train loss:   1.148769\n",
      "train loss:   0.996047\n",
      "train loss:   0.815842\n",
      "train loss:   0.567501\n",
      "train loss:   1.060229\n",
      "train loss:   1.220824\n",
      "train loss:   0.878676\n",
      "train loss:   0.948692\n",
      "train loss:   1.229846\n",
      "train loss:   0.773736\n",
      "train loss:   1.106336\n",
      "train loss:   0.774847\n",
      "train loss:   1.109217\n",
      "train loss:   1.384343\n",
      "train loss:   0.977253\n",
      "train loss:   0.691435\n",
      "train loss:   0.895658\n",
      "train loss:   0.904101\n",
      "train loss:   1.112372\n",
      "train loss:   1.097541\n",
      "train loss:   0.660230\n",
      "train loss:   1.027533\n",
      "train loss:   0.784400\n",
      "train loss:   1.036004\n",
      "train loss:   0.923059\n",
      "train loss:   0.718750\n",
      "train loss:   0.729478\n",
      "train loss:   0.918460\n",
      "train loss:   0.750807\n",
      "train loss:   0.927768\n",
      "train loss:   0.882999\n",
      "train loss:   0.835413\n",
      "train loss:   1.032837\n",
      "train loss:   0.756577\n",
      "train loss:   0.927751\n",
      "train loss:   0.600885\n",
      "train loss:   0.693087\n",
      "train loss:   1.008173\n",
      "train loss:   0.938734\n",
      "train loss:   1.020186\n",
      "train loss:   1.004284\n",
      "train loss:   1.204286\n",
      "train loss:   0.830470\n",
      "train loss:   0.789814\n",
      "train loss:   0.758988\n",
      "train loss:   1.228739\n",
      "train loss:   0.916667\n",
      "train loss:   0.822678\n",
      "train loss:   1.013858\n",
      "########### epoch 200 ###########\n",
      "########### loop 37500 ###########\n",
      "test loss:   0.242935   test accuracy:   0.916667\n",
      "########### loop 37500 ###########\n",
      "train loss:   0.597719\n",
      "train loss:   0.690846\n",
      "train loss:   1.042679\n",
      "train loss:   1.286394\n",
      "train loss:   1.295148\n",
      "train loss:   0.960354\n",
      "train loss:   1.102679\n",
      "train loss:   0.658722\n",
      "train loss:   1.096833\n",
      "train loss:   1.047578\n",
      "train loss:   1.009050\n",
      "train loss:   0.955287\n",
      "train loss:   1.407440\n",
      "train loss:   0.756328\n",
      "train loss:   1.018284\n",
      "train loss:   0.736130\n",
      "train loss:   1.309492\n",
      "train loss:   0.926331\n",
      "train loss:   0.804778\n",
      "train loss:   0.780939\n",
      "train loss:   1.050075\n",
      "train loss:   1.188794\n",
      "train loss:   0.767366\n",
      "train loss:   0.647521\n",
      "train loss:   1.134293\n",
      "train loss:   0.966442\n",
      "train loss:   0.941975\n",
      "train loss:   1.019564\n",
      "train loss:   1.091056\n",
      "train loss:   0.837492\n",
      "train loss:   1.042421\n",
      "train loss:   0.833832\n",
      "train loss:   1.188094\n",
      "train loss:   0.962514\n",
      "train loss:   0.866998\n",
      "train loss:   1.014238\n",
      "train loss:   1.227802\n",
      "train loss:   1.361623\n",
      "train loss:   0.801077\n",
      "train loss:   1.142650\n",
      "train loss:   0.604527\n",
      "train loss:   0.749786\n",
      "train loss:   1.268351\n",
      "train loss:   1.113547\n",
      "train loss:   1.245891\n",
      "train loss:   1.135242\n",
      "train loss:   0.790253\n",
      "train loss:   0.982441\n",
      "train loss:   0.970373\n",
      "train loss:   0.754228\n",
      "########### epoch 200 ###########\n",
      "########### loop 37550 ###########\n",
      "test loss:   0.216045   test accuracy:   0.916667\n",
      "########### loop 37550 ###########\n",
      "train loss:   1.001268\n",
      "train loss:   1.098132\n",
      "train loss:   0.740813\n",
      "train loss:   0.713642\n",
      "train loss:   0.929597\n",
      "train loss:   1.096664\n",
      "train loss:   0.745431\n",
      "train loss:   0.943581\n",
      "train loss:   0.895171\n",
      "train loss:   0.565028\n",
      "train loss:   0.985579\n",
      "train loss:   0.959706\n",
      "train loss:   0.935425\n",
      "train loss:   0.921043\n",
      "train loss:   0.892220\n",
      "train loss:   0.733681\n",
      "train loss:   0.925595\n",
      "train loss:   1.315923\n",
      "train loss:   0.995078\n",
      "train loss:   1.129166\n",
      "train loss:   0.899756\n",
      "train loss:   0.876988\n",
      "train loss:   1.127886\n",
      "train loss:   0.712723\n",
      "train loss:   1.265335\n",
      "train loss:   0.985217\n",
      "train loss:   0.884888\n",
      "train loss:   1.156632\n",
      "train loss:   1.226267\n",
      "train loss:   1.190768\n",
      "train loss:   1.120514\n",
      "train loss:   1.279282\n",
      "train loss:   0.889039\n",
      "train loss:   1.073622\n",
      "train loss:   0.765147\n",
      "train loss:   0.714590\n",
      "train loss:   0.898684\n",
      "train loss:   0.774914\n",
      "train loss:   0.953595\n",
      "train loss:   0.968994\n",
      "train loss:   1.157695\n",
      "train loss:   1.143093\n",
      "train loss:   0.822460\n",
      "train loss:   1.179981\n",
      "train loss:   1.049985\n",
      "train loss:   1.111051\n",
      "train loss:   0.768146\n",
      "train loss:   1.002343\n",
      "train loss:   0.820130\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "# build input\n",
    "imgs = tf.placeholder(tf.float32, shape = [None, image_size, image_size, 3], name = \"input_x\")\n",
    "targets = tf.placeholder(tf.int32, shape = [None, n_classes], name = \"input_y\")\n",
    "keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "\n",
    "# build cal graph\n",
    "bilinear_cnn = vgg_bilinear_model(imgs, \n",
    "                                  keep_prob, \n",
    "                                  weights_file = weights_file, \n",
    "                                  weights_file_last = weights_file_last_layer, \n",
    "                                  sess = sess, \n",
    "                                  finetune = True)\n",
    "\n",
    "# build cal node parameter\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = targets, logits = bilinear_cnn.fc3l)\n",
    "cost = tf.reduce_mean(cross_entropy, name = \"cost\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(bilinear_cnn.fc3l, name = \"predicted\")\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name = \"accuracy\")\n",
    "\n",
    "# start train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "bilinear_cnn.load_vgg_weights(sess)\n",
    "bilinear_cnn.load_last_layer_weights(sess)\n",
    "\n",
    "for v in tf.trainable_variables():\n",
    "    print(\"Trainable variables\", v)\n",
    "print('Starting training')\n",
    "\n",
    "\n",
    "###################################\n",
    "train_img_batch, train_label_batch = get_batchs(tfrecord_filename = \"train.tfrecords\", \n",
    "                                                    image_size = image_size, classes = n_classes,\n",
    "                                                    batch_size = batch_size, min_after_dequeue = 500,\n",
    "                                                   use_data_enhancement = True)\n",
    "test_img_batch, test_label_batch = get_batchs(tfrecord_filename = \"test.tfrecords\", \n",
    "                                                  image_size = image_size, classes = n_classes,\n",
    "                                                  batch_size = batch_size, min_after_dequeue = 500,\n",
    "                                                     use_data_enhancement = False)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "for i in range(int((4512 / batch_size) * epoch)):\n",
    "#for i in range(2):\n",
    "    # get data\n",
    "    train_imgs, train_labels= sess.run([train_img_batch, train_label_batch])\n",
    "    test_imgs, test_labels= sess.run([test_img_batch, test_label_batch])\n",
    "    \n",
    "#    train_imgs = image_random_enhancement(train_imgs)\n",
    "\n",
    "    # train single loop\n",
    "    _, train_loss = sess.run([optimizer,cost], feed_dict={imgs: train_imgs, targets: train_labels, keep_prob: 0.5})\n",
    "    #train_loss = sess.run(cost, feed_dict={imgs: train_imgs, targets: train_labels, keep_prob: 1.})\n",
    "    print(\"train loss: %10f\"%(train_loss))\n",
    "            \n",
    "    # print test stat\n",
    "    if (i % 50 == 0):\n",
    "        print(\"########### epoch %d ###########\"%((i / (4512 / batch_size)) + 1))\n",
    "        print(\"########### loop %d ###########\"%(i))\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], feed_dict={imgs: test_imgs, targets: test_labels, keep_prob: 1.})\n",
    "        print(\"test loss: %10f   test accuracy: %10f\"%(test_loss, test_acc))\n",
    "        print(\"########### loop %d ###########\"%(i))\n",
    "\n",
    "# close data queue\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "\n",
    "# save last layer parameters\n",
    "last_layer_weights = []\n",
    "\n",
    "for var in bilinear_cnn.last_layer_parameters:\n",
    "    last_layer_weights.append(sess.run(var))\n",
    "\n",
    "np.savez(weights_file_last_layer, last_layer = last_layer_weights)\n",
    "\n",
    "# save conv layer parameters\n",
    "conv_layer_weights = []\n",
    "\n",
    "for var in bilinear_cnn.parameters:\n",
    "    conv_layer_weights.append(sess.run(var))\n",
    "\n",
    "np.savez(weights_file_conv, conv_layer = conv_layer_weights)\n",
    "\n",
    "print('done')\n",
    " \n",
    "# end\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
