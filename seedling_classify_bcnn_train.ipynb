{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-A 初步训练\n",
    "\n",
    "### 1.加载VGG预训练参数\n",
    "### 2.冻结CONV层，训练FC层\n",
    "### 3.保存FC层参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from data_preprocess import *\n",
    "from image_preprocess import *\n",
    "from bcnn_model import *\n",
    "from image_enhancement import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_file = \"vgg16_weights.npz\"\n",
    "weights_file_last_layer = \"bcnn_last_weights.npz\"\n",
    "weights_file_conv = \"bcnn_conv_weights.npz\"\n",
    "\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "finetune_step = -1\n",
    "\n",
    "image_size = 224\n",
    "n_classes = 12\n",
    "\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Data Augmentation\n",
      "Shape of z (?, 262144)\n",
      "Adding weights to conv1_1/b:0\n",
      "Adding weights to conv1_1/W:0\n",
      "Adding weights to conv1_2/b:0\n",
      "Adding weights to conv1_2/W:0\n",
      "Adding weights to conv2_1/b:0\n",
      "Adding weights to conv2_1/W:0\n",
      "Adding weights to conv2_2/b:0\n",
      "Adding weights to conv2_2/W:0\n",
      "Adding weights to conv3_1/b:0\n",
      "Adding weights to conv3_1/W:0\n",
      "Adding weights to conv3_2/b:0\n",
      "Adding weights to conv3_2/W:0\n",
      "Adding weights to conv3_3/b:0\n",
      "Adding weights to conv3_3/W:0\n",
      "Adding weights to conv4_1/b:0\n",
      "Adding weights to conv4_1/W:0\n",
      "Adding weights to conv4_2/b:0\n",
      "Adding weights to conv4_2/W:0\n",
      "Adding weights to conv4_3/b:0\n",
      "Adding weights to conv4_3/W:0\n",
      "Adding weights to conv5_1/b:0\n",
      "Adding weights to conv5_1/W:0\n",
      "Adding weights to conv5_2/b:0\n",
      "Adding weights to conv5_2/W:0\n",
      "Adding weights to conv5_3/b:0\n",
      "Adding weights to conv5_3/W:0\n",
      "Trainable variables <tf.Variable 'fc-new/W:0' shape=(262144, 12) dtype=float32_ref>\n",
      "Trainable variables <tf.Variable 'fc-new/b:0' shape=(12,) dtype=float32_ref>\n",
      "Starting training\n",
      "train loss:   2.518847\n",
      "########### epoch 1 ###########\n",
      "########### loop 0 ###########\n",
      "test loss:   2.487820   test accuracy:   0.000000\n",
      "########### loop 0 ###########\n",
      "train loss:   2.486809\n",
      "train loss:   2.463114\n",
      "train loss:   2.470029\n",
      "train loss:   2.479569\n",
      "train loss:   2.516902\n",
      "train loss:   2.460419\n",
      "train loss:   2.476354\n",
      "train loss:   2.547026\n",
      "train loss:   2.423296\n",
      "train loss:   2.475719\n",
      "train loss:   2.421618\n",
      "train loss:   2.474670\n",
      "train loss:   2.400992\n",
      "train loss:   2.488810\n",
      "train loss:   2.446135\n",
      "train loss:   2.483782\n",
      "train loss:   2.485851\n",
      "train loss:   2.417353\n",
      "train loss:   2.460696\n",
      "train loss:   2.378510\n",
      "train loss:   2.449651\n",
      "train loss:   2.434621\n",
      "train loss:   2.413500\n",
      "train loss:   2.440831\n",
      "train loss:   2.430896\n",
      "train loss:   2.389671\n",
      "train loss:   2.347265\n",
      "train loss:   2.401543\n",
      "train loss:   2.420824\n",
      "train loss:   2.405292\n",
      "train loss:   2.432025\n",
      "train loss:   2.444357\n",
      "train loss:   2.407414\n",
      "train loss:   2.332394\n",
      "train loss:   2.366465\n",
      "train loss:   2.391817\n",
      "train loss:   2.322762\n",
      "train loss:   2.379324\n",
      "train loss:   2.343986\n",
      "train loss:   2.451800\n",
      "train loss:   2.479523\n",
      "train loss:   2.381688\n",
      "train loss:   2.355642\n",
      "train loss:   2.538642\n",
      "train loss:   2.346600\n",
      "train loss:   2.367236\n",
      "train loss:   2.386262\n",
      "train loss:   2.303059\n",
      "train loss:   2.262403\n",
      "train loss:   2.457149\n",
      "########### epoch 1 ###########\n",
      "########### loop 50 ###########\n",
      "test loss:   2.321639   test accuracy:   0.312500\n",
      "########### loop 50 ###########\n",
      "train loss:   2.308329\n",
      "train loss:   2.347959\n",
      "train loss:   2.350355\n",
      "train loss:   2.446524\n",
      "train loss:   2.286427\n",
      "train loss:   2.329712\n",
      "train loss:   2.400478\n",
      "train loss:   2.245580\n",
      "train loss:   2.394820\n",
      "train loss:   2.294147\n",
      "train loss:   2.422023\n",
      "train loss:   2.227125\n",
      "train loss:   2.335470\n",
      "train loss:   2.363412\n",
      "train loss:   2.343667\n",
      "train loss:   2.461989\n",
      "train loss:   2.316550\n",
      "train loss:   2.457701\n",
      "train loss:   2.364205\n",
      "train loss:   2.329459\n",
      "train loss:   2.391891\n",
      "train loss:   2.388024\n",
      "train loss:   2.298476\n",
      "train loss:   2.394204\n",
      "train loss:   2.310076\n",
      "train loss:   2.311323\n",
      "train loss:   2.350328\n",
      "train loss:   2.392066\n",
      "train loss:   2.467078\n",
      "train loss:   2.282460\n",
      "train loss:   2.317794\n",
      "train loss:   2.368805\n",
      "train loss:   2.235578\n",
      "train loss:   2.217068\n",
      "train loss:   2.272129\n",
      "train loss:   2.200906\n",
      "train loss:   2.400284\n",
      "train loss:   2.185749\n",
      "train loss:   2.174116\n",
      "train loss:   2.471737\n",
      "train loss:   2.273731\n",
      "train loss:   2.362994\n",
      "train loss:   2.347426\n",
      "train loss:   2.176860\n",
      "train loss:   2.359788\n",
      "train loss:   2.312507\n",
      "train loss:   2.337734\n",
      "train loss:   2.380981\n",
      "train loss:   2.355282\n",
      "train loss:   2.241866\n",
      "########### epoch 1 ###########\n",
      "########### loop 100 ###########\n",
      "test loss:   2.326274   test accuracy:   0.281250\n",
      "########### loop 100 ###########\n",
      "train loss:   2.391531\n",
      "train loss:   2.154959\n",
      "train loss:   2.320431\n",
      "train loss:   2.393314\n",
      "train loss:   2.253434\n",
      "train loss:   2.277260\n",
      "train loss:   2.399266\n",
      "train loss:   2.162690\n",
      "train loss:   2.253976\n",
      "train loss:   2.238946\n",
      "train loss:   2.246018\n",
      "train loss:   2.170121\n",
      "train loss:   2.175742\n",
      "train loss:   2.261156\n",
      "train loss:   2.254947\n",
      "train loss:   2.323918\n",
      "train loss:   2.326446\n",
      "train loss:   2.166162\n",
      "train loss:   2.353993\n",
      "train loss:   2.330275\n",
      "train loss:   2.211232\n",
      "train loss:   2.131419\n",
      "train loss:   2.287770\n",
      "train loss:   2.388338\n",
      "train loss:   2.245114\n",
      "train loss:   2.314362\n",
      "train loss:   2.424292\n",
      "train loss:   2.252162\n",
      "train loss:   2.241729\n",
      "train loss:   2.248965\n",
      "train loss:   2.254920\n",
      "train loss:   2.307563\n",
      "train loss:   2.214502\n",
      "train loss:   2.305715\n",
      "train loss:   2.361380\n",
      "train loss:   2.144444\n",
      "train loss:   2.416516\n",
      "train loss:   2.190424\n",
      "train loss:   2.307658\n",
      "train loss:   2.216548\n",
      "train loss:   2.252470\n",
      "train loss:   2.273157\n",
      "train loss:   2.162712\n",
      "train loss:   2.293976\n",
      "train loss:   2.206640\n",
      "train loss:   2.269721\n",
      "train loss:   2.292737\n",
      "train loss:   2.257720\n",
      "train loss:   2.318432\n",
      "train loss:   2.281666\n",
      "########### epoch 2 ###########\n",
      "########### loop 150 ###########\n",
      "test loss:   2.156486   test accuracy:   0.468750\n",
      "########### loop 150 ###########\n",
      "train loss:   2.228518\n",
      "train loss:   2.358279\n",
      "train loss:   2.220274\n",
      "train loss:   2.103722\n",
      "train loss:   2.147387\n",
      "train loss:   2.246013\n",
      "train loss:   2.175535\n",
      "train loss:   2.207616\n",
      "train loss:   2.300569\n",
      "train loss:   2.383895\n",
      "train loss:   2.102694\n",
      "train loss:   2.180449\n",
      "train loss:   2.256203\n",
      "train loss:   2.070924\n",
      "train loss:   2.221054\n",
      "train loss:   1.999809\n",
      "train loss:   2.160839\n",
      "train loss:   2.349809\n",
      "train loss:   2.202688\n",
      "train loss:   2.298874\n",
      "train loss:   2.269775\n",
      "train loss:   2.246429\n",
      "train loss:   2.254817\n",
      "train loss:   2.090549\n",
      "train loss:   2.160061\n",
      "train loss:   1.960645\n",
      "train loss:   2.379215\n",
      "train loss:   2.414992\n",
      "train loss:   2.340013\n",
      "train loss:   2.202446\n",
      "train loss:   2.125555\n",
      "train loss:   2.017206\n",
      "train loss:   2.429701\n",
      "train loss:   2.155948\n",
      "train loss:   2.264706\n",
      "train loss:   2.251468\n",
      "train loss:   2.373196\n",
      "train loss:   2.224321\n",
      "train loss:   2.297173\n",
      "train loss:   2.334394\n",
      "train loss:   2.175372\n",
      "train loss:   2.219149\n",
      "train loss:   2.195824\n",
      "train loss:   2.241889\n",
      "train loss:   2.033388\n",
      "train loss:   2.202537\n",
      "train loss:   2.301432\n",
      "train loss:   2.218669\n",
      "train loss:   2.222798\n",
      "train loss:   2.134992\n",
      "########### epoch 2 ###########\n",
      "########### loop 200 ###########\n",
      "test loss:   2.157979   test accuracy:   0.406250\n",
      "########### loop 200 ###########\n",
      "train loss:   2.182874\n",
      "train loss:   2.273095\n",
      "train loss:   2.133737\n",
      "train loss:   2.271202\n",
      "train loss:   2.222556\n",
      "train loss:   2.230345\n",
      "train loss:   2.174592\n",
      "train loss:   2.109249\n",
      "train loss:   2.168607\n",
      "train loss:   2.248362\n",
      "train loss:   2.138654\n",
      "train loss:   2.109719\n",
      "train loss:   2.237628\n",
      "train loss:   2.229704\n",
      "train loss:   2.293561\n",
      "train loss:   2.098627\n",
      "train loss:   2.169769\n",
      "train loss:   2.254776\n",
      "train loss:   2.160328\n",
      "train loss:   2.220633\n",
      "train loss:   2.355353\n",
      "train loss:   2.222187\n",
      "train loss:   2.225309\n",
      "train loss:   2.225374\n",
      "train loss:   2.171890\n",
      "train loss:   2.201900\n",
      "train loss:   2.237001\n",
      "train loss:   2.054154\n",
      "train loss:   2.287591\n",
      "train loss:   2.151345\n",
      "train loss:   2.174557\n",
      "train loss:   2.229064\n",
      "train loss:   2.256010\n",
      "train loss:   2.104736\n",
      "train loss:   2.322128\n",
      "train loss:   2.227314\n",
      "train loss:   2.104453\n",
      "train loss:   2.154526\n",
      "train loss:   2.180544\n",
      "train loss:   2.109153\n",
      "train loss:   2.150214\n",
      "train loss:   2.141279\n",
      "train loss:   2.188254\n",
      "train loss:   2.148025\n",
      "train loss:   2.161831\n",
      "train loss:   2.127696\n",
      "train loss:   2.236462\n",
      "train loss:   2.217242\n",
      "train loss:   2.082858\n",
      "train loss:   2.241879\n",
      "########### epoch 2 ###########\n",
      "########### loop 250 ###########\n",
      "test loss:   2.157374   test accuracy:   0.437500\n",
      "########### loop 250 ###########\n",
      "train loss:   2.192454\n",
      "train loss:   2.123574\n",
      "train loss:   2.110577\n",
      "train loss:   2.209313\n",
      "train loss:   2.101254\n",
      "train loss:   2.183928\n",
      "train loss:   2.194202\n",
      "train loss:   2.081998\n",
      "train loss:   2.151250\n",
      "train loss:   2.235907\n",
      "train loss:   2.114230\n",
      "train loss:   2.302726\n",
      "train loss:   2.189586\n",
      "train loss:   2.165261\n",
      "train loss:   2.040442\n",
      "train loss:   2.088670\n",
      "train loss:   2.068614\n",
      "train loss:   2.023835\n",
      "train loss:   2.068472\n",
      "train loss:   2.123302\n",
      "train loss:   2.006120\n",
      "train loss:   1.931949\n",
      "train loss:   2.333349\n",
      "train loss:   2.020135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   2.226629\n",
      "train loss:   2.126839\n",
      "train loss:   2.119179\n",
      "train loss:   1.984917\n",
      "train loss:   2.217154\n",
      "train loss:   2.312849\n",
      "train loss:   2.182417\n",
      "train loss:   2.133904\n",
      "train loss:   2.022936\n",
      "train loss:   2.066675\n",
      "train loss:   2.020809\n",
      "train loss:   2.147822\n",
      "train loss:   2.137505\n",
      "train loss:   2.230508\n",
      "train loss:   2.080014\n",
      "train loss:   2.302559\n",
      "train loss:   2.161315\n",
      "train loss:   2.285050\n",
      "train loss:   2.269837\n",
      "train loss:   2.324056\n",
      "train loss:   2.033614\n",
      "train loss:   1.932268\n",
      "train loss:   2.140348\n",
      "train loss:   2.180972\n",
      "train loss:   2.031968\n",
      "train loss:   2.277529\n",
      "########### epoch 3 ###########\n",
      "########### loop 300 ###########\n",
      "test loss:   2.017839   test accuracy:   0.625000\n",
      "########### loop 300 ###########\n",
      "train loss:   2.171234\n",
      "train loss:   2.103474\n",
      "train loss:   1.932008\n",
      "train loss:   1.903594\n",
      "train loss:   2.082652\n",
      "train loss:   2.091609\n",
      "train loss:   2.122979\n",
      "train loss:   1.956491\n",
      "train loss:   2.065399\n",
      "train loss:   2.249638\n",
      "train loss:   2.221706\n",
      "train loss:   2.131639\n",
      "train loss:   2.084470\n",
      "train loss:   2.244051\n",
      "train loss:   2.320312\n",
      "train loss:   2.219220\n",
      "train loss:   2.066258\n",
      "train loss:   2.087161\n",
      "train loss:   2.154825\n",
      "train loss:   2.118843\n",
      "train loss:   2.075162\n",
      "train loss:   2.186902\n",
      "train loss:   2.246939\n",
      "train loss:   2.059946\n",
      "train loss:   2.268122\n",
      "train loss:   2.175295\n",
      "train loss:   2.136577\n",
      "train loss:   2.149093\n",
      "train loss:   2.134541\n",
      "train loss:   2.043878\n",
      "train loss:   2.122310\n",
      "train loss:   2.045379\n",
      "train loss:   2.001329\n",
      "train loss:   2.294410\n",
      "train loss:   2.084950\n",
      "train loss:   2.125114\n",
      "train loss:   2.101995\n",
      "train loss:   2.041576\n",
      "train loss:   2.178040\n",
      "train loss:   2.046241\n",
      "train loss:   1.957113\n",
      "train loss:   2.173735\n",
      "train loss:   2.186200\n",
      "train loss:   2.122015\n",
      "train loss:   2.108398\n",
      "train loss:   2.007958\n",
      "train loss:   2.108765\n",
      "train loss:   2.004098\n",
      "train loss:   2.069902\n",
      "train loss:   2.022084\n",
      "########### epoch 3 ###########\n",
      "########### loop 350 ###########\n",
      "test loss:   2.036844   test accuracy:   0.593750\n",
      "########### loop 350 ###########\n",
      "train loss:   2.146797\n",
      "train loss:   2.189658\n",
      "train loss:   2.057497\n",
      "train loss:   2.049274\n",
      "train loss:   2.170071\n",
      "train loss:   2.150677\n",
      "train loss:   2.042021\n",
      "train loss:   2.118624\n",
      "train loss:   2.083413\n",
      "train loss:   2.143621\n",
      "train loss:   2.092264\n",
      "train loss:   2.073905\n",
      "train loss:   1.977148\n",
      "train loss:   2.028206\n",
      "train loss:   2.016764\n",
      "train loss:   2.048194\n",
      "train loss:   2.020348\n",
      "train loss:   2.061355\n",
      "train loss:   1.924121\n",
      "train loss:   2.107251\n",
      "train loss:   2.276064\n",
      "train loss:   2.106626\n",
      "train loss:   2.036057\n",
      "train loss:   2.087444\n",
      "train loss:   1.924353\n",
      "train loss:   2.112872\n",
      "train loss:   2.233645\n",
      "train loss:   1.958555\n",
      "train loss:   1.923797\n",
      "train loss:   2.019573\n",
      "train loss:   2.129716\n",
      "train loss:   1.907796\n",
      "train loss:   1.905943\n",
      "train loss:   1.868125\n",
      "train loss:   1.949635\n",
      "train loss:   2.163581\n",
      "train loss:   2.064178\n",
      "train loss:   2.343673\n",
      "train loss:   1.984419\n",
      "train loss:   2.285547\n",
      "train loss:   1.954542\n",
      "train loss:   2.208749\n",
      "train loss:   2.242892\n",
      "train loss:   2.315089\n",
      "train loss:   2.075064\n",
      "train loss:   2.042593\n",
      "train loss:   1.990388\n",
      "train loss:   2.080931\n",
      "train loss:   2.339283\n",
      "train loss:   2.113171\n",
      "########### epoch 3 ###########\n",
      "########### loop 400 ###########\n",
      "test loss:   1.982481   test accuracy:   0.562500\n",
      "########### loop 400 ###########\n",
      "train loss:   1.987144\n",
      "train loss:   1.958179\n",
      "train loss:   2.114246\n",
      "train loss:   2.142836\n",
      "train loss:   1.982521\n",
      "train loss:   2.126492\n",
      "train loss:   2.075622\n",
      "train loss:   1.993195\n",
      "train loss:   2.133594\n",
      "train loss:   1.917102\n",
      "train loss:   2.134392\n",
      "train loss:   2.221117\n",
      "train loss:   2.163346\n",
      "train loss:   2.118063\n",
      "train loss:   2.100379\n",
      "train loss:   2.165682\n",
      "train loss:   1.948650\n",
      "train loss:   2.050416\n",
      "train loss:   2.067291\n",
      "train loss:   2.016940\n",
      "train loss:   1.840531\n",
      "train loss:   2.029800\n",
      "train loss:   2.172837\n",
      "train loss:   2.111192\n",
      "train loss:   1.885347\n",
      "train loss:   2.216439\n",
      "train loss:   1.941938\n",
      "train loss:   2.011090\n",
      "train loss:   2.083596\n",
      "train loss:   2.154866\n",
      "train loss:   2.097266\n",
      "train loss:   2.027970\n",
      "train loss:   1.916942\n",
      "train loss:   2.155267\n",
      "train loss:   2.062978\n",
      "train loss:   1.932352\n",
      "train loss:   2.159734\n",
      "train loss:   1.976431\n",
      "train loss:   1.804181\n",
      "train loss:   2.203146\n",
      "train loss:   2.106001\n",
      "train loss:   1.939214\n",
      "train loss:   2.095631\n",
      "train loss:   2.040169\n",
      "train loss:   1.936075\n",
      "train loss:   2.084654\n",
      "train loss:   2.008314\n",
      "train loss:   2.053407\n",
      "train loss:   2.067585\n",
      "train loss:   2.103159\n",
      "########### epoch 4 ###########\n",
      "########### loop 450 ###########\n",
      "test loss:   1.928213   test accuracy:   0.625000\n",
      "########### loop 450 ###########\n",
      "train loss:   1.812151\n",
      "train loss:   1.870471\n",
      "train loss:   1.847698\n",
      "train loss:   2.136466\n",
      "train loss:   1.935680\n",
      "train loss:   2.157418\n",
      "train loss:   1.955062\n",
      "train loss:   2.031187\n",
      "train loss:   2.109540\n",
      "train loss:   2.147368\n",
      "train loss:   1.961146\n",
      "train loss:   1.965182\n",
      "train loss:   1.964622\n",
      "train loss:   2.192751\n",
      "train loss:   2.177487\n",
      "train loss:   1.969871\n",
      "train loss:   2.082038\n",
      "train loss:   1.997482\n",
      "train loss:   2.155448\n",
      "train loss:   2.017892\n",
      "train loss:   1.952671\n",
      "train loss:   1.974102\n",
      "train loss:   2.154578\n",
      "train loss:   2.240537\n",
      "train loss:   1.909332\n",
      "train loss:   2.083436\n",
      "train loss:   1.924675\n",
      "train loss:   2.159543\n",
      "train loss:   2.084589\n",
      "train loss:   2.029953\n",
      "train loss:   2.109888\n",
      "train loss:   2.004616\n",
      "train loss:   1.945606\n",
      "train loss:   2.024209\n",
      "train loss:   1.997342\n",
      "train loss:   2.108481\n",
      "train loss:   2.154129\n",
      "train loss:   2.128965\n",
      "train loss:   1.944295\n",
      "train loss:   1.903917\n",
      "train loss:   1.825676\n",
      "train loss:   2.023864\n",
      "train loss:   2.012644\n",
      "train loss:   2.026313\n",
      "train loss:   1.969125\n",
      "train loss:   2.056878\n",
      "train loss:   1.850139\n",
      "train loss:   2.041281\n",
      "train loss:   2.134214\n",
      "train loss:   1.815493\n",
      "########### epoch 4 ###########\n",
      "########### loop 500 ###########\n",
      "test loss:   1.959337   test accuracy:   0.500000\n",
      "########### loop 500 ###########\n",
      "train loss:   2.050622\n",
      "train loss:   1.935008\n",
      "train loss:   2.113376\n",
      "train loss:   2.058259\n",
      "train loss:   1.916788\n",
      "train loss:   2.035562\n",
      "train loss:   1.972808\n",
      "train loss:   1.983473\n",
      "train loss:   1.948319\n",
      "train loss:   1.746251\n",
      "train loss:   2.100760\n",
      "train loss:   1.998241\n",
      "train loss:   2.071742\n",
      "train loss:   2.002751\n",
      "train loss:   2.066412\n",
      "train loss:   2.104053\n",
      "train loss:   2.026492\n",
      "train loss:   1.859384\n",
      "train loss:   2.060952\n",
      "train loss:   2.109976\n",
      "train loss:   1.912418\n",
      "train loss:   1.869529\n",
      "train loss:   1.955277\n",
      "train loss:   2.012285\n",
      "train loss:   2.042145\n",
      "train loss:   1.933678\n",
      "train loss:   2.049463\n",
      "train loss:   2.103131\n",
      "train loss:   2.073528\n",
      "train loss:   1.969545\n",
      "train loss:   1.854274\n",
      "train loss:   2.175605\n",
      "train loss:   1.830185\n",
      "train loss:   2.038691\n",
      "train loss:   2.006617\n",
      "train loss:   1.953022\n",
      "train loss:   2.121980\n",
      "train loss:   1.656889\n",
      "train loss:   2.101501\n",
      "train loss:   2.188835\n",
      "train loss:   2.018673\n",
      "train loss:   1.719758\n",
      "train loss:   2.076688\n",
      "train loss:   1.771098\n",
      "train loss:   2.029006\n",
      "train loss:   2.205638\n",
      "train loss:   1.832856\n",
      "train loss:   1.784176\n",
      "train loss:   2.130268\n",
      "train loss:   2.001538\n",
      "########### epoch 4 ###########\n",
      "########### loop 550 ###########\n",
      "test loss:   1.947157   test accuracy:   0.531250\n",
      "########### loop 550 ###########\n",
      "train loss:   2.010475\n",
      "train loss:   1.989420\n",
      "train loss:   1.974667\n",
      "train loss:   2.349602\n",
      "train loss:   1.929863\n",
      "train loss:   1.990661\n",
      "train loss:   2.072366\n",
      "train loss:   1.936728\n",
      "train loss:   2.083215\n",
      "train loss:   1.973534\n",
      "train loss:   1.836767\n",
      "train loss:   1.852424\n",
      "train loss:   1.854399\n",
      "train loss:   1.884986\n",
      "train loss:   2.137569\n",
      "train loss:   1.787547\n",
      "train loss:   2.086512\n",
      "train loss:   1.954047\n",
      "train loss:   2.030545\n",
      "train loss:   1.768442\n",
      "train loss:   2.151182\n",
      "train loss:   1.893814\n",
      "train loss:   1.921205\n",
      "train loss:   1.953770\n",
      "train loss:   1.967843\n",
      "train loss:   1.885635\n",
      "train loss:   2.103611\n",
      "train loss:   2.130156\n",
      "train loss:   1.891944\n",
      "train loss:   1.950995\n",
      "train loss:   1.905178\n",
      "train loss:   2.005945\n",
      "train loss:   2.139787\n",
      "train loss:   2.174328\n",
      "train loss:   1.988116\n",
      "train loss:   1.947998\n",
      "train loss:   2.120468\n",
      "train loss:   1.998860\n",
      "train loss:   2.037955\n",
      "train loss:   1.979085\n",
      "train loss:   2.017315\n",
      "train loss:   2.034815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.936234\n",
      "train loss:   1.971342\n",
      "train loss:   2.058169\n",
      "train loss:   1.966018\n",
      "train loss:   2.166979\n",
      "train loss:   2.054907\n",
      "train loss:   1.908723\n",
      "train loss:   1.994505\n",
      "########### epoch 5 ###########\n",
      "########### loop 600 ###########\n",
      "test loss:   1.888732   test accuracy:   0.531250\n",
      "########### loop 600 ###########\n",
      "train loss:   1.868687\n",
      "train loss:   2.073086\n",
      "train loss:   1.972253\n",
      "train loss:   1.920221\n",
      "train loss:   2.114712\n",
      "train loss:   2.051690\n",
      "train loss:   1.987807\n",
      "train loss:   1.948579\n",
      "train loss:   1.994817\n",
      "train loss:   1.984885\n",
      "train loss:   1.890932\n",
      "train loss:   2.064524\n",
      "train loss:   1.764801\n",
      "train loss:   1.849709\n",
      "train loss:   1.731882\n",
      "train loss:   2.056951\n",
      "train loss:   2.068863\n",
      "train loss:   1.973662\n",
      "train loss:   1.760360\n",
      "train loss:   1.901813\n",
      "train loss:   2.121765\n",
      "train loss:   1.944566\n",
      "train loss:   1.720959\n",
      "train loss:   1.983501\n",
      "train loss:   2.082654\n",
      "train loss:   2.020010\n",
      "train loss:   2.126195\n",
      "train loss:   1.909852\n",
      "train loss:   1.963236\n",
      "train loss:   1.791758\n",
      "train loss:   1.872758\n",
      "train loss:   2.082011\n",
      "train loss:   1.949158\n",
      "train loss:   1.893084\n",
      "train loss:   1.851727\n",
      "train loss:   1.778621\n",
      "train loss:   2.103825\n",
      "train loss:   2.093467\n",
      "train loss:   1.960363\n",
      "train loss:   1.844177\n",
      "train loss:   1.916982\n",
      "train loss:   1.979716\n",
      "train loss:   1.930766\n",
      "train loss:   1.828553\n",
      "train loss:   1.814019\n",
      "train loss:   2.181426\n",
      "train loss:   1.964646\n",
      "train loss:   2.018720\n",
      "train loss:   1.890524\n",
      "train loss:   1.829997\n",
      "########### epoch 5 ###########\n",
      "########### loop 650 ###########\n",
      "test loss:   1.737868   test accuracy:   0.656250\n",
      "########### loop 650 ###########\n",
      "train loss:   2.060760\n",
      "train loss:   2.081666\n",
      "train loss:   1.996244\n",
      "train loss:   2.121569\n",
      "train loss:   2.123695\n",
      "train loss:   1.804237\n",
      "train loss:   1.980511\n",
      "train loss:   1.912981\n",
      "train loss:   1.847241\n",
      "train loss:   1.910936\n",
      "train loss:   1.941728\n",
      "train loss:   2.049809\n",
      "train loss:   1.895539\n",
      "train loss:   2.008115\n",
      "train loss:   2.042378\n",
      "train loss:   1.964811\n",
      "train loss:   1.887686\n",
      "train loss:   1.835778\n",
      "train loss:   2.026972\n",
      "train loss:   1.989646\n",
      "train loss:   1.731164\n",
      "train loss:   2.016498\n",
      "train loss:   2.076091\n",
      "train loss:   1.783785\n",
      "train loss:   1.884674\n",
      "train loss:   1.986328\n",
      "train loss:   1.972950\n",
      "train loss:   1.997817\n",
      "train loss:   2.054553\n",
      "train loss:   1.954765\n",
      "train loss:   2.073803\n",
      "train loss:   2.239438\n",
      "train loss:   1.902959\n",
      "train loss:   1.859198\n",
      "train loss:   1.554129\n",
      "train loss:   2.099608\n",
      "train loss:   1.854370\n",
      "train loss:   2.043161\n",
      "train loss:   1.719376\n",
      "train loss:   2.004133\n",
      "train loss:   1.860588\n",
      "train loss:   1.803792\n",
      "train loss:   1.830086\n",
      "train loss:   2.081924\n",
      "train loss:   2.013991\n",
      "train loss:   1.825172\n",
      "train loss:   1.992251\n",
      "train loss:   2.069233\n",
      "train loss:   1.701876\n",
      "train loss:   2.053068\n",
      "########### epoch 5 ###########\n",
      "########### loop 700 ###########\n",
      "test loss:   1.826944   test accuracy:   0.562500\n",
      "########### loop 700 ###########\n",
      "train loss:   2.140780\n",
      "train loss:   1.808361\n",
      "train loss:   1.936364\n",
      "train loss:   2.008620\n",
      "train loss:   2.004118\n",
      "train loss:   1.877234\n",
      "train loss:   1.948977\n",
      "train loss:   1.943223\n",
      "train loss:   1.935110\n",
      "train loss:   1.785790\n",
      "train loss:   1.941959\n",
      "train loss:   1.873929\n",
      "train loss:   1.947108\n",
      "train loss:   2.066666\n",
      "train loss:   1.866811\n",
      "train loss:   2.026444\n",
      "train loss:   1.954853\n",
      "train loss:   1.761978\n",
      "train loss:   1.961304\n",
      "train loss:   1.897442\n",
      "train loss:   1.794348\n",
      "train loss:   2.344345\n",
      "train loss:   1.736283\n",
      "train loss:   1.870021\n",
      "train loss:   1.925122\n",
      "train loss:   1.877896\n",
      "train loss:   1.900448\n",
      "train loss:   2.080495\n",
      "train loss:   1.803560\n",
      "train loss:   2.020846\n",
      "train loss:   1.863681\n",
      "train loss:   1.763214\n",
      "train loss:   1.858578\n",
      "train loss:   1.867901\n",
      "train loss:   1.836002\n",
      "train loss:   2.016769\n",
      "train loss:   1.883086\n",
      "train loss:   1.973260\n",
      "train loss:   1.963933\n",
      "train loss:   1.927690\n",
      "train loss:   1.878199\n",
      "train loss:   2.077915\n",
      "train loss:   1.849337\n",
      "train loss:   1.914606\n",
      "train loss:   1.880910\n",
      "train loss:   2.128348\n",
      "train loss:   1.923874\n",
      "train loss:   2.017932\n",
      "train loss:   1.930952\n",
      "train loss:   2.019931\n",
      "########### epoch 6 ###########\n",
      "########### loop 750 ###########\n",
      "test loss:   1.812071   test accuracy:   0.531250\n",
      "########### loop 750 ###########\n",
      "train loss:   1.973663\n",
      "train loss:   1.959727\n",
      "train loss:   2.002185\n",
      "train loss:   2.009778\n",
      "train loss:   1.879409\n",
      "train loss:   2.105477\n",
      "train loss:   2.041388\n",
      "train loss:   2.021788\n",
      "train loss:   2.047537\n",
      "train loss:   1.894915\n",
      "train loss:   1.638760\n",
      "train loss:   2.060071\n",
      "train loss:   1.887958\n",
      "train loss:   1.861292\n",
      "train loss:   1.937186\n",
      "train loss:   1.893219\n",
      "train loss:   1.981072\n",
      "train loss:   1.974990\n",
      "train loss:   2.071068\n",
      "train loss:   1.840529\n",
      "train loss:   2.197802\n",
      "train loss:   2.047992\n",
      "train loss:   1.943617\n",
      "train loss:   1.890409\n",
      "train loss:   1.841508\n",
      "train loss:   2.027658\n",
      "train loss:   1.875493\n",
      "train loss:   1.899210\n",
      "train loss:   1.851897\n",
      "train loss:   1.833740\n",
      "train loss:   1.695501\n",
      "train loss:   1.770231\n",
      "train loss:   1.729552\n",
      "train loss:   1.823637\n",
      "train loss:   2.065297\n",
      "train loss:   1.791649\n",
      "train loss:   1.991338\n",
      "train loss:   1.759104\n",
      "train loss:   1.918421\n",
      "train loss:   1.758076\n",
      "train loss:   1.725864\n",
      "train loss:   2.106953\n",
      "train loss:   2.060044\n",
      "train loss:   1.729438\n",
      "train loss:   1.984375\n",
      "train loss:   2.088166\n",
      "train loss:   1.926652\n",
      "train loss:   1.922564\n",
      "train loss:   1.725562\n",
      "train loss:   1.878675\n",
      "########### epoch 6 ###########\n",
      "########### loop 800 ###########\n",
      "test loss:   1.775544   test accuracy:   0.593750\n",
      "########### loop 800 ###########\n",
      "train loss:   1.763713\n",
      "train loss:   2.012462\n",
      "train loss:   1.834477\n",
      "train loss:   1.767510\n",
      "train loss:   2.081266\n",
      "train loss:   1.836891\n",
      "train loss:   1.960922\n",
      "train loss:   1.922509\n",
      "train loss:   1.890969\n",
      "train loss:   2.028787\n",
      "train loss:   1.773763\n",
      "train loss:   1.676331\n",
      "train loss:   1.906257\n",
      "train loss:   1.877978\n",
      "train loss:   1.716796\n",
      "train loss:   1.978200\n",
      "train loss:   1.915538\n",
      "train loss:   1.857393\n",
      "train loss:   2.072796\n",
      "train loss:   2.049156\n",
      "train loss:   2.136722\n",
      "train loss:   2.002807\n",
      "train loss:   1.974412\n",
      "train loss:   2.005919\n",
      "train loss:   1.831022\n",
      "train loss:   1.909237\n",
      "train loss:   2.164377\n",
      "train loss:   1.989404\n",
      "train loss:   1.944973\n",
      "train loss:   1.880481\n",
      "train loss:   1.938529\n",
      "train loss:   2.088631\n",
      "train loss:   2.000987\n",
      "train loss:   1.882212\n",
      "train loss:   2.007921\n",
      "train loss:   1.935370\n",
      "train loss:   1.918495\n",
      "train loss:   1.893832\n",
      "train loss:   1.845748\n",
      "train loss:   1.897648\n",
      "train loss:   2.112823\n",
      "train loss:   2.026265\n",
      "train loss:   2.001021\n",
      "train loss:   1.849473\n",
      "train loss:   2.033180\n",
      "train loss:   1.717898\n",
      "train loss:   1.897394\n",
      "train loss:   1.856630\n",
      "train loss:   1.852902\n",
      "train loss:   1.941388\n",
      "########### epoch 7 ###########\n",
      "########### loop 850 ###########\n",
      "test loss:   1.806849   test accuracy:   0.562500\n",
      "########### loop 850 ###########\n",
      "train loss:   1.976968\n",
      "train loss:   2.117428\n",
      "train loss:   1.826935\n",
      "train loss:   2.099969\n",
      "train loss:   1.769028\n",
      "train loss:   1.740079\n",
      "train loss:   1.824426\n",
      "train loss:   2.111659\n",
      "train loss:   1.864662\n",
      "train loss:   1.872137\n",
      "train loss:   1.721510\n",
      "train loss:   1.923429\n",
      "train loss:   1.854594\n",
      "train loss:   1.936775\n",
      "train loss:   1.814211\n",
      "train loss:   2.010187\n",
      "train loss:   1.949300\n",
      "train loss:   1.884717\n",
      "train loss:   1.948066\n",
      "train loss:   1.745482\n",
      "train loss:   1.857590\n",
      "train loss:   1.737855\n",
      "train loss:   1.976978\n",
      "train loss:   1.844000\n",
      "train loss:   1.697948\n",
      "train loss:   2.010623\n",
      "train loss:   1.740666\n",
      "train loss:   1.952897\n",
      "train loss:   1.683766\n",
      "train loss:   1.825821\n",
      "train loss:   1.931176\n",
      "train loss:   1.911282\n",
      "train loss:   1.998016\n",
      "train loss:   1.787376\n",
      "train loss:   1.996041\n",
      "train loss:   1.905989\n",
      "train loss:   1.988963\n",
      "train loss:   1.786297\n",
      "train loss:   1.776543\n",
      "train loss:   1.975346\n",
      "train loss:   2.084421\n",
      "train loss:   1.856721\n",
      "train loss:   1.808309\n",
      "train loss:   1.874307\n",
      "train loss:   1.938235\n",
      "train loss:   1.694824\n",
      "train loss:   1.851487\n",
      "train loss:   1.949286\n",
      "train loss:   1.835052\n",
      "train loss:   1.816544\n",
      "########### epoch 7 ###########\n",
      "########### loop 900 ###########\n",
      "test loss:   1.791061   test accuracy:   0.656250\n",
      "########### loop 900 ###########\n",
      "train loss:   1.727274\n",
      "train loss:   1.738383\n",
      "train loss:   1.827432\n",
      "train loss:   1.576930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.960027\n",
      "train loss:   2.023679\n",
      "train loss:   1.986275\n",
      "train loss:   1.923365\n",
      "train loss:   1.964178\n",
      "train loss:   2.042396\n",
      "train loss:   1.897279\n",
      "train loss:   1.782140\n",
      "train loss:   1.788378\n",
      "train loss:   1.730770\n",
      "train loss:   1.839459\n",
      "train loss:   1.882546\n",
      "train loss:   1.917056\n",
      "train loss:   2.126499\n",
      "train loss:   1.782447\n",
      "train loss:   1.736281\n",
      "train loss:   1.824094\n",
      "train loss:   1.791484\n",
      "train loss:   1.770582\n",
      "train loss:   1.892394\n",
      "train loss:   1.910802\n",
      "train loss:   2.051530\n",
      "train loss:   1.771577\n",
      "train loss:   1.990059\n",
      "train loss:   1.978802\n",
      "train loss:   1.770191\n",
      "train loss:   1.790993\n",
      "train loss:   2.038875\n",
      "train loss:   1.817622\n",
      "train loss:   2.053255\n",
      "train loss:   1.703997\n",
      "train loss:   1.759703\n",
      "train loss:   1.926943\n",
      "train loss:   1.761991\n",
      "train loss:   1.722384\n",
      "train loss:   1.888822\n",
      "train loss:   2.088958\n",
      "train loss:   1.684421\n",
      "train loss:   1.661585\n",
      "train loss:   2.121170\n",
      "train loss:   1.688696\n",
      "train loss:   1.848740\n",
      "train loss:   1.448784\n",
      "train loss:   2.003634\n",
      "train loss:   2.103957\n",
      "train loss:   1.908194\n",
      "########### epoch 7 ###########\n",
      "########### loop 950 ###########\n",
      "test loss:   1.846581   test accuracy:   0.531250\n",
      "########### loop 950 ###########\n",
      "train loss:   1.881678\n",
      "train loss:   1.803528\n",
      "train loss:   1.681451\n",
      "train loss:   1.707317\n",
      "train loss:   1.874343\n",
      "train loss:   1.809576\n",
      "train loss:   1.793176\n",
      "train loss:   1.683859\n",
      "train loss:   1.792808\n",
      "train loss:   1.656075\n",
      "train loss:   2.022014\n",
      "train loss:   1.648980\n",
      "train loss:   1.628467\n",
      "train loss:   1.998593\n",
      "train loss:   2.030961\n",
      "train loss:   1.974960\n",
      "train loss:   1.800436\n",
      "train loss:   1.915359\n",
      "train loss:   2.036872\n",
      "train loss:   1.896895\n",
      "train loss:   1.674157\n",
      "train loss:   1.696272\n",
      "train loss:   1.770015\n",
      "train loss:   1.653517\n",
      "train loss:   1.873093\n",
      "train loss:   1.965934\n",
      "train loss:   1.719424\n",
      "train loss:   1.828571\n",
      "train loss:   2.100243\n",
      "train loss:   1.755708\n",
      "train loss:   1.635007\n",
      "train loss:   1.740825\n",
      "train loss:   1.700107\n",
      "train loss:   2.066959\n",
      "train loss:   1.801728\n",
      "train loss:   1.626186\n",
      "train loss:   1.890448\n",
      "train loss:   2.052711\n",
      "train loss:   1.979889\n",
      "train loss:   1.880076\n",
      "train loss:   1.878655\n",
      "train loss:   2.096220\n",
      "train loss:   2.024498\n",
      "train loss:   1.860933\n",
      "train loss:   1.782703\n",
      "train loss:   1.857214\n",
      "train loss:   1.994175\n",
      "train loss:   1.916501\n",
      "train loss:   1.716928\n",
      "train loss:   1.567160\n",
      "########### epoch 8 ###########\n",
      "########### loop 1000 ###########\n",
      "test loss:   1.788830   test accuracy:   0.531250\n",
      "########### loop 1000 ###########\n",
      "train loss:   1.780835\n",
      "train loss:   1.775404\n",
      "train loss:   1.812433\n",
      "train loss:   1.755714\n",
      "train loss:   1.799172\n",
      "train loss:   2.061112\n",
      "train loss:   1.762144\n",
      "train loss:   1.794297\n",
      "train loss:   1.699930\n",
      "train loss:   1.941520\n",
      "train loss:   1.762784\n",
      "train loss:   1.706409\n",
      "train loss:   2.053420\n",
      "train loss:   1.587091\n",
      "train loss:   2.122012\n",
      "train loss:   1.863480\n",
      "train loss:   1.716776\n",
      "train loss:   1.792351\n",
      "train loss:   1.990277\n",
      "train loss:   1.861851\n",
      "train loss:   1.822294\n",
      "train loss:   1.842547\n",
      "train loss:   1.768009\n",
      "train loss:   1.779956\n",
      "train loss:   1.574963\n",
      "train loss:   1.835450\n",
      "train loss:   2.081074\n",
      "train loss:   1.720039\n",
      "train loss:   1.734520\n",
      "train loss:   1.914459\n",
      "train loss:   1.839285\n",
      "train loss:   1.847831\n",
      "train loss:   1.884457\n",
      "train loss:   1.689871\n",
      "train loss:   2.122053\n",
      "train loss:   1.790842\n",
      "train loss:   1.869431\n",
      "train loss:   1.825607\n",
      "train loss:   1.694526\n",
      "train loss:   1.849946\n",
      "train loss:   2.025241\n",
      "train loss:   1.904822\n",
      "train loss:   1.811193\n",
      "train loss:   1.913707\n",
      "train loss:   1.718685\n",
      "train loss:   2.042600\n",
      "train loss:   1.912024\n",
      "train loss:   1.942880\n",
      "train loss:   2.101007\n",
      "train loss:   1.868300\n",
      "########### epoch 8 ###########\n",
      "########### loop 1050 ###########\n",
      "test loss:   1.730248   test accuracy:   0.625000\n",
      "########### loop 1050 ###########\n",
      "train loss:   1.835489\n",
      "train loss:   1.924491\n",
      "train loss:   1.952070\n",
      "train loss:   1.994288\n",
      "train loss:   1.928648\n",
      "train loss:   2.037315\n",
      "train loss:   2.062127\n",
      "train loss:   2.036026\n",
      "train loss:   1.833118\n",
      "train loss:   1.866869\n",
      "train loss:   1.757064\n",
      "train loss:   1.743503\n",
      "train loss:   1.927174\n",
      "train loss:   1.838316\n",
      "train loss:   1.987385\n",
      "train loss:   1.758830\n",
      "train loss:   1.855295\n",
      "train loss:   1.504843\n",
      "train loss:   1.461333\n",
      "train loss:   1.973737\n",
      "train loss:   1.769328\n",
      "train loss:   1.499151\n",
      "train loss:   1.670758\n",
      "train loss:   1.773282\n",
      "train loss:   1.892400\n",
      "train loss:   1.763238\n",
      "train loss:   1.841931\n",
      "train loss:   1.864287\n",
      "train loss:   2.042774\n",
      "train loss:   1.675395\n",
      "train loss:   2.026879\n",
      "train loss:   1.911231\n",
      "train loss:   1.793756\n",
      "train loss:   1.659975\n",
      "train loss:   2.150037\n",
      "train loss:   1.778489\n",
      "train loss:   1.706841\n",
      "train loss:   1.822928\n",
      "train loss:   1.968377\n",
      "train loss:   1.803461\n",
      "train loss:   1.822852\n",
      "train loss:   1.928138\n",
      "train loss:   1.668940\n",
      "train loss:   1.792375\n",
      "train loss:   1.699100\n",
      "train loss:   1.738968\n",
      "train loss:   1.717452\n",
      "train loss:   1.781808\n",
      "train loss:   2.086228\n",
      "train loss:   1.669493\n",
      "########### epoch 8 ###########\n",
      "########### loop 1100 ###########\n",
      "test loss:   1.671891   test accuracy:   0.750000\n",
      "########### loop 1100 ###########\n",
      "train loss:   1.654959\n",
      "train loss:   2.114209\n",
      "train loss:   1.958191\n",
      "train loss:   1.918671\n",
      "train loss:   1.820906\n",
      "train loss:   1.751858\n",
      "train loss:   1.705663\n",
      "train loss:   1.731841\n",
      "train loss:   1.919985\n",
      "train loss:   1.846646\n",
      "train loss:   1.917200\n",
      "train loss:   1.819979\n",
      "train loss:   2.070849\n",
      "train loss:   1.838995\n",
      "train loss:   1.993344\n",
      "train loss:   1.910057\n",
      "train loss:   1.826829\n",
      "train loss:   1.459454\n",
      "train loss:   1.956256\n",
      "train loss:   1.630400\n",
      "train loss:   1.784732\n",
      "train loss:   1.902081\n",
      "train loss:   1.939220\n",
      "train loss:   1.802579\n",
      "train loss:   1.743929\n",
      "train loss:   1.621111\n",
      "train loss:   1.795569\n",
      "train loss:   1.839697\n",
      "train loss:   2.014741\n",
      "train loss:   1.628919\n",
      "train loss:   1.856415\n",
      "train loss:   1.755025\n",
      "train loss:   1.748106\n",
      "train loss:   1.818967\n",
      "train loss:   1.836400\n",
      "train loss:   1.646155\n",
      "train loss:   1.860729\n",
      "train loss:   1.965160\n",
      "train loss:   1.626299\n",
      "train loss:   1.654753\n",
      "train loss:   1.827826\n",
      "train loss:   1.818230\n",
      "train loss:   2.071059\n",
      "train loss:   1.795307\n",
      "train loss:   1.854939\n",
      "train loss:   1.797692\n",
      "train loss:   1.860785\n",
      "train loss:   1.874462\n",
      "train loss:   1.748112\n",
      "train loss:   1.899577\n",
      "########### epoch 9 ###########\n",
      "########### loop 1150 ###########\n",
      "test loss:   1.624428   test accuracy:   0.625000\n",
      "########### loop 1150 ###########\n",
      "train loss:   1.701049\n",
      "train loss:   1.576142\n",
      "train loss:   1.995388\n",
      "train loss:   1.921169\n",
      "train loss:   1.662401\n",
      "train loss:   1.773611\n",
      "train loss:   2.005275\n",
      "train loss:   1.733149\n",
      "train loss:   1.834497\n",
      "train loss:   2.018712\n",
      "train loss:   2.127773\n",
      "train loss:   1.855972\n",
      "train loss:   1.988369\n",
      "train loss:   1.515175\n",
      "train loss:   1.729439\n",
      "train loss:   1.958237\n",
      "train loss:   1.946664\n",
      "train loss:   1.996558\n",
      "train loss:   1.831994\n",
      "train loss:   1.697618\n",
      "train loss:   2.037214\n",
      "train loss:   1.723867\n",
      "train loss:   2.143445\n",
      "train loss:   1.798087\n",
      "train loss:   2.016653\n",
      "train loss:   1.562728\n",
      "train loss:   1.902531\n",
      "train loss:   1.753213\n",
      "train loss:   1.558517\n",
      "train loss:   1.657043\n",
      "train loss:   1.724717\n",
      "train loss:   1.904747\n",
      "train loss:   1.901529\n",
      "train loss:   1.779179\n",
      "train loss:   1.835515\n",
      "train loss:   1.691314\n",
      "train loss:   1.978607\n",
      "train loss:   1.731098\n",
      "train loss:   2.044103\n",
      "train loss:   2.028126\n",
      "train loss:   1.870855\n",
      "train loss:   1.745311\n",
      "train loss:   1.795484\n",
      "train loss:   1.942545\n",
      "train loss:   1.872993\n",
      "train loss:   1.789051\n",
      "train loss:   2.145368\n",
      "train loss:   1.862087\n",
      "train loss:   1.913122\n",
      "train loss:   1.891279\n",
      "########### epoch 9 ###########\n",
      "########### loop 1200 ###########\n",
      "test loss:   1.701860   test accuracy:   0.656250\n",
      "########### loop 1200 ###########\n",
      "train loss:   1.722700\n",
      "train loss:   1.528372\n",
      "train loss:   1.742393\n",
      "train loss:   1.719924\n",
      "train loss:   1.847824\n",
      "train loss:   1.769300\n",
      "train loss:   1.731723\n",
      "train loss:   1.666006\n",
      "train loss:   1.850538\n",
      "train loss:   1.737252\n",
      "train loss:   1.777020\n",
      "train loss:   1.542576\n",
      "train loss:   1.763725\n",
      "train loss:   1.563808\n",
      "train loss:   1.833120\n",
      "train loss:   1.680008\n",
      "train loss:   1.793647\n",
      "train loss:   1.876687\n",
      "train loss:   1.993586\n",
      "train loss:   1.816698\n",
      "train loss:   1.775112\n",
      "train loss:   1.845601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.557945\n",
      "train loss:   1.803572\n",
      "train loss:   1.642777\n",
      "train loss:   1.925966\n",
      "train loss:   1.929476\n",
      "train loss:   1.968287\n",
      "train loss:   1.768882\n",
      "train loss:   1.953325\n",
      "train loss:   1.931266\n",
      "train loss:   1.872196\n",
      "train loss:   1.704076\n",
      "train loss:   1.653341\n",
      "train loss:   1.717947\n",
      "train loss:   1.782638\n",
      "train loss:   1.886397\n",
      "train loss:   1.834176\n",
      "train loss:   1.695913\n",
      "train loss:   1.808748\n",
      "train loss:   1.713383\n",
      "train loss:   1.831076\n",
      "train loss:   1.678157\n",
      "train loss:   1.757102\n",
      "train loss:   1.525454\n",
      "train loss:   1.669239\n",
      "train loss:   1.649759\n",
      "train loss:   1.738259\n",
      "train loss:   1.958512\n",
      "train loss:   1.578258\n",
      "########### epoch 9 ###########\n",
      "########### loop 1250 ###########\n",
      "test loss:   1.596338   test accuracy:   0.718750\n",
      "########### loop 1250 ###########\n",
      "train loss:   1.893358\n",
      "train loss:   1.739036\n",
      "train loss:   1.712085\n",
      "train loss:   1.713762\n",
      "train loss:   1.894717\n",
      "train loss:   1.679202\n",
      "train loss:   1.961109\n",
      "train loss:   2.023680\n",
      "train loss:   1.482583\n",
      "train loss:   1.755502\n",
      "train loss:   1.883526\n",
      "train loss:   1.746295\n",
      "train loss:   1.917589\n",
      "train loss:   1.553468\n",
      "train loss:   1.778748\n",
      "train loss:   2.066656\n",
      "train loss:   1.722485\n",
      "train loss:   1.994890\n",
      "train loss:   1.776298\n",
      "train loss:   1.660796\n",
      "train loss:   1.700903\n",
      "train loss:   1.915058\n",
      "train loss:   1.954924\n",
      "train loss:   1.992061\n",
      "train loss:   1.580746\n",
      "train loss:   2.035780\n",
      "train loss:   1.720747\n",
      "train loss:   1.868620\n",
      "train loss:   1.581106\n",
      "train loss:   2.092310\n",
      "train loss:   1.917773\n",
      "train loss:   1.886316\n",
      "train loss:   1.771750\n",
      "train loss:   1.872988\n",
      "train loss:   1.917488\n",
      "train loss:   1.718755\n",
      "train loss:   2.082345\n",
      "train loss:   1.730994\n",
      "train loss:   2.057858\n",
      "train loss:   1.645343\n",
      "train loss:   1.982603\n",
      "train loss:   1.812052\n",
      "train loss:   1.633928\n",
      "train loss:   1.902590\n",
      "train loss:   1.724366\n",
      "train loss:   1.805266\n",
      "train loss:   1.821134\n",
      "train loss:   1.800416\n",
      "train loss:   1.930543\n",
      "train loss:   1.811298\n",
      "########### epoch 10 ###########\n",
      "########### loop 1300 ###########\n",
      "test loss:   1.585279   test accuracy:   0.687500\n",
      "########### loop 1300 ###########\n",
      "train loss:   1.639332\n",
      "train loss:   1.738462\n",
      "train loss:   1.875926\n",
      "train loss:   1.927790\n",
      "train loss:   1.597180\n",
      "train loss:   1.625433\n",
      "train loss:   2.015059\n",
      "train loss:   1.824884\n",
      "train loss:   2.017658\n",
      "train loss:   1.730865\n",
      "train loss:   1.663463\n",
      "train loss:   1.845450\n",
      "train loss:   1.899136\n",
      "train loss:   1.744618\n",
      "train loss:   1.681185\n",
      "train loss:   1.905832\n",
      "train loss:   1.748629\n",
      "train loss:   1.688716\n",
      "train loss:   1.763191\n",
      "train loss:   1.560764\n",
      "train loss:   1.710860\n",
      "train loss:   1.866897\n",
      "train loss:   1.773081\n",
      "train loss:   1.856119\n",
      "train loss:   1.937792\n",
      "train loss:   1.466968\n",
      "train loss:   1.636902\n",
      "train loss:   1.790799\n",
      "train loss:   1.820704\n",
      "train loss:   1.702849\n",
      "train loss:   1.972879\n",
      "train loss:   1.572856\n",
      "train loss:   1.597287\n",
      "train loss:   1.735875\n",
      "train loss:   1.790454\n",
      "train loss:   1.791846\n",
      "train loss:   1.746540\n",
      "train loss:   1.752477\n",
      "train loss:   1.743106\n",
      "train loss:   1.825269\n",
      "train loss:   1.717770\n",
      "train loss:   1.766886\n",
      "train loss:   1.751305\n",
      "train loss:   1.892440\n",
      "train loss:   1.818135\n",
      "train loss:   1.455119\n",
      "train loss:   2.067302\n",
      "train loss:   1.666679\n",
      "train loss:   1.841929\n",
      "train loss:   2.045561\n",
      "########### epoch 10 ###########\n",
      "########### loop 1350 ###########\n",
      "test loss:   1.702481   test accuracy:   0.625000\n",
      "########### loop 1350 ###########\n",
      "train loss:   1.769265\n",
      "train loss:   1.806474\n",
      "train loss:   1.723181\n",
      "train loss:   1.778019\n",
      "train loss:   1.902406\n",
      "train loss:   1.851178\n",
      "train loss:   1.962629\n",
      "train loss:   1.581567\n",
      "train loss:   1.869934\n",
      "train loss:   1.707761\n",
      "train loss:   1.784677\n",
      "train loss:   1.755650\n",
      "train loss:   1.676581\n",
      "train loss:   1.845746\n",
      "train loss:   1.732423\n",
      "train loss:   1.715267\n",
      "train loss:   1.965397\n",
      "train loss:   1.934734\n",
      "train loss:   1.863409\n",
      "train loss:   1.774400\n",
      "train loss:   1.770300\n",
      "train loss:   1.912238\n",
      "train loss:   2.129600\n",
      "train loss:   1.953798\n",
      "train loss:   1.939244\n",
      "train loss:   1.663077\n",
      "train loss:   1.645421\n",
      "train loss:   2.122822\n",
      "train loss:   1.836129\n",
      "train loss:   1.871203\n",
      "train loss:   1.776905\n",
      "train loss:   1.888271\n",
      "train loss:   1.720538\n",
      "train loss:   1.761064\n",
      "train loss:   1.778805\n",
      "train loss:   1.779654\n",
      "train loss:   1.624469\n",
      "train loss:   1.531570\n",
      "train loss:   1.533707\n",
      "train loss:   1.988370\n",
      "train loss:   1.675189\n",
      "train loss:   1.913300\n",
      "train loss:   1.780764\n",
      "train loss:   1.740894\n",
      "train loss:   1.675743\n",
      "train loss:   1.960810\n",
      "train loss:   1.772992\n",
      "train loss:   1.683999\n",
      "train loss:   2.146789\n",
      "train loss:   1.834845\n",
      "########### epoch 10 ###########\n",
      "########### loop 1400 ###########\n",
      "test loss:   1.665260   test accuracy:   0.687500\n",
      "########### loop 1400 ###########\n",
      "train loss:   1.574241\n",
      "train loss:   2.067711\n",
      "train loss:   1.802716\n",
      "train loss:   1.881042\n",
      "train loss:   1.573555\n",
      "train loss:   1.779032\n",
      "train loss:   1.974110\n",
      "train loss:   1.711289\n",
      "train loss:   1.656301\n",
      "train loss:   1.862217\n",
      "train loss:   1.699964\n",
      "train loss:   1.531641\n",
      "train loss:   1.770099\n",
      "train loss:   1.791099\n",
      "train loss:   1.870922\n",
      "train loss:   1.990963\n",
      "train loss:   1.833116\n",
      "train loss:   1.768487\n",
      "train loss:   1.705764\n",
      "train loss:   1.892958\n",
      "train loss:   1.776651\n",
      "train loss:   1.810982\n",
      "train loss:   1.748939\n",
      "train loss:   1.783095\n",
      "train loss:   1.941944\n",
      "train loss:   1.819146\n",
      "train loss:   1.767155\n",
      "train loss:   2.106060\n",
      "train loss:   1.778462\n",
      "train loss:   1.795322\n",
      "train loss:   1.802725\n",
      "train loss:   2.194112\n",
      "train loss:   1.651666\n",
      "train loss:   1.729602\n",
      "train loss:   1.602594\n",
      "train loss:   1.720785\n",
      "train loss:   1.656271\n",
      "train loss:   1.799810\n",
      "train loss:   1.716183\n",
      "train loss:   1.661346\n",
      "train loss:   1.828453\n",
      "train loss:   1.796885\n",
      "train loss:   1.708981\n",
      "train loss:   1.758417\n",
      "train loss:   1.843481\n",
      "train loss:   1.616034\n",
      "train loss:   1.705161\n",
      "train loss:   1.717626\n",
      "train loss:   1.953569\n",
      "train loss:   1.795236\n",
      "########### epoch 11 ###########\n",
      "########### loop 1450 ###########\n",
      "test loss:   1.687500   test accuracy:   0.625000\n",
      "########### loop 1450 ###########\n",
      "train loss:   1.662241\n",
      "train loss:   1.923486\n",
      "train loss:   1.881631\n",
      "train loss:   1.807448\n",
      "train loss:   1.511714\n",
      "train loss:   1.799041\n",
      "train loss:   1.648751\n",
      "train loss:   1.662375\n",
      "train loss:   1.784641\n",
      "train loss:   1.657721\n",
      "train loss:   1.627416\n",
      "train loss:   1.779239\n",
      "train loss:   1.643681\n",
      "train loss:   1.814632\n",
      "train loss:   1.641768\n",
      "train loss:   1.645328\n",
      "train loss:   1.820579\n",
      "train loss:   1.655427\n",
      "train loss:   1.608477\n",
      "train loss:   2.036635\n",
      "train loss:   1.919022\n",
      "train loss:   1.810106\n",
      "train loss:   1.323486\n",
      "train loss:   1.992270\n",
      "train loss:   1.852580\n",
      "train loss:   1.811734\n",
      "train loss:   1.702623\n",
      "train loss:   1.853451\n",
      "train loss:   1.798460\n",
      "train loss:   1.794391\n",
      "train loss:   1.682397\n",
      "train loss:   1.611841\n",
      "train loss:   1.960052\n",
      "train loss:   1.768701\n",
      "train loss:   1.741237\n",
      "train loss:   1.561800\n",
      "train loss:   1.844624\n",
      "train loss:   1.647916\n",
      "train loss:   1.683001\n",
      "train loss:   1.717661\n",
      "train loss:   1.890681\n",
      "train loss:   1.629157\n",
      "train loss:   1.907328\n",
      "train loss:   1.726990\n",
      "train loss:   1.853283\n",
      "train loss:   1.767898\n",
      "train loss:   1.602275\n",
      "train loss:   1.762974\n",
      "train loss:   1.738701\n",
      "train loss:   1.797534\n",
      "########### epoch 11 ###########\n",
      "########### loop 1500 ###########\n",
      "test loss:   1.545927   test accuracy:   0.625000\n",
      "########### loop 1500 ###########\n",
      "train loss:   1.820478\n",
      "train loss:   1.667731\n",
      "train loss:   2.026742\n",
      "train loss:   1.802481\n",
      "train loss:   1.872861\n",
      "train loss:   1.584782\n",
      "train loss:   1.693083\n",
      "train loss:   1.892583\n",
      "train loss:   1.763821\n",
      "train loss:   1.878592\n",
      "train loss:   1.687210\n",
      "train loss:   1.783996\n",
      "train loss:   1.738956\n",
      "train loss:   1.569726\n",
      "train loss:   1.861552\n",
      "train loss:   1.730047\n",
      "train loss:   1.554096\n",
      "train loss:   1.824880\n",
      "train loss:   2.004665\n",
      "train loss:   1.838253\n",
      "train loss:   1.879924\n",
      "train loss:   1.724405\n",
      "train loss:   1.630495\n",
      "train loss:   1.232050\n",
      "train loss:   1.868681\n",
      "train loss:   1.685948\n",
      "train loss:   1.921537\n",
      "train loss:   1.563340\n",
      "train loss:   1.775864\n",
      "train loss:   1.955855\n",
      "train loss:   1.797511\n",
      "train loss:   1.951685\n",
      "train loss:   1.895536\n",
      "train loss:   1.690750\n",
      "train loss:   1.803903\n",
      "train loss:   1.900255\n",
      "train loss:   1.535026\n",
      "train loss:   1.895655\n",
      "train loss:   1.817461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.946809\n",
      "train loss:   1.839997\n",
      "train loss:   1.892902\n",
      "train loss:   1.895123\n",
      "train loss:   1.842718\n",
      "train loss:   1.672526\n",
      "train loss:   1.755153\n",
      "train loss:   1.822498\n",
      "train loss:   1.881497\n",
      "train loss:   1.710246\n",
      "train loss:   1.729963\n",
      "########### epoch 11 ###########\n",
      "########### loop 1550 ###########\n",
      "test loss:   1.586509   test accuracy:   0.781250\n",
      "########### loop 1550 ###########\n",
      "train loss:   1.761675\n",
      "train loss:   1.522902\n",
      "train loss:   1.733208\n",
      "train loss:   1.604952\n",
      "train loss:   1.924763\n",
      "train loss:   1.546749\n",
      "train loss:   1.769189\n",
      "train loss:   1.667481\n",
      "train loss:   1.427672\n",
      "train loss:   1.696828\n",
      "train loss:   1.771345\n",
      "train loss:   1.606164\n",
      "train loss:   1.588723\n",
      "train loss:   1.704301\n",
      "train loss:   1.653208\n",
      "train loss:   1.692884\n",
      "train loss:   1.628071\n",
      "train loss:   1.919687\n",
      "train loss:   1.788189\n",
      "train loss:   1.708997\n",
      "train loss:   1.790128\n",
      "train loss:   1.785319\n",
      "train loss:   1.689580\n",
      "train loss:   1.646535\n",
      "train loss:   1.730152\n",
      "train loss:   1.741811\n",
      "train loss:   1.886171\n",
      "train loss:   1.718837\n",
      "train loss:   1.772109\n",
      "train loss:   1.693636\n",
      "train loss:   1.545801\n",
      "train loss:   1.732572\n",
      "train loss:   1.659067\n",
      "train loss:   1.942639\n",
      "train loss:   1.843032\n",
      "train loss:   1.710559\n",
      "train loss:   1.933506\n",
      "train loss:   1.837091\n",
      "train loss:   1.776745\n",
      "train loss:   1.694301\n",
      "train loss:   1.380601\n",
      "train loss:   1.743749\n",
      "train loss:   1.718581\n",
      "train loss:   1.648900\n",
      "train loss:   1.579199\n",
      "train loss:   1.881940\n",
      "train loss:   1.654506\n",
      "train loss:   1.567225\n",
      "train loss:   1.671476\n",
      "train loss:   1.660127\n",
      "########### epoch 12 ###########\n",
      "########### loop 1600 ###########\n",
      "test loss:   1.555686   test accuracy:   0.625000\n",
      "########### loop 1600 ###########\n",
      "train loss:   1.818433\n",
      "train loss:   1.889062\n",
      "train loss:   1.648182\n",
      "train loss:   1.749498\n",
      "train loss:   1.774509\n",
      "train loss:   1.829697\n",
      "train loss:   1.655591\n",
      "train loss:   1.453619\n",
      "train loss:   1.876838\n",
      "train loss:   1.713988\n",
      "train loss:   1.625500\n",
      "train loss:   1.500408\n",
      "train loss:   1.798181\n",
      "train loss:   1.625343\n",
      "train loss:   1.731445\n",
      "train loss:   1.564100\n",
      "train loss:   1.603414\n",
      "train loss:   1.636173\n",
      "train loss:   1.580238\n",
      "train loss:   1.719795\n",
      "train loss:   1.912935\n",
      "train loss:   1.782756\n",
      "train loss:   1.731734\n",
      "train loss:   1.492837\n",
      "train loss:   1.601917\n",
      "train loss:   1.762530\n",
      "train loss:   1.501489\n",
      "train loss:   1.641786\n",
      "train loss:   1.905236\n",
      "train loss:   1.672197\n",
      "train loss:   1.629537\n",
      "train loss:   1.773974\n",
      "train loss:   1.526724\n",
      "train loss:   1.913547\n",
      "train loss:   1.779985\n",
      "train loss:   1.800100\n",
      "train loss:   1.670205\n",
      "train loss:   1.662393\n",
      "train loss:   1.754581\n",
      "train loss:   1.632684\n",
      "train loss:   1.875437\n",
      "train loss:   1.746079\n",
      "train loss:   1.642605\n",
      "train loss:   1.752356\n",
      "train loss:   1.785317\n",
      "train loss:   1.782048\n",
      "train loss:   1.765119\n",
      "train loss:   1.848051\n",
      "train loss:   1.662133\n",
      "train loss:   1.709911\n",
      "########### epoch 12 ###########\n",
      "########### loop 1650 ###########\n",
      "test loss:   1.483541   test accuracy:   0.781250\n",
      "########### loop 1650 ###########\n",
      "train loss:   1.582394\n",
      "train loss:   1.722540\n",
      "train loss:   1.865745\n",
      "train loss:   1.702520\n",
      "train loss:   1.576018\n",
      "train loss:   1.736599\n",
      "train loss:   1.687013\n",
      "train loss:   1.803649\n",
      "train loss:   1.815667\n",
      "train loss:   1.626603\n",
      "train loss:   1.695819\n",
      "train loss:   1.794436\n",
      "train loss:   1.858878\n",
      "train loss:   1.733589\n",
      "train loss:   1.589550\n",
      "train loss:   1.613258\n",
      "train loss:   1.811888\n",
      "train loss:   1.759893\n",
      "train loss:   2.015400\n",
      "train loss:   1.657306\n",
      "train loss:   1.632757\n",
      "train loss:   1.678147\n",
      "train loss:   1.672170\n",
      "train loss:   1.829053\n",
      "train loss:   1.840724\n",
      "train loss:   1.713527\n",
      "train loss:   1.868392\n",
      "train loss:   1.547898\n",
      "train loss:   1.412297\n",
      "train loss:   1.729959\n",
      "train loss:   1.795164\n",
      "train loss:   1.894397\n",
      "train loss:   1.587141\n",
      "train loss:   1.614710\n",
      "train loss:   1.467087\n",
      "train loss:   1.714607\n",
      "train loss:   1.803255\n",
      "train loss:   1.705039\n",
      "train loss:   1.879148\n",
      "train loss:   1.851419\n",
      "train loss:   1.426036\n",
      "train loss:   1.574320\n",
      "train loss:   1.720483\n",
      "train loss:   1.750754\n",
      "train loss:   1.483514\n",
      "train loss:   1.836554\n",
      "train loss:   1.850777\n",
      "train loss:   2.009145\n",
      "train loss:   1.807378\n",
      "train loss:   1.778289\n",
      "########### epoch 13 ###########\n",
      "########### loop 1700 ###########\n",
      "test loss:   1.552113   test accuracy:   0.593750\n",
      "########### loop 1700 ###########\n",
      "train loss:   1.906662\n",
      "train loss:   1.686088\n",
      "train loss:   1.706160\n",
      "train loss:   1.924240\n",
      "train loss:   1.704808\n",
      "train loss:   1.712189\n",
      "train loss:   1.702889\n",
      "train loss:   1.762685\n",
      "train loss:   1.789592\n",
      "train loss:   1.723022\n",
      "train loss:   1.734002\n",
      "train loss:   1.728114\n",
      "train loss:   1.721864\n",
      "train loss:   1.793350\n",
      "train loss:   1.450813\n",
      "train loss:   1.541869\n",
      "train loss:   1.629975\n",
      "train loss:   2.015746\n",
      "train loss:   1.679216\n",
      "train loss:   1.818406\n",
      "train loss:   1.875643\n",
      "train loss:   1.829660\n",
      "train loss:   1.680760\n",
      "train loss:   1.580811\n",
      "train loss:   1.806126\n",
      "train loss:   1.614036\n",
      "train loss:   1.594597\n",
      "train loss:   1.587228\n",
      "train loss:   1.732552\n",
      "train loss:   1.806793\n",
      "train loss:   1.772839\n",
      "train loss:   1.693547\n",
      "train loss:   1.675978\n",
      "train loss:   1.647733\n",
      "train loss:   1.896986\n",
      "train loss:   1.700186\n",
      "train loss:   1.838622\n",
      "train loss:   1.617271\n",
      "train loss:   1.767312\n",
      "train loss:   1.567584\n",
      "train loss:   1.635682\n",
      "train loss:   1.774193\n",
      "train loss:   1.610887\n",
      "train loss:   1.754106\n",
      "train loss:   1.731818\n",
      "train loss:   1.535472\n",
      "train loss:   1.693307\n",
      "train loss:   1.748146\n",
      "train loss:   1.829227\n",
      "train loss:   1.775658\n",
      "########### epoch 13 ###########\n",
      "########### loop 1750 ###########\n",
      "test loss:   1.440706   test accuracy:   0.750000\n",
      "########### loop 1750 ###########\n",
      "train loss:   1.725795\n",
      "train loss:   1.652026\n",
      "train loss:   1.864456\n",
      "train loss:   1.761077\n",
      "train loss:   1.931454\n",
      "train loss:   1.842909\n",
      "train loss:   1.806497\n",
      "train loss:   1.884991\n",
      "train loss:   1.694133\n",
      "train loss:   1.830112\n",
      "train loss:   1.672838\n",
      "train loss:   1.668131\n",
      "train loss:   1.867545\n",
      "train loss:   1.646245\n",
      "train loss:   1.790192\n",
      "train loss:   1.733386\n",
      "train loss:   1.814664\n",
      "train loss:   1.604827\n",
      "train loss:   1.503822\n",
      "train loss:   1.682878\n",
      "train loss:   1.371749\n",
      "train loss:   1.818850\n",
      "train loss:   1.668680\n",
      "train loss:   1.577896\n",
      "train loss:   1.707678\n",
      "train loss:   1.597666\n",
      "train loss:   1.526076\n",
      "train loss:   1.940060\n",
      "train loss:   1.632084\n",
      "train loss:   1.608723\n",
      "train loss:   1.797353\n",
      "train loss:   1.588054\n",
      "train loss:   1.581069\n",
      "train loss:   1.977612\n",
      "train loss:   1.666117\n",
      "train loss:   1.748172\n",
      "train loss:   1.685512\n",
      "train loss:   2.023343\n",
      "train loss:   1.982284\n",
      "train loss:   1.518774\n",
      "train loss:   1.601698\n",
      "train loss:   1.617072\n",
      "train loss:   1.603293\n",
      "train loss:   1.434154\n",
      "train loss:   1.704575\n",
      "train loss:   1.682962\n",
      "train loss:   1.790122\n",
      "train loss:   1.683972\n",
      "train loss:   1.411592\n",
      "train loss:   1.568681\n",
      "########### epoch 13 ###########\n",
      "########### loop 1800 ###########\n",
      "test loss:   1.637175   test accuracy:   0.625000\n",
      "########### loop 1800 ###########\n",
      "train loss:   1.430399\n",
      "train loss:   1.808386\n",
      "train loss:   1.784988\n",
      "train loss:   1.498634\n",
      "train loss:   1.773667\n",
      "train loss:   1.630761\n",
      "train loss:   1.655179\n",
      "train loss:   1.778689\n",
      "train loss:   1.906375\n",
      "train loss:   1.786380\n",
      "train loss:   1.575202\n",
      "train loss:   1.914916\n",
      "train loss:   1.510836\n",
      "train loss:   1.586254\n",
      "train loss:   1.685343\n",
      "train loss:   1.626022\n",
      "train loss:   1.706798\n",
      "train loss:   1.563121\n",
      "train loss:   1.697648\n",
      "train loss:   1.998634\n",
      "train loss:   1.825739\n",
      "train loss:   1.527231\n",
      "train loss:   1.624562\n",
      "train loss:   1.694054\n",
      "train loss:   1.735009\n",
      "train loss:   1.716052\n",
      "train loss:   1.693084\n",
      "train loss:   1.822998\n",
      "train loss:   2.022447\n",
      "train loss:   1.718706\n",
      "train loss:   1.815770\n",
      "train loss:   1.610658\n",
      "train loss:   1.524033\n",
      "train loss:   1.714693\n",
      "train loss:   2.064586\n",
      "train loss:   1.589104\n",
      "train loss:   1.735404\n",
      "train loss:   1.937276\n",
      "train loss:   1.838995\n",
      "train loss:   1.944089\n",
      "train loss:   1.887988\n",
      "train loss:   1.657503\n",
      "train loss:   1.953148\n",
      "train loss:   1.637201\n",
      "train loss:   1.571712\n",
      "train loss:   1.589304\n",
      "train loss:   1.547222\n",
      "train loss:   1.911952\n",
      "train loss:   2.075383\n",
      "train loss:   1.645322\n",
      "########### epoch 14 ###########\n",
      "########### loop 1850 ###########\n",
      "test loss:   1.432341   test accuracy:   0.812500\n",
      "########### loop 1850 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.678549\n",
      "train loss:   1.857422\n",
      "train loss:   1.547640\n",
      "train loss:   1.568715\n",
      "train loss:   1.720920\n",
      "train loss:   1.645071\n",
      "train loss:   1.590324\n",
      "train loss:   1.737837\n",
      "train loss:   1.756824\n",
      "train loss:   1.850746\n",
      "train loss:   1.899270\n",
      "train loss:   1.446817\n",
      "train loss:   1.800043\n",
      "train loss:   1.491023\n",
      "train loss:   1.644123\n",
      "train loss:   1.544246\n",
      "train loss:   2.097611\n",
      "train loss:   1.812629\n",
      "train loss:   1.766626\n",
      "train loss:   1.640033\n",
      "train loss:   1.836681\n",
      "train loss:   1.676561\n",
      "train loss:   1.543140\n",
      "train loss:   1.872653\n",
      "train loss:   1.666541\n",
      "train loss:   1.841371\n",
      "train loss:   1.754565\n",
      "train loss:   1.682587\n",
      "train loss:   1.660887\n",
      "train loss:   1.782741\n",
      "train loss:   1.845770\n",
      "train loss:   1.868090\n",
      "train loss:   1.777811\n",
      "train loss:   1.571573\n",
      "train loss:   1.750908\n",
      "train loss:   1.721960\n",
      "train loss:   1.747342\n",
      "train loss:   1.608541\n",
      "train loss:   1.927135\n",
      "train loss:   1.636196\n",
      "train loss:   1.966134\n",
      "train loss:   1.713739\n",
      "train loss:   1.857434\n",
      "train loss:   1.668102\n",
      "train loss:   1.683672\n",
      "train loss:   1.829102\n",
      "train loss:   1.712895\n",
      "train loss:   1.724038\n",
      "train loss:   1.991039\n",
      "train loss:   1.558948\n",
      "########### epoch 14 ###########\n",
      "########### loop 1900 ###########\n",
      "test loss:   1.435913   test accuracy:   0.718750\n",
      "########### loop 1900 ###########\n",
      "train loss:   1.652130\n",
      "train loss:   1.626243\n",
      "train loss:   1.800778\n",
      "train loss:   1.910249\n",
      "train loss:   1.265697\n",
      "train loss:   1.937841\n",
      "train loss:   1.597693\n",
      "train loss:   1.548002\n",
      "train loss:   1.754218\n",
      "train loss:   1.493505\n",
      "train loss:   1.670528\n",
      "train loss:   1.736198\n",
      "train loss:   1.609238\n",
      "train loss:   1.526674\n",
      "train loss:   1.717856\n",
      "train loss:   1.582670\n",
      "train loss:   1.751865\n",
      "train loss:   1.395294\n",
      "train loss:   1.580413\n",
      "train loss:   1.916173\n",
      "train loss:   1.852007\n",
      "train loss:   1.704822\n",
      "train loss:   1.464942\n",
      "train loss:   1.433362\n",
      "train loss:   1.660832\n",
      "train loss:   1.537894\n",
      "train loss:   1.867019\n",
      "train loss:   1.807882\n",
      "train loss:   1.790552\n",
      "train loss:   1.868093\n",
      "train loss:   1.739234\n",
      "train loss:   1.562660\n",
      "train loss:   1.465811\n",
      "train loss:   1.584992\n",
      "train loss:   1.833311\n",
      "train loss:   1.537715\n",
      "train loss:   1.713120\n",
      "train loss:   1.780018\n",
      "train loss:   1.762362\n",
      "train loss:   1.679984\n",
      "train loss:   1.813796\n",
      "train loss:   1.817233\n",
      "train loss:   1.660838\n",
      "train loss:   1.459684\n",
      "train loss:   1.550540\n",
      "train loss:   1.480994\n",
      "train loss:   1.632150\n",
      "train loss:   1.845072\n",
      "train loss:   1.890793\n",
      "train loss:   1.835984\n",
      "########### epoch 14 ###########\n",
      "########### loop 1950 ###########\n",
      "test loss:   1.433321   test accuracy:   0.656250\n",
      "########### loop 1950 ###########\n",
      "train loss:   1.526066\n",
      "train loss:   1.506201\n",
      "train loss:   1.890103\n",
      "train loss:   1.572657\n",
      "train loss:   1.680073\n",
      "train loss:   1.725965\n",
      "train loss:   1.500510\n",
      "train loss:   1.582267\n",
      "train loss:   1.811884\n",
      "train loss:   1.751884\n",
      "train loss:   1.905017\n",
      "train loss:   1.831640\n",
      "train loss:   1.912698\n",
      "train loss:   1.578526\n",
      "train loss:   1.876175\n",
      "train loss:   1.861575\n",
      "train loss:   1.547343\n",
      "train loss:   1.643272\n",
      "train loss:   1.642344\n",
      "train loss:   1.613533\n",
      "train loss:   1.533465\n",
      "train loss:   1.674730\n",
      "train loss:   1.704625\n",
      "train loss:   1.810030\n",
      "train loss:   1.727682\n",
      "train loss:   1.555312\n",
      "train loss:   1.787468\n",
      "train loss:   1.872434\n",
      "train loss:   1.678938\n",
      "train loss:   2.082546\n",
      "train loss:   1.843987\n",
      "train loss:   1.696259\n",
      "train loss:   1.512217\n",
      "train loss:   1.698445\n",
      "train loss:   1.599160\n",
      "train loss:   1.709547\n",
      "train loss:   1.776599\n",
      "train loss:   1.683698\n",
      "train loss:   1.932614\n",
      "train loss:   1.188324\n",
      "train loss:   1.860512\n",
      "train loss:   1.732183\n",
      "train loss:   1.562712\n",
      "train loss:   1.864526\n",
      "train loss:   1.737541\n",
      "train loss:   1.791780\n",
      "train loss:   1.667687\n",
      "train loss:   1.697058\n",
      "train loss:   1.814812\n",
      "train loss:   1.784153\n",
      "########### epoch 15 ###########\n",
      "########### loop 2000 ###########\n",
      "test loss:   1.328790   test accuracy:   0.687500\n",
      "########### loop 2000 ###########\n",
      "train loss:   1.735631\n",
      "train loss:   1.538821\n",
      "train loss:   1.749600\n",
      "train loss:   1.599121\n",
      "train loss:   1.728700\n",
      "train loss:   1.860855\n",
      "train loss:   1.469871\n",
      "train loss:   1.894769\n",
      "train loss:   1.924734\n",
      "train loss:   1.966119\n",
      "train loss:   1.974332\n",
      "train loss:   1.694613\n",
      "train loss:   1.394626\n",
      "train loss:   1.696887\n",
      "train loss:   1.859313\n",
      "train loss:   1.876624\n",
      "train loss:   1.745533\n",
      "train loss:   1.503495\n",
      "train loss:   1.691044\n",
      "train loss:   1.609615\n",
      "train loss:   1.515018\n",
      "train loss:   1.848051\n",
      "train loss:   1.804866\n",
      "train loss:   1.599795\n",
      "train loss:   1.527508\n",
      "train loss:   1.663358\n",
      "train loss:   2.020979\n",
      "train loss:   1.667854\n",
      "train loss:   1.547779\n",
      "train loss:   1.605300\n",
      "train loss:   1.675075\n",
      "train loss:   1.549106\n",
      "train loss:   1.755713\n",
      "train loss:   1.911606\n",
      "train loss:   1.753228\n",
      "train loss:   1.588017\n",
      "train loss:   1.692310\n",
      "train loss:   1.691895\n",
      "train loss:   1.552893\n",
      "train loss:   1.952106\n",
      "train loss:   1.499619\n",
      "train loss:   1.657344\n",
      "train loss:   1.843764\n",
      "train loss:   1.704314\n",
      "train loss:   1.606070\n",
      "train loss:   1.761808\n",
      "train loss:   1.627384\n",
      "train loss:   1.811170\n",
      "train loss:   1.710428\n",
      "train loss:   1.913918\n",
      "########### epoch 15 ###########\n",
      "########### loop 2050 ###########\n",
      "test loss:   1.501554   test accuracy:   0.781250\n",
      "########### loop 2050 ###########\n",
      "train loss:   1.700628\n",
      "train loss:   1.738452\n",
      "train loss:   1.752067\n",
      "train loss:   1.543126\n",
      "train loss:   1.452586\n",
      "train loss:   1.640235\n",
      "train loss:   1.695110\n",
      "train loss:   1.841316\n",
      "train loss:   1.678250\n",
      "train loss:   1.833026\n",
      "train loss:   1.768491\n",
      "train loss:   1.689398\n",
      "train loss:   1.734271\n",
      "train loss:   1.797962\n",
      "train loss:   1.763439\n",
      "train loss:   1.868927\n",
      "train loss:   1.576198\n",
      "train loss:   1.646053\n",
      "train loss:   1.804074\n",
      "train loss:   1.655137\n",
      "train loss:   1.524254\n",
      "train loss:   1.528243\n",
      "train loss:   1.926145\n",
      "train loss:   1.807092\n",
      "train loss:   1.448393\n",
      "train loss:   1.692669\n",
      "train loss:   1.711799\n",
      "train loss:   1.630770\n",
      "train loss:   1.741892\n",
      "train loss:   1.538625\n",
      "train loss:   1.521307\n",
      "train loss:   1.688878\n",
      "train loss:   1.569483\n",
      "train loss:   1.783808\n",
      "train loss:   1.691278\n",
      "train loss:   1.838042\n",
      "train loss:   1.705641\n",
      "train loss:   1.588491\n",
      "train loss:   1.617029\n",
      "train loss:   1.622162\n",
      "train loss:   1.937827\n",
      "train loss:   1.567820\n",
      "train loss:   1.682998\n",
      "train loss:   1.746188\n",
      "train loss:   1.805023\n",
      "train loss:   1.957568\n",
      "train loss:   1.704938\n",
      "train loss:   1.608213\n",
      "train loss:   1.885527\n",
      "train loss:   1.724069\n",
      "########### epoch 15 ###########\n",
      "########### loop 2100 ###########\n",
      "test loss:   1.632095   test accuracy:   0.625000\n",
      "########### loop 2100 ###########\n",
      "train loss:   1.670052\n",
      "train loss:   1.768786\n",
      "train loss:   1.864482\n",
      "train loss:   1.584256\n",
      "train loss:   1.653178\n",
      "train loss:   1.660590\n",
      "train loss:   1.860140\n",
      "train loss:   1.750192\n",
      "train loss:   1.769503\n",
      "train loss:   1.608319\n",
      "train loss:   1.461541\n",
      "train loss:   1.879838\n",
      "train loss:   1.601441\n",
      "train loss:   1.565850\n",
      "train loss:   1.651337\n",
      "train loss:   1.720774\n",
      "train loss:   1.665858\n",
      "train loss:   1.463464\n",
      "train loss:   1.538861\n",
      "train loss:   1.706322\n",
      "train loss:   1.685941\n",
      "train loss:   1.834728\n",
      "train loss:   1.714269\n",
      "train loss:   1.683982\n",
      "train loss:   1.567056\n",
      "train loss:   1.801982\n",
      "train loss:   1.681640\n",
      "train loss:   1.748269\n",
      "train loss:   1.799761\n",
      "train loss:   1.579861\n",
      "train loss:   1.676263\n",
      "train loss:   1.870265\n",
      "train loss:   1.554420\n",
      "train loss:   1.599397\n",
      "train loss:   1.697291\n",
      "train loss:   1.681351\n",
      "train loss:   1.596852\n",
      "train loss:   1.705305\n",
      "train loss:   1.586637\n",
      "train loss:   1.464564\n",
      "train loss:   1.701201\n",
      "train loss:   1.818976\n",
      "train loss:   1.391531\n",
      "train loss:   1.442357\n",
      "train loss:   1.445511\n",
      "train loss:   1.437270\n",
      "train loss:   1.607084\n",
      "train loss:   1.697795\n",
      "train loss:   1.864046\n",
      "train loss:   1.714636\n",
      "########### epoch 16 ###########\n",
      "########### loop 2150 ###########\n",
      "test loss:   1.416234   test accuracy:   0.812500\n",
      "########### loop 2150 ###########\n",
      "train loss:   1.924431\n",
      "train loss:   1.323010\n",
      "train loss:   1.838348\n",
      "train loss:   1.673779\n",
      "train loss:   1.708938\n",
      "train loss:   1.783874\n",
      "train loss:   1.690197\n",
      "train loss:   1.589776\n",
      "train loss:   1.685300\n",
      "train loss:   1.655503\n",
      "train loss:   1.959818\n",
      "train loss:   1.695292\n",
      "train loss:   1.773056\n",
      "train loss:   1.656465\n",
      "train loss:   1.481457\n",
      "train loss:   1.726810\n",
      "train loss:   1.657980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.523397\n",
      "train loss:   1.675602\n",
      "train loss:   1.483563\n",
      "train loss:   1.652059\n",
      "train loss:   1.735207\n",
      "train loss:   1.925639\n",
      "train loss:   1.783029\n",
      "train loss:   1.652263\n",
      "train loss:   1.386620\n",
      "train loss:   1.568357\n",
      "train loss:   1.724465\n",
      "train loss:   1.650505\n",
      "train loss:   1.845822\n",
      "train loss:   1.514541\n",
      "train loss:   1.621506\n",
      "train loss:   1.773710\n",
      "train loss:   1.806356\n",
      "train loss:   1.497782\n",
      "train loss:   1.940193\n",
      "train loss:   1.773586\n",
      "train loss:   1.745374\n",
      "train loss:   1.826320\n",
      "train loss:   1.635346\n",
      "train loss:   1.743144\n",
      "train loss:   1.772358\n",
      "train loss:   1.733821\n",
      "train loss:   1.482179\n",
      "train loss:   1.731781\n",
      "train loss:   1.951983\n",
      "train loss:   1.672990\n",
      "train loss:   1.443880\n",
      "train loss:   1.682796\n",
      "train loss:   1.476501\n",
      "########### epoch 16 ###########\n",
      "########### loop 2200 ###########\n",
      "test loss:   1.536888   test accuracy:   0.625000\n",
      "########### loop 2200 ###########\n",
      "train loss:   1.705624\n",
      "train loss:   1.704234\n",
      "train loss:   1.553527\n",
      "train loss:   1.874109\n",
      "train loss:   1.443248\n",
      "train loss:   1.654571\n",
      "train loss:   1.747087\n",
      "train loss:   1.757081\n",
      "train loss:   1.663337\n",
      "train loss:   1.918482\n",
      "train loss:   1.760940\n",
      "train loss:   1.509002\n",
      "train loss:   1.850663\n",
      "train loss:   1.767643\n",
      "train loss:   1.822991\n",
      "train loss:   1.930172\n",
      "train loss:   1.526351\n",
      "train loss:   1.644472\n",
      "train loss:   1.680676\n",
      "train loss:   1.939627\n",
      "train loss:   1.641008\n",
      "train loss:   1.756254\n",
      "train loss:   1.765984\n",
      "train loss:   1.379623\n",
      "train loss:   1.493420\n",
      "train loss:   1.383687\n",
      "train loss:   1.968872\n",
      "train loss:   1.744038\n",
      "train loss:   1.439180\n",
      "train loss:   1.371335\n",
      "train loss:   1.955763\n",
      "train loss:   1.312864\n",
      "train loss:   1.625367\n",
      "train loss:   1.431006\n",
      "train loss:   1.586271\n",
      "train loss:   1.919850\n",
      "train loss:   1.897125\n",
      "train loss:   1.756341\n",
      "train loss:   1.547334\n",
      "train loss:   1.612106\n",
      "train loss:   1.729892\n",
      "train loss:   1.681822\n",
      "train loss:   1.640259\n",
      "train loss:   1.458464\n",
      "train loss:   1.796084\n",
      "train loss:   1.695431\n",
      "train loss:   1.774225\n",
      "train loss:   1.588022\n",
      "train loss:   1.621878\n",
      "train loss:   1.445042\n",
      "########### epoch 16 ###########\n",
      "########### loop 2250 ###########\n",
      "test loss:   1.386592   test accuracy:   0.687500\n",
      "########### loop 2250 ###########\n",
      "train loss:   1.559576\n",
      "train loss:   1.633475\n",
      "train loss:   1.814368\n",
      "train loss:   1.618795\n",
      "train loss:   1.563734\n",
      "train loss:   1.715602\n",
      "train loss:   1.488895\n",
      "train loss:   1.641621\n",
      "train loss:   1.707919\n",
      "train loss:   1.824701\n",
      "train loss:   1.531248\n",
      "train loss:   1.748679\n",
      "train loss:   1.820269\n",
      "train loss:   1.745612\n",
      "train loss:   1.743756\n",
      "train loss:   1.476990\n",
      "train loss:   1.642726\n",
      "train loss:   1.800659\n",
      "train loss:   1.535182\n",
      "train loss:   1.538313\n",
      "train loss:   1.684896\n",
      "train loss:   1.550030\n",
      "train loss:   1.692169\n",
      "train loss:   1.731358\n",
      "train loss:   1.677787\n",
      "train loss:   1.616046\n",
      "train loss:   1.454138\n",
      "train loss:   1.764584\n",
      "train loss:   1.468333\n",
      "train loss:   1.494215\n",
      "train loss:   1.918734\n",
      "train loss:   1.463197\n",
      "train loss:   1.744931\n",
      "train loss:   1.681698\n",
      "train loss:   1.780897\n",
      "train loss:   1.817959\n",
      "train loss:   1.813819\n",
      "train loss:   1.829845\n",
      "train loss:   1.555014\n",
      "train loss:   1.749603\n",
      "train loss:   1.563476\n",
      "train loss:   1.800388\n",
      "train loss:   1.737021\n",
      "train loss:   1.456860\n",
      "train loss:   1.862287\n",
      "train loss:   1.671286\n",
      "train loss:   1.784954\n",
      "train loss:   1.876780\n",
      "train loss:   1.576823\n",
      "train loss:   1.625731\n",
      "########### epoch 17 ###########\n",
      "########### loop 2300 ###########\n",
      "test loss:   1.520012   test accuracy:   0.687500\n",
      "########### loop 2300 ###########\n",
      "train loss:   1.805828\n",
      "train loss:   1.540101\n",
      "train loss:   1.927521\n",
      "train loss:   1.583797\n",
      "train loss:   1.664473\n",
      "train loss:   1.772603\n",
      "train loss:   1.908209\n",
      "train loss:   1.497888\n",
      "train loss:   1.457236\n",
      "train loss:   1.555875\n",
      "train loss:   1.677121\n",
      "train loss:   1.508776\n",
      "train loss:   1.762894\n",
      "train loss:   1.536664\n",
      "train loss:   1.887454\n",
      "train loss:   1.761250\n",
      "train loss:   1.685487\n",
      "train loss:   1.733473\n",
      "train loss:   1.685009\n",
      "train loss:   1.477345\n",
      "train loss:   1.780831\n",
      "train loss:   1.741794\n",
      "train loss:   1.796108\n",
      "train loss:   1.443457\n",
      "train loss:   1.684136\n",
      "train loss:   1.459655\n",
      "train loss:   1.784680\n",
      "train loss:   1.502974\n",
      "train loss:   1.655777\n",
      "train loss:   1.531201\n",
      "train loss:   1.738098\n",
      "train loss:   1.630951\n",
      "train loss:   1.627328\n",
      "train loss:   1.697927\n",
      "train loss:   1.739422\n",
      "train loss:   1.513825\n",
      "train loss:   1.617726\n",
      "train loss:   1.546997\n",
      "train loss:   1.492849\n",
      "train loss:   1.538646\n",
      "train loss:   1.568520\n",
      "train loss:   1.623625\n",
      "train loss:   1.455842\n",
      "train loss:   1.668764\n",
      "train loss:   1.821573\n",
      "train loss:   1.806715\n",
      "train loss:   1.658759\n",
      "train loss:   1.692662\n",
      "train loss:   1.554665\n",
      "train loss:   1.770303\n",
      "########### epoch 17 ###########\n",
      "########### loop 2350 ###########\n",
      "test loss:   1.537587   test accuracy:   0.625000\n",
      "########### loop 2350 ###########\n",
      "train loss:   1.744443\n",
      "train loss:   1.705337\n",
      "train loss:   1.590546\n",
      "train loss:   1.377145\n",
      "train loss:   1.638319\n",
      "train loss:   1.598080\n",
      "train loss:   1.731011\n",
      "train loss:   1.608881\n",
      "train loss:   1.810942\n",
      "train loss:   1.709599\n",
      "train loss:   1.743576\n",
      "train loss:   1.481688\n",
      "train loss:   1.585231\n",
      "train loss:   1.685985\n",
      "train loss:   1.538331\n",
      "train loss:   1.865561\n",
      "train loss:   1.806931\n",
      "train loss:   1.702720\n",
      "train loss:   1.843881\n",
      "train loss:   1.690724\n",
      "train loss:   1.771971\n",
      "train loss:   1.825104\n",
      "train loss:   1.490353\n",
      "train loss:   1.879988\n",
      "train loss:   1.745249\n",
      "train loss:   1.604361\n",
      "train loss:   1.731711\n",
      "train loss:   1.620570\n",
      "train loss:   1.459059\n",
      "train loss:   1.569479\n",
      "train loss:   1.473248\n",
      "train loss:   1.505774\n",
      "train loss:   1.809286\n",
      "train loss:   1.803953\n",
      "train loss:   1.509256\n",
      "train loss:   1.361110\n",
      "train loss:   1.487320\n",
      "train loss:   1.542977\n",
      "train loss:   1.570203\n",
      "train loss:   1.977783\n",
      "train loss:   1.766948\n",
      "train loss:   1.813834\n",
      "train loss:   1.758871\n",
      "train loss:   1.595246\n",
      "train loss:   1.820879\n",
      "train loss:   1.476892\n",
      "train loss:   1.749841\n",
      "train loss:   1.568320\n",
      "train loss:   1.860484\n",
      "train loss:   1.827691\n",
      "########### epoch 18 ###########\n",
      "########### loop 2400 ###########\n",
      "test loss:   1.606877   test accuracy:   0.500000\n",
      "########### loop 2400 ###########\n",
      "train loss:   1.881454\n",
      "train loss:   1.537408\n",
      "train loss:   1.632399\n",
      "train loss:   1.627619\n",
      "train loss:   1.727504\n",
      "train loss:   2.007846\n",
      "train loss:   1.545823\n",
      "train loss:   1.773974\n",
      "train loss:   1.899651\n",
      "train loss:   1.767291\n",
      "train loss:   1.845341\n",
      "train loss:   1.728018\n",
      "train loss:   1.679205\n",
      "train loss:   1.699235\n",
      "train loss:   1.474388\n",
      "train loss:   1.935097\n",
      "train loss:   1.601746\n",
      "train loss:   1.843367\n",
      "train loss:   1.873129\n",
      "train loss:   1.646788\n",
      "train loss:   1.604092\n",
      "train loss:   1.822424\n",
      "train loss:   1.605202\n",
      "train loss:   1.682240\n",
      "train loss:   1.648380\n",
      "train loss:   1.811605\n",
      "train loss:   1.694126\n",
      "train loss:   1.584413\n",
      "train loss:   1.600894\n",
      "train loss:   1.466597\n",
      "train loss:   1.747534\n",
      "train loss:   1.668762\n",
      "train loss:   1.948059\n",
      "train loss:   1.696248\n",
      "train loss:   1.868644\n",
      "train loss:   2.087662\n",
      "train loss:   1.264858\n",
      "train loss:   1.692775\n",
      "train loss:   1.664553\n",
      "train loss:   1.566548\n",
      "train loss:   1.446176\n",
      "train loss:   1.727107\n",
      "train loss:   1.706619\n",
      "train loss:   1.368069\n",
      "train loss:   1.649201\n",
      "train loss:   1.866357\n",
      "train loss:   1.719087\n",
      "train loss:   2.068678\n",
      "train loss:   1.863764\n",
      "train loss:   1.597321\n",
      "########### epoch 18 ###########\n",
      "########### loop 2450 ###########\n",
      "test loss:   1.376259   test accuracy:   0.750000\n",
      "########### loop 2450 ###########\n",
      "train loss:   1.581999\n",
      "train loss:   1.666898\n",
      "train loss:   1.566854\n",
      "train loss:   1.790479\n",
      "train loss:   1.660542\n",
      "train loss:   1.639058\n",
      "train loss:   1.687834\n",
      "train loss:   1.879962\n",
      "train loss:   1.848747\n",
      "train loss:   1.873304\n",
      "train loss:   1.901474\n",
      "train loss:   1.718957\n",
      "train loss:   1.435763\n",
      "train loss:   1.523748\n",
      "train loss:   1.545106\n",
      "train loss:   1.707633\n",
      "train loss:   1.570720\n",
      "train loss:   1.739108\n",
      "train loss:   1.705522\n",
      "train loss:   1.201240\n",
      "train loss:   1.742101\n",
      "train loss:   1.710528\n",
      "train loss:   1.722863\n",
      "train loss:   1.676303\n",
      "train loss:   1.509064\n",
      "train loss:   1.783642\n",
      "train loss:   1.564694\n",
      "train loss:   1.873257\n",
      "train loss:   1.498415\n",
      "train loss:   1.508260\n",
      "train loss:   1.473063\n",
      "train loss:   1.434584\n",
      "train loss:   1.556821\n",
      "train loss:   1.624011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.674214\n",
      "train loss:   1.453650\n",
      "train loss:   1.649825\n",
      "train loss:   1.306661\n",
      "train loss:   1.695244\n",
      "train loss:   1.454899\n",
      "train loss:   1.759104\n",
      "train loss:   1.743863\n",
      "train loss:   1.695302\n",
      "train loss:   1.555877\n",
      "train loss:   1.688919\n",
      "train loss:   1.581165\n",
      "train loss:   1.767099\n",
      "train loss:   1.515289\n",
      "train loss:   1.501889\n",
      "train loss:   1.256296\n",
      "########### epoch 18 ###########\n",
      "########### loop 2500 ###########\n",
      "test loss:   1.458588   test accuracy:   0.593750\n",
      "########### loop 2500 ###########\n",
      "train loss:   1.802284\n",
      "train loss:   1.670148\n",
      "train loss:   1.586455\n",
      "train loss:   1.683099\n",
      "train loss:   1.782206\n",
      "train loss:   1.641844\n",
      "train loss:   1.574941\n",
      "train loss:   1.464938\n",
      "train loss:   1.667911\n",
      "train loss:   1.560076\n",
      "train loss:   1.503020\n",
      "train loss:   1.600774\n",
      "train loss:   1.617686\n",
      "train loss:   1.744649\n",
      "train loss:   1.633008\n",
      "train loss:   1.674946\n",
      "train loss:   1.814821\n",
      "train loss:   1.732483\n",
      "train loss:   1.713080\n",
      "train loss:   1.644174\n",
      "train loss:   1.828832\n",
      "train loss:   1.811513\n",
      "train loss:   1.416915\n",
      "train loss:   1.594897\n",
      "train loss:   1.597789\n",
      "train loss:   1.871144\n",
      "train loss:   1.495180\n",
      "train loss:   1.533994\n",
      "train loss:   1.606372\n",
      "train loss:   1.610858\n",
      "train loss:   1.874071\n",
      "train loss:   1.432627\n",
      "train loss:   1.663985\n",
      "train loss:   1.713560\n",
      "train loss:   1.092046\n",
      "train loss:   1.565080\n",
      "train loss:   1.665787\n",
      "train loss:   1.570306\n",
      "train loss:   1.612647\n",
      "train loss:   1.956963\n",
      "train loss:   1.659655\n",
      "train loss:   1.464431\n",
      "train loss:   1.693230\n",
      "train loss:   2.012830\n",
      "train loss:   1.564848\n",
      "train loss:   1.748503\n",
      "train loss:   1.583329\n",
      "train loss:   1.344377\n",
      "train loss:   1.503805\n",
      "train loss:   1.562053\n",
      "########### epoch 19 ###########\n",
      "########### loop 2550 ###########\n",
      "test loss:   1.409180   test accuracy:   0.562500\n",
      "########### loop 2550 ###########\n",
      "train loss:   1.701498\n",
      "train loss:   1.777295\n",
      "train loss:   1.491441\n",
      "train loss:   1.577118\n",
      "train loss:   1.697883\n",
      "train loss:   1.648694\n",
      "train loss:   1.512277\n",
      "train loss:   1.531984\n",
      "train loss:   1.316636\n",
      "train loss:   1.528690\n",
      "train loss:   1.523013\n",
      "train loss:   1.764922\n",
      "train loss:   1.736557\n",
      "train loss:   1.700727\n",
      "train loss:   1.671952\n",
      "train loss:   1.672618\n",
      "train loss:   1.821333\n",
      "train loss:   1.410866\n",
      "train loss:   1.789082\n",
      "train loss:   1.452112\n",
      "train loss:   1.892342\n",
      "train loss:   1.440358\n",
      "train loss:   1.588274\n",
      "train loss:   1.701851\n",
      "train loss:   1.587785\n",
      "train loss:   1.589803\n",
      "train loss:   1.630181\n",
      "train loss:   1.608744\n",
      "train loss:   1.454231\n",
      "train loss:   1.538479\n",
      "train loss:   1.591111\n",
      "train loss:   1.561390\n",
      "train loss:   1.657525\n",
      "train loss:   1.741925\n",
      "train loss:   1.409625\n",
      "train loss:   1.445865\n",
      "train loss:   1.607150\n",
      "train loss:   1.705382\n",
      "train loss:   1.300017\n",
      "train loss:   1.620818\n",
      "train loss:   1.754774\n",
      "train loss:   1.677180\n",
      "train loss:   1.639227\n",
      "train loss:   1.505736\n",
      "train loss:   1.446386\n",
      "train loss:   1.726821\n",
      "train loss:   1.463054\n",
      "train loss:   1.722019\n",
      "train loss:   1.910341\n",
      "train loss:   1.768914\n",
      "########### epoch 19 ###########\n",
      "########### loop 2600 ###########\n",
      "test loss:   1.307903   test accuracy:   0.843750\n",
      "########### loop 2600 ###########\n",
      "train loss:   1.810161\n",
      "train loss:   1.742005\n",
      "train loss:   1.617228\n",
      "train loss:   1.770302\n",
      "train loss:   1.711696\n",
      "train loss:   1.296034\n",
      "train loss:   1.446305\n",
      "train loss:   1.441332\n",
      "train loss:   1.899765\n",
      "train loss:   1.917342\n",
      "train loss:   1.735502\n",
      "train loss:   1.742823\n",
      "train loss:   1.413840\n",
      "train loss:   1.518813\n",
      "train loss:   1.723393\n",
      "train loss:   1.919302\n",
      "train loss:   1.782404\n",
      "train loss:   1.370422\n",
      "train loss:   1.637948\n",
      "train loss:   1.635948\n",
      "train loss:   1.345446\n",
      "train loss:   1.607968\n",
      "train loss:   1.642583\n",
      "train loss:   1.618411\n",
      "train loss:   1.760362\n",
      "train loss:   1.775275\n",
      "train loss:   1.791522\n",
      "train loss:   1.706089\n",
      "train loss:   1.646630\n",
      "train loss:   1.788207\n",
      "train loss:   1.908834\n",
      "train loss:   1.835256\n",
      "train loss:   1.441114\n",
      "train loss:   1.506345\n",
      "train loss:   1.532187\n",
      "train loss:   1.691084\n",
      "train loss:   1.821375\n",
      "train loss:   1.636489\n",
      "train loss:   1.647391\n",
      "train loss:   1.738653\n",
      "train loss:   1.545704\n",
      "train loss:   1.579845\n",
      "train loss:   1.780842\n",
      "train loss:   1.772293\n",
      "train loss:   1.829298\n",
      "train loss:   1.736088\n",
      "train loss:   1.639067\n",
      "train loss:   1.403121\n",
      "train loss:   1.498106\n",
      "train loss:   1.844179\n",
      "########### epoch 19 ###########\n",
      "########### loop 2650 ###########\n",
      "test loss:   1.423474   test accuracy:   0.687500\n",
      "########### loop 2650 ###########\n",
      "train loss:   1.602692\n",
      "train loss:   1.724558\n",
      "train loss:   1.740801\n",
      "train loss:   1.778310\n",
      "train loss:   1.570552\n",
      "train loss:   1.743326\n",
      "train loss:   1.616680\n",
      "train loss:   1.722681\n",
      "train loss:   1.353460\n",
      "train loss:   1.672480\n",
      "train loss:   1.697098\n",
      "train loss:   1.346858\n",
      "train loss:   1.636484\n",
      "train loss:   1.692041\n",
      "train loss:   1.914768\n",
      "train loss:   1.590694\n",
      "train loss:   1.821332\n",
      "train loss:   1.279678\n",
      "train loss:   1.730222\n",
      "train loss:   1.461591\n",
      "train loss:   1.695550\n",
      "train loss:   1.860469\n",
      "train loss:   1.673596\n",
      "train loss:   1.569558\n",
      "train loss:   2.061822\n",
      "train loss:   1.538229\n",
      "train loss:   1.945839\n",
      "train loss:   1.669516\n",
      "train loss:   1.208264\n",
      "train loss:   1.637825\n",
      "train loss:   1.617002\n",
      "train loss:   1.901452\n",
      "train loss:   1.579552\n",
      "train loss:   1.544281\n",
      "train loss:   1.537723\n",
      "train loss:   1.931288\n",
      "train loss:   1.602652\n",
      "train loss:   1.489834\n",
      "train loss:   1.817960\n",
      "train loss:   1.665740\n",
      "train loss:   1.600309\n",
      "train loss:   1.754826\n",
      "train loss:   1.678404\n",
      "train loss:   1.681265\n",
      "train loss:   1.612386\n",
      "train loss:   1.489252\n",
      "train loss:   1.687772\n",
      "train loss:   1.536380\n",
      "train loss:   1.657637\n",
      "train loss:   1.863627\n",
      "########### epoch 20 ###########\n",
      "########### loop 2700 ###########\n",
      "test loss:   1.396950   test accuracy:   0.812500\n",
      "########### loop 2700 ###########\n",
      "train loss:   1.633593\n",
      "train loss:   1.584817\n",
      "train loss:   1.491196\n",
      "train loss:   1.777074\n",
      "train loss:   1.807336\n",
      "train loss:   1.726559\n",
      "train loss:   1.439722\n",
      "train loss:   1.815081\n",
      "train loss:   1.768549\n",
      "train loss:   1.354914\n",
      "train loss:   1.540644\n",
      "train loss:   1.608841\n",
      "train loss:   1.986877\n",
      "train loss:   1.568015\n",
      "train loss:   1.854075\n",
      "train loss:   1.385374\n",
      "train loss:   1.634411\n",
      "train loss:   1.567563\n",
      "train loss:   1.524036\n",
      "train loss:   1.704171\n",
      "train loss:   1.513182\n",
      "train loss:   1.498878\n",
      "train loss:   1.699330\n",
      "train loss:   1.921255\n",
      "train loss:   1.546295\n",
      "train loss:   1.785007\n",
      "train loss:   1.733531\n",
      "train loss:   1.752223\n",
      "train loss:   1.547037\n",
      "train loss:   1.506180\n",
      "train loss:   1.417403\n",
      "train loss:   1.878612\n",
      "train loss:   1.621223\n",
      "train loss:   1.503760\n",
      "train loss:   1.622911\n",
      "train loss:   1.695859\n",
      "train loss:   1.452609\n",
      "train loss:   1.557860\n",
      "train loss:   1.487134\n",
      "train loss:   1.470059\n",
      "train loss:   1.324133\n",
      "train loss:   1.684707\n",
      "train loss:   1.654307\n",
      "train loss:   1.757670\n",
      "train loss:   1.698177\n",
      "train loss:   1.481551\n",
      "train loss:   1.584359\n",
      "train loss:   1.785650\n",
      "train loss:   1.627694\n",
      "train loss:   1.618778\n",
      "########### epoch 20 ###########\n",
      "########### loop 2750 ###########\n",
      "test loss:   1.410011   test accuracy:   0.750000\n",
      "########### loop 2750 ###########\n",
      "train loss:   1.745360\n",
      "train loss:   1.885090\n",
      "train loss:   1.766952\n",
      "train loss:   1.812681\n",
      "train loss:   1.577671\n",
      "train loss:   1.642134\n",
      "train loss:   1.531176\n",
      "train loss:   1.615132\n",
      "train loss:   1.649434\n",
      "train loss:   1.610596\n",
      "train loss:   1.558504\n",
      "train loss:   1.419298\n",
      "train loss:   1.327117\n",
      "train loss:   1.779044\n",
      "train loss:   1.575826\n",
      "train loss:   1.490824\n",
      "train loss:   1.587882\n",
      "train loss:   1.744306\n",
      "train loss:   1.763392\n",
      "train loss:   1.702614\n",
      "train loss:   1.585017\n",
      "train loss:   1.730015\n",
      "train loss:   1.738715\n",
      "train loss:   1.744863\n",
      "train loss:   1.720925\n",
      "train loss:   1.500927\n",
      "train loss:   1.363795\n",
      "train loss:   1.528286\n",
      "train loss:   1.615826\n",
      "train loss:   1.593480\n",
      "train loss:   1.665216\n",
      "train loss:   1.397464\n",
      "train loss:   1.523089\n",
      "train loss:   1.659007\n",
      "train loss:   1.768682\n",
      "train loss:   1.495343\n",
      "train loss:   1.260067\n",
      "train loss:   1.697737\n",
      "train loss:   1.832850\n",
      "train loss:   2.025577\n",
      "train loss:   1.297130\n",
      "train loss:   1.443061\n",
      "train loss:   1.726751\n",
      "train loss:   1.712115\n",
      "train loss:   1.820883\n",
      "train loss:   1.494755\n",
      "train loss:   1.812814\n",
      "train loss:   1.807638\n",
      "train loss:   1.622036\n",
      "train loss:   1.571543\n",
      "########### epoch 20 ###########\n",
      "########### loop 2800 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   1.402359   test accuracy:   0.812500\n",
      "########### loop 2800 ###########\n",
      "train loss:   1.598919\n",
      "train loss:   1.740076\n",
      "train loss:   1.741899\n",
      "train loss:   1.503632\n",
      "train loss:   1.878619\n",
      "train loss:   1.728464\n",
      "train loss:   1.671850\n",
      "train loss:   1.500194\n",
      "train loss:   1.499749\n",
      "train loss:   1.691475\n",
      "train loss:   1.717026\n",
      "train loss:   1.766487\n",
      "train loss:   1.765782\n",
      "train loss:   1.800398\n",
      "train loss:   1.427855\n",
      "train loss:   1.579920\n",
      "train loss:   1.715535\n",
      "train loss:   1.734686\n",
      "train loss:   1.612434\n",
      "train loss:   1.680285\n",
      "train loss:   1.505435\n",
      "train loss:   1.531134\n",
      "train loss:   1.641117\n",
      "train loss:   1.719435\n",
      "train loss:   1.589291\n",
      "train loss:   1.748273\n",
      "train loss:   1.889598\n",
      "train loss:   1.386791\n",
      "train loss:   1.791227\n",
      "train loss:   1.637951\n",
      "train loss:   1.568340\n",
      "train loss:   1.678695\n",
      "train loss:   1.580965\n",
      "train loss:   1.850291\n",
      "train loss:   1.716165\n",
      "train loss:   1.615375\n",
      "train loss:   1.493018\n",
      "train loss:   1.714027\n",
      "train loss:   1.686312\n",
      "train loss:   1.431340\n",
      "train loss:   2.042206\n",
      "train loss:   1.462422\n",
      "train loss:   1.755725\n",
      "train loss:   1.650670\n",
      "train loss:   1.688655\n",
      "train loss:   1.614968\n",
      "train loss:   1.690171\n",
      "train loss:   1.534679\n",
      "train loss:   1.614122\n",
      "train loss:   1.856895\n",
      "########### epoch 21 ###########\n",
      "########### loop 2850 ###########\n",
      "test loss:   1.246533   test accuracy:   0.750000\n",
      "########### loop 2850 ###########\n",
      "train loss:   1.603615\n",
      "train loss:   1.737435\n",
      "train loss:   1.605521\n",
      "train loss:   1.761372\n",
      "train loss:   1.705416\n",
      "train loss:   1.547400\n",
      "train loss:   1.834348\n",
      "train loss:   1.674410\n",
      "train loss:   1.696933\n",
      "train loss:   1.612514\n",
      "train loss:   1.767551\n",
      "train loss:   1.578450\n",
      "train loss:   1.619967\n",
      "train loss:   1.664068\n",
      "train loss:   1.561192\n",
      "train loss:   1.616032\n",
      "train loss:   1.583854\n",
      "train loss:   1.268215\n",
      "train loss:   1.767850\n",
      "train loss:   1.420847\n",
      "train loss:   1.579215\n",
      "train loss:   1.788897\n",
      "train loss:   1.501933\n",
      "train loss:   1.455868\n",
      "train loss:   1.485435\n",
      "train loss:   1.344448\n",
      "train loss:   1.470091\n",
      "train loss:   1.712224\n",
      "train loss:   1.591287\n",
      "train loss:   1.779079\n",
      "train loss:   1.598860\n",
      "train loss:   1.444881\n",
      "train loss:   1.744783\n",
      "train loss:   1.554015\n",
      "train loss:   1.690414\n",
      "train loss:   1.631837\n",
      "train loss:   1.568392\n",
      "train loss:   1.628638\n",
      "train loss:   1.321391\n",
      "train loss:   1.598390\n",
      "train loss:   1.453955\n",
      "train loss:   1.900596\n",
      "train loss:   1.624149\n",
      "train loss:   1.628141\n",
      "train loss:   1.730725\n",
      "train loss:   1.737846\n",
      "train loss:   1.362954\n",
      "train loss:   1.492279\n",
      "train loss:   1.807485\n",
      "train loss:   1.536456\n",
      "########### epoch 21 ###########\n",
      "########### loop 2900 ###########\n",
      "test loss:   1.409388   test accuracy:   0.812500\n",
      "########### loop 2900 ###########\n",
      "train loss:   1.816587\n",
      "train loss:   1.684309\n",
      "train loss:   1.320194\n",
      "train loss:   1.586542\n",
      "train loss:   1.577714\n",
      "train loss:   1.516697\n",
      "train loss:   1.386476\n",
      "train loss:   1.670895\n",
      "train loss:   1.535949\n",
      "train loss:   1.344961\n",
      "train loss:   1.800997\n",
      "train loss:   1.659602\n",
      "train loss:   1.765621\n",
      "train loss:   1.553102\n",
      "train loss:   1.688243\n",
      "train loss:   1.692877\n",
      "train loss:   1.719103\n",
      "train loss:   1.793757\n",
      "train loss:   1.767047\n",
      "train loss:   1.443864\n",
      "train loss:   1.784346\n",
      "train loss:   1.799859\n",
      "train loss:   1.529392\n",
      "train loss:   1.395789\n",
      "train loss:   1.721575\n",
      "train loss:   1.515540\n",
      "train loss:   1.484610\n",
      "train loss:   1.491935\n",
      "train loss:   1.670744\n",
      "train loss:   1.588613\n",
      "train loss:   1.958481\n",
      "train loss:   1.646102\n",
      "train loss:   1.723099\n",
      "train loss:   1.817367\n",
      "train loss:   1.502694\n",
      "train loss:   1.472438\n",
      "train loss:   1.683705\n",
      "train loss:   1.567033\n",
      "train loss:   1.636926\n",
      "train loss:   1.649976\n",
      "train loss:   1.696536\n",
      "train loss:   1.576172\n",
      "train loss:   1.369555\n",
      "train loss:   1.546527\n",
      "train loss:   1.450386\n",
      "train loss:   1.777531\n",
      "train loss:   1.653408\n",
      "train loss:   1.630069\n",
      "train loss:   1.646493\n",
      "train loss:   1.305088\n",
      "########### epoch 21 ###########\n",
      "########### loop 2950 ###########\n",
      "test loss:   1.249259   test accuracy:   0.812500\n",
      "########### loop 2950 ###########\n",
      "train loss:   1.706865\n",
      "train loss:   1.491314\n",
      "train loss:   1.590693\n",
      "train loss:   1.702370\n",
      "train loss:   1.447423\n",
      "train loss:   1.527330\n",
      "train loss:   1.514014\n",
      "train loss:   1.325357\n",
      "train loss:   1.701124\n",
      "train loss:   1.482129\n",
      "train loss:   1.570689\n",
      "train loss:   1.749541\n",
      "train loss:   1.638558\n",
      "train loss:   1.864059\n",
      "train loss:   1.482007\n",
      "train loss:   1.477326\n",
      "train loss:   1.796724\n",
      "train loss:   1.722398\n",
      "train loss:   1.717623\n",
      "train loss:   1.544099\n",
      "train loss:   1.509597\n",
      "train loss:   1.761243\n",
      "train loss:   1.238479\n",
      "train loss:   1.688226\n",
      "train loss:   1.463624\n",
      "train loss:   1.415501\n",
      "train loss:   1.676052\n",
      "train loss:   1.582689\n",
      "train loss:   1.424506\n",
      "train loss:   1.771242\n",
      "train loss:   1.733347\n",
      "train loss:   1.660635\n",
      "train loss:   1.924638\n",
      "train loss:   1.715375\n",
      "train loss:   1.780252\n",
      "train loss:   1.603722\n",
      "train loss:   1.411741\n",
      "train loss:   1.767318\n",
      "train loss:   1.608587\n",
      "train loss:   1.569346\n",
      "train loss:   1.647105\n",
      "train loss:   1.457019\n",
      "train loss:   1.576824\n",
      "train loss:   1.554638\n",
      "train loss:   1.432250\n",
      "train loss:   1.607801\n",
      "train loss:   1.568386\n",
      "train loss:   1.774324\n",
      "train loss:   1.662280\n",
      "train loss:   1.384195\n",
      "########### epoch 22 ###########\n",
      "########### loop 3000 ###########\n",
      "test loss:   1.395865   test accuracy:   0.656250\n",
      "########### loop 3000 ###########\n",
      "train loss:   1.876684\n",
      "train loss:   1.674793\n",
      "train loss:   1.338995\n",
      "train loss:   1.400500\n",
      "train loss:   1.714440\n",
      "train loss:   1.466937\n",
      "train loss:   1.734608\n",
      "train loss:   1.862934\n",
      "train loss:   1.660913\n",
      "train loss:   1.689310\n",
      "train loss:   1.784388\n",
      "train loss:   1.532042\n",
      "train loss:   1.520242\n",
      "train loss:   1.696713\n",
      "train loss:   1.741041\n",
      "train loss:   1.614182\n",
      "train loss:   1.351454\n",
      "train loss:   1.650626\n",
      "train loss:   1.565174\n",
      "train loss:   1.604176\n",
      "train loss:   1.487094\n",
      "train loss:   1.359383\n",
      "train loss:   1.651384\n",
      "train loss:   1.220483\n",
      "train loss:   1.849706\n",
      "train loss:   1.651999\n",
      "train loss:   1.487193\n",
      "train loss:   1.583404\n",
      "train loss:   1.620713\n",
      "train loss:   1.483202\n",
      "train loss:   1.546384\n",
      "train loss:   1.544982\n",
      "train loss:   1.414713\n",
      "train loss:   1.815423\n",
      "train loss:   1.691246\n",
      "train loss:   1.210256\n",
      "train loss:   1.725607\n",
      "train loss:   1.819360\n",
      "train loss:   1.797665\n",
      "train loss:   1.589746\n",
      "train loss:   1.732130\n",
      "train loss:   1.664970\n",
      "train loss:   2.020230\n",
      "train loss:   1.839024\n",
      "train loss:   1.416404\n",
      "train loss:   1.489690\n",
      "train loss:   1.640208\n",
      "train loss:   1.805126\n",
      "train loss:   1.681522\n",
      "train loss:   1.675022\n",
      "########### epoch 22 ###########\n",
      "########### loop 3050 ###########\n",
      "test loss:   1.337133   test accuracy:   0.843750\n",
      "########### loop 3050 ###########\n",
      "train loss:   1.572824\n",
      "train loss:   1.502603\n",
      "train loss:   1.690261\n",
      "train loss:   1.733404\n",
      "train loss:   1.805028\n",
      "train loss:   1.270577\n",
      "train loss:   1.784360\n",
      "train loss:   1.579192\n",
      "train loss:   1.484084\n",
      "train loss:   1.407371\n",
      "train loss:   1.748201\n",
      "train loss:   1.725580\n",
      "train loss:   1.417232\n",
      "train loss:   1.658949\n",
      "train loss:   1.786500\n",
      "train loss:   1.476221\n",
      "train loss:   1.508181\n",
      "train loss:   1.493097\n",
      "train loss:   1.620206\n",
      "train loss:   1.544504\n",
      "train loss:   1.580925\n",
      "train loss:   1.757164\n",
      "train loss:   1.615359\n",
      "train loss:   1.140381\n",
      "train loss:   1.654134\n",
      "train loss:   1.675149\n",
      "train loss:   1.627241\n",
      "train loss:   1.619024\n",
      "train loss:   1.614018\n",
      "train loss:   1.543649\n",
      "train loss:   1.406002\n",
      "train loss:   1.349357\n",
      "train loss:   1.550099\n",
      "train loss:   1.423691\n",
      "train loss:   1.691906\n",
      "train loss:   1.615011\n",
      "train loss:   1.722992\n",
      "train loss:   1.764301\n",
      "train loss:   1.843245\n",
      "train loss:   1.763600\n",
      "train loss:   1.497826\n",
      "train loss:   1.729031\n",
      "train loss:   1.679909\n",
      "train loss:   1.623897\n",
      "train loss:   1.532852\n",
      "train loss:   1.630116\n",
      "train loss:   1.606542\n",
      "train loss:   1.526399\n",
      "train loss:   1.521461\n",
      "train loss:   1.419672\n",
      "########### epoch 22 ###########\n",
      "########### loop 3100 ###########\n",
      "test loss:   1.476547   test accuracy:   0.531250\n",
      "########### loop 3100 ###########\n",
      "train loss:   1.513077\n",
      "train loss:   1.681806\n",
      "train loss:   1.439952\n",
      "train loss:   1.494964\n",
      "train loss:   1.547959\n",
      "train loss:   1.792987\n",
      "train loss:   1.746024\n",
      "train loss:   1.761652\n",
      "train loss:   1.657722\n",
      "train loss:   1.781235\n",
      "train loss:   1.687600\n",
      "train loss:   1.682972\n",
      "train loss:   1.489944\n",
      "train loss:   1.484471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.512661\n",
      "train loss:   1.411224\n",
      "train loss:   1.845501\n",
      "train loss:   1.834579\n",
      "train loss:   1.558097\n",
      "train loss:   1.531363\n",
      "train loss:   1.618413\n",
      "train loss:   1.038679\n",
      "train loss:   1.530470\n",
      "train loss:   1.418529\n",
      "train loss:   1.693187\n",
      "train loss:   1.530541\n",
      "train loss:   1.798632\n",
      "train loss:   1.664932\n",
      "train loss:   1.435112\n",
      "train loss:   1.564359\n",
      "train loss:   1.634300\n",
      "train loss:   1.507298\n",
      "train loss:   1.521382\n",
      "train loss:   1.628711\n",
      "train loss:   1.677928\n",
      "train loss:   1.724670\n",
      "train loss:   1.705810\n",
      "train loss:   1.485097\n",
      "train loss:   1.576112\n",
      "train loss:   1.677435\n",
      "train loss:   1.655964\n",
      "train loss:   1.669128\n",
      "train loss:   1.515939\n",
      "train loss:   1.623340\n",
      "train loss:   1.558151\n",
      "train loss:   1.612787\n",
      "train loss:   1.505622\n",
      "train loss:   1.466737\n",
      "train loss:   1.739539\n",
      "train loss:   1.718848\n",
      "########### epoch 23 ###########\n",
      "########### loop 3150 ###########\n",
      "test loss:   1.194617   test accuracy:   0.812500\n",
      "########### loop 3150 ###########\n",
      "train loss:   1.500758\n",
      "train loss:   1.623514\n",
      "train loss:   1.796075\n",
      "train loss:   1.657490\n",
      "train loss:   1.705013\n",
      "train loss:   1.546240\n",
      "train loss:   1.439651\n",
      "train loss:   1.572944\n",
      "train loss:   1.648283\n",
      "train loss:   1.855302\n",
      "train loss:   1.582601\n",
      "train loss:   1.490604\n",
      "train loss:   1.331996\n",
      "train loss:   1.692974\n",
      "train loss:   1.677798\n",
      "train loss:   1.911359\n",
      "train loss:   1.258807\n",
      "train loss:   1.863534\n",
      "train loss:   1.429232\n",
      "train loss:   1.638408\n",
      "train loss:   1.511340\n",
      "train loss:   1.706901\n",
      "train loss:   1.524483\n",
      "train loss:   1.594919\n",
      "train loss:   1.367429\n",
      "train loss:   1.757817\n",
      "train loss:   1.769896\n",
      "train loss:   1.673870\n",
      "train loss:   1.636330\n",
      "train loss:   1.441407\n",
      "train loss:   1.625641\n",
      "train loss:   1.521098\n",
      "train loss:   1.353402\n",
      "train loss:   1.561086\n",
      "train loss:   1.442274\n",
      "train loss:   1.468220\n",
      "train loss:   1.492245\n",
      "train loss:   1.633606\n",
      "train loss:   1.749354\n",
      "train loss:   1.590289\n",
      "train loss:   1.671644\n",
      "train loss:   1.532647\n",
      "train loss:   1.625218\n",
      "train loss:   1.422030\n",
      "train loss:   1.286929\n",
      "train loss:   1.744174\n",
      "train loss:   1.506209\n",
      "train loss:   1.680865\n",
      "train loss:   1.849865\n",
      "train loss:   1.420835\n",
      "########### epoch 23 ###########\n",
      "########### loop 3200 ###########\n",
      "test loss:   1.336442   test accuracy:   0.750000\n",
      "########### loop 3200 ###########\n",
      "train loss:   1.475641\n",
      "train loss:   1.718689\n",
      "train loss:   1.639578\n",
      "train loss:   1.686382\n",
      "train loss:   1.809638\n",
      "train loss:   1.510939\n",
      "train loss:   1.422924\n",
      "train loss:   1.931310\n",
      "train loss:   1.705252\n",
      "train loss:   2.015373\n",
      "train loss:   1.541596\n",
      "train loss:   1.477759\n",
      "train loss:   1.580640\n",
      "train loss:   1.534082\n",
      "train loss:   1.841609\n",
      "train loss:   1.465077\n",
      "train loss:   1.457268\n",
      "train loss:   1.502011\n",
      "train loss:   1.349775\n",
      "train loss:   1.449938\n",
      "train loss:   1.683173\n",
      "train loss:   1.343872\n",
      "train loss:   1.651306\n",
      "train loss:   1.645974\n",
      "train loss:   1.497616\n",
      "train loss:   1.466488\n",
      "train loss:   1.704052\n",
      "train loss:   1.487508\n",
      "train loss:   1.708850\n",
      "train loss:   1.697679\n",
      "train loss:   1.469007\n",
      "train loss:   1.719911\n",
      "train loss:   1.626268\n",
      "train loss:   1.632046\n",
      "train loss:   1.757362\n",
      "train loss:   1.368281\n",
      "train loss:   1.284858\n",
      "train loss:   1.615797\n",
      "train loss:   1.385126\n",
      "train loss:   1.816331\n",
      "train loss:   1.640138\n",
      "train loss:   1.375248\n",
      "train loss:   1.520236\n",
      "train loss:   1.732013\n",
      "train loss:   1.371692\n",
      "train loss:   1.525309\n",
      "train loss:   1.763752\n",
      "train loss:   1.703081\n",
      "train loss:   1.775816\n",
      "train loss:   1.742334\n",
      "########### epoch 24 ###########\n",
      "########### loop 3250 ###########\n",
      "test loss:   1.337061   test accuracy:   0.687500\n",
      "########### loop 3250 ###########\n",
      "train loss:   1.783205\n",
      "train loss:   1.526358\n",
      "train loss:   1.536876\n",
      "train loss:   1.614182\n",
      "train loss:   1.816741\n",
      "train loss:   1.812778\n",
      "train loss:   1.816662\n",
      "train loss:   1.459519\n",
      "train loss:   1.707815\n",
      "train loss:   1.693263\n",
      "train loss:   1.557896\n",
      "train loss:   1.756980\n",
      "train loss:   1.605999\n",
      "train loss:   1.671819\n",
      "train loss:   1.614722\n",
      "train loss:   1.553413\n",
      "train loss:   1.435025\n",
      "train loss:   1.878587\n",
      "train loss:   1.601152\n",
      "train loss:   1.596531\n",
      "train loss:   1.614879\n",
      "train loss:   1.470213\n",
      "train loss:   1.725816\n",
      "train loss:   1.543071\n",
      "train loss:   1.760615\n",
      "train loss:   1.411783\n",
      "train loss:   1.547680\n",
      "train loss:   1.963281\n",
      "train loss:   1.680129\n",
      "train loss:   1.565316\n",
      "train loss:   1.648007\n",
      "train loss:   1.565311\n",
      "train loss:   1.622051\n",
      "train loss:   1.531569\n",
      "train loss:   1.894406\n",
      "train loss:   1.680047\n",
      "train loss:   1.600551\n",
      "train loss:   1.545509\n",
      "train loss:   1.819485\n",
      "train loss:   1.342910\n",
      "train loss:   1.648579\n",
      "train loss:   1.455071\n",
      "train loss:   1.697549\n",
      "train loss:   1.624005\n",
      "train loss:   1.701195\n",
      "train loss:   1.644922\n",
      "train loss:   1.233833\n",
      "train loss:   2.061279\n",
      "train loss:   1.811047\n",
      "train loss:   1.783398\n",
      "########### epoch 24 ###########\n",
      "########### loop 3300 ###########\n",
      "test loss:   1.208311   test accuracy:   0.843750\n",
      "########### loop 3300 ###########\n",
      "train loss:   1.564066\n",
      "train loss:   1.733189\n",
      "train loss:   1.474970\n",
      "train loss:   1.471050\n",
      "train loss:   1.496449\n",
      "train loss:   1.585125\n",
      "train loss:   1.668734\n",
      "train loss:   1.539876\n",
      "train loss:   1.724016\n",
      "train loss:   1.631793\n",
      "train loss:   1.518285\n",
      "train loss:   1.675712\n",
      "train loss:   1.535319\n",
      "train loss:   1.429379\n",
      "train loss:   1.573219\n",
      "train loss:   1.663842\n",
      "train loss:   1.371987\n",
      "train loss:   1.491378\n",
      "train loss:   1.669241\n",
      "train loss:   1.429761\n",
      "train loss:   1.533931\n",
      "train loss:   1.368584\n",
      "train loss:   1.755308\n",
      "train loss:   1.773178\n",
      "train loss:   1.385709\n",
      "train loss:   1.665740\n",
      "train loss:   1.560261\n",
      "train loss:   1.454292\n",
      "train loss:   1.449660\n",
      "train loss:   1.626215\n",
      "train loss:   1.350058\n",
      "train loss:   1.321012\n",
      "train loss:   1.454516\n",
      "train loss:   1.758803\n",
      "train loss:   1.671696\n",
      "train loss:   1.657693\n",
      "train loss:   1.244089\n",
      "train loss:   1.562577\n",
      "train loss:   1.582518\n",
      "train loss:   1.470921\n",
      "train loss:   1.431531\n",
      "train loss:   1.419317\n",
      "train loss:   1.897714\n",
      "train loss:   1.558956\n",
      "train loss:   1.415870\n",
      "train loss:   1.565595\n",
      "train loss:   1.250779\n",
      "train loss:   1.431932\n",
      "train loss:   1.561225\n",
      "train loss:   1.549618\n",
      "########### epoch 24 ###########\n",
      "########### loop 3350 ###########\n",
      "test loss:   1.332783   test accuracy:   0.718750\n",
      "########### loop 3350 ###########\n",
      "train loss:   1.674510\n",
      "train loss:   1.696539\n",
      "train loss:   1.761819\n",
      "train loss:   1.342204\n",
      "train loss:   1.442542\n",
      "train loss:   1.354478\n",
      "train loss:   1.757019\n",
      "train loss:   1.889955\n",
      "train loss:   1.844178\n",
      "train loss:   1.669431\n",
      "train loss:   1.642134\n",
      "train loss:   1.782969\n",
      "train loss:   1.553489\n",
      "train loss:   1.932391\n",
      "train loss:   1.712966\n",
      "train loss:   1.615022\n",
      "train loss:   1.608509\n",
      "train loss:   1.700063\n",
      "train loss:   1.640912\n",
      "train loss:   1.687627\n",
      "train loss:   1.389999\n",
      "train loss:   1.756895\n",
      "train loss:   1.709159\n",
      "train loss:   1.506873\n",
      "train loss:   1.449456\n",
      "train loss:   1.491786\n",
      "train loss:   1.503011\n",
      "train loss:   1.368250\n",
      "train loss:   1.829182\n",
      "train loss:   1.941815\n",
      "train loss:   1.508448\n",
      "train loss:   1.358241\n",
      "train loss:   1.542211\n",
      "train loss:   1.639749\n",
      "train loss:   1.370338\n",
      "train loss:   1.771754\n",
      "train loss:   1.817135\n",
      "train loss:   1.493735\n",
      "train loss:   1.559520\n",
      "train loss:   1.519986\n",
      "train loss:   1.451543\n",
      "train loss:   1.530304\n",
      "train loss:   1.156780\n",
      "train loss:   1.748846\n",
      "train loss:   1.638367\n",
      "train loss:   1.566367\n",
      "train loss:   1.544129\n",
      "train loss:   1.567011\n",
      "train loss:   1.424829\n",
      "train loss:   1.902815\n",
      "########### epoch 25 ###########\n",
      "########### loop 3400 ###########\n",
      "test loss:   1.335567   test accuracy:   0.718750\n",
      "########### loop 3400 ###########\n",
      "train loss:   1.505469\n",
      "train loss:   1.442504\n",
      "train loss:   1.601602\n",
      "train loss:   1.510882\n",
      "train loss:   1.516280\n",
      "train loss:   1.421257\n",
      "train loss:   1.771236\n",
      "train loss:   1.478550\n",
      "train loss:   1.636268\n",
      "train loss:   1.488167\n",
      "train loss:   1.743483\n",
      "train loss:   1.571672\n",
      "train loss:   1.693919\n",
      "train loss:   1.527214\n",
      "train loss:   1.772623\n",
      "train loss:   1.358893\n",
      "train loss:   1.637714\n",
      "train loss:   1.449412\n",
      "train loss:   1.649251\n",
      "train loss:   1.476218\n",
      "train loss:   1.592938\n",
      "train loss:   1.682718\n",
      "train loss:   1.521677\n",
      "train loss:   1.707988\n",
      "train loss:   1.544630\n",
      "train loss:   1.769753\n",
      "train loss:   1.404793\n",
      "train loss:   1.531601\n",
      "train loss:   1.728433\n",
      "train loss:   1.560905\n",
      "train loss:   1.555421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.451494\n",
      "train loss:   1.624006\n",
      "train loss:   1.384985\n",
      "train loss:   1.491464\n",
      "train loss:   1.534177\n",
      "train loss:   1.817216\n",
      "train loss:   1.468229\n",
      "train loss:   1.746644\n",
      "train loss:   1.644454\n",
      "train loss:   1.380660\n",
      "train loss:   1.731299\n",
      "train loss:   1.493196\n",
      "train loss:   1.378680\n",
      "train loss:   1.529062\n",
      "train loss:   1.671423\n",
      "train loss:   1.852672\n",
      "train loss:   1.444758\n",
      "train loss:   1.619086\n",
      "train loss:   1.735862\n",
      "########### epoch 25 ###########\n",
      "########### loop 3450 ###########\n",
      "test loss:   1.242023   test accuracy:   0.843750\n",
      "########### loop 3450 ###########\n",
      "train loss:   1.328223\n",
      "train loss:   1.520995\n",
      "train loss:   1.411416\n",
      "train loss:   1.733781\n",
      "train loss:   1.384638\n",
      "train loss:   1.687115\n",
      "train loss:   1.637855\n",
      "train loss:   1.823505\n",
      "train loss:   1.595708\n",
      "train loss:   1.423130\n",
      "train loss:   1.492624\n",
      "train loss:   1.361300\n",
      "train loss:   1.557187\n",
      "train loss:   1.579170\n",
      "train loss:   1.575711\n",
      "train loss:   1.527803\n",
      "train loss:   1.636427\n",
      "train loss:   1.527420\n",
      "train loss:   1.510853\n",
      "train loss:   1.463504\n",
      "train loss:   1.570358\n",
      "train loss:   1.567120\n",
      "train loss:   1.643878\n",
      "train loss:   1.841350\n",
      "train loss:   1.308520\n",
      "train loss:   1.549802\n",
      "train loss:   1.589010\n",
      "train loss:   1.476869\n",
      "train loss:   1.608716\n",
      "train loss:   1.490609\n",
      "train loss:   1.833024\n",
      "train loss:   1.538964\n",
      "train loss:   1.514128\n",
      "train loss:   1.458445\n",
      "train loss:   1.728474\n",
      "train loss:   1.764947\n",
      "train loss:   1.524198\n",
      "train loss:   1.414223\n",
      "train loss:   1.528583\n",
      "train loss:   1.348922\n",
      "train loss:   1.393765\n",
      "train loss:   1.488608\n",
      "train loss:   1.551275\n",
      "train loss:   1.379366\n",
      "train loss:   1.545141\n",
      "train loss:   1.604349\n",
      "train loss:   1.658406\n",
      "train loss:   1.435146\n",
      "train loss:   1.540037\n",
      "train loss:   1.273751\n",
      "########### epoch 25 ###########\n",
      "########### loop 3500 ###########\n",
      "test loss:   1.317010   test accuracy:   0.718750\n",
      "########### loop 3500 ###########\n",
      "train loss:   1.857678\n",
      "train loss:   1.745602\n",
      "train loss:   1.716084\n",
      "train loss:   1.420705\n",
      "train loss:   1.524238\n",
      "train loss:   1.307043\n",
      "train loss:   1.577113\n",
      "train loss:   1.750787\n",
      "train loss:   1.457094\n",
      "train loss:   1.441420\n",
      "train loss:   1.498175\n",
      "train loss:   1.397201\n",
      "train loss:   1.874162\n",
      "train loss:   1.393616\n",
      "train loss:   1.738492\n",
      "train loss:   1.394431\n",
      "train loss:   1.543455\n",
      "train loss:   1.723808\n",
      "train loss:   1.347945\n",
      "train loss:   1.618440\n",
      "train loss:   1.608976\n",
      "train loss:   1.413507\n",
      "train loss:   1.573682\n",
      "train loss:   1.797776\n",
      "train loss:   1.597666\n",
      "train loss:   1.682684\n",
      "train loss:   1.691968\n",
      "train loss:   1.497522\n",
      "train loss:   1.580724\n",
      "train loss:   1.664133\n",
      "train loss:   1.335854\n",
      "train loss:   1.423340\n",
      "train loss:   1.558831\n",
      "train loss:   1.748643\n",
      "train loss:   1.672176\n",
      "train loss:   1.484619\n",
      "train loss:   1.557831\n",
      "train loss:   1.599249\n",
      "train loss:   1.659756\n",
      "train loss:   1.677738\n",
      "train loss:   1.773975\n",
      "train loss:   1.533125\n",
      "train loss:   1.694597\n",
      "train loss:   1.716622\n",
      "train loss:   1.613788\n",
      "train loss:   1.681939\n",
      "train loss:   1.685581\n",
      "train loss:   1.498852\n",
      "train loss:   1.574004\n",
      "train loss:   1.570276\n",
      "########### epoch 26 ###########\n",
      "########### loop 3550 ###########\n",
      "test loss:   1.163711   test accuracy:   0.781250\n",
      "########### loop 3550 ###########\n",
      "train loss:   1.481287\n",
      "train loss:   1.749181\n",
      "train loss:   1.760342\n",
      "train loss:   1.598147\n",
      "train loss:   1.308577\n",
      "train loss:   1.439171\n",
      "train loss:   1.481741\n",
      "train loss:   1.467560\n",
      "train loss:   1.551050\n",
      "train loss:   1.695224\n",
      "train loss:   1.762648\n",
      "train loss:   1.671201\n",
      "train loss:   1.475047\n",
      "train loss:   1.423980\n",
      "train loss:   1.348998\n",
      "train loss:   1.840628\n",
      "train loss:   1.399500\n",
      "train loss:   1.682740\n",
      "train loss:   1.700671\n",
      "train loss:   1.584904\n",
      "train loss:   1.800494\n",
      "train loss:   1.589901\n",
      "train loss:   1.624589\n",
      "train loss:   1.760836\n",
      "train loss:   1.553412\n",
      "train loss:   1.435170\n",
      "train loss:   1.717547\n",
      "train loss:   1.487810\n",
      "train loss:   1.279519\n",
      "train loss:   1.696022\n",
      "train loss:   1.437291\n",
      "train loss:   1.564064\n",
      "train loss:   1.649675\n",
      "train loss:   1.210019\n",
      "train loss:   1.916010\n",
      "train loss:   1.926913\n",
      "train loss:   1.204412\n",
      "train loss:   1.781018\n",
      "train loss:   1.499071\n",
      "train loss:   1.391130\n",
      "train loss:   1.592013\n",
      "train loss:   1.702590\n",
      "train loss:   1.612249\n",
      "train loss:   1.635217\n",
      "train loss:   1.306474\n",
      "train loss:   1.761720\n",
      "train loss:   1.435148\n",
      "train loss:   1.521962\n",
      "train loss:   1.566563\n",
      "train loss:   1.618895\n",
      "########### epoch 26 ###########\n",
      "########### loop 3600 ###########\n",
      "test loss:   1.349665   test accuracy:   0.812500\n",
      "########### loop 3600 ###########\n",
      "train loss:   1.335869\n",
      "train loss:   1.817167\n",
      "train loss:   1.329261\n",
      "train loss:   1.553496\n",
      "train loss:   1.705774\n",
      "train loss:   1.344418\n",
      "train loss:   1.650327\n",
      "train loss:   1.801229\n",
      "train loss:   1.549998\n",
      "train loss:   1.406454\n",
      "train loss:   1.613118\n",
      "train loss:   1.549577\n",
      "train loss:   1.589936\n",
      "train loss:   1.677141\n",
      "train loss:   1.936871\n",
      "train loss:   1.514254\n",
      "train loss:   1.467098\n",
      "train loss:   1.562212\n",
      "train loss:   1.585040\n",
      "train loss:   1.678769\n",
      "train loss:   1.496425\n",
      "train loss:   1.437074\n",
      "train loss:   1.563552\n",
      "train loss:   1.838658\n",
      "train loss:   1.840874\n",
      "train loss:   1.439836\n",
      "train loss:   1.267745\n",
      "train loss:   1.624851\n",
      "train loss:   1.628450\n",
      "train loss:   1.469949\n",
      "train loss:   1.737810\n",
      "train loss:   1.309759\n",
      "train loss:   1.631416\n",
      "train loss:   1.562775\n",
      "train loss:   1.679841\n",
      "train loss:   1.700496\n",
      "train loss:   1.804014\n",
      "train loss:   1.684628\n",
      "train loss:   1.477771\n",
      "train loss:   1.542236\n",
      "train loss:   1.408280\n",
      "train loss:   1.548331\n",
      "train loss:   1.585074\n",
      "train loss:   1.676170\n",
      "train loss:   1.548498\n",
      "train loss:   1.335403\n",
      "train loss:   1.618368\n",
      "train loss:   1.410587\n",
      "train loss:   1.542576\n",
      "train loss:   1.882831\n",
      "########### epoch 26 ###########\n",
      "########### loop 3650 ###########\n",
      "test loss:   1.240761   test accuracy:   0.750000\n",
      "########### loop 3650 ###########\n",
      "train loss:   1.595701\n",
      "train loss:   1.591784\n",
      "train loss:   1.380961\n",
      "train loss:   1.565047\n",
      "train loss:   1.654955\n",
      "train loss:   1.294920\n",
      "train loss:   1.666396\n",
      "train loss:   1.363569\n",
      "train loss:   1.710341\n",
      "train loss:   1.700034\n",
      "train loss:   1.822472\n",
      "train loss:   1.622528\n",
      "train loss:   1.522387\n",
      "train loss:   1.383609\n",
      "train loss:   1.452990\n",
      "train loss:   1.653682\n",
      "train loss:   1.640430\n",
      "train loss:   1.649862\n",
      "train loss:   1.365277\n",
      "train loss:   1.406153\n",
      "train loss:   1.407070\n",
      "train loss:   1.479702\n",
      "train loss:   1.612305\n",
      "train loss:   1.719836\n",
      "train loss:   1.653381\n",
      "train loss:   1.956693\n",
      "train loss:   1.458261\n",
      "train loss:   1.652738\n",
      "train loss:   1.489923\n",
      "train loss:   1.620535\n",
      "train loss:   1.564933\n",
      "train loss:   1.271805\n",
      "train loss:   1.502794\n",
      "train loss:   1.548749\n",
      "train loss:   1.624270\n",
      "train loss:   1.556611\n",
      "train loss:   1.634843\n",
      "train loss:   1.725762\n",
      "train loss:   1.649826\n",
      "train loss:   1.413161\n",
      "train loss:   1.513005\n",
      "train loss:   1.607209\n",
      "train loss:   1.571236\n",
      "train loss:   1.743527\n",
      "train loss:   1.747874\n",
      "train loss:   1.645066\n",
      "train loss:   1.435106\n",
      "train loss:   1.410253\n",
      "train loss:   1.440966\n",
      "train loss:   1.418617\n",
      "########### epoch 27 ###########\n",
      "########### loop 3700 ###########\n",
      "test loss:   1.359571   test accuracy:   0.687500\n",
      "########### loop 3700 ###########\n",
      "train loss:   1.278223\n",
      "train loss:   1.512375\n",
      "train loss:   1.389456\n",
      "train loss:   1.479110\n",
      "train loss:   1.545621\n",
      "train loss:   1.530409\n",
      "train loss:   1.599233\n",
      "train loss:   1.821206\n",
      "train loss:   1.211739\n",
      "train loss:   1.558767\n",
      "train loss:   1.478953\n",
      "train loss:   1.768003\n",
      "train loss:   1.526412\n",
      "train loss:   1.678379\n",
      "train loss:   1.438202\n",
      "train loss:   1.496747\n",
      "train loss:   1.603932\n",
      "train loss:   1.168207\n",
      "train loss:   1.661538\n",
      "train loss:   1.560401\n",
      "train loss:   1.474408\n",
      "train loss:   1.702846\n",
      "train loss:   1.515036\n",
      "train loss:   1.576684\n",
      "train loss:   1.592425\n",
      "train loss:   1.440351\n",
      "train loss:   1.842187\n",
      "train loss:   1.915268\n",
      "train loss:   1.624388\n",
      "train loss:   1.396329\n",
      "train loss:   1.446605\n",
      "train loss:   1.369168\n",
      "train loss:   1.373034\n",
      "train loss:   1.734715\n",
      "train loss:   1.677937\n",
      "train loss:   1.486914\n",
      "train loss:   1.719754\n",
      "train loss:   1.617613\n",
      "train loss:   1.673490\n",
      "train loss:   1.382684\n",
      "train loss:   1.652765\n",
      "train loss:   1.620750\n",
      "train loss:   1.463434\n",
      "train loss:   1.521980\n",
      "train loss:   1.529894\n",
      "train loss:   1.435776\n",
      "train loss:   1.424906\n",
      "train loss:   1.484591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.524257\n",
      "train loss:   1.451745\n",
      "########### epoch 27 ###########\n",
      "########### loop 3750 ###########\n",
      "test loss:   1.109152   test accuracy:   0.875000\n",
      "########### loop 3750 ###########\n",
      "train loss:   1.615755\n",
      "train loss:   1.644919\n",
      "train loss:   1.303760\n",
      "train loss:   1.584658\n",
      "train loss:   1.732285\n",
      "train loss:   1.489872\n",
      "train loss:   1.432953\n",
      "train loss:   1.603645\n",
      "train loss:   1.332922\n",
      "train loss:   1.517010\n",
      "train loss:   1.728197\n",
      "train loss:   1.491245\n",
      "train loss:   1.616799\n",
      "train loss:   1.595736\n",
      "train loss:   1.268623\n",
      "train loss:   1.598446\n",
      "train loss:   1.730183\n",
      "train loss:   1.413325\n",
      "train loss:   1.432724\n",
      "train loss:   1.328052\n",
      "train loss:   1.591364\n",
      "train loss:   1.574358\n",
      "train loss:   1.546703\n",
      "train loss:   1.403439\n",
      "train loss:   1.538227\n",
      "train loss:   1.618961\n",
      "train loss:   1.468929\n",
      "train loss:   1.466558\n",
      "train loss:   1.579881\n",
      "train loss:   1.210149\n",
      "train loss:   1.557180\n",
      "train loss:   1.500639\n",
      "train loss:   1.562493\n",
      "train loss:   1.704237\n",
      "train loss:   1.354495\n",
      "train loss:   1.506633\n",
      "train loss:   1.758898\n",
      "train loss:   1.411122\n",
      "train loss:   1.482672\n",
      "train loss:   1.913002\n",
      "train loss:   1.509605\n",
      "train loss:   1.700861\n",
      "train loss:   1.351463\n",
      "train loss:   1.434668\n",
      "train loss:   1.793152\n",
      "train loss:   1.478613\n",
      "train loss:   1.511519\n",
      "train loss:   1.660337\n",
      "train loss:   1.510499\n",
      "train loss:   1.659725\n",
      "########### epoch 27 ###########\n",
      "########### loop 3800 ###########\n",
      "test loss:   1.274106   test accuracy:   0.718750\n",
      "########### loop 3800 ###########\n",
      "train loss:   1.612095\n",
      "train loss:   1.723953\n",
      "train loss:   1.546450\n",
      "train loss:   1.560034\n",
      "train loss:   1.358002\n",
      "train loss:   1.110389\n",
      "train loss:   1.373284\n",
      "train loss:   1.670409\n",
      "train loss:   1.474055\n",
      "train loss:   1.589063\n",
      "train loss:   1.726806\n",
      "train loss:   1.870199\n",
      "train loss:   1.511000\n",
      "train loss:   1.393694\n",
      "train loss:   1.665460\n",
      "train loss:   1.536279\n",
      "train loss:   1.625018\n",
      "train loss:   1.747217\n",
      "train loss:   1.573712\n",
      "train loss:   1.494609\n",
      "train loss:   1.668656\n",
      "train loss:   1.489551\n",
      "train loss:   1.419309\n",
      "train loss:   1.525400\n",
      "train loss:   1.618240\n",
      "train loss:   1.484262\n",
      "train loss:   1.523194\n",
      "train loss:   1.808306\n",
      "train loss:   1.495493\n",
      "train loss:   1.537464\n",
      "train loss:   1.595734\n",
      "train loss:   1.582498\n",
      "train loss:   1.334529\n",
      "train loss:   1.266836\n",
      "train loss:   1.653965\n",
      "train loss:   1.737931\n",
      "train loss:   1.454304\n",
      "train loss:   1.602324\n",
      "train loss:   1.529615\n",
      "train loss:   1.559292\n",
      "train loss:   1.550682\n",
      "train loss:   1.605785\n",
      "train loss:   1.413195\n",
      "train loss:   1.441464\n",
      "train loss:   1.565880\n",
      "train loss:   1.536619\n",
      "train loss:   1.427584\n",
      "train loss:   1.826989\n",
      "train loss:   1.685420\n",
      "train loss:   1.732090\n",
      "########### epoch 28 ###########\n",
      "########### loop 3850 ###########\n",
      "test loss:   1.278186   test accuracy:   0.781250\n",
      "########### loop 3850 ###########\n",
      "train loss:   1.627753\n",
      "train loss:   1.703015\n",
      "train loss:   1.769806\n",
      "train loss:   1.536551\n",
      "train loss:   1.621692\n",
      "train loss:   1.672041\n",
      "train loss:   1.606519\n",
      "train loss:   1.457204\n",
      "train loss:   1.363572\n",
      "train loss:   1.641908\n",
      "train loss:   1.720514\n",
      "train loss:   1.649600\n",
      "train loss:   1.453080\n",
      "train loss:   1.534449\n",
      "train loss:   1.648098\n",
      "train loss:   1.741764\n",
      "train loss:   1.707821\n",
      "train loss:   1.641306\n",
      "train loss:   1.185103\n",
      "train loss:   1.608714\n",
      "train loss:   1.502428\n",
      "train loss:   1.336964\n",
      "train loss:   1.625479\n",
      "train loss:   1.418487\n",
      "train loss:   1.619037\n",
      "train loss:   1.511490\n",
      "train loss:   1.542308\n",
      "train loss:   1.390466\n",
      "train loss:   1.438207\n",
      "train loss:   1.647479\n",
      "train loss:   1.249076\n",
      "train loss:   1.539580\n",
      "train loss:   1.507409\n",
      "train loss:   1.764693\n",
      "train loss:   1.625514\n",
      "train loss:   1.651872\n",
      "train loss:   1.511821\n",
      "train loss:   1.543169\n",
      "train loss:   1.640358\n",
      "train loss:   1.663019\n",
      "train loss:   1.788638\n",
      "train loss:   1.450914\n",
      "train loss:   1.509546\n",
      "train loss:   1.255722\n",
      "train loss:   1.470672\n",
      "train loss:   1.763386\n",
      "train loss:   1.606821\n",
      "train loss:   1.756653\n",
      "train loss:   1.718039\n",
      "train loss:   1.528893\n",
      "########### epoch 28 ###########\n",
      "########### loop 3900 ###########\n",
      "test loss:   1.345768   test accuracy:   0.750000\n",
      "########### loop 3900 ###########\n",
      "train loss:   2.010287\n",
      "train loss:   1.704984\n",
      "train loss:   1.306343\n",
      "train loss:   1.580379\n",
      "train loss:   1.907491\n",
      "train loss:   1.703012\n",
      "train loss:   1.466091\n",
      "train loss:   1.226908\n",
      "train loss:   1.596675\n",
      "train loss:   1.749017\n",
      "train loss:   1.426309\n",
      "train loss:   1.404537\n",
      "train loss:   1.586549\n",
      "train loss:   1.350030\n",
      "train loss:   1.706790\n",
      "train loss:   1.702345\n",
      "train loss:   1.113195\n",
      "train loss:   1.508777\n",
      "train loss:   1.463365\n",
      "train loss:   1.563492\n",
      "train loss:   1.665679\n",
      "train loss:   1.564345\n",
      "train loss:   1.356613\n",
      "train loss:   1.545678\n",
      "train loss:   1.734998\n",
      "train loss:   1.478976\n",
      "train loss:   1.611920\n",
      "train loss:   1.536908\n",
      "train loss:   1.102733\n",
      "train loss:   1.475878\n",
      "train loss:   1.660741\n",
      "train loss:   1.338961\n",
      "train loss:   1.630426\n",
      "train loss:   1.673545\n",
      "train loss:   1.368507\n",
      "train loss:   1.538634\n",
      "train loss:   1.476270\n",
      "train loss:   1.521187\n",
      "train loss:   1.299956\n",
      "train loss:   1.773062\n",
      "train loss:   1.412983\n",
      "train loss:   1.535508\n",
      "train loss:   1.533353\n",
      "train loss:   1.539056\n",
      "train loss:   1.520459\n",
      "train loss:   1.395979\n",
      "train loss:   1.487621\n",
      "train loss:   1.659029\n",
      "train loss:   1.332784\n",
      "train loss:   1.500424\n",
      "########### epoch 29 ###########\n",
      "########### loop 3950 ###########\n",
      "test loss:   1.414958   test accuracy:   0.625000\n",
      "########### loop 3950 ###########\n",
      "train loss:   1.590366\n",
      "train loss:   1.928007\n",
      "train loss:   1.558750\n",
      "train loss:   1.706087\n",
      "train loss:   1.583164\n",
      "train loss:   1.501714\n",
      "train loss:   1.719783\n",
      "train loss:   1.584538\n",
      "train loss:   1.622278\n",
      "train loss:   1.472964\n",
      "train loss:   1.681965\n",
      "train loss:   1.512094\n",
      "train loss:   1.605040\n",
      "train loss:   1.552518\n",
      "train loss:   1.493978\n",
      "train loss:   1.363080\n",
      "train loss:   1.362567\n",
      "train loss:   1.570993\n",
      "train loss:   1.277885\n",
      "train loss:   1.764278\n",
      "train loss:   1.765490\n",
      "train loss:   1.667543\n",
      "train loss:   1.475998\n",
      "train loss:   1.601883\n",
      "train loss:   1.659745\n",
      "train loss:   1.340177\n",
      "train loss:   1.749257\n",
      "train loss:   1.550052\n",
      "train loss:   1.636934\n",
      "train loss:   1.734080\n",
      "train loss:   1.712051\n",
      "train loss:   1.479091\n",
      "train loss:   1.458780\n",
      "train loss:   1.530434\n",
      "train loss:   1.503653\n",
      "train loss:   1.697013\n",
      "train loss:   1.621793\n",
      "train loss:   1.450678\n",
      "train loss:   1.642610\n",
      "train loss:   1.420332\n",
      "train loss:   1.367194\n",
      "train loss:   1.579998\n",
      "train loss:   1.540561\n",
      "train loss:   1.570186\n",
      "train loss:   1.519741\n",
      "train loss:   1.511186\n",
      "train loss:   1.575827\n",
      "train loss:   1.486902\n",
      "train loss:   1.505851\n",
      "train loss:   1.436257\n",
      "########### epoch 29 ###########\n",
      "########### loop 4000 ###########\n",
      "test loss:   1.154256   test accuracy:   0.843750\n",
      "########### loop 4000 ###########\n",
      "train loss:   1.594901\n",
      "train loss:   1.616936\n",
      "train loss:   1.644744\n",
      "train loss:   1.453633\n",
      "train loss:   1.594377\n",
      "train loss:   1.330821\n",
      "train loss:   1.482788\n",
      "train loss:   1.396248\n",
      "train loss:   1.338315\n",
      "train loss:   1.387609\n",
      "train loss:   1.730610\n",
      "train loss:   1.534887\n",
      "train loss:   1.691616\n",
      "train loss:   1.486673\n",
      "train loss:   1.750285\n",
      "train loss:   1.454649\n",
      "train loss:   1.412330\n",
      "train loss:   1.473015\n",
      "train loss:   1.676515\n",
      "train loss:   1.625417\n",
      "train loss:   1.803455\n",
      "train loss:   1.597914\n",
      "train loss:   1.620367\n",
      "train loss:   1.535852\n",
      "train loss:   1.429719\n",
      "train loss:   1.468676\n",
      "train loss:   1.599125\n",
      "train loss:   1.741109\n",
      "train loss:   1.173932\n",
      "train loss:   1.631834\n",
      "train loss:   1.490083\n",
      "train loss:   1.537888\n",
      "train loss:   1.251036\n",
      "train loss:   1.693468\n",
      "train loss:   1.291242\n",
      "train loss:   1.462701\n",
      "train loss:   1.280596\n",
      "train loss:   1.378206\n",
      "train loss:   1.615046\n",
      "train loss:   1.229434\n",
      "train loss:   1.611622\n",
      "train loss:   1.257851\n",
      "train loss:   1.461869\n",
      "train loss:   1.428484\n",
      "train loss:   1.493177\n",
      "train loss:   1.486926\n",
      "train loss:   1.635317\n",
      "train loss:   1.562231\n",
      "train loss:   1.517804\n",
      "train loss:   1.533600\n",
      "########### epoch 29 ###########\n",
      "########### loop 4050 ###########\n",
      "test loss:   1.113441   test accuracy:   0.843750\n",
      "########### loop 4050 ###########\n",
      "train loss:   1.695333\n",
      "train loss:   1.605878\n",
      "train loss:   1.156275\n",
      "train loss:   1.615705\n",
      "train loss:   1.476478\n",
      "train loss:   1.669695\n",
      "train loss:   1.649628\n",
      "train loss:   1.368119\n",
      "train loss:   1.225310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.496647\n",
      "train loss:   1.718087\n",
      "train loss:   1.508472\n",
      "train loss:   1.560681\n",
      "train loss:   1.610186\n",
      "train loss:   1.536955\n",
      "train loss:   1.536559\n",
      "train loss:   1.314329\n",
      "train loss:   1.475691\n",
      "train loss:   1.322454\n",
      "train loss:   1.603572\n",
      "train loss:   1.786473\n",
      "train loss:   1.511738\n",
      "train loss:   1.279263\n",
      "train loss:   1.492458\n",
      "train loss:   1.664609\n",
      "train loss:   1.442644\n",
      "train loss:   1.397800\n",
      "train loss:   1.619717\n",
      "train loss:   1.859827\n",
      "train loss:   1.663489\n",
      "train loss:   1.636337\n",
      "train loss:   1.501187\n",
      "train loss:   1.482970\n",
      "train loss:   1.652242\n",
      "train loss:   1.737265\n",
      "train loss:   1.416630\n",
      "train loss:   1.264657\n",
      "train loss:   1.631060\n",
      "train loss:   1.660419\n",
      "train loss:   1.495117\n",
      "train loss:   1.594453\n",
      "train loss:   1.517373\n",
      "train loss:   1.688225\n",
      "train loss:   1.469148\n",
      "train loss:   1.462458\n",
      "train loss:   1.191723\n",
      "train loss:   1.324480\n",
      "train loss:   1.652896\n",
      "train loss:   1.461847\n",
      "train loss:   1.551249\n",
      "########### epoch 30 ###########\n",
      "########### loop 4100 ###########\n",
      "test loss:   1.185231   test accuracy:   0.812500\n",
      "########### loop 4100 ###########\n",
      "train loss:   1.809037\n",
      "train loss:   1.734782\n",
      "train loss:   1.324316\n",
      "train loss:   1.735511\n",
      "train loss:   1.672606\n",
      "train loss:   1.464967\n",
      "train loss:   1.556803\n",
      "train loss:   1.668291\n",
      "train loss:   1.323432\n",
      "train loss:   1.584775\n",
      "train loss:   1.692880\n",
      "train loss:   1.575805\n",
      "train loss:   1.621734\n",
      "train loss:   1.450392\n",
      "train loss:   1.506632\n",
      "train loss:   1.506524\n",
      "train loss:   1.461450\n",
      "train loss:   1.673892\n",
      "train loss:   1.603071\n",
      "train loss:   1.487274\n",
      "train loss:   1.437217\n",
      "train loss:   1.568017\n",
      "train loss:   1.401327\n",
      "train loss:   1.538642\n",
      "train loss:   1.751033\n",
      "train loss:   1.523929\n",
      "train loss:   1.449600\n",
      "train loss:   1.666106\n",
      "train loss:   1.335373\n",
      "train loss:   1.823094\n",
      "train loss:   1.336321\n",
      "train loss:   1.558436\n",
      "train loss:   1.618972\n",
      "train loss:   1.829511\n",
      "train loss:   1.558937\n",
      "train loss:   1.536370\n",
      "train loss:   1.651882\n",
      "train loss:   1.386762\n",
      "train loss:   1.517995\n",
      "train loss:   1.582224\n",
      "train loss:   1.496861\n",
      "train loss:   1.567450\n",
      "train loss:   1.720828\n",
      "train loss:   1.752928\n",
      "train loss:   1.382959\n",
      "train loss:   1.782248\n",
      "train loss:   1.470871\n",
      "train loss:   1.400194\n",
      "train loss:   1.170721\n",
      "train loss:   1.692445\n",
      "########### epoch 30 ###########\n",
      "########### loop 4150 ###########\n",
      "test loss:   1.103919   test accuracy:   0.812500\n",
      "########### loop 4150 ###########\n",
      "train loss:   1.855056\n",
      "train loss:   1.777745\n",
      "train loss:   1.696636\n",
      "train loss:   1.639467\n",
      "train loss:   1.356452\n",
      "train loss:   1.639504\n",
      "train loss:   1.472263\n",
      "train loss:   1.356230\n",
      "train loss:   1.598162\n",
      "train loss:   1.627283\n",
      "train loss:   1.544335\n",
      "train loss:   1.550921\n",
      "train loss:   1.734020\n",
      "train loss:   1.185692\n",
      "train loss:   1.435728\n",
      "train loss:   1.557660\n",
      "train loss:   1.450988\n",
      "train loss:   1.384978\n",
      "train loss:   1.515634\n",
      "train loss:   1.783930\n",
      "train loss:   1.578143\n",
      "train loss:   1.580934\n",
      "train loss:   1.541520\n",
      "train loss:   1.549747\n",
      "train loss:   1.890315\n",
      "train loss:   1.407726\n",
      "train loss:   1.782386\n",
      "train loss:   1.459138\n",
      "train loss:   1.578271\n",
      "train loss:   1.355858\n",
      "train loss:   1.385997\n",
      "train loss:   1.588538\n",
      "train loss:   1.599213\n",
      "train loss:   1.636819\n",
      "train loss:   1.486974\n",
      "train loss:   1.576581\n",
      "train loss:   1.412432\n",
      "train loss:   1.603457\n",
      "train loss:   1.577040\n",
      "train loss:   1.541973\n",
      "train loss:   1.190715\n",
      "train loss:   1.246143\n",
      "train loss:   1.649788\n",
      "train loss:   1.546314\n",
      "train loss:   1.352202\n",
      "train loss:   1.740604\n",
      "train loss:   1.591240\n",
      "train loss:   1.375694\n",
      "train loss:   1.376385\n",
      "train loss:   1.647233\n",
      "########### epoch 30 ###########\n",
      "########### loop 4200 ###########\n",
      "test loss:   1.216514   test accuracy:   0.750000\n",
      "########### loop 4200 ###########\n",
      "train loss:   1.278834\n",
      "train loss:   1.802424\n",
      "train loss:   1.885135\n",
      "train loss:   1.283740\n",
      "train loss:   1.464317\n",
      "train loss:   1.672025\n",
      "train loss:   1.561229\n",
      "train loss:   1.395289\n",
      "train loss:   1.613839\n",
      "train loss:   1.668502\n",
      "train loss:   1.626932\n",
      "train loss:   1.670918\n",
      "train loss:   1.410957\n",
      "train loss:   1.686735\n",
      "train loss:   1.575182\n",
      "train loss:   1.468037\n",
      "train loss:   1.489859\n",
      "train loss:   1.612339\n",
      "train loss:   1.638033\n",
      "train loss:   1.761183\n",
      "train loss:   1.508053\n",
      "train loss:   1.519500\n",
      "train loss:   1.332879\n",
      "train loss:   1.652423\n",
      "train loss:   1.610924\n",
      "train loss:   1.461451\n",
      "train loss:   1.150744\n",
      "train loss:   1.525459\n",
      "train loss:   1.655170\n",
      "train loss:   1.310302\n",
      "train loss:   1.322985\n",
      "train loss:   1.389611\n",
      "train loss:   1.631692\n",
      "train loss:   1.609068\n",
      "train loss:   1.483937\n",
      "train loss:   1.452479\n",
      "train loss:   1.385663\n",
      "train loss:   1.680990\n",
      "train loss:   1.674229\n",
      "train loss:   1.598267\n",
      "train loss:   1.332142\n",
      "train loss:   1.614409\n",
      "train loss:   1.391074\n",
      "train loss:   1.752051\n",
      "train loss:   1.414254\n",
      "train loss:   1.828143\n",
      "train loss:   1.444996\n",
      "train loss:   1.484121\n",
      "train loss:   1.748900\n",
      "train loss:   1.815383\n",
      "########### epoch 31 ###########\n",
      "########### loop 4250 ###########\n",
      "test loss:   1.091803   test accuracy:   0.812500\n",
      "########### loop 4250 ###########\n",
      "train loss:   1.528708\n",
      "train loss:   1.495575\n",
      "train loss:   1.574109\n",
      "train loss:   1.547163\n",
      "train loss:   1.311141\n",
      "train loss:   1.443789\n",
      "train loss:   1.617420\n",
      "train loss:   1.620485\n",
      "train loss:   1.672117\n",
      "train loss:   1.367300\n",
      "train loss:   1.438146\n",
      "train loss:   1.325809\n",
      "train loss:   1.755258\n",
      "train loss:   1.501091\n",
      "train loss:   1.783683\n",
      "train loss:   1.555033\n",
      "train loss:   1.419770\n",
      "train loss:   1.458801\n",
      "train loss:   1.576993\n",
      "train loss:   1.425457\n",
      "train loss:   1.383475\n",
      "train loss:   1.486886\n",
      "train loss:   1.350405\n",
      "train loss:   1.439183\n",
      "train loss:   1.726887\n",
      "train loss:   1.502441\n",
      "train loss:   1.437989\n",
      "train loss:   1.758543\n",
      "train loss:   1.448405\n",
      "train loss:   1.589071\n",
      "train loss:   1.379052\n",
      "train loss:   1.579919\n",
      "train loss:   1.520281\n",
      "train loss:   1.428984\n",
      "train loss:   1.329910\n",
      "train loss:   1.586107\n",
      "train loss:   1.485670\n",
      "train loss:   1.237060\n",
      "train loss:   1.561101\n",
      "train loss:   1.472648\n",
      "train loss:   1.351272\n",
      "train loss:   1.511067\n",
      "train loss:   1.613350\n",
      "train loss:   1.439575\n",
      "train loss:   1.298713\n",
      "train loss:   1.605427\n",
      "train loss:   1.546425\n",
      "train loss:   1.865393\n",
      "train loss:   1.320123\n",
      "train loss:   1.519161\n",
      "########### epoch 31 ###########\n",
      "########### loop 4300 ###########\n",
      "test loss:   1.316693   test accuracy:   0.718750\n",
      "########### loop 4300 ###########\n",
      "train loss:   1.460445\n",
      "train loss:   1.509320\n",
      "train loss:   1.353031\n",
      "train loss:   1.536923\n",
      "train loss:   1.829154\n",
      "train loss:   1.491907\n",
      "train loss:   1.334413\n",
      "train loss:   1.347331\n",
      "train loss:   1.713447\n",
      "train loss:   1.505459\n",
      "train loss:   1.736314\n",
      "train loss:   1.549544\n",
      "train loss:   1.281783\n",
      "train loss:   1.169441\n",
      "train loss:   1.984603\n",
      "train loss:   1.490625\n",
      "train loss:   1.638921\n",
      "train loss:   1.790849\n",
      "train loss:   1.701216\n",
      "train loss:   1.539974\n",
      "train loss:   1.886997\n",
      "train loss:   1.606092\n",
      "train loss:   1.569624\n",
      "train loss:   1.834373\n",
      "train loss:   1.612419\n",
      "train loss:   1.637930\n",
      "train loss:   1.569733\n",
      "train loss:   1.776748\n",
      "train loss:   1.752567\n",
      "train loss:   1.493366\n",
      "train loss:   1.329001\n",
      "train loss:   1.585664\n",
      "train loss:   1.616584\n",
      "train loss:   1.494689\n",
      "train loss:   1.398204\n",
      "train loss:   1.271688\n",
      "train loss:   1.456697\n",
      "train loss:   1.665252\n",
      "train loss:   1.474196\n",
      "train loss:   1.571123\n",
      "train loss:   1.191431\n",
      "train loss:   1.383215\n",
      "train loss:   1.404321\n",
      "train loss:   1.701029\n",
      "train loss:   1.682400\n",
      "train loss:   1.362205\n",
      "train loss:   1.669299\n",
      "train loss:   1.333657\n",
      "train loss:   1.214553\n",
      "train loss:   1.579694\n",
      "########### epoch 31 ###########\n",
      "########### loop 4350 ###########\n",
      "test loss:   1.291238   test accuracy:   0.781250\n",
      "########### loop 4350 ###########\n",
      "train loss:   1.495582\n",
      "train loss:   1.442158\n",
      "train loss:   1.441657\n",
      "train loss:   1.430808\n",
      "train loss:   1.530762\n",
      "train loss:   1.685629\n",
      "train loss:   1.337809\n",
      "train loss:   1.723945\n",
      "train loss:   1.380258\n",
      "train loss:   1.250182\n",
      "train loss:   1.704076\n",
      "train loss:   1.213162\n",
      "train loss:   1.566881\n",
      "train loss:   1.449112\n",
      "train loss:   1.435197\n",
      "train loss:   1.680646\n",
      "train loss:   1.511758\n",
      "train loss:   1.424901\n",
      "train loss:   1.695923\n",
      "train loss:   1.707681\n",
      "train loss:   1.443993\n",
      "train loss:   1.461716\n",
      "train loss:   1.546031\n",
      "train loss:   1.618747\n",
      "train loss:   1.500164\n",
      "train loss:   1.408423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.440685\n",
      "train loss:   1.624227\n",
      "train loss:   1.497643\n",
      "train loss:   1.411692\n",
      "train loss:   1.626330\n",
      "train loss:   1.628100\n",
      "train loss:   1.429145\n",
      "train loss:   1.633085\n",
      "train loss:   1.506794\n",
      "train loss:   1.239584\n",
      "train loss:   1.410013\n",
      "train loss:   1.312598\n",
      "train loss:   1.522577\n",
      "train loss:   1.673640\n",
      "train loss:   1.405549\n",
      "train loss:   1.684419\n",
      "train loss:   1.428711\n",
      "train loss:   1.319360\n",
      "train loss:   1.599184\n",
      "train loss:   1.596358\n",
      "train loss:   1.547017\n",
      "train loss:   1.298344\n",
      "train loss:   1.509961\n",
      "train loss:   1.415731\n",
      "########### epoch 32 ###########\n",
      "########### loop 4400 ###########\n",
      "test loss:   1.161897   test accuracy:   0.718750\n",
      "########### loop 4400 ###########\n",
      "train loss:   1.131450\n",
      "train loss:   1.550356\n",
      "train loss:   1.454186\n",
      "train loss:   1.530746\n",
      "train loss:   1.507383\n",
      "train loss:   1.750600\n",
      "train loss:   1.376271\n",
      "train loss:   1.716220\n",
      "train loss:   1.462932\n",
      "train loss:   1.746241\n",
      "train loss:   1.531627\n",
      "train loss:   1.598259\n",
      "train loss:   1.532362\n",
      "train loss:   1.673165\n",
      "train loss:   1.648352\n",
      "train loss:   1.581458\n",
      "train loss:   1.519597\n",
      "train loss:   1.523217\n",
      "train loss:   1.348379\n",
      "train loss:   1.478130\n",
      "train loss:   1.502097\n",
      "train loss:   1.443300\n",
      "train loss:   1.615802\n",
      "train loss:   1.819006\n",
      "train loss:   1.581481\n",
      "train loss:   1.664705\n",
      "train loss:   1.670140\n",
      "train loss:   1.461482\n",
      "train loss:   1.438108\n",
      "train loss:   1.558462\n",
      "train loss:   1.612500\n",
      "train loss:   1.590411\n",
      "train loss:   1.566252\n",
      "train loss:   1.465015\n",
      "train loss:   1.541600\n",
      "train loss:   1.611123\n",
      "train loss:   1.272265\n",
      "train loss:   1.871858\n",
      "train loss:   1.562578\n",
      "train loss:   1.524945\n",
      "train loss:   1.318853\n",
      "train loss:   1.522434\n",
      "train loss:   1.446256\n",
      "train loss:   1.655956\n",
      "train loss:   1.345628\n",
      "train loss:   1.815307\n",
      "train loss:   1.492301\n",
      "train loss:   1.676767\n",
      "train loss:   1.585660\n",
      "train loss:   1.611731\n",
      "########### epoch 32 ###########\n",
      "########### loop 4450 ###########\n",
      "test loss:   1.201646   test accuracy:   0.718750\n",
      "########### loop 4450 ###########\n",
      "train loss:   1.629939\n",
      "train loss:   1.719733\n",
      "train loss:   1.489649\n",
      "train loss:   1.250507\n",
      "train loss:   1.423420\n",
      "train loss:   1.571717\n",
      "train loss:   1.580170\n",
      "train loss:   1.401389\n",
      "train loss:   1.461507\n",
      "train loss:   1.460041\n",
      "train loss:   1.842761\n",
      "train loss:   1.460377\n",
      "train loss:   1.360698\n",
      "train loss:   1.398588\n",
      "train loss:   1.710728\n",
      "train loss:   1.609987\n",
      "train loss:   1.567965\n",
      "train loss:   1.662022\n",
      "train loss:   1.494151\n",
      "train loss:   1.566877\n",
      "train loss:   1.399226\n",
      "train loss:   1.308811\n",
      "train loss:   1.725242\n",
      "train loss:   1.624172\n",
      "train loss:   1.967426\n",
      "train loss:   1.394728\n",
      "train loss:   1.649123\n",
      "train loss:   1.578438\n",
      "train loss:   1.717150\n",
      "train loss:   1.402441\n",
      "train loss:   1.621267\n",
      "train loss:   1.396345\n",
      "train loss:   1.375564\n",
      "train loss:   1.824572\n",
      "train loss:   1.218393\n",
      "train loss:   1.653616\n",
      "train loss:   1.513098\n",
      "train loss:   1.509446\n",
      "train loss:   1.405455\n",
      "train loss:   1.557360\n",
      "train loss:   1.638595\n",
      "train loss:   1.538642\n",
      "train loss:   1.559158\n",
      "train loss:   1.689944\n",
      "train loss:   1.524254\n",
      "train loss:   1.559616\n",
      "train loss:   1.695265\n",
      "train loss:   1.591995\n",
      "train loss:   1.676383\n",
      "train loss:   1.412973\n",
      "########### epoch 32 ###########\n",
      "########### loop 4500 ###########\n",
      "test loss:   1.194703   test accuracy:   0.812500\n",
      "########### loop 4500 ###########\n",
      "train loss:   1.609685\n",
      "train loss:   1.553225\n",
      "train loss:   1.627786\n",
      "train loss:   1.729504\n",
      "train loss:   1.620496\n",
      "train loss:   1.537982\n",
      "train loss:   1.529190\n",
      "train loss:   1.418569\n",
      "train loss:   1.266244\n",
      "train loss:   1.756478\n",
      "train loss:   1.555192\n",
      "train loss:   1.463515\n",
      "train loss:   1.668963\n",
      "train loss:   1.340833\n",
      "train loss:   1.273876\n",
      "train loss:   1.554894\n",
      "train loss:   1.597710\n",
      "train loss:   1.457355\n",
      "train loss:   2.021474\n",
      "train loss:   1.238455\n",
      "train loss:   1.804625\n",
      "train loss:   1.404006\n",
      "train loss:   1.468897\n",
      "train loss:   1.373901\n",
      "train loss:   1.418007\n",
      "train loss:   1.426717\n",
      "train loss:   1.584668\n",
      "train loss:   1.386714\n",
      "train loss:   1.807729\n",
      "train loss:   1.223400\n",
      "train loss:   1.319786\n",
      "train loss:   1.946770\n",
      "train loss:   1.486273\n",
      "train loss:   1.725241\n",
      "train loss:   1.400463\n",
      "train loss:   1.718127\n",
      "train loss:   1.541277\n",
      "train loss:   1.662118\n",
      "train loss:   1.473004\n",
      "train loss:   1.468839\n",
      "train loss:   1.424557\n",
      "train loss:   1.437937\n",
      "train loss:   1.707589\n",
      "train loss:   1.571470\n",
      "train loss:   1.364585\n",
      "train loss:   1.211251\n",
      "train loss:   1.567483\n",
      "train loss:   1.601531\n",
      "train loss:   1.829153\n",
      "train loss:   1.672232\n",
      "########### epoch 33 ###########\n",
      "########### loop 4550 ###########\n",
      "test loss:   1.234511   test accuracy:   0.718750\n",
      "########### loop 4550 ###########\n",
      "train loss:   1.761164\n",
      "train loss:   1.423761\n",
      "train loss:   1.814716\n",
      "train loss:   1.465775\n",
      "train loss:   1.362959\n",
      "train loss:   1.687035\n",
      "train loss:   1.200697\n",
      "train loss:   1.713582\n",
      "train loss:   1.397752\n",
      "train loss:   1.497460\n",
      "train loss:   1.733621\n",
      "train loss:   1.491499\n",
      "train loss:   1.417371\n",
      "train loss:   1.794535\n",
      "train loss:   1.533579\n",
      "train loss:   1.443230\n",
      "train loss:   1.439467\n",
      "train loss:   1.449552\n",
      "train loss:   1.509081\n",
      "train loss:   1.567146\n",
      "train loss:   1.613077\n",
      "train loss:   1.530268\n",
      "train loss:   1.728420\n",
      "train loss:   1.646930\n",
      "train loss:   1.423022\n",
      "train loss:   1.549878\n",
      "train loss:   1.347691\n",
      "train loss:   1.636482\n",
      "train loss:   1.475476\n",
      "train loss:   1.418037\n",
      "train loss:   1.515212\n",
      "train loss:   1.275959\n",
      "train loss:   1.190441\n",
      "train loss:   1.510607\n",
      "train loss:   1.678857\n",
      "train loss:   1.596921\n",
      "train loss:   1.884189\n",
      "train loss:   1.286856\n",
      "train loss:   1.524503\n",
      "train loss:   1.125457\n",
      "train loss:   1.507815\n",
      "train loss:   1.322035\n",
      "train loss:   1.714382\n",
      "train loss:   1.691377\n",
      "train loss:   1.760419\n",
      "train loss:   1.570840\n",
      "train loss:   1.442399\n",
      "train loss:   1.519045\n",
      "train loss:   1.632847\n",
      "train loss:   1.905035\n",
      "########### epoch 33 ###########\n",
      "########### loop 4600 ###########\n",
      "test loss:   1.346016   test accuracy:   0.812500\n",
      "########### loop 4600 ###########\n",
      "train loss:   1.528959\n",
      "train loss:   1.510266\n",
      "train loss:   1.539972\n",
      "train loss:   1.587538\n",
      "train loss:   1.424679\n",
      "train loss:   1.520086\n",
      "train loss:   1.802663\n",
      "train loss:   1.589735\n",
      "train loss:   1.463997\n",
      "train loss:   1.593156\n",
      "train loss:   1.234138\n",
      "train loss:   1.257144\n",
      "train loss:   1.561262\n",
      "train loss:   1.583712\n",
      "train loss:   1.559637\n",
      "train loss:   1.169496\n",
      "train loss:   1.400907\n",
      "train loss:   1.583557\n",
      "train loss:   1.467861\n",
      "train loss:   1.146194\n",
      "train loss:   1.306149\n",
      "train loss:   1.434951\n",
      "train loss:   1.577282\n",
      "train loss:   1.467396\n",
      "train loss:   1.592085\n",
      "train loss:   1.436583\n",
      "train loss:   1.520370\n",
      "train loss:   1.462886\n",
      "train loss:   1.594626\n",
      "train loss:   1.432800\n",
      "train loss:   1.512785\n",
      "train loss:   1.423733\n",
      "train loss:   1.375064\n",
      "train loss:   1.384830\n",
      "train loss:   1.452010\n",
      "train loss:   1.595944\n",
      "train loss:   1.704545\n",
      "train loss:   1.596886\n",
      "train loss:   1.440324\n",
      "train loss:   1.600756\n",
      "train loss:   1.694970\n",
      "train loss:   1.419868\n",
      "train loss:   1.628572\n",
      "train loss:   1.463780\n",
      "train loss:   1.691902\n",
      "train loss:   1.705812\n",
      "train loss:   1.566053\n",
      "train loss:   1.419686\n",
      "train loss:   1.659253\n",
      "train loss:   1.540858\n",
      "########### epoch 33 ###########\n",
      "########### loop 4650 ###########\n",
      "test loss:   1.079440   test accuracy:   0.812500\n",
      "########### loop 4650 ###########\n",
      "train loss:   1.446611\n",
      "train loss:   1.408377\n",
      "train loss:   1.561026\n",
      "train loss:   1.612642\n",
      "train loss:   1.610549\n",
      "train loss:   1.555653\n",
      "train loss:   1.853170\n",
      "train loss:   1.285018\n",
      "train loss:   1.617203\n",
      "train loss:   1.550177\n",
      "train loss:   1.431299\n",
      "train loss:   1.520573\n",
      "train loss:   1.394715\n",
      "train loss:   1.398346\n",
      "train loss:   1.652050\n",
      "train loss:   1.651205\n",
      "train loss:   1.526237\n",
      "train loss:   1.575703\n",
      "train loss:   1.477718\n",
      "train loss:   1.559992\n",
      "train loss:   1.268159\n",
      "train loss:   1.367854\n",
      "train loss:   1.583159\n",
      "train loss:   1.397874\n",
      "train loss:   1.873477\n",
      "train loss:   1.481430\n",
      "train loss:   1.437447\n",
      "train loss:   1.524361\n",
      "train loss:   1.650578\n",
      "train loss:   1.692101\n",
      "train loss:   1.386916\n",
      "train loss:   1.604958\n",
      "train loss:   1.575996\n",
      "train loss:   1.511935\n",
      "train loss:   1.523302\n",
      "train loss:   1.429824\n",
      "train loss:   1.428605\n",
      "train loss:   1.585530\n",
      "train loss:   1.748459\n",
      "train loss:   1.298186\n",
      "train loss:   1.712790\n",
      "train loss:   1.291054\n",
      "train loss:   1.755417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.702896\n",
      "train loss:   1.472536\n",
      "train loss:   1.334592\n",
      "train loss:   1.416496\n",
      "train loss:   1.378786\n",
      "train loss:   1.648157\n",
      "train loss:   1.423300\n",
      "########### epoch 34 ###########\n",
      "########### loop 4700 ###########\n",
      "test loss:   1.054110   test accuracy:   0.875000\n",
      "########### loop 4700 ###########\n",
      "train loss:   1.298237\n",
      "train loss:   1.752095\n",
      "train loss:   1.495976\n",
      "train loss:   1.601548\n",
      "train loss:   1.536119\n",
      "train loss:   1.479490\n",
      "train loss:   1.420846\n",
      "train loss:   1.552958\n",
      "train loss:   1.447473\n",
      "train loss:   1.662090\n",
      "train loss:   1.594042\n",
      "train loss:   1.614987\n",
      "train loss:   1.181239\n",
      "train loss:   1.521758\n",
      "train loss:   1.664727\n",
      "train loss:   1.618202\n",
      "train loss:   1.431309\n",
      "train loss:   1.481754\n",
      "train loss:   1.546513\n",
      "train loss:   1.659693\n",
      "train loss:   1.511643\n",
      "train loss:   1.630628\n",
      "train loss:   1.262014\n",
      "train loss:   1.709621\n",
      "train loss:   1.724479\n",
      "train loss:   1.460074\n",
      "train loss:   1.463959\n",
      "train loss:   1.742692\n",
      "train loss:   1.527940\n",
      "train loss:   1.394808\n",
      "train loss:   1.582910\n",
      "train loss:   1.248356\n",
      "train loss:   1.761885\n",
      "train loss:   1.596520\n",
      "train loss:   1.627928\n",
      "train loss:   1.574826\n",
      "train loss:   1.418625\n",
      "train loss:   1.677637\n",
      "train loss:   1.624428\n",
      "train loss:   1.598064\n",
      "train loss:   1.396590\n",
      "train loss:   1.643322\n",
      "train loss:   1.418682\n",
      "train loss:   1.789464\n",
      "train loss:   1.503400\n",
      "train loss:   1.411128\n",
      "train loss:   1.532449\n",
      "train loss:   1.496228\n",
      "train loss:   1.342800\n",
      "train loss:   1.602755\n",
      "########### epoch 34 ###########\n",
      "########### loop 4750 ###########\n",
      "test loss:   1.159060   test accuracy:   0.750000\n",
      "########### loop 4750 ###########\n",
      "train loss:   1.716008\n",
      "train loss:   1.520452\n",
      "train loss:   1.576900\n",
      "train loss:   1.557020\n",
      "train loss:   1.363330\n",
      "train loss:   1.660868\n",
      "train loss:   1.409335\n",
      "train loss:   1.401806\n",
      "train loss:   1.938063\n",
      "train loss:   1.265429\n",
      "train loss:   1.317961\n",
      "train loss:   1.486468\n",
      "train loss:   1.369044\n",
      "train loss:   1.782571\n",
      "train loss:   1.514497\n",
      "train loss:   1.385457\n",
      "train loss:   1.645201\n",
      "train loss:   1.588264\n",
      "train loss:   1.478249\n",
      "train loss:   1.580465\n",
      "train loss:   1.787145\n",
      "train loss:   1.670624\n",
      "train loss:   1.595032\n",
      "train loss:   1.414327\n",
      "train loss:   1.416035\n",
      "train loss:   1.550955\n",
      "train loss:   1.484979\n",
      "train loss:   1.678213\n",
      "train loss:   1.598568\n",
      "train loss:   1.443774\n",
      "train loss:   1.452039\n",
      "train loss:   1.752942\n",
      "train loss:   1.441722\n",
      "train loss:   1.641819\n",
      "train loss:   1.425149\n",
      "train loss:   1.626364\n",
      "train loss:   1.558726\n",
      "train loss:   1.934772\n",
      "train loss:   1.656788\n",
      "train loss:   1.504489\n",
      "train loss:   1.280621\n",
      "train loss:   1.468861\n",
      "train loss:   1.140743\n",
      "train loss:   1.231515\n",
      "train loss:   1.643357\n",
      "train loss:   1.635127\n",
      "train loss:   1.314516\n",
      "train loss:   1.767190\n",
      "train loss:   1.490494\n",
      "train loss:   1.566923\n",
      "########### epoch 35 ###########\n",
      "########### loop 4800 ###########\n",
      "test loss:   1.192127   test accuracy:   0.843750\n",
      "########### loop 4800 ###########\n",
      "train loss:   1.346897\n",
      "train loss:   1.293584\n",
      "train loss:   1.427679\n",
      "train loss:   1.519815\n",
      "train loss:   1.434757\n",
      "train loss:   1.537014\n",
      "train loss:   1.741395\n",
      "train loss:   1.668686\n",
      "train loss:   1.834602\n",
      "train loss:   1.414258\n",
      "train loss:   1.579144\n",
      "train loss:   1.564456\n",
      "train loss:   1.209327\n",
      "train loss:   1.490401\n",
      "train loss:   1.295039\n",
      "train loss:   1.713222\n",
      "train loss:   1.457105\n",
      "train loss:   1.247284\n",
      "train loss:   1.377137\n",
      "train loss:   1.579877\n",
      "train loss:   1.475042\n",
      "train loss:   1.462876\n",
      "train loss:   1.543722\n",
      "train loss:   1.587998\n",
      "train loss:   1.426915\n",
      "train loss:   1.650588\n",
      "train loss:   1.437725\n",
      "train loss:   1.788175\n",
      "train loss:   1.641504\n",
      "train loss:   1.688037\n",
      "train loss:   1.532981\n",
      "train loss:   1.612901\n",
      "train loss:   1.092115\n",
      "train loss:   1.475905\n",
      "train loss:   1.520237\n",
      "train loss:   1.457296\n",
      "train loss:   1.272581\n",
      "train loss:   1.497499\n",
      "train loss:   1.591703\n",
      "train loss:   1.398489\n",
      "train loss:   1.451356\n",
      "train loss:   1.340354\n",
      "train loss:   1.392760\n",
      "train loss:   1.536638\n",
      "train loss:   1.619438\n",
      "train loss:   1.941942\n",
      "train loss:   1.656710\n",
      "train loss:   1.539743\n",
      "train loss:   1.714500\n",
      "train loss:   1.070139\n",
      "########### epoch 35 ###########\n",
      "########### loop 4850 ###########\n",
      "test loss:   1.370938   test accuracy:   0.593750\n",
      "########### loop 4850 ###########\n",
      "train loss:   1.586366\n",
      "train loss:   1.381526\n",
      "train loss:   1.701167\n",
      "train loss:   1.585991\n",
      "train loss:   1.592661\n",
      "train loss:   1.268918\n",
      "train loss:   1.246132\n",
      "train loss:   1.703191\n",
      "train loss:   1.626169\n",
      "train loss:   1.268538\n",
      "train loss:   1.387967\n",
      "train loss:   1.743411\n",
      "train loss:   1.685303\n",
      "train loss:   1.783809\n",
      "train loss:   1.443894\n",
      "train loss:   1.617675\n",
      "train loss:   1.402807\n",
      "train loss:   1.586480\n",
      "train loss:   1.428337\n",
      "train loss:   1.694178\n",
      "train loss:   1.573928\n",
      "train loss:   1.519776\n",
      "train loss:   1.529678\n",
      "train loss:   1.422209\n",
      "train loss:   1.363713\n",
      "train loss:   1.591594\n",
      "train loss:   1.548999\n",
      "train loss:   1.659320\n",
      "train loss:   1.712562\n",
      "train loss:   1.769202\n",
      "train loss:   1.674776\n",
      "train loss:   1.346347\n",
      "train loss:   1.332898\n",
      "train loss:   1.780204\n",
      "train loss:   1.328854\n",
      "train loss:   1.373542\n",
      "train loss:   1.641639\n",
      "train loss:   1.471286\n",
      "train loss:   1.534767\n",
      "train loss:   1.578601\n",
      "train loss:   1.463359\n",
      "train loss:   1.659136\n",
      "train loss:   1.714162\n",
      "train loss:   1.297066\n",
      "train loss:   1.608866\n",
      "train loss:   1.403793\n",
      "train loss:   1.474005\n",
      "train loss:   1.653071\n",
      "train loss:   1.701169\n",
      "train loss:   1.257625\n",
      "########### epoch 35 ###########\n",
      "########### loop 4900 ###########\n",
      "test loss:   1.229624   test accuracy:   0.687500\n",
      "########### loop 4900 ###########\n",
      "train loss:   1.455773\n",
      "train loss:   1.311353\n",
      "train loss:   1.420738\n",
      "train loss:   1.708124\n",
      "train loss:   1.745567\n",
      "train loss:   1.567465\n",
      "train loss:   1.357056\n",
      "train loss:   1.499117\n",
      "train loss:   1.666646\n",
      "train loss:   1.760724\n",
      "train loss:   1.622288\n",
      "train loss:   1.591759\n",
      "train loss:   1.647182\n",
      "train loss:   1.682733\n",
      "train loss:   1.575439\n",
      "train loss:   1.601059\n",
      "train loss:   1.426015\n",
      "train loss:   1.523156\n",
      "train loss:   1.502482\n",
      "train loss:   1.611417\n",
      "train loss:   1.531624\n",
      "train loss:   1.561938\n",
      "train loss:   1.424523\n",
      "train loss:   1.315476\n",
      "train loss:   1.648297\n",
      "train loss:   1.579949\n",
      "train loss:   1.472349\n",
      "train loss:   1.458406\n",
      "train loss:   1.564749\n",
      "train loss:   1.346224\n",
      "train loss:   1.733047\n",
      "train loss:   1.532849\n",
      "train loss:   1.396472\n",
      "train loss:   1.559357\n",
      "train loss:   1.515622\n",
      "train loss:   1.580703\n",
      "train loss:   1.261752\n",
      "train loss:   1.688339\n",
      "train loss:   1.484256\n",
      "train loss:   1.597443\n",
      "train loss:   1.731014\n",
      "train loss:   1.510093\n",
      "train loss:   1.438299\n",
      "train loss:   1.428376\n",
      "train loss:   1.564230\n",
      "train loss:   1.495433\n",
      "train loss:   1.311025\n",
      "train loss:   1.278579\n",
      "train loss:   1.724312\n",
      "train loss:   1.602233\n",
      "########### epoch 36 ###########\n",
      "########### loop 4950 ###########\n",
      "test loss:   1.268800   test accuracy:   0.750000\n",
      "########### loop 4950 ###########\n",
      "train loss:   1.736782\n",
      "train loss:   1.307510\n",
      "train loss:   1.519319\n",
      "train loss:   1.427585\n",
      "train loss:   1.676830\n",
      "train loss:   1.416409\n",
      "train loss:   1.299930\n",
      "train loss:   1.480289\n",
      "train loss:   1.446210\n",
      "train loss:   1.433230\n",
      "train loss:   1.284959\n",
      "train loss:   1.609779\n",
      "train loss:   1.645670\n",
      "train loss:   1.596827\n",
      "train loss:   1.739908\n",
      "train loss:   1.456650\n",
      "train loss:   1.576940\n",
      "train loss:   1.450803\n",
      "train loss:   1.331496\n",
      "train loss:   1.598788\n",
      "train loss:   1.460438\n",
      "train loss:   1.595850\n",
      "train loss:   1.310776\n",
      "train loss:   1.279984\n",
      "train loss:   1.377434\n",
      "train loss:   1.390834\n",
      "train loss:   1.583145\n",
      "train loss:   1.329188\n",
      "train loss:   1.585138\n",
      "train loss:   1.532576\n",
      "train loss:   1.625437\n",
      "train loss:   1.361216\n",
      "train loss:   1.618426\n",
      "train loss:   1.608361\n",
      "train loss:   1.493277\n",
      "train loss:   1.704145\n",
      "train loss:   1.602709\n",
      "train loss:   1.495230\n",
      "train loss:   1.465188\n",
      "train loss:   1.429240\n",
      "train loss:   1.744672\n",
      "train loss:   1.364307\n",
      "train loss:   1.574349\n",
      "train loss:   1.399541\n",
      "train loss:   1.370287\n",
      "train loss:   1.529729\n",
      "train loss:   1.437504\n",
      "train loss:   1.386643\n",
      "train loss:   1.576728\n",
      "train loss:   1.840018\n",
      "########### epoch 36 ###########\n",
      "########### loop 5000 ###########\n",
      "test loss:   1.043296   test accuracy:   0.812500\n",
      "########### loop 5000 ###########\n",
      "train loss:   1.414106\n",
      "train loss:   1.164016\n",
      "train loss:   1.630575\n",
      "train loss:   1.800336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.679788\n",
      "train loss:   1.346083\n",
      "train loss:   1.727801\n",
      "train loss:   1.483172\n",
      "train loss:   1.745068\n",
      "train loss:   1.638682\n",
      "train loss:   1.509881\n",
      "train loss:   1.508323\n",
      "train loss:   1.713898\n",
      "train loss:   1.732283\n",
      "train loss:   1.314267\n",
      "train loss:   1.281095\n",
      "train loss:   1.566833\n",
      "train loss:   1.764829\n",
      "train loss:   1.583862\n",
      "train loss:   1.227656\n",
      "train loss:   1.412154\n",
      "train loss:   1.729704\n",
      "train loss:   1.502631\n",
      "train loss:   1.230553\n",
      "train loss:   1.083701\n",
      "train loss:   1.267682\n",
      "train loss:   1.605854\n",
      "train loss:   1.582927\n",
      "train loss:   1.547310\n",
      "train loss:   1.546348\n",
      "train loss:   1.614187\n",
      "train loss:   1.527167\n",
      "train loss:   1.600708\n",
      "train loss:   1.593701\n",
      "train loss:   1.383423\n",
      "train loss:   1.745953\n",
      "train loss:   1.649138\n",
      "train loss:   1.417912\n",
      "train loss:   1.359031\n",
      "train loss:   1.356188\n",
      "train loss:   1.649174\n",
      "train loss:   1.642807\n",
      "train loss:   1.285428\n",
      "train loss:   1.499239\n",
      "train loss:   1.147115\n",
      "train loss:   1.502657\n",
      "train loss:   1.289253\n",
      "train loss:   1.527939\n",
      "train loss:   1.224780\n",
      "train loss:   1.107789\n",
      "########### epoch 36 ###########\n",
      "########### loop 5050 ###########\n",
      "test loss:   1.248983   test accuracy:   0.718750\n",
      "########### loop 5050 ###########\n",
      "train loss:   1.454461\n",
      "train loss:   1.715961\n",
      "train loss:   1.647067\n",
      "train loss:   1.431653\n",
      "train loss:   1.443573\n",
      "train loss:   1.299825\n",
      "train loss:   1.835556\n",
      "train loss:   1.598389\n",
      "train loss:   1.368494\n",
      "train loss:   1.464996\n",
      "train loss:   1.276931\n",
      "train loss:   1.504370\n",
      "train loss:   1.567757\n",
      "train loss:   1.602415\n",
      "train loss:   1.217845\n",
      "train loss:   1.796736\n",
      "train loss:   1.558404\n",
      "train loss:   1.889640\n",
      "train loss:   1.397096\n",
      "train loss:   1.709067\n",
      "train loss:   1.759993\n",
      "train loss:   1.314288\n",
      "train loss:   1.345154\n",
      "train loss:   1.429755\n",
      "train loss:   1.353336\n",
      "train loss:   1.542007\n",
      "train loss:   1.665824\n",
      "train loss:   1.614076\n",
      "train loss:   1.414722\n",
      "train loss:   1.692381\n",
      "train loss:   1.780547\n",
      "train loss:   1.611922\n",
      "train loss:   1.265166\n",
      "train loss:   1.639041\n",
      "train loss:   1.660900\n",
      "train loss:   1.625829\n",
      "train loss:   1.467560\n",
      "train loss:   1.704566\n",
      "train loss:   1.412759\n",
      "train loss:   1.553284\n",
      "train loss:   1.354951\n",
      "train loss:   1.548874\n",
      "train loss:   1.751019\n",
      "train loss:   1.592238\n",
      "train loss:   1.462405\n",
      "train loss:   1.642376\n",
      "train loss:   1.547386\n",
      "train loss:   1.553306\n",
      "train loss:   1.530610\n",
      "train loss:   1.246931\n",
      "########### epoch 37 ###########\n",
      "########### loop 5100 ###########\n",
      "test loss:   1.183398   test accuracy:   0.718750\n",
      "########### loop 5100 ###########\n",
      "train loss:   1.600320\n",
      "train loss:   1.437844\n",
      "train loss:   1.198471\n",
      "train loss:   1.329163\n",
      "train loss:   1.559327\n",
      "train loss:   1.570393\n",
      "train loss:   1.613370\n",
      "train loss:   1.331081\n",
      "train loss:   1.550031\n",
      "train loss:   1.501979\n",
      "train loss:   1.504158\n",
      "train loss:   1.392158\n",
      "train loss:   1.143977\n",
      "train loss:   1.810733\n",
      "train loss:   1.379636\n",
      "train loss:   1.589515\n",
      "train loss:   1.368680\n",
      "train loss:   1.661018\n",
      "train loss:   1.611439\n",
      "train loss:   1.406868\n",
      "train loss:   1.567203\n",
      "train loss:   1.571473\n",
      "train loss:   1.969778\n",
      "train loss:   1.615745\n",
      "train loss:   1.381528\n",
      "train loss:   1.261898\n",
      "train loss:   1.652275\n",
      "train loss:   1.546455\n",
      "train loss:   1.379118\n",
      "train loss:   1.553200\n",
      "train loss:   1.385667\n",
      "train loss:   1.624290\n",
      "train loss:   1.332063\n",
      "train loss:   1.514009\n",
      "train loss:   1.542779\n",
      "train loss:   1.442442\n",
      "train loss:   1.087203\n",
      "train loss:   1.521172\n",
      "train loss:   1.713619\n",
      "train loss:   1.457143\n",
      "train loss:   1.502817\n",
      "train loss:   1.443311\n",
      "train loss:   1.469457\n",
      "train loss:   1.451065\n",
      "train loss:   1.289402\n",
      "train loss:   1.584813\n",
      "train loss:   1.639029\n",
      "train loss:   1.416951\n",
      "train loss:   1.687312\n",
      "train loss:   1.753747\n",
      "########### epoch 37 ###########\n",
      "########### loop 5150 ###########\n",
      "test loss:   1.157774   test accuracy:   0.812500\n",
      "########### loop 5150 ###########\n",
      "train loss:   1.944789\n",
      "train loss:   1.467687\n",
      "train loss:   1.542325\n",
      "train loss:   1.393014\n",
      "train loss:   1.294805\n",
      "train loss:   1.562565\n",
      "train loss:   1.463015\n",
      "train loss:   1.676316\n",
      "train loss:   1.448791\n",
      "train loss:   1.225353\n",
      "train loss:   1.612190\n",
      "train loss:   1.355437\n",
      "train loss:   1.702916\n",
      "train loss:   1.350137\n",
      "train loss:   1.387238\n",
      "train loss:   1.643024\n",
      "train loss:   1.260062\n",
      "train loss:   1.653119\n",
      "train loss:   1.327537\n",
      "train loss:   1.575768\n",
      "train loss:   1.461226\n",
      "train loss:   1.648626\n",
      "train loss:   1.120673\n",
      "train loss:   1.565268\n",
      "train loss:   1.366201\n",
      "train loss:   1.529941\n",
      "train loss:   1.536793\n",
      "train loss:   1.285444\n",
      "train loss:   1.391317\n",
      "train loss:   1.462866\n",
      "train loss:   1.583169\n",
      "train loss:   1.651674\n",
      "train loss:   1.444185\n",
      "train loss:   1.094043\n",
      "train loss:   1.317753\n",
      "train loss:   1.673299\n",
      "train loss:   1.617896\n",
      "train loss:   1.599402\n",
      "train loss:   1.457270\n",
      "train loss:   1.475505\n",
      "train loss:   1.434900\n",
      "train loss:   1.657074\n",
      "train loss:   1.132480\n",
      "train loss:   1.589796\n",
      "train loss:   1.509469\n",
      "train loss:   1.649680\n",
      "train loss:   1.295039\n",
      "train loss:   1.690478\n",
      "train loss:   1.447366\n",
      "train loss:   1.240582\n",
      "########### epoch 37 ###########\n",
      "########### loop 5200 ###########\n",
      "test loss:   1.179531   test accuracy:   0.750000\n",
      "########### loop 5200 ###########\n",
      "train loss:   1.489299\n",
      "train loss:   1.294238\n",
      "train loss:   1.405338\n",
      "train loss:   1.675635\n",
      "train loss:   1.646851\n",
      "train loss:   1.429916\n",
      "train loss:   1.426629\n",
      "train loss:   1.698276\n",
      "train loss:   1.451335\n",
      "train loss:   1.577893\n",
      "train loss:   1.508039\n",
      "train loss:   1.688955\n",
      "train loss:   1.388270\n",
      "train loss:   1.445335\n",
      "train loss:   1.700094\n",
      "train loss:   1.740276\n",
      "train loss:   1.725792\n",
      "train loss:   1.530383\n",
      "train loss:   1.690154\n",
      "train loss:   1.268106\n",
      "train loss:   1.576361\n",
      "train loss:   1.337726\n",
      "train loss:   1.427135\n",
      "train loss:   1.442342\n",
      "train loss:   1.835147\n",
      "train loss:   1.350259\n",
      "train loss:   1.658741\n",
      "train loss:   1.310141\n",
      "train loss:   1.436238\n",
      "train loss:   1.626688\n",
      "train loss:   1.535712\n",
      "train loss:   1.781240\n",
      "train loss:   1.595376\n",
      "train loss:   1.613698\n",
      "train loss:   1.315712\n",
      "train loss:   1.623429\n",
      "train loss:   1.446661\n",
      "train loss:   1.242085\n",
      "train loss:   1.404135\n",
      "train loss:   1.716646\n",
      "train loss:   1.367594\n",
      "train loss:   1.487097\n",
      "train loss:   1.512107\n",
      "train loss:   1.401846\n",
      "train loss:   1.585685\n",
      "train loss:   1.418764\n",
      "train loss:   1.603621\n",
      "train loss:   1.608629\n",
      "train loss:   1.759882\n",
      "train loss:   1.184520\n",
      "########### epoch 38 ###########\n",
      "########### loop 5250 ###########\n",
      "test loss:   1.118625   test accuracy:   0.843750\n",
      "########### loop 5250 ###########\n",
      "train loss:   1.512044\n",
      "train loss:   1.573333\n",
      "train loss:   1.141731\n",
      "train loss:   1.307582\n",
      "train loss:   1.476325\n",
      "train loss:   1.581498\n",
      "train loss:   1.509382\n",
      "train loss:   1.432970\n",
      "train loss:   1.681562\n",
      "train loss:   1.590002\n",
      "train loss:   1.604721\n",
      "train loss:   1.481027\n",
      "train loss:   1.251718\n",
      "train loss:   1.329876\n",
      "train loss:   1.544662\n",
      "train loss:   1.606488\n",
      "train loss:   1.468975\n",
      "train loss:   1.470407\n",
      "train loss:   1.350184\n",
      "train loss:   1.371090\n",
      "train loss:   1.409732\n",
      "train loss:   1.507882\n",
      "train loss:   1.460053\n",
      "train loss:   1.698166\n",
      "train loss:   1.628213\n",
      "train loss:   1.420595\n",
      "train loss:   1.665126\n",
      "train loss:   1.143951\n",
      "train loss:   1.342674\n",
      "train loss:   1.474596\n",
      "train loss:   1.769721\n",
      "train loss:   1.137944\n",
      "train loss:   1.667254\n",
      "train loss:   1.542934\n",
      "train loss:   1.537029\n",
      "train loss:   1.320443\n",
      "train loss:   1.229652\n",
      "train loss:   1.230182\n",
      "train loss:   1.272593\n",
      "train loss:   1.336930\n",
      "train loss:   1.506013\n",
      "train loss:   1.606136\n",
      "train loss:   1.436040\n",
      "train loss:   1.595396\n",
      "train loss:   1.395926\n",
      "train loss:   1.629730\n",
      "train loss:   1.252082\n",
      "train loss:   1.541395\n",
      "train loss:   1.422030\n",
      "train loss:   1.622848\n",
      "########### epoch 38 ###########\n",
      "########### loop 5300 ###########\n",
      "test loss:   1.255540   test accuracy:   0.656250\n",
      "########### loop 5300 ###########\n",
      "train loss:   1.416860\n",
      "train loss:   1.487947\n",
      "train loss:   1.355035\n",
      "train loss:   1.343128\n",
      "train loss:   1.238438\n",
      "train loss:   1.435479\n",
      "train loss:   1.782111\n",
      "train loss:   1.602307\n",
      "train loss:   1.427571\n",
      "train loss:   1.257371\n",
      "train loss:   1.515195\n",
      "train loss:   1.408448\n",
      "train loss:   1.515030\n",
      "train loss:   1.422886\n",
      "train loss:   1.611646\n",
      "train loss:   1.655032\n",
      "train loss:   1.583016\n",
      "train loss:   1.755513\n",
      "train loss:   1.520286\n",
      "train loss:   1.487317\n",
      "train loss:   1.424720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.267258\n",
      "train loss:   1.317208\n",
      "train loss:   1.508049\n",
      "train loss:   1.474211\n",
      "train loss:   1.329367\n",
      "train loss:   1.776750\n",
      "train loss:   1.706407\n",
      "train loss:   1.405706\n",
      "train loss:   1.495168\n",
      "train loss:   1.538468\n",
      "train loss:   1.374752\n",
      "train loss:   1.701064\n",
      "train loss:   1.549347\n",
      "train loss:   1.184296\n",
      "train loss:   1.549375\n",
      "train loss:   1.342556\n",
      "train loss:   1.313079\n",
      "train loss:   1.810148\n",
      "train loss:   1.152537\n",
      "train loss:   1.446061\n",
      "train loss:   1.486713\n",
      "train loss:   1.272714\n",
      "train loss:   1.423076\n",
      "train loss:   1.351955\n",
      "train loss:   1.275732\n",
      "train loss:   1.461764\n",
      "train loss:   1.888480\n",
      "train loss:   1.730987\n",
      "train loss:   1.485729\n",
      "########### epoch 38 ###########\n",
      "########### loop 5350 ###########\n",
      "test loss:   1.254095   test accuracy:   0.750000\n",
      "########### loop 5350 ###########\n",
      "train loss:   1.427398\n",
      "train loss:   1.288189\n",
      "train loss:   1.105256\n",
      "train loss:   1.168312\n",
      "train loss:   1.499806\n",
      "train loss:   1.593249\n",
      "train loss:   1.333127\n",
      "train loss:   1.444937\n",
      "train loss:   1.534753\n",
      "train loss:   1.618839\n",
      "train loss:   1.698762\n",
      "train loss:   1.482220\n",
      "train loss:   1.398694\n",
      "train loss:   1.503728\n",
      "train loss:   1.357126\n",
      "train loss:   1.693344\n",
      "train loss:   1.322772\n",
      "train loss:   1.342525\n",
      "train loss:   1.437873\n",
      "train loss:   1.776085\n",
      "train loss:   1.821844\n",
      "train loss:   1.526744\n",
      "train loss:   1.398694\n",
      "train loss:   1.500263\n",
      "train loss:   1.760846\n",
      "train loss:   1.607880\n",
      "train loss:   1.409055\n",
      "train loss:   1.386962\n",
      "train loss:   1.517702\n",
      "train loss:   1.862978\n",
      "train loss:   1.711596\n",
      "train loss:   1.624159\n",
      "train loss:   1.632239\n",
      "train loss:   1.624599\n",
      "train loss:   1.252305\n",
      "train loss:   1.561821\n",
      "train loss:   1.467755\n",
      "train loss:   1.509305\n",
      "train loss:   1.051202\n",
      "train loss:   1.057088\n",
      "train loss:   1.300269\n",
      "train loss:   1.526361\n",
      "train loss:   1.472213\n",
      "train loss:   1.243678\n",
      "train loss:   1.408010\n",
      "train loss:   1.465250\n",
      "train loss:   1.429745\n",
      "train loss:   1.312085\n",
      "train loss:   1.594543\n",
      "train loss:   1.379736\n",
      "########### epoch 39 ###########\n",
      "########### loop 5400 ###########\n",
      "test loss:   1.243196   test accuracy:   0.687500\n",
      "########### loop 5400 ###########\n",
      "train loss:   1.632227\n",
      "train loss:   1.426836\n",
      "train loss:   1.680963\n",
      "train loss:   1.715389\n",
      "train loss:   1.650659\n",
      "train loss:   1.539562\n",
      "train loss:   1.433920\n",
      "train loss:   1.363065\n",
      "train loss:   1.470462\n",
      "train loss:   1.443382\n",
      "train loss:   1.687140\n",
      "train loss:   1.194503\n",
      "train loss:   1.593117\n",
      "train loss:   1.258328\n",
      "train loss:   1.336912\n",
      "train loss:   1.442534\n",
      "train loss:   1.303874\n",
      "train loss:   1.425101\n",
      "train loss:   1.274796\n",
      "train loss:   1.443051\n",
      "train loss:   1.469760\n",
      "train loss:   1.524129\n",
      "train loss:   1.238380\n",
      "train loss:   1.677611\n",
      "train loss:   1.353615\n",
      "train loss:   1.439102\n",
      "train loss:   1.611955\n",
      "train loss:   1.295881\n",
      "train loss:   1.659419\n",
      "train loss:   1.498081\n",
      "train loss:   1.859146\n",
      "train loss:   1.438829\n",
      "train loss:   1.525344\n",
      "train loss:   1.360505\n",
      "train loss:   1.504869\n",
      "train loss:   1.388717\n",
      "train loss:   1.255044\n",
      "train loss:   1.388210\n",
      "train loss:   1.525657\n",
      "train loss:   1.660315\n",
      "train loss:   1.099358\n",
      "train loss:   1.597511\n",
      "train loss:   1.577427\n",
      "train loss:   1.216047\n",
      "train loss:   1.450953\n",
      "train loss:   1.720583\n",
      "train loss:   1.549414\n",
      "train loss:   1.361014\n",
      "train loss:   1.410159\n",
      "train loss:   1.524460\n",
      "########### epoch 39 ###########\n",
      "########### loop 5450 ###########\n",
      "test loss:   1.014177   test accuracy:   0.843750\n",
      "########### loop 5450 ###########\n",
      "train loss:   1.341060\n",
      "train loss:   1.509743\n",
      "train loss:   1.337929\n",
      "train loss:   1.231102\n",
      "train loss:   1.604873\n",
      "train loss:   1.518546\n",
      "train loss:   1.287795\n",
      "train loss:   1.455095\n",
      "train loss:   1.580368\n",
      "train loss:   1.477847\n",
      "train loss:   1.493333\n",
      "train loss:   1.113698\n",
      "train loss:   1.377324\n",
      "train loss:   1.496150\n",
      "train loss:   1.163703\n",
      "train loss:   1.595821\n",
      "train loss:   1.506570\n",
      "train loss:   1.607945\n",
      "train loss:   1.388673\n",
      "train loss:   1.391320\n",
      "train loss:   1.314859\n",
      "train loss:   1.499606\n",
      "train loss:   1.448122\n",
      "train loss:   1.411391\n",
      "train loss:   1.370549\n",
      "train loss:   1.701490\n",
      "train loss:   1.560361\n",
      "train loss:   1.513560\n",
      "train loss:   1.508602\n",
      "train loss:   1.509640\n",
      "train loss:   1.626908\n",
      "train loss:   1.571681\n",
      "train loss:   1.639264\n",
      "train loss:   1.670920\n",
      "train loss:   1.514362\n",
      "train loss:   1.558851\n",
      "train loss:   1.374451\n",
      "train loss:   1.388092\n",
      "train loss:   1.550161\n",
      "train loss:   1.317071\n",
      "train loss:   1.509411\n",
      "train loss:   1.504323\n",
      "train loss:   1.414399\n",
      "train loss:   1.346699\n",
      "train loss:   1.370100\n",
      "train loss:   1.552748\n",
      "train loss:   1.698146\n",
      "train loss:   1.582133\n",
      "train loss:   1.666145\n",
      "train loss:   1.463120\n",
      "########### epoch 40 ###########\n",
      "########### loop 5500 ###########\n",
      "test loss:   1.364522   test accuracy:   0.656250\n",
      "########### loop 5500 ###########\n",
      "train loss:   1.584215\n",
      "train loss:   1.595632\n",
      "train loss:   1.516118\n",
      "train loss:   1.688812\n",
      "train loss:   1.285253\n",
      "train loss:   1.501273\n",
      "train loss:   1.451355\n",
      "train loss:   1.380173\n",
      "train loss:   1.436504\n",
      "train loss:   1.472882\n",
      "train loss:   1.128449\n",
      "train loss:   1.719877\n",
      "train loss:   1.860071\n",
      "train loss:   1.494893\n",
      "train loss:   1.545819\n",
      "train loss:   1.656452\n",
      "train loss:   1.629537\n",
      "train loss:   1.255710\n",
      "train loss:   1.511380\n",
      "train loss:   1.711518\n",
      "train loss:   1.608629\n",
      "train loss:   1.318742\n",
      "train loss:   1.357525\n",
      "train loss:   1.617289\n",
      "train loss:   1.124443\n",
      "train loss:   1.426016\n",
      "train loss:   1.299622\n",
      "train loss:   1.345157\n",
      "train loss:   1.315116\n",
      "train loss:   1.602751\n",
      "train loss:   1.556427\n",
      "train loss:   1.356743\n",
      "train loss:   1.375529\n",
      "train loss:   1.498536\n",
      "train loss:   1.671494\n",
      "train loss:   1.455318\n",
      "train loss:   1.058845\n",
      "train loss:   1.641443\n",
      "train loss:   1.560891\n",
      "train loss:   1.828996\n",
      "train loss:   1.331319\n",
      "train loss:   1.358629\n",
      "train loss:   1.339066\n",
      "train loss:   1.287658\n",
      "train loss:   1.509151\n",
      "train loss:   1.634374\n",
      "train loss:   1.397171\n",
      "train loss:   1.276530\n",
      "train loss:   1.614761\n",
      "train loss:   1.291375\n",
      "########### epoch 40 ###########\n",
      "########### loop 5550 ###########\n",
      "test loss:   1.225276   test accuracy:   0.687500\n",
      "########### loop 5550 ###########\n",
      "train loss:   1.546037\n",
      "train loss:   1.345309\n",
      "train loss:   1.455437\n",
      "train loss:   1.274560\n",
      "train loss:   1.590002\n",
      "train loss:   1.340809\n",
      "train loss:   1.650072\n",
      "train loss:   1.624384\n",
      "train loss:   1.882249\n",
      "train loss:   1.835084\n",
      "train loss:   1.295843\n",
      "train loss:   1.490365\n",
      "train loss:   1.737125\n",
      "train loss:   1.443996\n",
      "train loss:   1.410426\n",
      "train loss:   1.530222\n",
      "train loss:   1.334010\n",
      "train loss:   1.490663\n",
      "train loss:   1.714127\n",
      "train loss:   1.385088\n",
      "train loss:   1.551113\n",
      "train loss:   1.327629\n",
      "train loss:   1.333333\n",
      "train loss:   1.630870\n",
      "train loss:   1.395850\n",
      "train loss:   1.614579\n",
      "train loss:   1.327795\n",
      "train loss:   1.249217\n",
      "train loss:   1.310650\n",
      "train loss:   1.566965\n",
      "train loss:   1.531995\n",
      "train loss:   1.067568\n",
      "train loss:   1.455517\n",
      "train loss:   1.536360\n",
      "train loss:   1.621294\n",
      "train loss:   1.465801\n",
      "train loss:   1.666958\n",
      "train loss:   1.385038\n",
      "train loss:   1.472637\n",
      "train loss:   1.539274\n",
      "train loss:   1.816662\n",
      "train loss:   1.324674\n",
      "train loss:   1.248362\n",
      "train loss:   1.413645\n",
      "train loss:   1.342638\n",
      "train loss:   1.596951\n",
      "train loss:   1.578606\n",
      "train loss:   1.493789\n",
      "train loss:   1.533082\n",
      "train loss:   1.460285\n",
      "########### epoch 40 ###########\n",
      "########### loop 5600 ###########\n",
      "test loss:   1.163800   test accuracy:   0.843750\n",
      "########### loop 5600 ###########\n",
      "train loss:   1.411430\n",
      "train loss:   1.557880\n",
      "train loss:   1.440746\n",
      "train loss:   1.472433\n",
      "train loss:   1.589020\n",
      "train loss:   1.546823\n",
      "train loss:   1.309070\n",
      "train loss:   1.175869\n",
      "train loss:   1.777757\n",
      "train loss:   1.326056\n",
      "train loss:   1.611404\n",
      "train loss:   1.256283\n",
      "train loss:   1.179565\n",
      "train loss:   1.460636\n",
      "train loss:   1.616327\n",
      "train loss:   1.615068\n",
      "train loss:   1.522424\n",
      "train loss:   1.572152\n",
      "train loss:   1.316714\n",
      "train loss:   1.201830\n",
      "train loss:   1.581326\n",
      "train loss:   1.405337\n",
      "train loss:   1.380733\n",
      "train loss:   1.520238\n",
      "train loss:   1.472229\n",
      "train loss:   1.417134\n",
      "train loss:   1.293416\n",
      "train loss:   1.221829\n",
      "train loss:   1.391641\n",
      "train loss:   1.647073\n",
      "train loss:   1.175343\n",
      "train loss:   1.609165\n",
      "train loss:   1.467146\n",
      "train loss:   1.585071\n",
      "train loss:   1.786763\n",
      "train loss:   1.419693\n",
      "train loss:   1.442698\n",
      "train loss:   1.663677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.610539\n",
      "train loss:   1.506005\n",
      "train loss:   1.674107\n",
      "train loss:   1.452937\n",
      "train loss:   1.540440\n",
      "train loss:   1.467504\n",
      "train loss:   1.701408\n",
      "train loss:   1.601833\n",
      "train loss:   1.439782\n",
      "train loss:   1.596969\n",
      "train loss:   1.399081\n",
      "train loss:   1.348963\n",
      "########### epoch 41 ###########\n",
      "########### loop 5650 ###########\n",
      "test loss:   1.090608   test accuracy:   0.812500\n",
      "########### loop 5650 ###########\n",
      "train loss:   1.461149\n",
      "train loss:   1.402635\n",
      "train loss:   1.271405\n",
      "train loss:   1.530966\n",
      "train loss:   1.645345\n",
      "train loss:   1.561751\n",
      "train loss:   1.601410\n",
      "train loss:   1.580663\n",
      "train loss:   1.563493\n",
      "train loss:   1.661891\n",
      "train loss:   1.638674\n",
      "train loss:   1.833112\n",
      "train loss:   1.651967\n",
      "train loss:   1.540776\n",
      "train loss:   1.391548\n",
      "train loss:   1.304595\n",
      "train loss:   1.661882\n",
      "train loss:   1.910814\n",
      "train loss:   1.540810\n",
      "train loss:   1.329898\n",
      "train loss:   1.734295\n",
      "train loss:   1.486135\n",
      "train loss:   1.329811\n",
      "train loss:   1.702499\n",
      "train loss:   1.634395\n",
      "train loss:   1.378511\n",
      "train loss:   1.399838\n",
      "train loss:   1.616725\n",
      "train loss:   1.550033\n",
      "train loss:   1.369256\n",
      "train loss:   1.364827\n",
      "train loss:   1.789724\n",
      "train loss:   1.651040\n",
      "train loss:   1.535365\n",
      "train loss:   1.104035\n",
      "train loss:   1.351455\n",
      "train loss:   1.179043\n",
      "train loss:   1.295430\n",
      "train loss:   1.674811\n",
      "train loss:   1.442761\n",
      "train loss:   1.224078\n",
      "train loss:   1.549475\n",
      "train loss:   1.727206\n",
      "train loss:   1.460196\n",
      "train loss:   1.627681\n",
      "train loss:   1.292892\n",
      "train loss:   1.655571\n",
      "train loss:   1.341907\n",
      "train loss:   1.460948\n",
      "train loss:   1.511746\n",
      "########### epoch 41 ###########\n",
      "########### loop 5700 ###########\n",
      "test loss:   1.181567   test accuracy:   0.812500\n",
      "########### loop 5700 ###########\n",
      "train loss:   1.264862\n",
      "train loss:   1.596334\n",
      "train loss:   1.399089\n",
      "train loss:   1.503875\n",
      "train loss:   1.410314\n",
      "train loss:   1.430133\n",
      "train loss:   1.099230\n",
      "train loss:   1.451574\n",
      "train loss:   1.219448\n",
      "train loss:   1.652153\n",
      "train loss:   1.675734\n",
      "train loss:   1.891353\n",
      "train loss:   1.388490\n",
      "train loss:   1.419974\n",
      "train loss:   1.478125\n",
      "train loss:   1.273005\n",
      "train loss:   1.576051\n",
      "train loss:   1.430542\n",
      "train loss:   1.504986\n",
      "train loss:   1.242445\n",
      "train loss:   1.688945\n",
      "train loss:   1.721661\n",
      "train loss:   1.475542\n",
      "train loss:   1.392819\n",
      "train loss:   1.561627\n",
      "train loss:   1.685607\n",
      "train loss:   1.457385\n",
      "train loss:   1.454185\n",
      "train loss:   1.461907\n",
      "train loss:   1.249471\n",
      "train loss:   1.606883\n",
      "train loss:   1.655528\n",
      "train loss:   1.445315\n",
      "train loss:   1.341044\n",
      "train loss:   1.367333\n",
      "train loss:   1.482452\n",
      "train loss:   1.548734\n",
      "train loss:   1.687248\n",
      "train loss:   1.136758\n",
      "train loss:   1.560033\n",
      "train loss:   1.630494\n",
      "train loss:   1.303561\n",
      "train loss:   1.409409\n",
      "train loss:   1.395938\n",
      "train loss:   1.187343\n",
      "train loss:   1.403135\n",
      "train loss:   1.697550\n",
      "train loss:   1.182631\n",
      "train loss:   1.426486\n",
      "train loss:   1.565768\n",
      "########### epoch 41 ###########\n",
      "########### loop 5750 ###########\n",
      "test loss:   1.148958   test accuracy:   0.781250\n",
      "########### loop 5750 ###########\n",
      "train loss:   1.796722\n",
      "train loss:   1.206773\n",
      "train loss:   1.475447\n",
      "train loss:   1.433862\n",
      "train loss:   1.449255\n",
      "train loss:   1.331094\n",
      "train loss:   1.610071\n",
      "train loss:   1.511543\n",
      "train loss:   1.238920\n",
      "train loss:   1.625712\n",
      "train loss:   1.561838\n",
      "train loss:   1.592983\n",
      "train loss:   1.674742\n",
      "train loss:   1.436023\n",
      "train loss:   1.357396\n",
      "train loss:   1.101311\n",
      "train loss:   1.439346\n",
      "train loss:   1.391847\n",
      "train loss:   1.723636\n",
      "train loss:   1.575471\n",
      "train loss:   1.395382\n",
      "train loss:   1.761392\n",
      "train loss:   1.473189\n",
      "train loss:   1.551528\n",
      "train loss:   1.313625\n",
      "train loss:   1.415092\n",
      "train loss:   1.447595\n",
      "train loss:   1.863777\n",
      "train loss:   1.193138\n",
      "train loss:   1.847404\n",
      "train loss:   1.471566\n",
      "train loss:   1.655771\n",
      "train loss:   1.536821\n",
      "train loss:   1.580199\n",
      "train loss:   1.710212\n",
      "train loss:   1.615985\n",
      "train loss:   1.632001\n",
      "train loss:   1.624939\n",
      "train loss:   1.294539\n",
      "train loss:   1.356493\n",
      "train loss:   1.558614\n",
      "train loss:   1.358032\n",
      "train loss:   1.632498\n",
      "train loss:   1.211316\n",
      "train loss:   1.658270\n",
      "train loss:   1.450036\n",
      "train loss:   1.572179\n",
      "train loss:   1.609615\n",
      "train loss:   1.549556\n",
      "train loss:   1.486927\n",
      "########### epoch 42 ###########\n",
      "########### loop 5800 ###########\n",
      "test loss:   1.012487   test accuracy:   0.812500\n",
      "########### loop 5800 ###########\n",
      "train loss:   1.306866\n",
      "train loss:   1.377630\n",
      "train loss:   1.554525\n",
      "train loss:   1.334472\n",
      "train loss:   1.842565\n",
      "train loss:   1.519533\n",
      "train loss:   1.411386\n",
      "train loss:   1.363906\n",
      "train loss:   1.699992\n",
      "train loss:   1.663399\n",
      "train loss:   1.478030\n",
      "train loss:   1.574624\n",
      "train loss:   1.440516\n",
      "train loss:   1.618802\n",
      "train loss:   1.401854\n",
      "train loss:   1.540933\n",
      "train loss:   1.481333\n",
      "train loss:   1.144523\n",
      "train loss:   1.490347\n",
      "train loss:   1.144338\n",
      "train loss:   1.629350\n",
      "train loss:   1.288201\n",
      "train loss:   1.484678\n",
      "train loss:   1.544080\n",
      "train loss:   1.495446\n",
      "train loss:   1.449720\n",
      "train loss:   1.732894\n",
      "train loss:   1.490004\n",
      "train loss:   1.472886\n",
      "train loss:   1.642280\n",
      "train loss:   1.298464\n",
      "train loss:   1.597167\n",
      "train loss:   1.379671\n",
      "train loss:   1.747585\n",
      "train loss:   1.417655\n",
      "train loss:   1.941493\n",
      "train loss:   1.540817\n",
      "train loss:   1.347113\n",
      "train loss:   1.715811\n",
      "train loss:   1.236540\n",
      "train loss:   1.537870\n",
      "train loss:   1.384845\n",
      "train loss:   1.683043\n",
      "train loss:   1.643503\n",
      "train loss:   1.574284\n",
      "train loss:   1.422593\n",
      "train loss:   1.498896\n",
      "train loss:   1.722799\n",
      "train loss:   1.681399\n",
      "train loss:   1.501038\n",
      "########### epoch 42 ###########\n",
      "########### loop 5850 ###########\n",
      "test loss:   1.132803   test accuracy:   0.781250\n",
      "########### loop 5850 ###########\n",
      "train loss:   1.565330\n",
      "train loss:   1.762889\n",
      "train loss:   1.549077\n",
      "train loss:   1.608146\n",
      "train loss:   1.483180\n",
      "train loss:   1.377279\n",
      "train loss:   1.457094\n",
      "train loss:   1.510355\n",
      "train loss:   1.474966\n",
      "train loss:   1.401415\n",
      "train loss:   1.328825\n",
      "train loss:   1.328944\n",
      "train loss:   1.755164\n",
      "train loss:   1.402872\n",
      "train loss:   1.322762\n",
      "train loss:   1.383558\n",
      "train loss:   1.353363\n",
      "train loss:   1.594959\n",
      "train loss:   1.382597\n",
      "train loss:   1.442138\n",
      "train loss:   1.620078\n",
      "train loss:   1.355827\n",
      "train loss:   1.375817\n",
      "train loss:   1.331989\n",
      "train loss:   1.522525\n",
      "train loss:   1.466698\n",
      "train loss:   1.605730\n",
      "train loss:   1.289943\n",
      "train loss:   1.748017\n",
      "train loss:   1.524913\n",
      "train loss:   1.464651\n",
      "train loss:   1.567690\n",
      "train loss:   1.677445\n",
      "train loss:   1.497627\n",
      "train loss:   1.726508\n",
      "train loss:   1.444245\n",
      "train loss:   1.202653\n",
      "train loss:   1.701757\n",
      "train loss:   1.340012\n",
      "train loss:   1.379480\n",
      "train loss:   1.236634\n",
      "train loss:   1.429158\n",
      "train loss:   1.434809\n",
      "train loss:   1.431021\n",
      "train loss:   1.243979\n",
      "train loss:   1.431025\n",
      "train loss:   1.410091\n",
      "train loss:   1.431251\n",
      "train loss:   1.335929\n",
      "train loss:   1.775454\n",
      "########### epoch 42 ###########\n",
      "########### loop 5900 ###########\n",
      "test loss:   1.206729   test accuracy:   0.750000\n",
      "########### loop 5900 ###########\n",
      "train loss:   1.623919\n",
      "train loss:   1.346632\n",
      "train loss:   1.309678\n",
      "train loss:   1.753280\n",
      "train loss:   1.249934\n",
      "train loss:   1.297765\n",
      "train loss:   1.403724\n",
      "train loss:   1.517013\n",
      "train loss:   1.494577\n",
      "train loss:   1.420864\n",
      "train loss:   1.426084\n",
      "train loss:   1.727927\n",
      "train loss:   1.720725\n",
      "train loss:   1.067700\n",
      "train loss:   1.404647\n",
      "train loss:   1.275312\n",
      "train loss:   1.709571\n",
      "train loss:   1.418064\n",
      "train loss:   1.333547\n",
      "train loss:   1.293062\n",
      "train loss:   1.443668\n",
      "train loss:   1.450453\n",
      "train loss:   1.375950\n",
      "train loss:   1.711339\n",
      "train loss:   1.312740\n",
      "train loss:   1.728772\n",
      "train loss:   1.296443\n",
      "train loss:   1.748802\n",
      "train loss:   1.299321\n",
      "train loss:   1.583706\n",
      "train loss:   1.513358\n",
      "train loss:   1.676185\n",
      "train loss:   1.476306\n",
      "train loss:   1.285050\n",
      "train loss:   1.584636\n",
      "train loss:   1.577171\n",
      "train loss:   1.460454\n",
      "train loss:   1.664054\n",
      "train loss:   1.668044\n",
      "train loss:   1.222698\n",
      "train loss:   1.547431\n",
      "train loss:   1.448000\n",
      "train loss:   1.557363\n",
      "train loss:   1.439197\n",
      "train loss:   1.359004\n",
      "train loss:   1.504977\n",
      "train loss:   1.467552\n",
      "train loss:   1.665097\n",
      "train loss:   1.544209\n",
      "train loss:   1.628927\n",
      "########### epoch 43 ###########\n",
      "########### loop 5950 ###########\n",
      "test loss:   1.181580   test accuracy:   0.750000\n",
      "########### loop 5950 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.364202\n",
      "train loss:   1.477433\n",
      "train loss:   1.594065\n",
      "train loss:   1.543890\n",
      "train loss:   1.566730\n",
      "train loss:   1.333349\n",
      "train loss:   1.366247\n",
      "train loss:   1.625342\n",
      "train loss:   1.302697\n",
      "train loss:   1.580124\n",
      "train loss:   1.529928\n",
      "train loss:   1.724994\n",
      "train loss:   1.444337\n",
      "train loss:   1.508520\n",
      "train loss:   1.663736\n",
      "train loss:   1.870498\n",
      "train loss:   1.309711\n",
      "train loss:   1.495309\n",
      "train loss:   1.690284\n",
      "train loss:   1.522724\n",
      "train loss:   1.419843\n",
      "train loss:   1.523471\n",
      "train loss:   1.343464\n",
      "train loss:   1.568757\n",
      "train loss:   1.353173\n",
      "train loss:   1.832630\n",
      "train loss:   1.402715\n",
      "train loss:   1.187060\n",
      "train loss:   1.546044\n",
      "train loss:   1.499399\n",
      "train loss:   1.553142\n",
      "train loss:   1.363056\n",
      "train loss:   1.468595\n",
      "train loss:   1.562726\n",
      "train loss:   1.416784\n",
      "train loss:   1.515698\n",
      "train loss:   1.511946\n",
      "train loss:   1.392161\n",
      "train loss:   1.488318\n",
      "train loss:   1.325569\n",
      "train loss:   1.098652\n",
      "train loss:   1.590992\n",
      "train loss:   1.468733\n",
      "train loss:   1.155832\n",
      "train loss:   1.103335\n",
      "train loss:   1.565366\n",
      "train loss:   1.575333\n",
      "train loss:   1.228023\n",
      "train loss:   1.647206\n",
      "train loss:   1.437181\n",
      "########### epoch 43 ###########\n",
      "########### loop 6000 ###########\n",
      "test loss:   1.136282   test accuracy:   0.750000\n",
      "########### loop 6000 ###########\n",
      "train loss:   1.723563\n",
      "train loss:   1.366517\n",
      "train loss:   1.316467\n",
      "train loss:   1.690241\n",
      "train loss:   1.528809\n",
      "train loss:   1.566155\n",
      "train loss:   1.154119\n",
      "train loss:   1.330793\n",
      "train loss:   1.339791\n",
      "train loss:   1.468175\n",
      "train loss:   1.644821\n",
      "train loss:   1.554089\n",
      "train loss:   1.347656\n",
      "train loss:   1.310554\n",
      "train loss:   1.480735\n",
      "train loss:   1.352828\n",
      "train loss:   1.392803\n",
      "train loss:   1.291499\n",
      "train loss:   1.472991\n",
      "train loss:   1.624676\n",
      "train loss:   1.694185\n",
      "train loss:   1.611850\n",
      "train loss:   1.180126\n",
      "train loss:   1.356543\n",
      "train loss:   1.460402\n",
      "train loss:   1.433727\n",
      "train loss:   1.559862\n",
      "train loss:   1.778774\n",
      "train loss:   1.588348\n",
      "train loss:   1.294538\n",
      "train loss:   1.690352\n",
      "train loss:   1.305529\n",
      "train loss:   1.543454\n",
      "train loss:   1.386383\n",
      "train loss:   1.350368\n",
      "train loss:   1.598547\n",
      "train loss:   1.535495\n",
      "train loss:   1.468406\n",
      "train loss:   1.653431\n",
      "train loss:   1.321290\n",
      "train loss:   1.064429\n",
      "train loss:   1.537444\n",
      "train loss:   1.606964\n",
      "train loss:   1.512836\n",
      "train loss:   1.426198\n",
      "train loss:   1.366431\n",
      "train loss:   1.269199\n",
      "train loss:   1.514663\n",
      "train loss:   1.487089\n",
      "train loss:   1.616090\n",
      "########### epoch 43 ###########\n",
      "########### loop 6050 ###########\n",
      "test loss:   1.184285   test accuracy:   0.750000\n",
      "########### loop 6050 ###########\n",
      "train loss:   1.520092\n",
      "train loss:   1.444247\n",
      "train loss:   1.720337\n",
      "train loss:   1.470188\n",
      "train loss:   1.525033\n",
      "train loss:   1.410920\n",
      "train loss:   1.316430\n",
      "train loss:   1.654451\n",
      "train loss:   1.396253\n",
      "train loss:   1.441116\n",
      "train loss:   1.430449\n",
      "train loss:   1.689113\n",
      "train loss:   1.482573\n",
      "train loss:   1.407588\n",
      "train loss:   1.576335\n",
      "train loss:   1.549587\n",
      "train loss:   1.516453\n",
      "train loss:   1.689833\n",
      "train loss:   1.218840\n",
      "train loss:   1.619766\n",
      "train loss:   1.357687\n",
      "train loss:   1.606810\n",
      "train loss:   1.507649\n",
      "train loss:   1.481122\n",
      "train loss:   1.459820\n",
      "train loss:   1.208696\n",
      "train loss:   1.455558\n",
      "train loss:   1.455193\n",
      "train loss:   1.412970\n",
      "train loss:   1.518806\n",
      "train loss:   1.599213\n",
      "train loss:   1.304312\n",
      "train loss:   1.486642\n",
      "train loss:   1.405141\n",
      "train loss:   1.571485\n",
      "train loss:   1.631048\n",
      "train loss:   1.325431\n",
      "train loss:   1.330369\n",
      "train loss:   1.530085\n",
      "train loss:   1.370467\n",
      "train loss:   1.522274\n",
      "train loss:   1.720175\n",
      "train loss:   1.440857\n",
      "train loss:   1.442751\n",
      "train loss:   1.804085\n",
      "train loss:   1.617564\n",
      "train loss:   1.562819\n",
      "train loss:   1.081315\n",
      "train loss:   1.631348\n",
      "train loss:   1.433484\n",
      "########### epoch 44 ###########\n",
      "########### loop 6100 ###########\n",
      "test loss:   1.257091   test accuracy:   0.687500\n",
      "########### loop 6100 ###########\n",
      "train loss:   1.484656\n",
      "train loss:   1.477116\n",
      "train loss:   1.620353\n",
      "train loss:   1.553079\n",
      "train loss:   1.471634\n",
      "train loss:   1.614034\n",
      "train loss:   1.428882\n",
      "train loss:   1.389991\n",
      "train loss:   1.614670\n",
      "train loss:   1.104273\n",
      "train loss:   1.517468\n",
      "train loss:   1.217922\n",
      "train loss:   1.650276\n",
      "train loss:   1.567672\n",
      "train loss:   1.305207\n",
      "train loss:   1.226560\n",
      "train loss:   1.288101\n",
      "train loss:   1.529058\n",
      "train loss:   1.517187\n",
      "train loss:   1.424573\n",
      "train loss:   1.530081\n",
      "train loss:   1.485383\n",
      "train loss:   1.215843\n",
      "train loss:   1.599508\n",
      "train loss:   1.538240\n",
      "train loss:   1.398851\n",
      "train loss:   1.431788\n",
      "train loss:   1.605144\n",
      "train loss:   1.587736\n",
      "train loss:   1.379105\n",
      "train loss:   1.472097\n",
      "train loss:   1.271327\n",
      "train loss:   1.591322\n",
      "train loss:   1.723357\n",
      "train loss:   1.422975\n",
      "train loss:   1.682152\n",
      "train loss:   1.202537\n",
      "train loss:   0.954717\n",
      "train loss:   1.451027\n",
      "train loss:   1.427573\n",
      "train loss:   1.329677\n",
      "train loss:   1.250844\n",
      "train loss:   1.524076\n",
      "train loss:   1.337905\n",
      "train loss:   1.597743\n",
      "train loss:   1.305059\n",
      "train loss:   1.499897\n",
      "train loss:   1.609642\n",
      "train loss:   1.596521\n",
      "train loss:   1.527368\n",
      "########### epoch 44 ###########\n",
      "########### loop 6150 ###########\n",
      "test loss:   1.235024   test accuracy:   0.687500\n",
      "########### loop 6150 ###########\n",
      "train loss:   1.599972\n",
      "train loss:   1.418597\n",
      "train loss:   0.956127\n",
      "train loss:   1.251459\n",
      "train loss:   1.460726\n",
      "train loss:   1.394991\n",
      "train loss:   1.491469\n",
      "train loss:   1.457093\n",
      "train loss:   1.178174\n",
      "train loss:   1.504481\n",
      "train loss:   1.334002\n",
      "train loss:   1.627620\n",
      "train loss:   1.184909\n",
      "train loss:   1.263951\n",
      "train loss:   1.179664\n",
      "train loss:   1.333789\n",
      "train loss:   1.482232\n",
      "train loss:   1.454891\n",
      "train loss:   1.494179\n",
      "train loss:   1.278421\n",
      "train loss:   1.280830\n",
      "train loss:   1.610482\n",
      "train loss:   1.493012\n",
      "train loss:   1.302256\n",
      "train loss:   1.596458\n",
      "train loss:   1.408411\n",
      "train loss:   1.551429\n",
      "train loss:   1.478653\n",
      "train loss:   1.624111\n",
      "train loss:   1.455615\n",
      "train loss:   1.339698\n",
      "train loss:   1.236200\n",
      "train loss:   1.695969\n",
      "train loss:   1.537224\n",
      "train loss:   1.418306\n",
      "train loss:   1.626240\n",
      "train loss:   1.531144\n",
      "train loss:   1.794356\n",
      "train loss:   1.270436\n",
      "train loss:   1.734206\n",
      "train loss:   1.645712\n",
      "train loss:   1.402135\n",
      "train loss:   1.567644\n",
      "train loss:   1.339142\n",
      "train loss:   1.577577\n",
      "train loss:   1.343318\n",
      "train loss:   1.542674\n",
      "train loss:   1.639754\n",
      "train loss:   1.427989\n",
      "train loss:   1.440263\n",
      "########### epoch 44 ###########\n",
      "########### loop 6200 ###########\n",
      "test loss:   0.929660   test accuracy:   0.906250\n",
      "########### loop 6200 ###########\n",
      "train loss:   1.552961\n",
      "train loss:   1.376470\n",
      "train loss:   1.621400\n",
      "train loss:   1.370959\n",
      "train loss:   1.640573\n",
      "train loss:   1.471118\n",
      "train loss:   1.261480\n",
      "train loss:   1.651635\n",
      "train loss:   1.210522\n",
      "train loss:   1.376399\n",
      "train loss:   1.575491\n",
      "train loss:   1.553348\n",
      "train loss:   1.439097\n",
      "train loss:   1.304757\n",
      "train loss:   1.512623\n",
      "train loss:   1.378524\n",
      "train loss:   1.413353\n",
      "train loss:   1.256776\n",
      "train loss:   1.618280\n",
      "train loss:   1.409129\n",
      "train loss:   1.976276\n",
      "train loss:   1.253578\n",
      "train loss:   1.405372\n",
      "train loss:   1.426683\n",
      "train loss:   1.626141\n",
      "train loss:   1.376203\n",
      "train loss:   1.524958\n",
      "train loss:   1.531286\n",
      "train loss:   1.398611\n",
      "train loss:   1.716810\n",
      "train loss:   1.765341\n",
      "train loss:   1.336146\n",
      "train loss:   1.170936\n",
      "train loss:   1.297978\n",
      "train loss:   1.474994\n",
      "train loss:   1.394400\n",
      "train loss:   1.753019\n",
      "train loss:   1.448004\n",
      "train loss:   1.508372\n",
      "train loss:   1.657460\n",
      "train loss:   1.805452\n",
      "train loss:   1.781496\n",
      "train loss:   1.490616\n",
      "train loss:   1.389489\n",
      "train loss:   1.494929\n",
      "train loss:   1.598962\n",
      "train loss:   1.451438\n",
      "train loss:   1.540610\n",
      "train loss:   1.570165\n",
      "train loss:   1.325844\n",
      "########### epoch 45 ###########\n",
      "########### loop 6250 ###########\n",
      "test loss:   1.069750   test accuracy:   0.843750\n",
      "########### loop 6250 ###########\n",
      "train loss:   1.305606\n",
      "train loss:   1.405128\n",
      "train loss:   1.683716\n",
      "train loss:   1.151392\n",
      "train loss:   1.548406\n",
      "train loss:   1.590641\n",
      "train loss:   1.532299\n",
      "train loss:   1.261661\n",
      "train loss:   1.405265\n",
      "train loss:   1.519617\n",
      "train loss:   1.406759\n",
      "train loss:   1.793825\n",
      "train loss:   1.447917\n",
      "train loss:   1.358010\n",
      "train loss:   1.257377\n",
      "train loss:   1.279339\n",
      "train loss:   1.479377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.461969\n",
      "train loss:   1.401904\n",
      "train loss:   1.327950\n",
      "train loss:   1.405113\n",
      "train loss:   1.375396\n",
      "train loss:   1.488687\n",
      "train loss:   1.315737\n",
      "train loss:   1.685784\n",
      "train loss:   1.683614\n",
      "train loss:   1.350252\n",
      "train loss:   1.714650\n",
      "train loss:   1.418190\n",
      "train loss:   1.424150\n",
      "train loss:   1.563927\n",
      "train loss:   1.333528\n",
      "train loss:   1.532577\n",
      "train loss:   1.537470\n",
      "train loss:   1.555873\n",
      "train loss:   1.528166\n",
      "train loss:   1.560899\n",
      "train loss:   1.211607\n",
      "train loss:   1.578369\n",
      "train loss:   1.445204\n",
      "train loss:   1.172449\n",
      "train loss:   1.398249\n",
      "train loss:   1.338835\n",
      "train loss:   1.223405\n",
      "train loss:   1.712476\n",
      "train loss:   1.359718\n",
      "train loss:   1.638365\n",
      "train loss:   1.307589\n",
      "train loss:   1.088282\n",
      "train loss:   1.360795\n",
      "########### epoch 45 ###########\n",
      "########### loop 6300 ###########\n",
      "test loss:   1.045294   test accuracy:   0.812500\n",
      "########### loop 6300 ###########\n",
      "train loss:   1.523690\n",
      "train loss:   1.388853\n",
      "train loss:   1.537597\n",
      "train loss:   1.478057\n",
      "train loss:   1.450768\n",
      "train loss:   1.485623\n",
      "train loss:   1.521207\n",
      "train loss:   1.544233\n",
      "train loss:   1.559419\n",
      "train loss:   1.135471\n",
      "train loss:   1.719303\n",
      "train loss:   1.777921\n",
      "train loss:   1.401108\n",
      "train loss:   1.527319\n",
      "train loss:   1.516396\n",
      "train loss:   1.383556\n",
      "train loss:   1.400028\n",
      "train loss:   1.435977\n",
      "train loss:   1.531271\n",
      "train loss:   1.341966\n",
      "train loss:   1.334233\n",
      "train loss:   1.478040\n",
      "train loss:   1.261781\n",
      "train loss:   1.188668\n",
      "train loss:   1.464227\n",
      "train loss:   1.591816\n",
      "train loss:   1.741838\n",
      "train loss:   1.092933\n",
      "train loss:   1.533233\n",
      "train loss:   1.302145\n",
      "train loss:   1.371391\n",
      "train loss:   1.240291\n",
      "train loss:   1.430990\n",
      "train loss:   1.329520\n",
      "train loss:   1.675951\n",
      "train loss:   1.646756\n",
      "train loss:   1.272653\n",
      "train loss:   1.625237\n",
      "train loss:   1.403700\n",
      "train loss:   1.443721\n",
      "train loss:   1.285582\n",
      "train loss:   1.341707\n",
      "train loss:   1.342140\n",
      "train loss:   1.461530\n",
      "train loss:   1.685191\n",
      "train loss:   1.405125\n",
      "train loss:   1.555655\n",
      "train loss:   1.522896\n",
      "train loss:   1.636737\n",
      "train loss:   1.533587\n",
      "########### epoch 46 ###########\n",
      "########### loop 6350 ###########\n",
      "test loss:   1.010741   test accuracy:   0.812500\n",
      "########### loop 6350 ###########\n",
      "train loss:   1.571174\n",
      "train loss:   1.414899\n",
      "train loss:   1.837290\n",
      "train loss:   1.412964\n",
      "train loss:   1.550337\n",
      "train loss:   1.513726\n",
      "train loss:   1.585743\n",
      "train loss:   1.590660\n",
      "train loss:   1.461886\n",
      "train loss:   1.166944\n",
      "train loss:   1.758743\n",
      "train loss:   1.152182\n",
      "train loss:   1.637870\n",
      "train loss:   1.414624\n",
      "train loss:   1.511963\n",
      "train loss:   1.593809\n",
      "train loss:   1.358707\n",
      "train loss:   1.456534\n",
      "train loss:   1.475376\n",
      "train loss:   1.411442\n",
      "train loss:   1.349206\n",
      "train loss:   1.658435\n",
      "train loss:   1.488541\n",
      "train loss:   1.398407\n",
      "train loss:   1.580965\n",
      "train loss:   1.477004\n",
      "train loss:   1.365877\n",
      "train loss:   1.492166\n",
      "train loss:   1.464307\n",
      "train loss:   1.341944\n",
      "train loss:   1.596972\n",
      "train loss:   1.496157\n",
      "train loss:   1.579276\n",
      "train loss:   1.409114\n",
      "train loss:   1.521142\n",
      "train loss:   1.593043\n",
      "train loss:   1.376162\n",
      "train loss:   1.689454\n",
      "train loss:   1.520640\n",
      "train loss:   1.641323\n",
      "train loss:   1.469275\n",
      "train loss:   1.199804\n",
      "train loss:   1.473357\n",
      "train loss:   1.486918\n",
      "train loss:   1.495316\n",
      "train loss:   1.585919\n",
      "train loss:   1.215683\n",
      "train loss:   1.586995\n",
      "train loss:   1.412043\n",
      "train loss:   1.243587\n",
      "########### epoch 46 ###########\n",
      "########### loop 6400 ###########\n",
      "test loss:   1.084129   test accuracy:   0.718750\n",
      "########### loop 6400 ###########\n",
      "train loss:   1.430847\n",
      "train loss:   1.562551\n",
      "train loss:   1.729316\n",
      "train loss:   1.654557\n",
      "train loss:   1.612636\n",
      "train loss:   1.391987\n",
      "train loss:   1.348131\n",
      "train loss:   1.490460\n",
      "train loss:   1.602307\n",
      "train loss:   1.511579\n",
      "train loss:   1.602563\n",
      "train loss:   1.501687\n",
      "train loss:   1.500195\n",
      "train loss:   1.616783\n",
      "train loss:   1.649489\n",
      "train loss:   1.258216\n",
      "train loss:   1.403145\n",
      "train loss:   1.798555\n",
      "train loss:   1.523629\n",
      "train loss:   1.543910\n",
      "train loss:   1.635331\n",
      "train loss:   1.582466\n",
      "train loss:   1.514149\n",
      "train loss:   1.412262\n",
      "train loss:   1.675190\n",
      "train loss:   1.441193\n",
      "train loss:   1.406082\n",
      "train loss:   1.317211\n",
      "train loss:   1.069257\n",
      "train loss:   1.416769\n",
      "train loss:   1.477860\n",
      "train loss:   1.497827\n",
      "train loss:   1.538159\n",
      "train loss:   1.443071\n",
      "train loss:   1.510030\n",
      "train loss:   1.256317\n",
      "train loss:   1.633445\n",
      "train loss:   1.267732\n",
      "train loss:   1.286376\n",
      "train loss:   1.575849\n",
      "train loss:   1.673070\n",
      "train loss:   1.570177\n",
      "train loss:   1.481262\n",
      "train loss:   1.415080\n",
      "train loss:   1.457716\n",
      "train loss:   1.384979\n",
      "train loss:   1.414165\n",
      "train loss:   1.549433\n",
      "train loss:   1.374063\n",
      "train loss:   1.533050\n",
      "########### epoch 46 ###########\n",
      "########### loop 6450 ###########\n",
      "test loss:   0.985512   test accuracy:   0.781250\n",
      "########### loop 6450 ###########\n",
      "train loss:   1.659071\n",
      "train loss:   1.335440\n",
      "train loss:   1.667410\n",
      "train loss:   1.711207\n",
      "train loss:   1.428566\n",
      "train loss:   1.405120\n",
      "train loss:   1.642688\n",
      "train loss:   1.431717\n",
      "train loss:   1.497640\n",
      "train loss:   1.180736\n",
      "train loss:   1.316044\n",
      "train loss:   1.475710\n",
      "train loss:   1.151868\n",
      "train loss:   1.389816\n",
      "train loss:   1.167607\n",
      "train loss:   1.502895\n",
      "train loss:   1.494974\n",
      "train loss:   1.348563\n",
      "train loss:   1.348521\n",
      "train loss:   1.251082\n",
      "train loss:   1.191554\n",
      "train loss:   1.396861\n",
      "train loss:   1.365699\n",
      "train loss:   1.174517\n",
      "train loss:   1.115269\n",
      "train loss:   1.269827\n",
      "train loss:   1.547974\n",
      "train loss:   1.336144\n",
      "train loss:   1.328736\n",
      "train loss:   1.428350\n",
      "train loss:   1.538450\n",
      "train loss:   1.459320\n",
      "train loss:   1.478963\n",
      "train loss:   1.381324\n",
      "train loss:   1.367965\n",
      "train loss:   1.566437\n",
      "train loss:   1.401703\n",
      "train loss:   1.639780\n",
      "train loss:   1.759366\n",
      "train loss:   1.206680\n",
      "train loss:   1.497665\n",
      "train loss:   1.247237\n",
      "train loss:   1.615642\n",
      "train loss:   1.220181\n",
      "train loss:   1.395970\n",
      "train loss:   1.693103\n",
      "train loss:   1.388816\n",
      "train loss:   1.829774\n",
      "train loss:   1.949958\n",
      "train loss:   1.320712\n",
      "########### epoch 47 ###########\n",
      "########### loop 6500 ###########\n",
      "test loss:   1.136223   test accuracy:   0.781250\n",
      "########### loop 6500 ###########\n",
      "train loss:   1.183694\n",
      "train loss:   1.264154\n",
      "train loss:   1.482329\n",
      "train loss:   1.545094\n",
      "train loss:   1.347640\n",
      "train loss:   1.409969\n",
      "train loss:   1.296801\n",
      "train loss:   1.543873\n",
      "train loss:   1.821145\n",
      "train loss:   1.687507\n",
      "train loss:   1.476312\n",
      "train loss:   1.472141\n",
      "train loss:   1.650329\n",
      "train loss:   1.333898\n",
      "train loss:   1.513866\n",
      "train loss:   1.215173\n",
      "train loss:   1.671065\n",
      "train loss:   1.446325\n",
      "train loss:   1.432151\n",
      "train loss:   1.333538\n",
      "train loss:   1.680438\n",
      "train loss:   1.486397\n",
      "train loss:   1.618223\n",
      "train loss:   1.519106\n",
      "train loss:   1.351491\n",
      "train loss:   1.589461\n",
      "train loss:   1.330492\n",
      "train loss:   1.717846\n",
      "train loss:   1.508910\n",
      "train loss:   1.420671\n",
      "train loss:   1.318078\n",
      "train loss:   1.496887\n",
      "train loss:   1.160127\n",
      "train loss:   1.456228\n",
      "train loss:   1.609516\n",
      "train loss:   1.509406\n",
      "train loss:   1.473283\n",
      "train loss:   1.511376\n",
      "train loss:   1.458198\n",
      "train loss:   1.452526\n",
      "train loss:   1.351273\n",
      "train loss:   1.420491\n",
      "train loss:   1.462703\n",
      "train loss:   1.543208\n",
      "train loss:   1.365149\n",
      "train loss:   1.521392\n",
      "train loss:   1.341419\n",
      "train loss:   1.601899\n",
      "train loss:   1.488596\n",
      "train loss:   1.520793\n",
      "########### epoch 47 ###########\n",
      "########### loop 6550 ###########\n",
      "test loss:   1.165356   test accuracy:   0.656250\n",
      "########### loop 6550 ###########\n",
      "train loss:   1.569856\n",
      "train loss:   1.555629\n",
      "train loss:   1.499826\n",
      "train loss:   1.422707\n",
      "train loss:   1.547422\n",
      "train loss:   1.420392\n",
      "train loss:   1.576259\n",
      "train loss:   1.546067\n",
      "train loss:   1.240284\n",
      "train loss:   1.572186\n",
      "train loss:   1.612981\n",
      "train loss:   1.394572\n",
      "train loss:   1.834783\n",
      "train loss:   1.531926\n",
      "train loss:   1.468376\n",
      "train loss:   1.458223\n",
      "train loss:   1.386106\n",
      "train loss:   1.542383\n",
      "train loss:   1.527702\n",
      "train loss:   1.595885\n",
      "train loss:   1.107349\n",
      "train loss:   1.459216\n",
      "train loss:   1.270194\n",
      "train loss:   1.592568\n",
      "train loss:   1.530775\n",
      "train loss:   1.626651\n",
      "train loss:   1.416332\n",
      "train loss:   1.858746\n",
      "train loss:   1.549161\n",
      "train loss:   1.424941\n",
      "train loss:   1.591189\n",
      "train loss:   1.577436\n",
      "train loss:   1.704775\n",
      "train loss:   1.497400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:   1.622998\n",
      "train loss:   1.533355\n",
      "train loss:   1.263442\n",
      "train loss:   1.528119\n",
      "train loss:   1.589951\n",
      "train loss:   1.371045\n",
      "train loss:   1.184609\n",
      "train loss:   1.686881\n",
      "train loss:   1.338908\n",
      "train loss:   1.413891\n",
      "train loss:   1.603187\n",
      "train loss:   1.470304\n",
      "train loss:   1.233437\n",
      "train loss:   1.280746\n",
      "train loss:   1.186832\n",
      "train loss:   1.213359\n",
      "########### epoch 47 ###########\n",
      "########### loop 6600 ###########\n",
      "test loss:   0.998325   test accuracy:   0.750000\n",
      "########### loop 6600 ###########\n",
      "train loss:   1.574151\n",
      "train loss:   1.502000\n",
      "train loss:   1.508882\n",
      "train loss:   1.306795\n",
      "train loss:   1.544613\n",
      "train loss:   1.286957\n",
      "train loss:   1.237090\n",
      "train loss:   1.426738\n",
      "train loss:   1.703570\n",
      "train loss:   1.569356\n",
      "train loss:   1.261774\n",
      "train loss:   1.222030\n",
      "train loss:   1.694701\n",
      "train loss:   1.251956\n",
      "train loss:   1.666167\n",
      "train loss:   1.245322\n",
      "train loss:   1.610581\n",
      "train loss:   1.379456\n",
      "train loss:   1.414232\n",
      "train loss:   1.539229\n",
      "train loss:   1.464033\n",
      "train loss:   1.258456\n",
      "train loss:   1.367054\n",
      "train loss:   1.735737\n",
      "train loss:   1.137092\n",
      "train loss:   1.615517\n",
      "train loss:   1.365569\n",
      "train loss:   1.147600\n",
      "train loss:   1.522946\n",
      "train loss:   1.379548\n",
      "train loss:   1.471133\n",
      "train loss:   1.668886\n",
      "train loss:   1.248139\n",
      "train loss:   1.416629\n",
      "train loss:   1.286782\n",
      "train loss:   1.548045\n",
      "train loss:   1.609086\n",
      "train loss:   1.434961\n",
      "train loss:   1.459058\n",
      "train loss:   1.561035\n",
      "train loss:   1.743268\n",
      "train loss:   1.553800\n",
      "train loss:   1.771747\n",
      "train loss:   1.628649\n",
      "train loss:   1.452486\n",
      "train loss:   1.284202\n",
      "train loss:   1.476881\n",
      "train loss:   1.198209\n",
      "train loss:   1.318711\n",
      "train loss:   1.472621\n",
      "########### epoch 48 ###########\n",
      "########### loop 6650 ###########\n",
      "test loss:   1.052716   test accuracy:   0.718750\n",
      "########### loop 6650 ###########\n",
      "train loss:   1.792933\n",
      "train loss:   1.329116\n",
      "train loss:   1.345911\n",
      "train loss:   1.450174\n",
      "train loss:   1.228121\n",
      "train loss:   1.845974\n",
      "train loss:   1.426395\n",
      "train loss:   1.457721\n",
      "train loss:   1.388117\n",
      "train loss:   1.308836\n",
      "train loss:   1.636383\n",
      "train loss:   1.421160\n",
      "train loss:   1.396200\n",
      "train loss:   1.566096\n",
      "train loss:   1.403038\n",
      "train loss:   1.636995\n",
      "train loss:   1.371205\n",
      "train loss:   1.490458\n",
      "train loss:   1.561076\n",
      "train loss:   1.427985\n",
      "train loss:   1.658499\n",
      "train loss:   1.320648\n",
      "train loss:   1.398781\n",
      "train loss:   1.475172\n",
      "train loss:   1.546314\n",
      "train loss:   1.472705\n",
      "train loss:   1.565001\n",
      "train loss:   1.118583\n",
      "train loss:   1.750144\n",
      "train loss:   1.476302\n",
      "train loss:   1.689751\n",
      "train loss:   1.255110\n",
      "train loss:   1.475215\n",
      "train loss:   1.297589\n",
      "train loss:   1.608242\n",
      "train loss:   1.473857\n",
      "train loss:   1.717594\n",
      "train loss:   1.485638\n",
      "train loss:   1.513166\n",
      "train loss:   1.357785\n",
      "train loss:   1.406531\n",
      "train loss:   1.488406\n",
      "train loss:   1.316621\n",
      "train loss:   1.157493\n",
      "train loss:   1.699985\n",
      "train loss:   1.324465\n",
      "train loss:   1.704201\n",
      "train loss:   1.715989\n",
      "train loss:   1.438750\n",
      "train loss:   1.334763\n",
      "########### epoch 48 ###########\n",
      "########### loop 6700 ###########\n",
      "test loss:   1.050298   test accuracy:   0.812500\n",
      "########### loop 6700 ###########\n",
      "train loss:   1.551830\n",
      "train loss:   1.601720\n",
      "train loss:   1.360673\n",
      "train loss:   1.637646\n",
      "train loss:   1.173222\n",
      "train loss:   1.428535\n",
      "train loss:   1.475770\n",
      "train loss:   1.347989\n",
      "train loss:   1.234538\n",
      "train loss:   1.256828\n",
      "train loss:   1.748049\n",
      "train loss:   1.238975\n",
      "train loss:   1.307921\n",
      "train loss:   1.437857\n",
      "train loss:   1.697633\n",
      "train loss:   1.545516\n",
      "train loss:   1.467138\n",
      "train loss:   1.499654\n",
      "train loss:   1.336718\n",
      "train loss:   1.197167\n",
      "train loss:   1.480378\n",
      "train loss:   1.416521\n",
      "train loss:   1.184177\n",
      "train loss:   1.552604\n",
      "train loss:   1.251789\n",
      "train loss:   1.734766\n",
      "train loss:   1.514032\n",
      "train loss:   1.530843\n",
      "train loss:   1.224041\n",
      "train loss:   1.698758\n",
      "train loss:   1.366892\n",
      "train loss:   1.417863\n",
      "train loss:   1.225584\n",
      "train loss:   1.528259\n",
      "train loss:   1.525617\n",
      "train loss:   1.159124\n",
      "train loss:   1.354608\n",
      "train loss:   1.422230\n",
      "train loss:   1.660258\n",
      "train loss:   1.346105\n",
      "train loss:   1.498928\n",
      "train loss:   1.478904\n",
      "train loss:   1.725091\n",
      "train loss:   1.386664\n",
      "train loss:   1.443184\n",
      "train loss:   1.421718\n",
      "train loss:   1.572739\n",
      "train loss:   1.555716\n",
      "train loss:   1.260336\n",
      "train loss:   1.637266\n",
      "########### epoch 48 ###########\n",
      "########### loop 6750 ###########\n",
      "test loss:   1.294017   test accuracy:   0.687500\n",
      "########### loop 6750 ###########\n",
      "train loss:   1.562842\n",
      "train loss:   1.460205\n",
      "train loss:   1.414524\n",
      "train loss:   1.359970\n",
      "train loss:   1.496183\n",
      "train loss:   1.549039\n",
      "train loss:   1.249829\n",
      "train loss:   1.385401\n",
      "train loss:   1.659571\n",
      "train loss:   1.614619\n",
      "train loss:   1.525816\n",
      "train loss:   1.474403\n",
      "train loss:   1.510991\n",
      "train loss:   1.185102\n",
      "train loss:   1.351441\n",
      "train loss:   1.566631\n",
      "train loss:   1.502938\n",
      "train loss:   1.454239\n",
      "train loss:   1.712598\n",
      "train loss:   1.379239\n",
      "train loss:   1.107659\n",
      "train loss:   1.471334\n",
      "train loss:   1.657418\n",
      "train loss:   1.519229\n",
      "train loss:   1.415828\n",
      "train loss:   1.582018\n",
      "train loss:   1.589486\n",
      "train loss:   1.293725\n",
      "train loss:   1.635534\n",
      "train loss:   1.507090\n",
      "train loss:   1.575379\n",
      "train loss:   1.379414\n",
      "train loss:   1.616676\n",
      "train loss:   1.540958\n",
      "train loss:   1.378351\n",
      "train loss:   1.660163\n",
      "train loss:   1.552371\n",
      "train loss:   1.482177\n",
      "train loss:   1.465210\n",
      "train loss:   1.575483\n",
      "train loss:   1.777227\n",
      "train loss:   1.418269\n",
      "train loss:   1.554218\n",
      "train loss:   1.755609\n",
      "train loss:   1.275084\n",
      "train loss:   1.349262\n",
      "train loss:   1.384069\n",
      "train loss:   1.464316\n",
      "train loss:   1.487144\n",
      "train loss:   1.251409\n",
      "########### epoch 49 ###########\n",
      "########### loop 6800 ###########\n",
      "test loss:   1.035691   test accuracy:   0.812500\n",
      "########### loop 6800 ###########\n",
      "train loss:   1.039338\n",
      "train loss:   1.355312\n",
      "train loss:   1.490240\n",
      "train loss:   1.613737\n",
      "train loss:   1.434858\n",
      "train loss:   1.467206\n",
      "train loss:   1.286561\n",
      "train loss:   1.127776\n",
      "train loss:   1.555387\n",
      "train loss:   1.220129\n",
      "train loss:   1.562755\n",
      "train loss:   1.523645\n",
      "train loss:   1.590962\n",
      "train loss:   1.489143\n",
      "train loss:   1.364393\n",
      "train loss:   1.715340\n",
      "train loss:   1.340829\n",
      "train loss:   1.587764\n",
      "train loss:   1.448341\n",
      "train loss:   1.401994\n",
      "train loss:   1.530862\n",
      "train loss:   1.284317\n",
      "train loss:   1.454940\n",
      "train loss:   1.528453\n",
      "train loss:   1.432846\n",
      "train loss:   1.453076\n",
      "train loss:   1.419065\n",
      "train loss:   1.435641\n",
      "train loss:   1.360918\n",
      "train loss:   1.458208\n",
      "train loss:   1.374193\n",
      "train loss:   1.438719\n",
      "train loss:   1.629384\n",
      "train loss:   1.508723\n",
      "train loss:   1.502319\n",
      "train loss:   1.527535\n",
      "train loss:   1.777251\n",
      "train loss:   1.376338\n",
      "train loss:   1.307263\n",
      "train loss:   1.282446\n",
      "train loss:   1.596687\n",
      "train loss:   1.467027\n",
      "train loss:   1.337039\n",
      "train loss:   1.152054\n",
      "train loss:   1.655265\n",
      "train loss:   1.577210\n",
      "train loss:   1.766984\n",
      "train loss:   1.221462\n",
      "train loss:   1.453424\n",
      "train loss:   1.594522\n",
      "########### epoch 49 ###########\n",
      "########### loop 6850 ###########\n",
      "test loss:   1.180559   test accuracy:   0.781250\n",
      "########### loop 6850 ###########\n",
      "train loss:   1.578790\n",
      "train loss:   1.397746\n",
      "train loss:   1.527471\n",
      "train loss:   1.304384\n",
      "train loss:   1.442015\n",
      "train loss:   1.243537\n",
      "train loss:   1.334490\n",
      "train loss:   1.584793\n",
      "train loss:   1.407302\n",
      "train loss:   1.259658\n",
      "train loss:   1.431631\n",
      "train loss:   1.179334\n",
      "train loss:   1.665372\n",
      "train loss:   1.182030\n",
      "train loss:   1.442840\n",
      "train loss:   1.439104\n",
      "train loss:   1.517757\n",
      "train loss:   1.348077\n",
      "train loss:   1.773739\n",
      "train loss:   1.333293\n",
      "train loss:   1.559034\n",
      "train loss:   0.911379\n",
      "train loss:   1.100950\n",
      "train loss:   1.403447\n",
      "train loss:   1.456030\n",
      "train loss:   1.471268\n",
      "train loss:   1.499982\n",
      "train loss:   1.453607\n",
      "train loss:   1.676589\n",
      "train loss:   1.389754\n",
      "train loss:   1.492298\n",
      "train loss:   1.493859\n",
      "train loss:   1.337262\n",
      "train loss:   1.429170\n",
      "train loss:   1.447915\n",
      "train loss:   1.572598\n",
      "train loss:   1.397176\n",
      "train loss:   1.335479\n",
      "train loss:   1.357231\n",
      "train loss:   1.370264\n",
      "train loss:   1.342398\n",
      "train loss:   1.212233\n",
      "train loss:   1.461965\n",
      "train loss:   1.277876\n",
      "train loss:   1.276080\n",
      "train loss:   1.590636\n",
      "train loss:   1.252904\n",
      "train loss:   1.231478\n",
      "train loss:   1.533440\n",
      "train loss:   1.203850\n",
      "########### epoch 49 ###########\n",
      "########### loop 6900 ###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:   0.894696   test accuracy:   0.875000\n",
      "########### loop 6900 ###########\n",
      "train loss:   1.617668\n",
      "train loss:   1.467458\n",
      "train loss:   1.255551\n",
      "train loss:   1.229854\n",
      "train loss:   1.045715\n",
      "train loss:   1.577470\n",
      "train loss:   1.459185\n",
      "train loss:   1.243940\n",
      "train loss:   1.372841\n",
      "train loss:   1.356395\n",
      "train loss:   1.543184\n",
      "train loss:   1.417125\n",
      "train loss:   1.697636\n",
      "train loss:   1.371930\n",
      "train loss:   1.283865\n",
      "train loss:   1.149129\n",
      "train loss:   1.426818\n",
      "train loss:   1.569421\n",
      "train loss:   1.693427\n",
      "train loss:   1.370137\n",
      "train loss:   1.378608\n",
      "train loss:   1.621001\n",
      "train loss:   1.526272\n",
      "train loss:   1.257567\n",
      "train loss:   1.616010\n",
      "train loss:   1.364344\n",
      "train loss:   1.621508\n",
      "train loss:   1.328825\n",
      "train loss:   1.291275\n",
      "train loss:   1.611421\n",
      "train loss:   1.402042\n",
      "train loss:   1.413630\n",
      "train loss:   1.400083\n",
      "train loss:   1.272099\n",
      "train loss:   1.682045\n",
      "train loss:   1.547058\n",
      "train loss:   1.444280\n",
      "train loss:   1.650397\n",
      "train loss:   1.498969\n",
      "train loss:   1.392534\n",
      "train loss:   1.281912\n",
      "train loss:   1.459854\n",
      "train loss:   1.142103\n",
      "train loss:   1.684238\n",
      "train loss:   1.354389\n",
      "train loss:   1.414948\n",
      "train loss:   1.545924\n",
      "train loss:   1.759947\n",
      "train loss:   1.154688\n",
      "train loss:   1.223218\n",
      "########### epoch 50 ###########\n",
      "########### loop 6950 ###########\n",
      "test loss:   1.082804   test accuracy:   0.718750\n",
      "########### loop 6950 ###########\n",
      "train loss:   1.043141\n",
      "train loss:   1.015418\n",
      "train loss:   1.458873\n",
      "train loss:   1.430869\n",
      "train loss:   1.316232\n",
      "train loss:   1.746829\n",
      "train loss:   1.323705\n",
      "train loss:   1.426114\n",
      "train loss:   1.730053\n",
      "train loss:   1.100182\n",
      "train loss:   1.363062\n",
      "train loss:   1.539868\n",
      "train loss:   1.249961\n",
      "train loss:   1.281672\n",
      "train loss:   1.528335\n",
      "train loss:   1.532506\n",
      "train loss:   1.467790\n",
      "train loss:   1.219850\n",
      "train loss:   1.215790\n",
      "train loss:   1.256753\n",
      "train loss:   1.419578\n",
      "train loss:   1.575799\n",
      "train loss:   1.593141\n",
      "train loss:   1.364571\n",
      "train loss:   1.598409\n",
      "train loss:   1.541330\n",
      "train loss:   1.348075\n",
      "train loss:   1.431957\n",
      "train loss:   1.445176\n",
      "train loss:   1.565634\n",
      "train loss:   1.274356\n",
      "train loss:   1.322628\n",
      "train loss:   1.670181\n",
      "train loss:   1.251362\n",
      "train loss:   1.416800\n",
      "train loss:   1.440592\n",
      "train loss:   1.767243\n",
      "train loss:   1.252004\n",
      "train loss:   1.805463\n",
      "train loss:   1.563241\n",
      "train loss:   1.368982\n",
      "train loss:   1.547242\n",
      "train loss:   1.343440\n",
      "train loss:   1.624918\n",
      "train loss:   1.571319\n",
      "train loss:   1.725988\n",
      "train loss:   1.421216\n",
      "train loss:   1.617899\n",
      "train loss:   1.217340\n",
      "train loss:   1.401697\n",
      "########### epoch 50 ###########\n",
      "########### loop 7000 ###########\n",
      "test loss:   1.084508   test accuracy:   0.812500\n",
      "########### loop 7000 ###########\n",
      "train loss:   1.296081\n",
      "train loss:   1.328675\n",
      "train loss:   1.587514\n",
      "train loss:   1.379959\n",
      "train loss:   1.284748\n",
      "train loss:   1.490289\n",
      "train loss:   1.696180\n",
      "train loss:   1.256099\n",
      "train loss:   1.210219\n",
      "train loss:   1.767628\n",
      "train loss:   1.040086\n",
      "train loss:   1.567236\n",
      "train loss:   1.316274\n",
      "train loss:   1.452985\n",
      "train loss:   1.629124\n",
      "train loss:   1.525044\n",
      "train loss:   1.417311\n",
      "train loss:   1.526291\n",
      "train loss:   1.447816\n",
      "train loss:   1.495768\n",
      "train loss:   1.295841\n",
      "train loss:   1.307967\n",
      "train loss:   1.453660\n",
      "train loss:   1.438181\n",
      "train loss:   1.686362\n",
      "train loss:   1.078575\n",
      "train loss:   1.418382\n",
      "train loss:   1.149938\n",
      "train loss:   1.556099\n",
      "train loss:   1.394023\n",
      "train loss:   1.326622\n",
      "train loss:   1.411540\n",
      "train loss:   1.674437\n",
      "train loss:   1.629814\n",
      "train loss:   1.557955\n",
      "train loss:   1.650444\n",
      "train loss:   1.363510\n",
      "train loss:   1.416648\n",
      "train loss:   1.551997\n",
      "train loss:   1.144859\n",
      "train loss:   1.306420\n",
      "train loss:   1.293618\n",
      "train loss:   1.487824\n",
      "train loss:   1.172259\n",
      "train loss:   1.507535\n",
      "train loss:   1.523527\n",
      "train loss:   1.474384\n",
      "train loss:   1.200151\n",
      "train loss:   1.534392\n",
      "[array([[ 9.5026232e-02,  7.2930790e-02, -3.6813654e-02, ...,\n",
      "        -3.4357134e-02, -8.7767147e-02,  5.0819542e-02],\n",
      "       [ 1.3950452e-03, -1.4508091e-03, -3.7143724e-03, ...,\n",
      "         4.9801045e-03, -6.6605175e-04, -8.1326594e-05],\n",
      "       [ 5.5598471e-02,  4.8295103e-02, -4.3294623e-02, ...,\n",
      "        -5.8805291e-02, -2.5589159e-02,  5.1891845e-02],\n",
      "       ...,\n",
      "       [-2.8579900e-04,  7.9469429e-03, -5.1174024e-03, ...,\n",
      "        -1.3578797e-03, -7.5880910e-04, -4.8587504e-03],\n",
      "       [ 8.6970087e-03,  4.8119997e-05, -3.0925651e-03, ...,\n",
      "        -6.2323422e-03, -6.1725550e-03, -2.7743427e-03],\n",
      "       [ 7.4410692e-02,  5.7588834e-02,  5.7855304e-02, ...,\n",
      "        -5.0982535e-02,  4.5402646e-02, -7.4410759e-02]], dtype=float32), array([ 0.21044499, -1.1371075 , -1.1756704 , -1.2705263 , -0.66082335,\n",
      "       -0.851597  ,  0.58093846, -0.27838677, -0.9823012 , -1.0829741 ,\n",
      "       -0.34868932, -1.1569897 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "# build input\n",
    "imgs = tf.placeholder(tf.float32, shape = [None, image_size, image_size, 3], name = \"input_x\")\n",
    "targets = tf.placeholder(tf.int32, shape = [None, n_classes], name = \"input_y\")\n",
    "keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "\n",
    "# build cal graph\n",
    "bilinear_cnn = vgg_bilinear_model(imgs, \n",
    "                                  keep_prob, \n",
    "                                  weights_file = weights_file, \n",
    "                                  weights_file_last = weights_file_last_layer, \n",
    "                                  sess = sess, \n",
    "                                  finetune = False)\n",
    "\n",
    "# build cal node parameter\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = targets, logits = bilinear_cnn.fc3l)\n",
    "cost = tf.reduce_mean(cross_entropy, name = \"cost\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.2, momentum=0.5).minimize(cost)\n",
    "\n",
    "predicted = tf.nn.softmax(bilinear_cnn.fc3l, name = \"predicted\")\n",
    "correct_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name = \"accuracy\")\n",
    "\n",
    "# start train\n",
    "sess.run(tf.global_variables_initializer())\n",
    "bilinear_cnn.load_vgg_weights(sess)\n",
    "\n",
    "for v in tf.trainable_variables():\n",
    "    print(\"Trainable variables\", v)\n",
    "print('Starting training')\n",
    "\n",
    "\n",
    "###################################\n",
    "train_img_batch, train_label_batch = get_batchs(tfrecord_filename = \"train.tfrecords\", \n",
    "                                                    image_size = image_size, classes = n_classes,\n",
    "                                                    batch_size = batch_size, min_after_dequeue = 500,\n",
    "                                                   use_data_enhancement = True)\n",
    "test_img_batch, test_label_batch = get_batchs(tfrecord_filename = \"test.tfrecords\", \n",
    "                                                  image_size = image_size, classes = n_classes,\n",
    "                                                  batch_size = batch_size, min_after_dequeue = 500,\n",
    "                                                     use_data_enhancement = False)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "for i in range(int((4512 / batch_size) * epoch)):\n",
    "#for i in range(2):\n",
    "    # get data\n",
    "    train_imgs, train_labels= sess.run([train_img_batch, train_label_batch])\n",
    "    test_imgs, test_labels= sess.run([test_img_batch, test_label_batch])\n",
    "\n",
    "#    not random rotate\n",
    "#    train_imgs = image_random_enhancement(train_imgs)\n",
    "\n",
    "    # train single loop\n",
    "    _, train_loss = sess.run([optimizer,cost], feed_dict={imgs: train_imgs, targets: train_labels, keep_prob: 0.5})\n",
    "    #train_loss = sess.run(cost, feed_dict={imgs: train_imgs, targets: train_labels, keep_prob: 1.})\n",
    "    print(\"train loss: %10f\"%(train_loss))\n",
    "            \n",
    "    # print test stat\n",
    "    if (i % 50 == 0):\n",
    "        print(\"########### epoch %d ###########\"%((i / (4512 / batch_size)) + 1))\n",
    "        print(\"########### loop %d ###########\"%(i))\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], feed_dict={imgs: test_imgs, targets: test_labels, keep_prob: 1.})\n",
    "        print(\"test loss: %10f   test accuracy: %10f\"%(test_loss, test_acc))\n",
    "        print(\"########### loop %d ###########\"%(i))\n",
    "\n",
    "# close data queue\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "\n",
    "# save first train parameters\n",
    "last_layer_weights = []\n",
    "\n",
    "for var in bilinear_cnn.last_layer_parameters:\n",
    "    last_layer_weights.append(sess.run(var))\n",
    "print(last_layer_weights)\n",
    "\n",
    "np.savez(weights_file_last_layer, last_layer = last_layer_weights)\n",
    "\n",
    "# end\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_dict_test = np.load(weights_file_last_layer, encoding = 'bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last_layer']\n"
     ]
    }
   ],
   "source": [
    "print(weights_dict_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00041662 -0.00353788  0.00106028 ... -0.00134582  0.00190614\n",
      "  -0.00243913]\n",
      " [ 0.00408528  0.00418111 -0.00251761 ... -0.00158784 -0.0001661\n",
      "   0.00020998]\n",
      " [-0.00277867 -0.0027039   0.00085511 ... -0.00456285  0.00487587\n",
      "   0.00440324]\n",
      " ...\n",
      " [-0.00144065 -0.00295138 -0.00382247 ...  0.00146416  0.00394172\n",
      "   0.00060865]\n",
      " [ 0.00153437 -0.00270551 -0.00297345 ...  0.00369073 -0.00077833\n",
      "   0.00110156]\n",
      " [-0.0022927  -0.00305246  0.00286247 ...  0.00209831  0.00114476\n",
      "   0.0038535 ]]\n",
      "(12,)\n",
      "[ 0.20129488  0.04828681  0.03133622  0.20117898  0.0758582  -0.13442625\n",
      "  0.2165261  -0.04616061  0.11876933 -0.10707303  0.20810203  0.16723256]\n"
     ]
    }
   ],
   "source": [
    "print(weights_dict_test['last_layer'][0])\n",
    "print(weights_dict_test['last_layer'][1].shape)\n",
    "print(weights_dict_test['last_layer'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
